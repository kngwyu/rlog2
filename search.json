[
  {
    "objectID": "posts/understanding-attention.html",
    "href": "posts/understanding-attention.html",
    "title": "AttentionãŒä½•ã‚’ã‚„ã£ã¦ã„ã‚‹ã®ã‹ç†è§£ã—ã‚ˆã†",
    "section": "",
    "text": "ChatGPTãŒå¤§ãƒã‚ºãƒªã—ã¦ã„ã‚‹æ˜¨ä»Šã§ã™ã€‚åƒ•ã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è€ƒãˆã‚‹ã®ãŒé¢å€’ãªã®ã§ï¼ˆãˆãˆâ€¦)ã‚ã¾ã‚Šä½¿ã‚ãªã„ã®ã§ã™ãŒã€å‹äººãŒè«–æ–‡ã‚’æ›¸ãã®ã«ä½¿ã£ã¦ã„ãŸã‚Šã€åƒ•ã®æ¯è¦ªãŒè©±ã—ç›¸æ‰‹ã«ä½¿ã£ã¦ã„ãŸã‚Šã™ã‚‹ã‚ˆã†ã§ã™ã€‚è¦ªä¸å­ãªæ¯å­ã§ã”ã‚ã‚“ãªã•ã„ã¨ã„ã†æ„Ÿã˜ã‚‚ã—ã¾ã™ã€‚ ã¨ã“ã‚ã§ã€ChatGPTã®ã‚ˆã†ãªè¨€èªç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã§ã—ã°ã—ã°åˆ©ç”¨ã•ã‚Œã¦ã„ã‚‹ã®ãŒã€Transformerã¨å‘¼ã°ã‚Œã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ§‹æˆã§ã™ã€‚Transformerã§ã¯ã€Multihead Attentionã¨å‘¼ã°ã‚Œã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’å…¥åŠ›ã®ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã«å¯¾ã—ç¹°ã‚Šè¿”ã—é©ç”¨ã—è¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«ã‚’æ§‹æˆã—ã¾ã™ã€‚ã“ã®ãƒ–ãƒ­ã‚°ã§ã¯ã€ãã®ç°¡ç•¥ç‰ˆã§ã‚ã‚‹1ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®Attention(Multiheadã§ã¯ãªã„)ã«ç€ç›®ã—ã€ã“ã‚ŒãŒä½•ã‚’ã—ã¦ã„ã‚‹ã®ã‹ã‚’å‹‰å¼·ã—ã€ã¤ã„ã§ã«ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦å‹•ã‹ã—ã¦ã¿ã¾ã™ã€‚"
  },
  {
    "objectID": "posts/understanding-attention.html#æœ€åˆã«åˆ—ãŒã‚ã£ãŸ",
    "href": "posts/understanding-attention.html#æœ€åˆã«åˆ—ãŒã‚ã£ãŸ",
    "title": "AttentionãŒä½•ã‚’ã‚„ã£ã¦ã„ã‚‹ã®ã‹ç†è§£ã—ã‚ˆã†",
    "section": "æœ€åˆã«åˆ—ãŒã‚ã£ãŸ",
    "text": "æœ€åˆã«åˆ—ãŒã‚ã£ãŸ\nãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã¨ã„ã†ã®ã¯æ–‡å­—é€šã‚Šãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰ãªã‚‹åˆ—ã®ã“ã¨ã§ã™ã€‚ãƒˆãƒ¼ã‚¯ãƒ³ã¯æœ‰é™é›†åˆã®è¦ç´ ã§ã™ã€‚å®Ÿç”¨ä¸Šã¯byte pair encodingã«ã‚ˆã‚Šå¾—ã‚‰ã‚ŒãŸéƒ¨åˆ†æ–‡å­—åˆ—ãªã©ãŒã“ã‚Œã«è©²å½“ã—ã¾ã™ãŒã€ã¨ã‚Šã‚ãˆãšæ°—ã«ã—ãªãã¦ã„ã„ã§ã™ã€‚ãƒˆãƒ¼ã‚¯ãƒ³ã®é›†åˆã‚’\\(V\\)ã¨ã—ã€\\([Nv] := {1, ..., Nv}\\)ã¨ç•ªå·ä»˜ã‘ã—ã¦ãŠãã¾ã™ã€‚ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‚’\\(x = x[1: l]\\)ã¨æ›¸ãã¾ã™ã€‚ã¾ãŸã€ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã®æœ€å¤§ã®é•·ã•ã‚’\\(L\\)ã¨ã—ã¾ã™ã€‚ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦é€£ç¶šå€¤ã‚„ç„¡é™é›†åˆã¯æ‰±ãˆãªã„ã¨æ€ã„ã¾ã™ãŒã€ç´ äººãªã®ã§ä½•ã‹æŠœã‘é“ãŒã‚ã‚‹ã‹ã©ã†ã‹ã¯çŸ¥ã‚Šã¾ã›ã‚“ã€‚"
  },
  {
    "objectID": "posts/understanding-attention.html#ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«ã«",
    "href": "posts/understanding-attention.html#ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«ã«",
    "title": "AttentionãŒä½•ã‚’ã‚„ã£ã¦ã„ã‚‹ã®ã‹ç†è§£ã—ã‚ˆã†",
    "section": "ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«ã«",
    "text": "ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«ã«\né©å½“ãª\\(d_e \\times Nv\\)æ¬¡å…ƒã®è¡Œåˆ—\\(W_e\\)ã‚’ä½¿ã£ã¦ã€\\(v\\)ç•ªç›®ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰åŸ‹ã‚è¾¼ã¿ï¼ˆToken embeddingï¼‰ã‚’ \\(e = W_e[:, v]\\)ã«ã‚ˆã‚Šå¾—ã¾ã™ã€‚ã“ã‚Œã¯\\(d_e\\)æ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ã«ãªã‚Šã¾ã™ã€‚ãªãŠã€numpyé¢¨ã«\\(i\\)ç•ªç›®ã®è¡Œãƒ™ã‚¯ãƒˆãƒ«ã‚’\\(W[i, :]\\)ã€\\(j\\)ç•ªç›®ã®åˆ—ãƒ™ã‚¯ãƒˆãƒ«ã‚’\\(W[:, j]\\)ã¨æ›¸ã„ã¦ã„ã¾ã™ã€‚ã“ã®è¡Œåˆ—\\(W_e\\)ã¯å‹¾é…é™ä¸‹ã«ã‚ˆã‚Šå­¦ç¿’ã•ã‚Œã‚‹ã‚ˆã†ã§ã™ã€‚"
  },
  {
    "objectID": "posts/understanding-attention.html#ã¤ã„ã§ã«ä½ç½®ã‚‚ãƒ™ã‚¯ãƒˆãƒ«ã«",
    "href": "posts/understanding-attention.html#ã¤ã„ã§ã«ä½ç½®ã‚‚ãƒ™ã‚¯ãƒˆãƒ«ã«",
    "title": "AttentionãŒä½•ã‚’ã‚„ã£ã¦ã„ã‚‹ã®ã‹ç†è§£ã—ã‚ˆã†",
    "section": "ã¤ã„ã§ã«ä½ç½®ã‚‚ãƒ™ã‚¯ãƒˆãƒ«ã«",
    "text": "ã¤ã„ã§ã«ä½ç½®ã‚‚ãƒ™ã‚¯ãƒˆãƒ«ã«\né©å½“ãª\\(d_p \\times L\\)æ¬¡å…ƒã®è¡Œåˆ—\\(W_p\\)ã‚’ä½¿ã£ã¦ã€ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ä¸­ã®\\(l\\)ç•ªç›®ã«ãƒˆãƒ¼ã‚¯ãƒ³ãŒã‚ã‚‹ã¨ã„ã†æƒ…å ±ã‹ã‚‰ã€ä½ç½®åŸ‹ã‚è¾¼ã¿ï¼ˆPositional embeddingï¼‰ã‚’ \\(p = W_p[:, l]\\)ã«ã‚ˆã‚Šå¾—ã¾ã™ã€‚ã“ã‚Œã‚‚\\(d_e\\)æ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ã«ãªã‚Šã¾ã™ã€‚æ­£ç›´ãªã‚“ã®æ„å‘³ãŒã‚ã‚‹ã®ã‹ã‚ˆãã‚ã‹ã‚‰ãªã„ã®ã§ã™ãŒã€ã“ã‚Œã‚’å…ˆç¨‹ã®ãƒˆãƒ¼ã‚¯ãƒ³åŸ‹ã‚è¾¼ã¿ã«è¶³ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³åˆ—\\(x\\)ã®\\(t\\)ç•ªç›®ã®ãƒˆãƒ¼ã‚¯ãƒ³\\(x[t]\\)ã«å¯¾ã™ã‚‹åŸ‹ã‚è¾¼ã¿ã‚’\\(e = W_e[:, x[t]] + W_p[:, t]\\)ã«ã‚ˆã£ã¦å¾—ã¾ã™ã€‚ã“ã‚Œè¶³ã—ã¦å¤§ä¸ˆå¤«ãªã®ã‹ãªï¼Ÿã£ã¦æ€ã†ã‚“ã§ã™ãŒã€‚ ä½ç½®åŸ‹ã‚è¾¼ã¿ã¯ã€å­¦ç¿’ã•ã‚Œã‚‹ã“ã¨ã‚‚ã‚ã‚‹ã‚ˆã†ã§ã™ãŒã€TransformerãŒæœ€åˆã«ææ¡ˆã•ã‚ŒãŸAttention Is All You Needã®è«–æ–‡ã§ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚ \\[\n\\begin{align*}\nW_p[2i - 1, t] &= \\sin (\\frac{t}{L^{2i / d_e}}) \\\\\nW_p[2i, t] &= \\cos (\\frac{t}{L^{2i / d_e}}) \\\\\n&~~~~~(0 &lt; 2i \\leq d_e)\n\\end{align*}\n\\] ã“ã‚Œã‚’\\(L=50, d_e = 5\\)ã¨ã—ã¦å¯è¦–åŒ–ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\nimport matplotlib\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nmatplotlib.font_manager.fontManager.addfont(\"NotoEmoji-Medium.ttf\")\n\nL = 50\nd_e = 5\nx = np.arange(L)\nfor i in range(1, 1 + d_e):\n    if i % 2 == 0:\n        w_p = np.sin(x / L ** (i / d_e))\n    else:\n        w_p = np.cos(x / L ** ((i - 1) / d_e))\n    _ = plt.plot(x, w_p, label=f\"i={i}\")\nplt.legend()\n\n\n&lt;matplotlib.legend.Legend at 0x7f9028bcbee0&gt;\n\n\n\n\n\nã¨ã„ã†ã‚ã‘ã§ã€ã“ã®åŸ‹ã‚ã“ã¿ã¯å„æˆåˆ†ã”ã¨ã«ç•°ãªã‚‹å‘¨æ³¢æ•°ã§ã®å˜èªã‚’åŸ‹ã‚è¾¼ã‚€ã‚ˆã†ã§ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€çŸ­ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ä¸­ã§ã®ä½ç½®ã‚‚åŒæ™‚ã«è€ƒæ…®ã§ãã‚‹ã®ã‹ãªã€‚"
  },
  {
    "objectID": "posts/understanding-attention.html#ãƒãƒ«ã‚³ãƒ•ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’",
    "href": "posts/understanding-attention.html#ãƒãƒ«ã‚³ãƒ•ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’",
    "title": "AttentionãŒä½•ã‚’ã‚„ã£ã¦ã„ã‚‹ã®ã‹ç†è§£ã—ã‚ˆã†",
    "section": "ãƒãƒ«ã‚³ãƒ•ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’",
    "text": "ãƒãƒ«ã‚³ãƒ•ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’\nã¾ãšç°¡å˜ãªãƒ¢ãƒ‡ãƒ«ã§å¤©æ°—ã‚’ç”Ÿæˆã—ã¦ã¿ã¾ã™ã€‚æ¬¡ã®æ—¥ã®å¤©æ°—ã¯ã€å‰ã®æ—¥ã®å¤©æ°—ã«ã‚‚ã¨ã¥ã„ã¦ç¢ºç‡çš„ã«æ±ºã¾ã‚‹ã“ã¨ã«ã—ã¾ã—ã‚‡ã†ã€‚ğŸŒ§ï¸ãƒ»â˜ï¸ãƒ»â˜€ï¸ãŒãƒãƒ«ãƒãƒã‚¤ãƒˆæ–‡å­—ã§ã‚ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ã€ä»¥ä¸‹ã®ã‚ˆã†ã«å®Ÿè£…ã—ã¾ã™ã€‚\n\n\nCode\nimport dataclasses\n\n_GEN = np.random.Generator(np.random.PCG64(20230508))\n_MARKOV = {\n    \"\": [0.3, 0.4, 0.3],\n    \"ğŸŒ§ï¸\": [0.6, 0.3, 0.1],\n    \"â˜ï¸\": [0.3, 0.4, 0.3],\n    \"â˜€ï¸\": [0.2, 0.3, 0.5],\n}\n\ndef markov(prev: str) -&gt; str:\n    prob = _MARKOV[prev[-2:]]\n    return prev + _GEN.choice(WEATHERS, p=prob)\n\n\ndef generate(f, n: int, init: str = \"\"):\n    value = init\n    for _ in range(n):\n        value = f(value)\n    return value\n\n\n@dataclasses.dataclass\nclass Dataset:\n    weathers: list[str]\n    embeddings: jax.Array\n    next_weather_indices: jax.Array\n    \n    def __len__(self) -&gt; int:\n        return len(self.weathers)\n\n\ndef make_dataset(f, seq_len, size) -&gt; Dataset:\n    w_list, e_list, nw_list = [], [], []\n    for _ in range(size):\n        weathers = generate(f, seq_len + 1)\n        e = jnp.array(get_embedding(weathers[:-2]))\n        w_list.append(weathers)\n        e_list.append(e)\n        nw_list.append(WEATHERS.index(weathers[-2:]))\n    return Dataset(w_list, jnp.stack(e_list), jnp.array(nw_list))\n\n\ngenerated = generate(markov, 10)\ngenerated, get_embedding(generated)\n\n\n('ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸â˜ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸â˜€ï¸',\n array([[1.  , 1.  , 1.  , 0.  , 1.  , 0.  , 1.  , 1.  , 0.  , 0.  ],\n        [0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  ],\n        [0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 1.  , 1.  ],\n        [0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 ]]))\n\n\nã“ã‚“ãªæ„Ÿã˜ã§ã™ã€‚ã„ã¾ã€æ¬¡ã®æ—¥ã®å¤©æ°—ã ã‘äºˆæ¸¬ã—ãŸã„ã®ã§ã€ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã¯é›†åˆ{ğŸŒ§ï¸ãƒ»â˜ï¸ãƒ»â˜€ï¸}ä¸Šã§ã®ç¢ºç‡åˆ†å¸ƒãŒé©åˆ‡ã§ã—ã‚‡ã†ã€‚Attentionã¯é•·ã•\\(T\\)ã®åŸ‹ã‚è¾¼ã¿åˆ—ã«å¯¾ã—ã¦é•·ã•\\(d_\\textrm{out} \\times T\\)ã®è¡Œåˆ—ã‚’ã‹ãˆã—ã¾ã™ã€‚ãªã®ã§ã€\\(d_\\textrm{out} = 3\\)ã¨ã—ã€Attentionã®å‡ºåŠ›\\(\\tilde{V}\\)ã«å¯¾ã—ã¦ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é–¢æ•°ã‚’é©ç”¨ã—ã€\\(P_t = \\textrm{softmax}(\\tilde{V}[:, t])\\)ã¨ã—ã¾ã™ã€‚ã“ã®ã¨ãã€\\(P_t\\)ã®å„è¦ç´ ãŒæ¬¡ã®æ—¥ğŸŒ§ï¸ãƒ»â˜ï¸ãƒ»â˜€ï¸ã«ãªã‚‹ç¢ºç‡ã‚’è¡¨ã™ã¨ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã¾ã™ã€‚ã“ã‚Œã‚’ã€å¯¾æ•°å°¤åº¦ã®å’Œ\\(\\sum_t \\log P_t(\\textrm{next weather})\\)ã‚’æœ€å¤§åŒ–ã™ã‚‹ã‚ˆã†ã«å­¦ç¿’ã—ã¾ã—ã‚‡ã†ã€‚å­¦ç¿’ã®ã‚³ãƒ¼ãƒ‰ã‚’å®šç¾©ã—ã¾ã™ã€‚\n\n\nCode\nfrom typing import Callable\n\nimport optax\n\n\ndef attn_neglogp(model: eqx.Module, seq: jax.Array, next_w: jax.Array) -&gt; jax.Array:\n    batch_size = seq.shape[0]\n    tilde_v = jax.vmap(model)(seq)  # B x OUT x SEQ_LEN\n    logp = jax.nn.log_softmax(tilde_v, axis=1)  # B x OUT x SEQ_LEN\n    logp_masked = logp * jax.nn.one_hot(next_w, num_classes=3).reshape(-1, 3, 1)\n    return -jnp.mean(jnp.sum(logp_masked.reshape(batch_size, -1), axis=-1))\n\ndef train(\n    n_total_epochs: int,\n    minibatch_size: int,\n    model: eqx.Module,\n    ds: Dataset,\n    test_ds: Dataset,\n    key: jax.Array,\n    learning_rate: float = 1e-2,\n    loss_fn: Callable[[eqx.Module, jax.Array, jax.Array], jax.Array] = attn_neglogp,\n) -&gt; tuple[eqx.Module, jax.Array, list[float], list[float]]:\n    n_data = len(ds)\n    optim = optax.adam(learning_rate)\n\n    @eqx.filter_jit\n    def train_1step(\n        model: eqx.Module,\n        seq: jax.Array,\n        next_w: jax.Array,\n        opt_state: optax.OptState,\n    ) -&gt; tuple[jax.Array, eqx.Module, optax.OptState]:\n        loss, grads = eqx.filter_value_and_grad(loss_fn)(model, seq, next_w)\n        updates, opt_state = optim.update(grads, opt_state)\n        model = eqx.apply_updates(model, updates)\n        return loss, model, opt_state\n\n    opt_state = optim.init(model)\n    n_optim_epochs = n_data // minibatch_size\n    loss_list, eval_list = [], []\n    for epoch in range(n_total_epochs // n_optim_epochs):\n        key, perm_key = jax.random.split(key)\n        indices = jax.random.permutation(perm_key, n_data, independent=True)\n        for _ in range(n_optim_epochs):\n            e = ds.embeddings[indices]\n            next_w = ds.next_weather_indices[indices]\n            loss, model, opt_state = train_1step(model, e, next_w, opt_state)\n            loss_list.append(loss.item())\n            test_loss = jax.jit(loss_fn)(\n                model,\n                test_ds.embeddings,\n                test_ds.next_weather_indices,\n            )\n            eval_list.append(test_loss.item())\n    return model, key, loss_list, eval_list\n\n\nã“ã‚Œã‚’å®Ÿéš›ã«èµ°ã‚‰ã›ã¦ã¿ã¾ã™ã€‚é©å½“ã«ã€Attentionã®æ¬¡å…ƒã‚’6ã€å¤©æ°—åˆ—ã®é•·ã•ã‚’10ã«ã—ã¾ã™ã€‚\n\n\nCode\nD_ATTN = 6\nSEQ_LEN = 10\nkey = jax.random.PRNGKey(1234)\nmodel = MaskedAttention(4, D_ATTN, 3, key)\nds = make_dataset(markov, SEQ_LEN, 1000)\ntest_ds = make_dataset(markov, SEQ_LEN, 1000)\nmodel, key, loss_list, eval_list = train(500, 100, model, ds, test_ds, key, 1e-2)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on Markov model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\n@jax.jit\ndef accuracy(model: eqx.Module, seq: jax.Array, next_w: jax.Array) -&gt; float:\n    tilde_v = jax.vmap(model)(seq)  # B x OUT x SEQ_LEN\n    inferred = jnp.argmax(tilde_v[:, :, 0], axis=1)\n    n_correct = jnp.sum(inferred == next_w)\n    return n_correct / seq.shape[0]\n\nf\"Accuracy: {accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.49800002574920654'\n\n\n\n\n\n100ã‚¨ãƒãƒƒã‚¯ã‚ãŸã‚Šã§ãƒ­ã‚¹ãŒè½ã¡ãªããªã£ã¦ã„ã‚‹ã®ã§åæŸã¯ã—ã¦ã„ãã†ã§ã™ã€‚å®Ÿéš›ã«ä½•ã‚’å­¦ç¿’ã—ãŸã®ã‹ç¢ºèªã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã¨ã‚Šã‚ãˆãšå¤©æ°—ã‚’ç”Ÿæˆã—ã¦ã¿ã¾ã™ã€‚ã“ã‚“ãªã‚‚ã®è¦‹ã¦ã‚‚ä½•ã‚‚ã‚ã‹ã‚‰ãªã„ã®ã§ã™ãŒã€ç”Ÿæˆã®æµã‚Œã‚’ç¢ºèªã—ã¦ãŠãã«ã¯ã„ã„ã‹ãªã¨ã€‚ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒãŒä½¿ã‚ã‚Œã‚‹ã“ã¨ãŒå¤šã„ã‚ˆã†ã§ã™ãŒã€é¢å€’ãªã®ã§ä»Šå›ã¯ã‚‚ã£ã¨ç°¡å˜ãªæ–¹æ³•ã‚’ä½¿ã„ã¾ã™ã€‚â˜ï¸ã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆã—ã¦ã€ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«åˆ†å¸ƒã‹ã‚‰æ¬¡ã®å¤©æ°—ã‚’ã‚µãƒ³ãƒ—ãƒ«ã—ã€ã©ã‚“ã©ã‚“è¶³ã—ã¦ã„ãã“ã¨ã«ã—ã¾ã™ã€‚\n\n\nCode\ndef generate_from_model(\n    model: eqx.Module,\n    key: jax.Array,\n    seq_len: int,\n    init: str = \"â˜ï¸\",\n) -&gt; tuple[str, jax.Array]:\n    @jax.jit\n    def step(\n        model: eqx.Module,\n        seq: jax.Array,\n        key: jax.Array,\n    ) -&gt; tuple[jax.Array, jax.Array]:\n        sample_key, key = jax.random.split(key)\n        tilde_v = model(seq)  # 3 x len(seq)\n        sampled = jax.random.categorical(key=sample_key, logits=tilde_v[:, 0])\n        return sampled, key\n\n    generated = init\n    for _ in range(seq_len):\n        next_w, key = step(model, get_embedding(generated), key)\n        generated += WEATHERS[next_w.item()]\n    return generated, key\n\n\ngenerated, key = generate_from_model(model, key, 20)\ngenerated\n\n\n'â˜ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸â˜ï¸ğŸŒ§ï¸â˜ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜ï¸â˜€ï¸â˜ï¸'\n\n\nã“ã‚“ãªæ„Ÿã˜ã«ãªã‚Šã¾ã—ãŸã€‚å½“ãŸã‚Šå‰ã§ã™ãŒã“ã‚Œã‚’è¦‹ãŸã¨ã“ã‚ã§ä½•ã‚‚ã‚ã‹ã‚‰ãªã„ã§ã™ã­ã€‚æ¬¡ã«ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä¸­ã®é©å½“ãªãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—Attentionã®ä¸­èº«ã‚’å¯è¦–åŒ–ã—ã¦ã¿ã¾ã™ã€‚\n\n\nCode\n@jax.jit\ndef get_attn(model: eqx.Module, seq: jax.Array) -&gt; jax.Array:\n    q = model.w_q @ seq + model.b_q\n    k = model.w_k @ seq + model.b_k\n    score = causal_mask(q.T @ k) / model.sqrt_d_attn\n    return jax.nn.softmax(score, axis=-1)\n\n\ndef visualize_attn(ax, model: eqx.Module, ds: Dataset, index: int = 0) -&gt; None:\n    attn = np.array(get_attn(model, ds.embeddings[index]))\n    im = ax.imshow(attn)\n    ax.set_xticks(\n        np.arange(10),\n        labels=[ds.weathers[index][i * 2] for i in range(10)],\n        fontname=\"Noto Emoji\",\n    )\n    ax.set_yticks(\n        np.arange(10),\n        labels=[ds.weathers[index][i * 2] for i in range(10)],\n        fontname=\"Noto Emoji\",\n    )\n    for i in [np.argmin(attn), np.argmax(attn)]:\n        # Show min and max values\n        im.axes.text(i % 10, i // 10, f\"{attn.flatten()[i]:.1f}\", color=\"gray\")\n\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\nvisualize_attn(ax1, model, test_ds, 1)\nvisualize_attn(ax2, model, test_ds, 2)\n\n\n\n\n\nmatplotlibã§ã‚«ãƒ©ãƒ¼çµµæ–‡å­—ãŒä½¿ãˆãªã‹ã£ãŸã®ã§ãƒ¢ãƒã‚¯ãƒ­ã®çµµæ–‡å­—ã«ã—ã¾ã—ãŸã€‚ã¨ã„ã†ã‚ã‘ã§ã€ 1. ç›´å‰ã®å¤©æ°—â†’ç›´å‰ã®å¤©æ°— ã®AttentionãŒæœ€ã‚‚å¤§ãã„ 2. ä»–ã®æ—¥ã®å¤©æ°—â†’ç›´å‰ã®å¤©æ°— ã®Attentionã‚‚å¤§ãã„ 3. ä»–ã¯ã»ã¨ã‚“ã©é–¢ä¿‚ãªã„\nã¨ã„ã£ãŸã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ãƒãƒ«ã‚³ãƒ•ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ç”Ÿæˆã—ãŸå¤©æ°—åˆ—ã‚’å­¦ç¿’ã•ã›ãŸã®ã§ã€1ã¯å½“ãŸã‚Šå‰ã§ã™ã‚ˆã­ã€‚2ã®ä»–ã®æ—¥ã®å¤©æ°—â†’ç›´å‰ã®å¤©æ°—ã®é–¢ä¿‚ã‚‚å®Ÿéš›ã¯ã„ã‚‰ãªã„ã®ã§ã™ãŒã€æ³¨æ„ã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ã§ã™ã€‚"
  },
  {
    "objectID": "posts/understanding-attention.html#ç‹¬ç«‹ã«ç™ºç”Ÿã—ãŸéå»ã®è¤‡æ•°ã®äº‹è±¡ã«ä¾å­˜ã—ã¦å°†æ¥ã®å‡ºæ¥äº‹ãŒæ±ºã¾ã‚‹å ´åˆ",
    "href": "posts/understanding-attention.html#ç‹¬ç«‹ã«ç™ºç”Ÿã—ãŸéå»ã®è¤‡æ•°ã®äº‹è±¡ã«ä¾å­˜ã—ã¦å°†æ¥ã®å‡ºæ¥äº‹ãŒæ±ºã¾ã‚‹å ´åˆ",
    "title": "AttentionãŒä½•ã‚’ã‚„ã£ã¦ã„ã‚‹ã®ã‹ç†è§£ã—ã‚ˆã†",
    "section": "ç‹¬ç«‹ã«ç™ºç”Ÿã—ãŸéå»ã®è¤‡æ•°ã®äº‹è±¡ã«ä¾å­˜ã—ã¦å°†æ¥ã®å‡ºæ¥äº‹ãŒæ±ºã¾ã‚‹å ´åˆ",
    "text": "ç‹¬ç«‹ã«ç™ºç”Ÿã—ãŸéå»ã®è¤‡æ•°ã®äº‹è±¡ã«ä¾å­˜ã—ã¦å°†æ¥ã®å‡ºæ¥äº‹ãŒæ±ºã¾ã‚‹å ´åˆ\næ¬¡ã«ã€ã‚‚ã†å°‘ã—è¤‡é›‘ãªãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã•ã›ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ä»Šåº¦ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ãªæ–¹æ³•ã§11æ—¥ã¶ã‚“ã®å¤©æ°—ã‚’ç”Ÿæˆã—ã¾ã™ã€‚ 1. 1æ—¥ç›®ã€4æ—¥ç›®ã€8æ—¥ç›®ã®å¤©æ°—ã‚’ç‹¬ç«‹ã«ç”Ÿæˆã™ã‚‹ 2. 2,3æ—¥ç›®ã®å¤©æ°—ã‚’1æ—¥ç›®ã®å¤©æ°—ã‚’åˆæœŸçŠ¶æ…‹ã¨ã™ã‚‹ãƒãƒ«ã‚³ãƒ•é€£é–ã«ã‚ˆã‚Šç”Ÿæˆã™ã‚‹ã€‚5,6,7,9,10æ—¥ç›®ã®å¤©æ°—ã«ã¤ã„ã¦ã‚‚ã€4æ—¥ç›®ãƒ»8æ—¥ç›®ã®å¤©æ°—ã«ã‚‚ã¨ã¥ã„ã¦åŒæ§˜ã«ç”Ÿæˆã™ã‚‹ã€‚ 3. 11æ—¥ç›®ã®å¤©æ°—ã‚’1æ—¥ç›®ã€4æ—¥ç›®ã€8æ—¥ç›®ã®å¤©æ°—ã‹ã‚‰ç¢ºç‡çš„ã«ç”Ÿæˆã™ã‚‹ã€‚\nã“ã‚Œã‚’å­¦ç¿’ã§ãã‚‹ã‹è©¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\ndef _make_table() -&gt; dict[str, list[float]]:\n    candidates = []\n    for i in range(1, 9):\n        for j in range(1, 9):\n            for k in range(1, 9):\n                if i + j + k == 10:\n                    candidates.append((i, j, k))\n    table = {}\n    for i in WEATHERS:\n        for j in WEATHERS:\n            for k in WEATHERS:\n                table[i + j + k] = [p / 10 for p in _GEN.choice(candidates)]\n    return table\n\n_ONE_FOUR_8_TABLE = _make_table()\n\ndef one_four_8(prev: str) -&gt; str:\n    length = len(prev) // 2\n    if length == 10:\n        p = _ONE_FOUR_8_TABLE[prev[0: 2] + prev[6: 8] + prev[14: 16]]\n        return prev + _GEN.choice(WEATHERS, p=p)\n    elif length == 4 or length == 8:\n        return prev + _GEN.choice(WEATHERS, p=_MARKOV[\"\"])\n    else:\n        return markov(prev)\n    \ngenerate(one_four_8, 11)\n\n\n'ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜ï¸â˜ï¸â˜ï¸â˜ï¸'\n\n\nã“ã‚“ãªæ„Ÿã˜ã§ã™ã­ã€‚ã§ã¯å­¦ç¿’ã•ã›ã¾ã—ã‚‡ã†ã€‚ã•ã£ãã‚ˆã‚Šã‚‚å°‘ã—ãƒ‡ãƒ¼ã‚¿ãŒè¤‡é›‘ãªã®ã§ã€ã‚µãƒ³ãƒ—ãƒ«ã®æ•°ã‚’å¢—ã‚„ã—ã¦ã¿ã¾ã™ã€‚\n\n\nCode\nmodel = MaskedAttention(4, D_ATTN, 3, key)\nds = make_dataset(one_four_8, SEQ_LEN, 5000)\ntest_ds = make_dataset(one_four_8, SEQ_LEN, 1000)\nmodel, key, loss_list, eval_list = train(500, 100, model, ds, test_ds, key, 1e-2)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on 1-4-8 model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\nvisualize_attn(ax1, model, test_ds, 1)\nvisualize_attn(ax2, model, test_ds, 2)\n\nf\"Accuracy: {accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.33800002932548523'\n\n\n\n\n\n\n\n\næå¤±ã¯å°ã•ããªã£ã¦ã„ã¾ã™ãŒAccuracyãŒæ‚ªãã¾ãŸAttentionã®å‡ºæ–¹ã‚‚å¾®å¦™ã§ã™ã­ã€‚ä¸€å¿œ1ãƒ»48æ—¥ç›®ã«ã‚‚æ³¨æ„ãŒã„ã£ã¦ã„ã¾ã™ãŒã€å…ˆã»ã©ã®å®Ÿé¨“ã¨åŒã˜ãæœ€å¾Œã®æ—¥ã®æ³¨æ„ãŒå¤§ãã‚ã«å‡ºã¦ã„ã¾ã™ã­ã€‚"
  },
  {
    "objectID": "posts/understanding-attention.html#attentionã„ã‚‰ãªã„ã‚“ã˜ã‚ƒ",
    "href": "posts/understanding-attention.html#attentionã„ã‚‰ãªã„ã‚“ã˜ã‚ƒ",
    "title": "AttentionãŒä½•ã‚’ã‚„ã£ã¦ã„ã‚‹ã®ã‹ç†è§£ã—ã‚ˆã†",
    "section": "Attentionã„ã‚‰ãªã„ã‚“ã˜ã‚ƒâ€¦",
    "text": "Attentionã„ã‚‰ãªã„ã‚“ã˜ã‚ƒâ€¦\nå‹˜ã®ã„ã„èª­è€…ã®æ–¹ã¯ãŠæ°—ã¥ãã‹ã¨æ€ã„ã¾ã™ãŒã€ã“ã“ã¾ã§å­¦ç¿’ã•ã›ãŸ2ã¤ã®å¤©æ°—åˆ—ã‚’è¡¨ç¾ã™ã‚‹ã®ã«ã€Attentionãªã‚“ã¦å°é›£ã—ã„ã‚‚ã®ã¯ã„ã‚‰ãªã„ã§ã™ã‚ˆã­ã€‚æœ€åˆã®ã‚‚ã®ã¯å‰æ—¥ï¼ˆ10æ—¥ç›®)ã®å¤©æ°—ã€æ¬¡ã®ã‚„ã¤ã¯1ãƒ»4ãƒ»8æ—¥ç›®ã‹ã‚‰11æ—¥ç›®ã®å¤©æ°—ãŒæ±ºå®šã•ã‚Œã‚‹ãŸã‚ã€å…¥åŠ›ã•ã‚ŒãŸå¤©æ°—åˆ—ã®å†…éƒ¨ç›¸é–¢ãŒã‚¿ã‚¹ã‚¯ã«ä¸€åˆ‡é–¢ä¿‚ãªã„ã‹ã‚‰ã§ã™ã€‚ã¨ã„ã†ã‚ã‘ã§ã€ç·šå½¢ãƒ¢ãƒ‡ãƒ«+ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹(ã„ã‚ã‚†ã‚‹multinomial logistic regressionã¨ã„ã†ã‚„ã¤)ã§å­¦ç¿’ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\nclass LinearModel(eqx.Module):\n    w: jax.Array\n    b: jax.Array\n\n    def __init__(self, d_in: int, d_out: int, key: jax.Array) -&gt; None:\n        w_key, b_key = jax.random.split(key)\n        self.w = jax.random.normal(w_key, (d_out, d_in))\n        self.b = jax.random.normal(b_key, (d_out,))\n\n    def __call__(self, seq: jax.Array) -&gt; jax.Array:\n        return self.w @ seq.flatten() + self.b\n\n\ndef linear_neglogp(model: eqx.Module, seq: jax.Array, next_w: jax.Array) -&gt; jax.Array:\n    logp = jax.nn.log_softmax(jax.vmap(model)(seq), axis=1)  # B x OUT\n    logp_masked = logp * jax.nn.one_hot(next_w, num_classes=3)\n    return -jnp.mean(jnp.sum(logp_masked, axis=1))\n\n\nmodel = LinearModel(4 * SEQ_LEN, 3, key)\nmodel, key, loss_list, eval_list = train(\n    500, 100, model, ds, test_ds, key, 1e-2, linear_neglogp\n)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on 1-4-8 model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\n\n@jax.jit\ndef linear_accuracy(model: eqx.Module, seq: jax.Array, next_w: jax.Array) -&gt; float:\n    tilde_v = jax.vmap(model)(seq)  # B x OUT\n    inferred = jnp.argmax(tilde_v, axis=1)\n    n_correct = jnp.sum(inferred == next_w)\n    return n_correct / seq.shape[0]\n\n\nf\"Accuracy: {linear_accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.37700000405311584'\n\n\n\n\n\næ™®é€šã«ã“ã£ã¡ã®ã»ã†ãŒè‰¯ã•ãã†ã§ã™ã­â€¦ã€‚ã§ã¯ã€Attentionã¯ã©ã†ã„ã†æ™‚ã«å½¹ã«ç«‹ã¤ã®ã§ã—ã‚‡ã†ã‹ã€‚\n\n(MLPç­‰ã¨æ¯”è¼ƒ) ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã®é•·ã•\\(L\\)ã«ä¾å­˜ã•ã›ãŸããªã„ã¨ã\n\nAttentionã§ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ãŒ \\((d_\\textrm{in} + 1)(2d_\\textrm{attn} + d_\\textrm{out})\\)ã«ãªã‚‹ã®ã«å¯¾ã—ã€ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã§ã¯\\((d_\\textrm{in}L + 1)d_\\textrm{out}\\)ã«ãªã‚‹ã“ã¨ã«æ³¨æ„ã—ã¾ã—ã‚‡ã†ã€‚ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã§ã¯ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã®é•·ã•ã«æ¯”ä¾‹ã—ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ãŒå¢—ãˆã¦ã—ã¾ã„ã¾ã™ã€‚ãŸã ã—ã€Attentionã§ã¯\\(q^\\top k\\)ã‚’ä¿æŒã™ã‚‹ã®ã«\\(O(L^2)\\)ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¿…è¦ãªç‚¹ã«æ³¨æ„ãŒå¿…è¦ã§ã™ã€‚ã‚‚ã£ã¨ã‚‚ã€Self-attention Does Not Need \\(O(n^2)\\) Memoryã§ã¯åŠ¹ç‡çš„ãª\\(O(\\sqrt{L})\\)ã®å®Ÿè£…ãŒç¤ºã•ã‚Œã¦ãŠã‚Šã€ã¾ã‚ä½•ã¨ã‹ãªã‚‹ã¨ã„ãˆã°ãªã‚‹ã‚ˆã†ã§ã™ãŒã€ãã‚Œã§ã‚‚å˜ç´”ãªRNNã‚„CNNã‚ˆã‚Šé…ããªã‚Šã¾ã™ã€‚\n\n(RNNãƒ»CNNç­‰ã¨æ¯”è¼ƒ) ãƒˆãƒ¼ã‚¯ãƒ³ç³»åˆ—ã«é•·æœŸé–“ã®ä¾å­˜é–¢ä¿‚ãŒå­˜åœ¨ã™ã‚‹å ´åˆ\n\nCNNã‚„RNNã¨æ¯”ã¹ãŸã¨ãã€\\(q^\\top k\\)ã«ã‚ˆã‚Šä¸€å±¤ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§ä»»æ„ã®ãƒˆãƒ¼ã‚¯ãƒ³é–“ã®ä¾å­˜é–¢ä¿‚ãŒè¡¨ç¾ã§ãã‚‹ã®ã¯Attentionã®åˆ©ç‚¹ã¨è¨€ãˆã‚‹ã§ã—ã‚‡ã†ã€‚ãŸã ã—\\(q^\\top k[i, j]\\)ã¯2ã¤ã®åŸ‹ã‚è¾¼ã¿\\(e[i], e[j]\\)ã«å¯¾ã—ã¦ç·šå½¢ãªæ¼”ç®—ã®ã¿ã§å¾—ã‚‰ã‚Œã‚‹ãŸã‚ã€ã“ã®2ã¤ã®åŸ‹ã‚è¾¼ã¿ãŒä½•ã‹éç·šå½¢ãªé–¢æ•°ã‚’ä»‹ã—ã¦ä¾å­˜ã—ã¦ã„ã‚‹å ´åˆã€ãã®é–¢ä¿‚ã¯ä¸€å±¤ã®Attentionã§ã¯è¡¨ç¾ã§ãã¾ã›ã‚“ã€‚\nã¨ã„ã†ã‚ã‘ã§ã€ä¸€å±¤ã®ç·šå½¢ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¨æ¯”è¼ƒã™ã‚‹ã¨ã€ãƒ‘ãƒ©ãƒ¡ã‚¿æ•°ãŒ\\(L\\)ã«ä¾å­˜ã—ãªã„ã¨ã„ã†ãƒ¡ãƒªãƒƒãƒˆã¯ã‚ã‚‹ã‚‚ã®ã®ã€å®Ÿéš›Attentionã‚’ä½¿ã†ã¨ã‚‚ã£ã¨è‰²ã€…ãªé–¢æ•°ãŒå­¦ç¿’ã§ãã‚‹ã®ã‹ã¨ã„ã†ã¨ã‚ˆãã‚ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚‚ã†å°‘ã—è©¦ã—ã¦ã¿ã¾ã™ã€‚"
  },
  {
    "objectID": "posts/understanding-attention.html#éš ã‚Œå¤‰æ•°ãŒã‚ã‚‹å ´åˆ",
    "href": "posts/understanding-attention.html#éš ã‚Œå¤‰æ•°ãŒã‚ã‚‹å ´åˆ",
    "title": "AttentionãŒä½•ã‚’ã‚„ã£ã¦ã„ã‚‹ã®ã‹ç†è§£ã—ã‚ˆã†",
    "section": "éš ã‚Œå¤‰æ•°ãŒã‚ã‚‹å ´åˆ",
    "text": "éš ã‚Œå¤‰æ•°ãŒã‚ã‚‹å ´åˆ\nä»¥ä¸‹ã®æ–¹æ³•ã§å¤©æ°—åˆ—ã‚’ç”Ÿæˆã—ã¾ã™ã€‚éå»\\(n\\)æ—¥é–“ã®å¤©æ°—ã‚’è¦‹ã¦ã€ğŸŒ§ï¸ã®ç™»å ´å›æ•°ãŒ\\(k\\)å›ãªã‚‰ã€æ¬¡ã®æ—¥ã®å¤©æ°—ãŒğŸŒ§ï¸ã«ãªã‚‹ç¢ºç‡ã‚’\\(\\frac{n - k}{2n}\\)ã¨ã—ã¾ã™ã€‚â˜ï¸ã€â˜€ï¸ã«ã¤ã„ã¦ã‚‚åŒæ§˜ã«ç¢ºç‡ã‚’å‰²ã‚Šå½“ã¦ã¾ã™ã€‚ã“ã®æ–¹æ³•ã§å¤§é‡ã«å¤©æ°—åˆ—ã‚’ç”Ÿæˆã—ã¦é©å½“ãªéƒ¨åˆ†åˆ—ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œã‚Šã¾ã™ã€‚\n\n\nCode\nfrom functools import partial\n\ndef ndays_model(prev: str, n: int = 10) -&gt; str:\n    counts = np.zeros(3)\n    prev_n = prev[-2 * n: ]\n    for i in range(n):\n        prev_w_i = prev_n[i * 2: i * 2 + 2]\n        counts[WEATHERS.index(prev_w_i)] += 1\n    prob = (n - counts) / (n * 2)\n    return prev + _GEN.choice(WEATHERS, p=prob)\n\ngenerate(ndays_model, 100, generate(markov, 10))                \n\n\n'â˜ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜ï¸â˜ï¸â˜ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸â˜ï¸â˜ï¸â˜ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜€ï¸ğŸŒ§ï¸â˜ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸â˜ï¸â˜ï¸â˜ï¸â˜€ï¸ğŸŒ§ï¸â˜ï¸â˜€ï¸â˜ï¸â˜ï¸â˜€ï¸â˜€ï¸â˜€ï¸ğŸŒ§ï¸â˜ï¸â˜ï¸ğŸŒ§ï¸â˜ï¸â˜ï¸â˜ï¸â˜€ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜ï¸â˜ï¸â˜€ï¸â˜ï¸â˜€ï¸ğŸŒ§ï¸â˜€ï¸â˜ï¸â˜ï¸â˜€ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜ï¸â˜€ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸â˜ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸â˜ï¸â˜ï¸â˜€ï¸â˜ï¸â˜€ï¸â˜ï¸â˜ï¸ğŸŒ§ï¸â˜€ï¸â˜ï¸â˜€ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸â˜€ï¸â˜ï¸â˜ï¸â˜ï¸ğŸŒ§ï¸â˜ï¸ğŸŒ§ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜ï¸â˜€ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸'\n\n\nç”Ÿæˆã•ã‚ŒãŸå¤©æ°—åˆ—ã¯ã“ã‚“ãªæ„Ÿã˜ã§ã™ã€‚ã¾ãšç·šå½¢ãƒ¢ãƒ‡ãƒ«ã‚’10æ—¥ãƒ¢ãƒ‡ãƒ«ã§å­¦ç¿’ã•ã›ã¾ã™ã€‚ã“ã®å ´åˆéš ã‚Œå¤‰æ•°ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n\n\nCode\ndef make_ndays_dataset(seq_len, size, n: int = 10) -&gt; Dataset:\n    weathers = generate(partial(ndays_model, n=n), seq_len * size * 2, generate(markov, n * 2))\n    w_list, e_list, nw_list = [], [], []\n    for _ in range(size):\n        start = _GEN.integers(0, seq_len * size * 2 - 11)\n        w = weathers[start * 2 : start * 2 + (seq_len + 1) * 2]\n        e = jnp.array(get_embedding(w[:-2]))\n        w_list.append(w)\n        e_list.append(e)\n        nw_list.append(WEATHERS.index(w[-2:]))\n    return Dataset(w_list, jnp.stack(e_list), jnp.array(nw_list))\n\n\nds = make_ndays_dataset(SEQ_LEN, 5000, n=10)\ntest_ds = make_ndays_dataset(SEQ_LEN, 1000, n=10)\n\nmodel = LinearModel(4 * SEQ_LEN, 3, key)\nmodel, key, loss_list, eval_list = train(\n    500, 100, model, ds, test_ds, key, 1e-2, linear_neglogp\n)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on 10days model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nf\"Accuracy: {linear_accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.3960000276565552'\n\n\n\n\n\næ¬¡ã«Attentionã‚’å­¦ç¿’ã•ã›ã¾ã™ã€‚\n\n\nCode\nmodel = MaskedAttention(4, D_ATTN, 3, key)\nmodel, key, loss_list, eval_list = train(500, 100, model, ds, test_ds, key, 1e-2)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on 10days model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\nvisualize_attn(ax1, model, test_ds, 1)\nvisualize_attn(ax2, model, test_ds, 2)\n\nf\"Accuracy: {accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.3710000216960907'\n\n\n\n\n\n\n\n\nAttentionã®ã»ã†ãŒç·šå½¢ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚ˆã‚Šæ‚ªããªã£ã¦ã—ã¾ã„ã¾ã—ãŸã€‚æ¬¡ã«éš ã‚Œå¤‰æ•°ãŒã‚ã‚‹ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã€å…ˆç¨‹ã®10æ—¥ãƒ¢ãƒ‡ãƒ«ã‚’15æ—¥ãƒ¢ãƒ‡ãƒ«ã«ã—ã¦ã¿ã¾ã™ã€‚ã¾ãšã€ç·šå½¢ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’å­¦ç¿’ã•ã›ã¾ã™ã€‚\n\n\nCode\nds = make_ndays_dataset(SEQ_LEN, 5000, n=15)\ntest_ds = make_ndays_dataset(SEQ_LEN, 1000, n=15)\n\nmodel = LinearModel(4 * SEQ_LEN, 3, key)\nmodel, key, loss_list, eval_list = train(\n    500, 100, model, ds, test_ds, key, 1e-2, linear_neglogp\n)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on 15days model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nf\"Accuracy: {linear_accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.3790000081062317'\n\n\n\n\n\næ¬¡ã«Attentionã‚’å­¦ç¿’ã•ã›ã¾ã™ã€‚\n\n\nCode\nmodel = MaskedAttention(4, D_ATTN, 3, key)\nmodel, key, loss_list, eval_list = train(500, 100, model, ds, test_ds, key, 1e-2)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on 15days model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\nvisualize_attn(ax1, model, test_ds, 1)\nvisualize_attn(ax2, model, test_ds, 2)\n\nf\"Accuracy: {accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.328000009059906'\n\n\n\n\n\n\n\n\nã“ã®å ´åˆã‚‚çµå±€Attentionã®ã»ã†ãŒæ‚ªããªã£ã¦ã—ã¾ã„ã¾ã—ãŸã€‚æ‚²ã—ã„ã€‚"
  },
  {
    "objectID": "posts/understanding-attention.html#éç·šå½¢ãªã‚‰ã©ã†ã‹",
    "href": "posts/understanding-attention.html#éç·šå½¢ãªã‚‰ã©ã†ã‹",
    "title": "AttentionãŒä½•ã‚’ã‚„ã£ã¦ã„ã‚‹ã®ã‹ç†è§£ã—ã‚ˆã†",
    "section": "éç·šå½¢ãªã‚‰ã©ã†ã‹",
    "text": "éç·šå½¢ãªã‚‰ã©ã†ã‹\néš ã‚Œå¤‰æ•°ãŒã‚ã£ã¦ã‚‚ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã®æ–¹ãŒæ€§èƒ½ãŒã„ã„ã¨ã„ã†ã“ã¨ã¯ã€ãŸã¶ã‚“ç·šå½¢ã§è§£ã‘ã‚‹ã‚¿ã‚¹ã‚¯ã§ã¯ã©ã†ã‚ãŒã„ã¦ã‚‚ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã«å‹ã¦ãªã„ã¨ã„ã†ã“ã¨ãªã®ã§ã—ã‚‡ã†ã€‚ãªã®ã§ã‚‚ã£ã¨é›£ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’è€ƒãˆã¾ã™ã€‚ 10æ—¥ã¶ã‚“ã®å¤©æ°—åˆ—ã®ğŸŒ§ï¸ã€â˜ï¸ã€â˜€ï¸ã«ãã‚Œãã‚Œ0, 1, 2ã‚’å‰²ã‚Šå½“ã¦ã¦ä½œã£ãŸãƒ™ã‚¯ãƒˆãƒ«ã‚’\\(y\\)ã¨ã—ã¾ã™ã€‚ã¾ãŸã€\\(\\beta = (0, 1, 2, 3, 2, 1, 0, 1, 2, 3)^\\top\\)ã¨ã—ã¾ã™ã€‚ã“ã®ã¨ãã€æ¬¡ã®æ—¥ã®å¤©æ°—ã‚’\\((y(2 - y)\\cdot \\beta) \\mod 3\\)ã¨ã—ã¾ã™ã€‚ã“ã‚Œã ã¨èŠ¸ãŒãªã„ã®ã§ä¸€å¿œä»–ã®å¤©æ°—ã‚‚2%ãã‚‰ã„ã®ç¢ºç‡ã§å‡ºã‚‹ã‚ˆã†ã«ã—ã¦ãŠãã¾ã™ã€‚\n\n\nCode\n_BETA = np.tile([0, 1, 2, 3, 2, 1], (10,))\n\ndef dotmod_model(prev: str, n: int =10) -&gt; str:\n    y = np.zeros(n, dtype=int)\n    prev_n = prev[-2 * n:]\n    for i in range(n):\n        prev_w_i = prev_n[i * 2: i * 2 + 2]\n        y[i] = WEATHERS.index(prev_w_i) + 1\n    prob = [0.02, 0.02, 0.02]\n    prob[np.dot(y * (2 - y), _BETA[: n]) % 3] = 0.96\n    return prev + _GEN.choice(WEATHERS, p=prob)\n\ndef make_dotmod_dataset(seq_len, size, n: int = 10) -&gt; Dataset:\n    weathers = generate(partial(ndays_mod4_model, n=n), seq_len * size * 2, generate(markov, n * 2))\n    w_list, e_list, nw_list = [], [], []\n    for _ in range(size):\n        start = _GEN.integers(0, seq_len * size * 2 - 11)\n        w = weathers[start * 2 : start * 2 + (seq_len + 1) * 2]\n        e = jnp.array(get_embedding(w[:-2]))\n        w_list.append(w)\n        e_list.append(e)\n        nw_list.append(WEATHERS.index(w[-2:]))\n    return Dataset(w_list, jnp.stack(e_list), jnp.array(nw_list))\n\ngenerate(dotmod_model, 100, generate(markov, 10))\n\n\n'â˜ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜ï¸â˜ï¸â˜€ï¸â˜€ï¸ğŸŒ§ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜ï¸â˜ï¸â˜€ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜ï¸â˜ï¸â˜€ï¸â˜ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸â˜ï¸ğŸŒ§ï¸â˜ï¸â˜ï¸â˜ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜ï¸â˜€ï¸â˜ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸â˜ï¸ğŸŒ§ï¸â˜ï¸â˜ï¸â˜ï¸â˜€ï¸â˜€ï¸â˜ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸â˜ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜ï¸â˜ï¸â˜€ï¸â˜ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸â˜ï¸ğŸŒ§ï¸â˜ï¸â˜ï¸â˜ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜ï¸â˜€ï¸â˜ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸â˜ï¸ğŸŒ§ï¸â˜ï¸â˜ï¸â˜ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜ï¸â˜€ï¸â˜ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸â˜ï¸ğŸŒ§ï¸â˜ï¸â˜ï¸â˜ï¸â˜€ï¸â˜€ï¸â˜ï¸'\n\n\nã±ã£ã¨è¦‹ã§ã¯ã¾ã‚‹ã§æ³•å‰‡æ€§ãŒã‚ã‹ã‚‰ãªã„å¤©æ°—åˆ—ãŒç”Ÿæˆã§ãã¾ã—ãŸã€‚ã“ã‚Œã‚’å­¦ç¿’ã•ã›ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã¾ãšã¯ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚\n\n\nCode\nds = make_dotmod_dataset(SEQ_LEN, 5000)\ntest_ds = make_dotmod_dataset(SEQ_LEN, 1000)\n\nmodel = LinearModel(4 * SEQ_LEN, 3, key)\nmodel, key, loss_list, eval_list = train(\n    500, 100, model, ds, test_ds, key, 1e-2, linear_neglogp\n)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on Dotmod model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nf\"Accuracy: {linear_accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.36500000953674316'\n\n\n\n\n\næ¬¡ã«Attentionã‚’å­¦ç¿’ã—ã¦ã¿ã¾ã™ã€‚\n\n\nCode\nmodel = MaskedAttention(4, D_ATTN, 3, key)\nmodel, key, loss_list, eval_list = train(500, 100, model, ds, test_ds, key, 1e-2)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on Dotmod model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\nvisualize_attn(ax1, model, test_ds, 1)\nvisualize_attn(ax2, model, test_ds, 2)\n\nf\"Accuracy: {accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.33900001645088196'\n\n\n\n\n\n\n\n\nã¾ãŸã—ã¦ã‚‚Attentionã®ã»ã†ãŒã ã‚ã¨ã„ã†çµæœã«ãªã‚Šã¾ã—ãŸãŒã€ã“ã®ã‚¿ã‚¹ã‚¯ã¯ä»Šã¾ã§ã®ï¼’ã¤ã¨ã¯é•ã„æ­£ç­”ç‡96%ã¾ã§ä¸Šã’ã‚‰ã‚Œã‚‹ã®ã§ã€ä¸¡æ–¹ã¨ã‚‚å…¨ç„¶ã ã‚ã§ã—ãŸã€‚"
  },
  {
    "objectID": "posts/sandb-exercise-racetrack.html",
    "href": "posts/sandb-exercise-racetrack.html",
    "title": "Exercise 5.12 Racetrack from the Reinforcement Learning textbook",
    "section": "",
    "text": "Here I demonstrate the execise 5.12 of the textbook Reinforcement Learning: An Introduction by Richard Sutton and Andrew G. Barto, using both of planning method and Monte Carlo method. Basic knowledge of Python (&gt;= 3.7) and NumPy are assumed. Some konwledge of matplotlib and Python typing library also helps.\nContact: yuji.kanagawa@oist.jp"
  },
  {
    "objectID": "posts/sandb-exercise-racetrack.html#modeling-the-problem-in-code",
    "href": "posts/sandb-exercise-racetrack.html#modeling-the-problem-in-code",
    "title": "Exercise 5.12 Racetrack from the Reinforcement Learning textbook",
    "section": "Modeling the problem in code",
    "text": "Modeling the problem in code\nLetâ€™s start from writing the problem in code. What are important in this phase? Here, Iâ€™d like to emphasize the importantness of looking back at the definition of the environment. I.e., in reinforcement learning (RL), environments are modelled by Markov decision process (MDP), consisting of states, actions, transition function, and reward function. So first letâ€™s check the definition of states and actions in the problem statement. Itâ€™s (somehow) not very straightforward, but we can find &gt; In our simplified racetrack, the car is at one of a discrete set of grid positions, the cells in the diagram. The velocity is also discrete, a number of grid cells moved horizontally and vertically per time step.\nSo, a state consists of position and velocity of the car (a.k.a. agent). What about actions?\n\nThe actions are increments to the velocity components. Each may be changed by +1, âˆ’1, or 0 in each step, for a total of nine (\\(3 \\times 3\\)) actions.\n\nSo there are 9 actions for each direction (â†“â†™â†â†–â†‘â†—â†’â†˜ or no acceleration). Here, we can also notice that the total number of states is given by \\(\\textrm{Num. positions} \\times \\textrm{Num. choices of velocity}\\). And the texbook also says &gt; Both velocity components are restricted to be nonnegative and less than 5, and they cannot both be zero except at the starting line.\nSo there are 24 possible velocity at the non-starting positions:\n\n\nCode\nimport itertools\nlist(itertools.product(range(5), range(5)))[1:]\n\n\n[(0, 1),\n (0, 2),\n (0, 3),\n (0, 4),\n (1, 0),\n (1, 1),\n (1, 2),\n (1, 3),\n (1, 4),\n (2, 0),\n (2, 1),\n (2, 2),\n (2, 3),\n (2, 4),\n (3, 0),\n (3, 1),\n (3, 2),\n (3, 3),\n (3, 4),\n (4, 0),\n (4, 1),\n (4, 2),\n (4, 3),\n (4, 4)]\n\n\nAgain, number of states is given by (roughly) \\(24 \\times \\textrm{Num. positions}\\) and number of actions is \\(9\\). Sounds not very easy problem with many positions.\nSo, letâ€™s start the coding from representing the state and actions. There are multiple ways, but I prefer to NumPy array for representing everything.\nLetâ€™s consider a ASCII representation of the map (or track) like this:\n\n\nCode\nSMALL_TRACK = \"\"\"\n###      F\n##       F\n##       F\n#      ###\n#      ###\n#      ###\n#SSSSS####\n\"\"\"\n\n\nHere, S denotes a starting positiona, F denotes a finishing position, # denotes a wall, and  denotes a road. We have this track as a 2D NumPy Array, and encode agentâ€™s position as an index of this array.\n\n\nCode\nimport numpy as np\n\ndef ascii_to_array(ascii_track: str) -&gt; np.ndarray:\n    \"\"\"Convert the ascii (string) map to a NumPy array.\"\"\"\n\n    lines = [line for line in ascii_track.split(\"\\n\") if len(line) &gt; 0]\n    byte_lines = [list(bytes(line, encoding=\"utf-8\")) for line in lines]\n    return np.array(byte_lines, dtype=np.uint8)\n\ntrack = ascii_to_array(SMALL_TRACK)\nprint(track)\nposition = np.array([0, 0])\ntrack[tuple(position)] == int.from_bytes(b'#', \"big\")\n\n\n[[35 35 35 32 32 32 32 32 32 70]\n [35 35 32 32 32 32 32 32 32 70]\n [35 35 32 32 32 32 32 32 32 70]\n [35 32 32 32 32 32 32 35 35 35]\n [35 32 32 32 32 32 32 35 35 35]\n [35 32 32 32 32 32 32 35 35 35]\n [35 83 83 83 83 83 35 35 35 35]]\n\n\nTrue\n\n\nThen, agentâ€™s velocity and acceleration are also naturally represented by an array. And, we represent an action as an index of an array consisting of all possible acceleration vetors:\n\n\nCode\nnp.array(list(itertools.product([-1, 0, 1], [-1, 0, 1])))\n\n\narray([[-1, -1],\n       [-1,  0],\n       [-1,  1],\n       [ 0, -1],\n       [ 0,  0],\n       [ 0,  1],\n       [ 1, -1],\n       [ 1,  0],\n       [ 1,  1]])\n\n\nThe next step is to represent a transition function as a black box simulator. Note that we will visit another representation by transition matrix, but implementing the simulator is easier. Basically, the simulator should take an agentâ€™s action and current state, and then return the next state. Letâ€™s call this function step. However, letâ€™s make it return some other things to make the implementation easier. Reward function sounds fairly easy to implement given the agentâ€™s position. &gt; The rewards are âˆ’1 for each step until the car crosses the finish line.\nAlso, we have to handle the termination of the episode. &gt; Each episode begins in one of the randomly selected start states with both velocity components zero and ends when the car crosses the finish line.\nSo the resulting step function should return a tuple (state, reward, termination). The below cell contains my implementation of the simulator with matplotlib visualization. The step function is so complicated to handle the case where the agent goes through a wall, so readers are encouraged to just run their eyes through.\n\n\nCode\n# collapse-hide\n\nfrom typing import List, NamedTuple, Optional, Tuple\n\nfrom IPython.display import display\nfrom matplotlib import pyplot as plt\nfrom matplotlib.axes import Axes\nfrom matplotlib.colors import ListedColormap\n\n\nclass State(NamedTuple):\n    position: np.ndarray\n    velocity: np.ndarray\n\n\nclass RacetrackEnv:\n    \"\"\"Racetrack environment\"\"\"\n\n    EMPTY = int.from_bytes(b\" \", \"big\")\n    WALL = int.from_bytes(b\"#\", \"big\")\n    START = int.from_bytes(b\"S\", \"big\")\n    FINISH = int.from_bytes(b\"F\", \"big\")\n\n    def __init__(\n        self,\n        ascii_track: str,\n        noise_prob: float = 0.1,\n        seed: int = 0,\n    ) -&gt; None:\n        self._track = ascii_to_array(ascii_track)\n        self._max_height, self._max_width = self._track.shape\n        self._noise_prob = noise_prob\n        self._actions = np.array(list(itertools.product([-1, 0, 1], [-1, 0, 1])))\n        self._no_accel = 4\n        self._random_state = np.random.RandomState(seed=seed)\n        self._start_positions = np.argwhere(self._track == self.START)\n        self._state_indices = None\n        self._ax = None\n        self._agent_fig = None\n        self._arrow_fig = None\n        \n    def state_index(self, state: State) -&gt; int:\n        \"\"\"Returns a state index\"\"\"\n        (y, x), (vy, vx) = state\n        return y * self._max_width * 25 + x * 25 + vy * 5 + vx\n        \n\n    def _all_passed_positions(\n        self,\n        start: np.ndarray,\n        velocity: np.ndarray,\n    ) -&gt; Tuple[List[np.ndarray], bool]:\n        \"\"\"\n        List all positions that the agent passes over.\n        Here we assume that the y-directional velocity is already flipped by -1.\n        \"\"\"\n\n        maxv = np.max(np.abs(velocity))\n        if maxv == 0:\n            return [start], False\n        one_step_vector = velocity / maxv\n        pos = start + 0.0\n        traj = []\n        for i in range(maxv):\n            pos = pos + one_step_vector\n            ceiled = np.ceil(pos).astype(int)\n            if self._is_out(ceiled):\n                return traj, True\n            traj.append(ceiled)\n        # To prevent numerical issue\n        traj[-1] = start + velocity\n        return traj, False\n\n    def _is_out(self, position: np.ndarray) -&gt; bool:\n        \"\"\"Returns whether the given position is out of the map.\"\"\"\n        y, x = position\n        return y &lt; 0 or x &gt;= self._max_width\n\n    def step(self, state: State, action: int) -&gt; Tuple[State, float, bool]:\n        \"\"\"\n        Taking the current state and an agents' action, returns the next state,\n        reward and a boolean flag that indicates that the current episode terminates.\n        \"\"\"\n        position, velocity = state\n        if self._random_state.rand() &lt; self._noise_prob:\n            accel = self._actions[self._no_accel]\n        else:\n            accel = self._actions[action]\n        # velocity is clipped so that only â†‘â†’ directions are possible\n        next_velocity = np.clip(velocity + accel, a_min=0, a_max=4)\n        # If both of velocity is 0, cancel the acceleration\n        if np.sum(next_velocity) == 0:\n            next_velocity = velocity\n        # List up trajectory. y_velocity is flipped to adjust the coordinate system.\n        traj, went_out = self._all_passed_positions(\n            position,\n            next_velocity * np.array([-1, 1]),\n        )\n        passed_wall, passed_finish = False, False\n        for track in map(lambda pos: self._track[tuple(pos)], traj):\n            passed_wall |= track == self.WALL\n            passed_finish |= track == self.FINISH\n        if not passed_wall and passed_finish:  # Goal!\n            return State(traj[-1], next_velocity), 0, True\n        elif passed_wall or went_out:  # Crasshed to the wall or run outside\n            return self.reset(), -1.0, False\n        else:\n            return State(traj[-1], next_velocity), -1, False\n\n    def reset(self) -&gt; State:\n        \"\"\"Randomly assigns a start position of the agent.\"\"\"\n        n_starts = len(self._start_positions)\n        initial_pos_idx = self._random_state.choice(n_starts)\n        initial_pos = self._start_positions[initial_pos_idx]\n        initial_velocity = np.array([0, 0])\n        return State(initial_pos, initial_velocity)\n\n    def render(\n        self,\n        state: Optional[State] = None,\n        movie: bool = False,\n        ax: Optional[Axes] = None,\n    ) -&gt; Axes:\n        \"\"\"Render the map and (optinally) the agents' position and velocity.\"\"\"\n        if self._ax is None or ax is not None:\n            if ax is None:\n                _, ax = plt.subplots(1, 1, figsize=(8, 8))\n            ax.set_aspect(\"equal\")\n            ax.set_xticks([])\n            ax.set_yticks([])\n            # Map the track to one of [0, 1, 2, 3] to that simple colormap works\n            map_array = np.zeros_like(track)\n            symbols = [self.EMPTY, self.WALL, self.START, self.FINISH]\n            for i in range(track.shape[0]):\n                for j in range(track.shape[1]):\n                    map_array[i, j] = symbols.index(self._track[i, j])\n            cm = ListedColormap(\n                [\"w\", \".75\", \"xkcd:reddish orange\", \"xkcd:kermit green\"]\n            )\n            map_img = ax.imshow(\n                map_array,\n                cmap=cm,\n                vmin=0,\n                vmax=4,\n                alpha=0.8,\n            )\n            if ax.get_legend() is None:\n                descriptions = [\"Empty\", \"Wall\", \"Start\", \"Finish\"]\n                for i in range(1, 4):\n                    if np.any(map_array == i):\n                        ax.plot([0.0], [0.0], color=cm(i), label=descriptions[i])\n                ax.legend(fontsize=12, loc=\"lower right\")\n            self._ax = ax\n        if state is not None:\n            if not movie and self._agent_fig is not None:\n                self._agent_fig.remove()\n            if not movie and self._arrow_fig is not None:\n                self._arrow_fig.remove()\n            pos, vel = state\n            self._agent_fig = self._ax.plot(pos[1], pos[0], \"k^\", markersize=20)[0]\n            # Show velocity\n            self._arrow_fig = self._ax.annotate(\n                \"\",\n                xy=(pos[1], pos[0] + 0.2),\n                xycoords=\"data\",\n                xytext=(pos[1] - vel[1], pos[0] + vel[0] + 0.2),\n                textcoords=\"data\",\n                arrowprops={\"color\": \"xkcd:blueberry\", \"alpha\": 0.6, \"width\": 2},\n            )\n        return self._ax\n\n\nsmalltrack = RacetrackEnv(SMALL_TRACK)\nstate = smalltrack.reset()\nprint(state)\ndisplay(smalltrack.render(state=state).get_figure())\nnext_state, reward, termination = smalltrack.step(state, 7)\nprint(next_state)\nsmalltrack.render(state=next_state)\n\n\nState(position=array([6, 5]), velocity=array([0, 0]))\nState(position=array([5, 5]), velocity=array([1, 0]))\n\n\n\n\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\nNote that the vertical velocity is negative, so that we can simply represent the coordinate by an array index."
  },
  {
    "objectID": "posts/sandb-exercise-racetrack.html#solve-a-small-problem-by-dynamic-programming",
    "href": "posts/sandb-exercise-racetrack.html#solve-a-small-problem-by-dynamic-programming",
    "title": "Exercise 5.12 Racetrack from the Reinforcement Learning textbook",
    "section": "Solve a small problem by dynamic programming",
    "text": "Solve a small problem by dynamic programming\nOK, now we have a simulator, so letâ€™s solve the problem! However, before stepping into reinforcement learning, itâ€™s better to compute an optimal policy in a small problem for sanity check. To do so, we need a transition matrix p, which is a \\(|\\mathcal{S}| \\times |\\mathcal{A}| \\times |\\mathcal{S}|\\) NumPy array where p[i][j][k] is the probability of transiting from i to k when action j is taken. Also, we need a \\(|\\mathcal{S}| \\times |\\mathcal{A}| \\times |\\mathcal{S}|\\) reward matrix r. Note that this representation is not general as \\(R_t\\) can be stochastic, but since the only stochasticity of rewards is the noise to actions in this problem, this notion suffices. It is often not very straightforward to get p and r from the problem definition, but basically we need to give 0-indexd indices to each state (0, 1, 2, ...) and fill the array. Here, I index each state by \\(\\textrm{Idx(S)} = y \\times 25 \\times W + x \\times 25 + vy \\times 5 + vx\\), where \\((x, y)\\) is a position, \\((vx, vy\\)) is a velocity, and \\(W\\) is the width of the map.\n\n\nCode\n# collapse-hide\nfrom typing import Iterable\n\n\ndef get_p_and_r(env: RacetrackEnv) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Taking RacetrackEnv, returns the transition probability p and reward fucntion r of the env.\"\"\"\n    n_states = env._max_height * env._max_width * 25\n    n_actions = len(env._actions)\n    p = np.zeros((n_states, n_actions, n_states))\n    r = np.ones((n_states, n_actions, n_states)) * -1\n    noise = env._noise_prob\n\n    def state_prob(*indices):\n        \"\"\"Returns a |S| length zero-initialized array where specified elements are filled\"\"\"\n        prob = 1.0 / len(indices)\n        res = np.zeros(n_states)\n        for idx in indices:\n            res[idx] = prob\n        return res\n\n    # List up all states and memonize starting states\n    states = []\n    starting_states = []\n    for y in range(env._max_height):\n        for x in range(env._max_width):\n            track = env._track[y][x]\n            for y_velocity in range(5):\n                for x_velocity in range(5):\n                    state = State(np.array([y, x]), np.array([y_velocity, x_velocity]))\n                    states.append(state)\n                    if track == env.START:\n                        starting_states.append(env.state_index(state))\n\n    for state in states:\n        position, velocity = state\n        i = env.state_index(state)\n        track = env._track[tuple(position)]\n        # At a terminating state or unreachable, the agent cannot move\n        if (\n            track == env.FINISH\n            or track == env.WALL\n            or (np.sum(velocity) == 0 and track != env.START)\n        ):\n            r[i] = 0\n            p[i, :] = state_prob(i)\n        # Start or empty states\n        else:\n            # First, compute next state probs without noise\n            next_state_prob = []\n            for j, action in enumerate(env._actions):\n                next_velocity = np.clip(velocity + action, a_min=0, a_max=4)\n                if np.sum(next_velocity) == 0:\n                    next_velocity = velocity\n                traj, went_out = env._all_passed_positions(\n                    position,\n                    next_velocity * np.array([-1, 1]),\n                )\n                passed_wall, passed_finish = False, False\n                for track in map(lambda pos: env._track[tuple(pos)], traj):\n                    passed_wall |= track == env.WALL\n                    passed_finish |= track == env.FINISH\n                if passed_wall or (went_out and not passed_finish):\n                    #  Run outside or crasheed to the wall\n                    next_state_prob.append(state_prob(*starting_states))\n                else:\n                    next_state_idx = env.state_index(State(traj[-1], next_velocity))\n                    next_state_prob.append(state_prob(next_state_idx))\n                    if passed_finish:\n                        r[i, j, next_state_idx] = 0.0\n            # Then linearly mix the transition probs with noise\n            for j in range(n_actions):\n                p[i][j] = (\n                    noise * next_state_prob[env._no_accel]\n                    + (1.0 - noise) * next_state_prob[j]\n                )\n\n    return p, r\n\n\nThen, letâ€™s compute the optimal Q value by value iteration. So far, we only learned dynamic programming with discount factor \\(\\gamma\\), so letâ€™s use \\(\\gamma =0.95\\) that is sufficiently large for this small problem. \\(\\epsilon = 0.000001\\) is used as a convergence threshold. Letâ€™s show the elapsed time and the required number of iteration.\n\n\nCode\nimport datetime\n\n\nclass ValueIterationResult(NamedTuple):\n    q: np.ndarray\n    v: np.ndarray\n    elapsed: datetime.timedelta\n    n_iterations: int\n\n\ndef value_iteration(\n    p: np.ndarray,\n    r: np.ndarray,\n    discount: float,\n    epsilon: float = 1e-6,\n) -&gt; ValueIterationResult:\n    n_states, n_actions, _ = p.shape\n    q = np.zeros((n_states, n_actions))\n    v = np.zeros(n_states)\n    n_iterations = 0\n    start = datetime.datetime.now()\n    while True:\n        n_iterations += 1\n        v_old = v.copy()\n        for s in range(n_states):\n            # Q(s, a) = âˆ‘ p(s, a, s') * (r(s, a, s') + Î³ v(s')\n            for a in range(n_actions):\n                q[s, a] = np.dot(p[s, a], r[s, a] + discount * v)\n            # V(s) = max_a Q(s, a)\n            v[s] = np.max(q[s])\n        if np.linalg.norm(v - v_old, ord=np.inf) &lt; epsilon:\n            break\n    return ValueIterationResult(q, v, datetime.datetime.now() - start, n_iterations)\n\n\np, r = get_p_and_r(smalltrack)\nvi_result = value_iteration(p, r, discount=0.95)\nprint(f\"Elapsed: {vi_result.elapsed.total_seconds()} n_iter: {vi_result.n_iterations}\")\n\n\nElapsed: 4.235676 n_iter: 10\n\n\nIt took longer that a second on my laptop, even for this small problem. Some technical notes on value iteration (\\(R_\\textrm{max} = 1.0\\) is assumed for simplicity): - Each iteration takes \\(O(|\\mathcal{S}| ^ 2 |\\mathcal{A}|)\\) time - The required iteration number is bounded by \\(\\frac{\\log \\epsilon}{\\gamma - 1}\\) - In our case, \\(\\frac{\\log \\epsilon}{\\gamma - 1} \\approx 270\\), so our computation converged quicker than theory - Thus the total computation time is bounded by \\(O(|\\mathcal{S}| ^ 2 |\\mathcal{A}|\\frac{\\log \\epsilon}{\\gamma - 1})\\) - For a convergence threshold \\(\\epsilon\\), \\(\\max |V(s) - V^*(s)| &lt; \\frac{\\gamma\\epsilon}{1 - \\gamma}\\) is guranteed - In our case, \\(\\frac{\\gamma\\epsilon}{1 - \\gamma} \\approx 0.00002\\) - This is called relative error\nLetâ€™s visualize the V value and an optimal trajectory. celluloid) is used for making an animation.\n\n\nCode\nfrom typing import Callable\n\nfrom IPython.display import HTML\n\ntry:\n    from celluloid import Camera\nexcept ImportError as _e:\n    ! pip install celluloid --user\n    from celluloid import Camera\n\nPolicy = Callable[[int], int]\n\n\ndef smalltrack_optimal_policy(state_idx: int) -&gt; int:\n    return np.argmax(vi_result.q[state_idx])\n\n\ndef show_rollout(\n    env: RacetrackEnv,\n    policy: Policy,\n    v: np.ndarray = vi_result.v,\n    title: Optional[str] = None,\n) -&gt; HTML:\n    state = env.reset()\n    prev_termination = False\n    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n    camera = Camera(fig)\n    initial = True\n    while True:\n        env.render(state=state, movie=True, ax=ax)\n        state_idx = env.state_index(state)\n        ax.text(3, 0.5, f\"V(s): {v[state_idx]:02}\", c=\"red\")\n        camera.snap()\n        if prev_termination:\n            break\n        state, _, prev_termination = env.step(state, policy(state_idx))\n    if title is not None:\n        ax.text(3, 0.1, title, c=\"k\")\n    return HTML(camera.animate(interval=1000).to_jshtml())\n\n\nshow_rollout(smalltrack, smalltrack_optimal_policy)\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\nThe computed optimal policy and value seems correct."
  },
  {
    "objectID": "posts/sandb-exercise-racetrack.html#monte-carlo-prediction",
    "href": "posts/sandb-exercise-racetrack.html#monte-carlo-prediction",
    "title": "Exercise 5.12 Racetrack from the Reinforcement Learning textbook",
    "section": "Monte-Carlo prediction",
    "text": "Monte-Carlo prediction\nThen letâ€™s try â€˜reinforcement learningâ€™. First, I implemeted â€˜First visit Monte-Carlo predictionâ€™, which evaluates a (Markovian) policy \\(\\pi\\) by doing a simulation multiple times, and calculates the average of received returns. Here, I evaluate the optimal policy \\(\\pi^*\\) obtained by value iteration.\n\n\nCode\n# collapse-hide\n\nfrom typing import Union\n\n\ndef first_visit_mc_prediction(\n    policy: Policy,\n    env: RacetrackEnv,\n    n_episodes: int,\n    discount: float = 0.95,\n    record_all_values: bool = False,\n) -&gt; Tuple[np.ndarray, List[np.ndarray]]:\n    \"\"\"Predict value function corresponding to the policy by First-visit MC prediction\"\"\"\n    n_states = env._max_width * env._max_height * 25\n    v = np.zeros(n_states)\n    all_values = []\n    # Note that we have to have a list of returns for each state!\n    # So the maximum memory usage would be Num.States x Num.Episodes\n    all_returns = [[] for _ in range(n_states)]\n    for i in range(n_episodes):\n        if record_all_values:\n            all_values.append(v.copy())\n        state = env.reset()\n        visited_states = [env.state_index(state)]\n        received_rewards = []\n        # Rollout the policy until the episode ends\n        while True:\n            # Sample an action from the policy\n            action = policy(env.state_index(state))\n            # Step the simulator\n            state, reward, termination = env.step(state, action)\n            visited_states.append(env.state_index(state))\n            received_rewards.append(reward)\n            if termination:\n                break\n        # Compute return\n        traj_len = len(received_rewards)\n        returns = np.zeros(traj_len)\n        # Gt = Rt when t = T\n        returns[-1] = received_rewards[-1]\n        # Iterating from T - 2, T - 1, ..., to 0\n        for t in reversed(range(traj_len - 1)):\n            # Gt = Rt + Î³Gt+1\n            returns[t] = received_rewards[t] + discount * returns[t + 1]\n        updated = set()\n        # Update the value\n        for i, state in enumerate(visited_states[: -1]):\n            # If the state is already visited, skip it\n            if state in updated:\n                continue\n            updated.add(state)\n            all_returns[state].append(returns[i].item())\n            # V(St) â† average(Returns(St))\n            v[state] = np.mean(all_returns[state])\n    return v, all_values\n\n\nv, all_values = first_visit_mc_prediction(\n    smalltrack_optimal_policy,\n    smalltrack,\n    1000,\n    record_all_values=True,\n)\nvalue_diff = []\nfor i, mc_value in enumerate(all_values + [v]):\n    value_diff.append(np.mean(np.abs(mc_value - vi_result.v)))\nplt.plot(value_diff)\nplt.xlabel(\"Num. Episodes\")\nplt.ylabel(\"Avg. |V* - V|\")\n\n\nText(0, 0.5, 'Avg. |V* - V|')\n\n\n\n\n\nIt looks like that the difference between \\(V^*\\) and the value function estimated by MC prediction converges after 600 steps, but itâ€™s still larger than \\(0\\), because \\(\\pi^*\\) doesnâ€™t visit all states. Letâ€™s plot the difference between value functions only on starting states.\n\n\nCode\nstart_states = []\nfor x in range(1, 6):\n    idx = smalltrack.state_index(State(position=np.array([6, x]), velocity=np.array([0, 0])))\n    start_states.append(idx)\nstart_values = []\nfor i, mc_value in enumerate(all_values + [v]):\n    start_values.append(np.mean(np.abs(mc_value - vi_result.v)[start_states]))\nplt.plot(start_values)\nplt.xlabel(\"Num. Episodes\")\nplt.ylabel(\"|V*(start) - V(start)| only on possible states\")\nplt.ylim((0.0, 0.5))\n\n\n(0.0, 0.5)\n\n\n\n\n\nHere, we can confirm that the estimated value certainly converged close to 0.0, while fractuating a bit. Note that the magnitude of fractuation is larger than the relative error we allowed for value iteration (\\(0.00002\\)), implying the difficulty of convergence."
  },
  {
    "objectID": "posts/sandb-exercise-racetrack.html#monte-carlo-control",
    "href": "posts/sandb-exercise-racetrack.html#monte-carlo-control",
    "title": "Exercise 5.12 Racetrack from the Reinforcement Learning textbook",
    "section": "Monte-Carlo Control",
    "text": "Monte-Carlo Control\nNow we successfully estimate \\(V^\\pi\\) using Monte Carlo method, so then letâ€™s try to learn a sub-optimal \\(\\pi\\) directly using Monte Carlo method. In the textbook, three methods are introduced: - Monte Carlo ES (Exploring Starts) - On-policy first visit Monte Carlo Control - Off-policy first visit Monte Carlo Control\nHere, letâ€™s try all three methods and compare the resulting value functions. However, we cannot naively implement the pseudo code in the textbook, due to a â€˜loopâ€™ problem. Since the car that crashed into the wall is returned to a starting point, the episode length can be infinitte depending on a policy. As a remedy for this problem, I limit the length of the episode as \\(H\\). Supposing that we ignore the future rewards smaller than \\(\\epsilon\\), how to set \\(H\\)? Just by solving \\(\\gamma^H R &lt; \\epsilon\\), we get \\(H &gt; \\frac{\\log \\epsilon}{\\log \\gamma}\\), which is about \\(270\\) in case \\(\\gamma = 0.95\\) and \\(\\epsilon = 0.000001\\).\nBelow are the implementation of three methods. A few notes about implementation: - Monte Carlo ES requires a set of all possible states, which is implemented in valid_states function. - For On-Policy MC, \\(\\epsilon\\) is decreased from 0.5 to 0.01 - We can use arbitary policy in Off-Policy MC, but I used the same \\(\\epsilon\\)-soft policy as On-Policy MC.\n\n\nCode\n# collapse-hide\n\ndef valid_states(env: RacetrackEnv) -&gt; List[State]:\n    states = []\n    for y in range(env._max_height):\n        for x in range(env._max_width):\n            track = env._track[y][x]\n            if track == env.WALL:\n                continue\n            for y_velocity in range(5):\n                for x_velocity in range(5):\n                    state = State(np.array([y, x]), np.array([y_velocity, x_velocity]))\n                    if track != env.START and (x_velocity &gt; 0 or y_velocity &gt; 0):\n                        states.append(state)\n    return states\n\n\ndef mc_es(\n    env: RacetrackEnv,\n    n_episodes: int,\n    discount: float = 0.95,\n    record_all_values: bool = False,\n    seed: int = 999,\n) -&gt; Tuple[np.ndarray, List[np.ndarray]]:\n    \"\"\"Monte-Carlo Control with Exploring Starts\"\"\"\n    n_states = env._max_width * env._max_height * 25\n    n_actions = len(env._actions)\n    random_state = np.random.RandomState(seed=seed)\n    q = np.zeros((n_states, n_actions))\n    pi = random_state.randint(9, size=n_states)\n    all_values = []\n    all_returns = [[[] for _ in range(n_actions)] for _ in range(n_states)]\n    possible_starts = valid_states(env)\n    max_episode_length = int(np.ceil(np.log(1e-6) / np.log(discount)))\n    for i in range(n_episodes):\n        if record_all_values:\n            all_values.append(q.copy())\n        state = possible_starts[random_state.choice(len(possible_starts))]\n        visited_states = [env.state_index(state)]\n        taken_actions = []\n        received_rewards = []\n        initial = True\n        for _ in range(max_episode_length):\n            if initial:\n                # Randomly sample the first action\n                action = random_state.randint(9)\n                initial = False\n            else:\n                # Take an action following the policy\n                action = pi[env.state_index(state)]\n            taken_actions.append(action)\n            # Step the simulator\n            state, reward, termination = env.step(state, action)\n            visited_states.append(env.state_index(state))\n            received_rewards.append(reward)\n            if termination:\n                break\n        # Compute return\n        traj_len = len(received_rewards)\n        returns = np.zeros(traj_len)\n        # Gt = Rt when t = T\n        returns[-1] = received_rewards[-1]\n        # Iterating from T - 2, T - 1, ..., to 0\n        for t in reversed(range(traj_len - 1)):\n            # Gt = Rt + Î³Gt+1\n            returns[t] = received_rewards[t] + discount * returns[t + 1]\n        updated = set()\n        # Update the value\n        for i, (state, action) in enumerate(zip(visited_states[:-1], taken_actions)):\n            # If the state is already visited, skip it\n            if (state, action) in updated:\n                continue\n            updated.add((state, action))\n            all_returns[state][action].append(returns[i].item())\n            # Q(St, At) â† average(Returns(St, At))\n            q[state, action] = np.mean(all_returns[state][action])\n            pi[state] = np.argmax(q[state])\n    return q, all_values\n\n\ndef on_policy_fist_visit_mc(\n    env: RacetrackEnv,\n    n_episodes: int,\n    discount: float = 0.95,\n    epsilon: float = 0.1,\n    epsilon_final: float = 0.1,\n    record_all_values: bool = False,\n    seed: int = 999,\n) -&gt; Tuple[np.ndarray, List[np.ndarray]]:\n    \"\"\"On-policy first visit Monte-Carlo\"\"\"\n\n    n_states = env._max_width * env._max_height * 25\n    n_actions = len(env._actions)\n    random_state = np.random.RandomState(seed=seed)\n    q = np.zeros((n_states, n_actions))\n    pi = random_state.randint(9, size=n_states)\n    all_values = []\n    all_returns = [[[] for _ in range(n_actions)] for _ in range(n_states)]\n    possible_starts = valid_states(env)\n    max_episode_length = int(np.ceil(np.log(1e-6) / np.log(discount)))\n    epsilon_decay = (epsilon - epsilon_final) / n_episodes\n    for i in range(n_episodes):\n        if record_all_values:\n            all_values.append(q.copy())\n        state = env.reset()\n        visited_states = [env.state_index(state)]\n        taken_actions = []\n        received_rewards = []\n        for _ in range(max_episode_length):\n            # Îµ-soft policy\n            if random_state.rand() &lt; epsilon:\n                action = random_state.randint(9)\n            else:\n                action = pi[env.state_index(state)]\n            taken_actions.append(action)\n            # Step the simulator\n            state, reward, termination = env.step(state, action)\n            visited_states.append(env.state_index(state))\n            received_rewards.append(reward)\n            if termination:\n                break\n        # Below code is the same as mc_es\n        # Compute return\n        traj_len = len(received_rewards)\n        returns = np.zeros(traj_len)\n        # Gt = Rt when t = T\n        returns[-1] = received_rewards[-1]\n        # Iterating from T - 2, T - 1, ..., to 0\n        for t in reversed(range(traj_len - 1)):\n            # Gt = Rt + Î³Gt+1\n            returns[t] = received_rewards[t] + discount * returns[t + 1]\n        updated = set()\n        # Update the value\n        for i, (state, action) in enumerate(zip(visited_states[:-1], taken_actions)):\n            # If the state is already visited, skip it\n            if (state, action) in updated:\n                continue\n            updated.add((state, action))\n            all_returns[state][action].append(returns[i].item())\n            # Q(St, At) â† average(Returns(St, At))\n            q[state, action] = np.mean(all_returns[state][action])\n            pi[state] = np.argmax(q[state])\n        epsilon -= epsilon_decay\n    return q, all_values\n\n\ndef off_policy_mc(\n    env: RacetrackEnv,\n    n_episodes: int,\n    discount: float = 0.95,\n    record_all_values: bool = False,\n    epsilon: float = 0.1,\n    epsilon_final: float = 0.1,\n    seed: int = 999,\n) -&gt; Tuple[np.ndarray, List[np.ndarray]]:\n    \"\"\"Off-policy MC control\"\"\"\n\n    n_states = env._max_width * env._max_height * 25\n    n_actions = len(env._actions)\n    random_state = np.random.RandomState(seed=seed)\n    q = np.zeros((n_states, n_actions))\n    c = np.zeros_like(q)\n    pi = np.argmax(q, axis=1)\n    all_values = []\n    possible_starts = valid_states(env)\n    max_episode_length = int(np.ceil(np.log(1e-6) / np.log(discount)))\n    epsilon_decay = (epsilon - epsilon_final) / n_episodes\n    for i in range(n_episodes):\n        if record_all_values:\n            all_values.append(q.copy())\n        state = env.reset()\n        visited_states = [env.state_index(state)]\n        taken_actions = []\n        received_rewards = []\n        acted_optimally = []\n        for _ in range(max_episode_length):\n            # Îµ-soft policy\n            if random_state.rand() &lt; epsilon:\n                action = random_state.randint(9)\n            else:\n                action = pi[env.state_index(state)]\n            acted_optimally.append(action == pi[env.state_index(state)])\n            taken_actions.append(action)\n            # Step the simulator\n            state, reward, termination = env.step(state, action)\n            visited_states.append(env.state_index(state))\n            received_rewards.append(reward)\n            if termination:\n                break\n        g = 0\n        w = 1.0\n        for i, (state, action) in enumerate(zip(visited_states[:-1], taken_actions)):\n            g = discount * g + received_rewards[i]\n            c[state, action] += w\n            q[state, action] += w / c[state, action] * (g - q[state, action])\n            pi[state] = np.argmax(q[state])\n            if action == pi[state]:\n                break\n            else:\n                if acted_optimally[i]:\n                    w *= 1.0 - epsilon + epsilon / n_actions\n                else:\n                    w *= epsilon / n_actions\n        epsilon -= epsilon_decay\n    return q, all_values\n\n\nmces_result = mc_es(smalltrack, 3000, record_all_values=True)\non_mc_result = on_policy_fist_visit_mc(\n    smalltrack,\n    3000,\n    epsilon=0.5,\n    epsilon_final=0.01,\n    record_all_values=True,\n)\noff_mc_result = off_policy_mc(\n    smalltrack,\n    3000,\n    epsilon=0.5,\n    epsilon_final=0.01,\n    record_all_values=True,\n)\n\n\nLetâ€™s plot the results. Here I plotted the difference from the optimal value function and the number of states that the policy choices the optimal action.\n\n\nCode\ndef value_diff(q_values: List[np.ndarray]) -&gt; List[float]:\n    diffs = []\n    for i, q in enumerate(q_values):\n        diff = np.abs(np.max(q, axis=-1) - vi_result.v)[start_states]\n        diffs.append(np.mean(diff))\n    return diffs\n\n\ndef n_optimal_actions(q_values: List[np.ndarray]) -&gt; List[int]:\n    n_optimal = []\n    optimal_actions = np.argmax(vi_result.q, axis=-1)\n    for i, q in enumerate(q_values):\n        greedy = np.argmax(q, axis=-1)\n        n_optimal.append(np.sum(greedy == optimal_actions))\n    return n_optimal\n\n\n_, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\nmc_es_value_diff = value_diff([mces_result[0]] + mces_result[1])\non_mc_value_diff = value_diff([on_mc_result[0]] + on_mc_result[1])\noff_mc_value_diff = value_diff([off_mc_result[0]] + off_mc_result[1])\nax1.plot(mc_es_value_diff, label=\"MC-ES\")\nax1.plot(on_mc_value_diff, label=\"On-Policy\")\nax1.plot(off_mc_value_diff, label=\"Off-Policy\")\nax1.set_xlabel(\"Num. Episodes\")\nax1.set_ylabel(\"Avg. |V* - V|\")\nax1.set_title(\"Diff. from V*\")\nmc_es_nopt = n_optimal_actions([mces_result[0]] + mces_result[1])\non_mc_nopt =  n_optimal_actions([on_mc_result[0]] + on_mc_result[1])\noff_mc_nopt =  n_optimal_actions([off_mc_result[0]] + off_mc_result[1])\nax1.legend(fontsize=12, loc=\"upper right\")\nax2.plot(mc_es_nopt, label=\"MC-ES\")\nax2.plot(on_mc_nopt, label=\"On-Policy\")\nax2.plot(off_mc_nopt, label=\"Off-Policy\")\nax2.set_xlabel(\"Num. Episodes\")\nax2.set_ylabel(\"Num. Optimal Actions\")\nax2.legend(fontsize=12, loc=\"upper right\")\nax2.set_title(\"Num. of optimal actions\")\n\n\nText(0.5, 1.0, 'Num. of optimal actions')\n\n\n\n\n\nSome observations from the results: - On-Policy MC converges to the optimal policy the fastest, though the convergence of its value function is the slowest - MC-ES struggles to distinguish optimal and non-optimal actions at some states, probably because of the lack of exploration during an episode. - Compared to MC-ES and On-Policy MC, the peak of value differences of Off-Policy MC is much milder. - A randomly initialized policy is often caught in a loop and cannot reach to the goal. The value of such a policy is really small (\\(-1 -1 * 0.95 - 1 * 0.95^2 - ... \\approx -20\\)). However, Off-Policy MC uses important sampling to decay the rewards by uncertain actions, resulting the smaller value differences.\nHere I visualized sampled plays from all three methods. On-Policy MC looks the most efficient.\n\n\nCode\nfor q, name in zip([mces_result[0], on_mc_result[0], off_mc_result[0]], [\"MC-ES\", \"On-Policy\", \"Off-Policy\"]):\n    display(show_rollout(smalltrack, lambda i: np.argmax(q[i]), np.argmax(q, axis=-1), name))\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/jax-brax-haiku.html",
    "href": "posts/jax-brax-haiku.html",
    "title": "Jaxãƒ»Braxãƒ»Haikuã§GPUå¼•ãã“ã‚‚ã‚Šå­¦ç¿’",
    "section": "",
    "text": "å¼·åŒ–å­¦ç¿’è‹¥æ‰‹ã®ä¼š Advent Calendar 2021 18æ—¥ç›®\n\n\n0. æ™‚å‹¢ã®ã‚ã„ã•ã¤ã¨ã‹\n\nNote: ã“ã®ãƒ–ãƒ­ã‚°ã¯å¼·åŒ–å­¦ç¿’è‹¥æ‰‹ã®ä¼š Advent Calendar 2021 18æ—¥ç›®ã®è¨˜äº‹ã¨ã—ã¦æ›¸ã‹ã‚Œã¾ã—ãŸ\n\nã“ã‚“ã«ã¡ã¯ã€‚ ã‚³ãƒ­ãƒŠç¦ã‚‚çµ‚ã‚ã‚ŠãŒè¦‹ãˆã¤ã¤ã‚ã‚‹ï¼ˆã¨æ€ã£ãŸã‚‰ã‚ªãƒŸã‚¯ãƒ­ãƒ³æ ªãŒâ€¦ï¼‰2021å¹´ã‚‚ã‚ã¨ã‚ãšã‹ã€‚å¯’ã•ã‚‚å³ã—ããªã£ã¦ãã¾ã—ãŸãŒã€çš†ã•ã‚“å¦‚ä½•ãŠéã”ã—ã§ã—ã‚‡ã†ã‹ã€‚ ã¨ã¯è¨€ã£ãŸã‚‚ã®ã®ã€åƒ•ã¯æ²–ç¸„ã«ã„ã‚‹ã®ã§ã€ãã‚Œã»ã©å¯’ãã¯ãªã„ã®ã§ã™ãŒâ€¦ã€‚\nè‹¥æ‰‹ã®ä¼šã®ã‚¢ãƒ‰ãƒ™ãƒ³ãƒˆã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã¨ã„ã†ã“ã¨ã§ã€å›½å†…ã§ã®ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®æ´»å‹•ã«ã¤ã„ã¦ã€æœ€åˆã«ç·æ‹¬ã—ã¦ã¿ãŸã„ã¨æ€ã„ã¾ã™ã€‚\n\nè‹¥æ‰‹ã®ä¼šã§ã¯æ¨¡å€£å­¦ç¿’ã®å‹‰å¼·ä¼šã‚’ã—ã¾ã—ãŸãŒã€çµå±€2å›ã—ã‹ç¶šãã¾ã›ã‚“ã§ã—ãŸã€‚\nè‹¦æ‰‹ã®ä¼šã®ã‚‚ãã‚‚ãä¼šã¯ãƒ•ãƒªã‚¹ãƒ“ãƒ¼ã‚„ãƒãƒ©ã‚½ãƒ³ã®ç·´ç¿’ã¨ãƒãƒƒãƒ†ã‚£ãƒ³ã‚°ã—ã¦ã‚„ã‚‰ãªããªã£ã¦ã—ã¾ã„ã¾ã—ãŸãŒã€æœ€è¿‘æ—¥ç¨‹ã‚’å¤‰ãˆã¦ã€ç«æ›œæ—¥ã®å¤œã«å§‹ã‚ã¾ã—ãŸã€‚æš‡ãªæ–¹ä¸€ç·’ã«ã‚‚ãã‚‚ãã—ã¾ã—ã‚‡ã†ã€‚\nå¼·åŒ–å­¦ç¿’ã®è¬›ç¾©è³‡æ–™ã®ç¿»è¨³ã‚’ã—ã¦ã„ã¾ã™ã€‚é›£ã—ã„ã§ã™ãŒã€ã‘ã£ã“ã†å‹‰å¼·ã«ãªã‚Šã¾ã™ã€‚æœ‰é™ã‚µãƒ³ãƒ—ãƒ«ã§ã®ãƒã‚¦ãƒ³ãƒ‰ã‚’ã€åˆã‚ã¦å‹‰å¼·ã—ã¾ã—ãŸã€‚èˆˆå‘³ãŒã‚ã‚‹æ–¹ã¯ã€ãœã²ä¸€ç·’ã«ã‚„ã‚Šã¾ã—ã‚‡ã†ã€‚\næ˜¨å¹´ã®ãƒ–ãƒ­ã‚°ã‚’æ›¸ã„ã¦ã‹ã‚‰ã¯ã‚„ä¸€å¹´ã€ã›ã£ã‹ãå°‚ç”¨ã®ãƒ–ãƒ­ã‚°ã‚’ä½œã£ãŸã®ã§ä»Šå¹´ã‚‚ã„ã„æ„Ÿã˜ã«matplotlibèŠ¸å¼·åŒ–å­¦ç¿’ã®è¨˜äº‹ã‚’æ›¸ã„ã¦ã„ããŸã„ã¨æ€ã£ã¦ã„ã¾ã—ãŸãŒã€çµå±€ä½•ã‚‚æ›¸ãã¾ã›ã‚“ã§ã—ãŸã€‚\n\næœ€è¿‘ã¯äººå·¥é€²åŒ–ã‚„äººå·¥ç”Ÿå‘½ã®ç ”ç©¶ã‚‚å§‹ã‚ãŸã®ã§ã€ã‚‚ã¯ã‚„ã€Œå¼·åŒ–å­¦ç¿’ã®äººã€ã¨åä¹—ã£ã¦ã„ã„ã®ã‹ã‚ˆãã‚ã‹ã‚Šã¾ã›ã‚“ãŒã€ä»Šå¾Œã‚‚å›½å†…ã®ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã«ä½•ã‹è²¢çŒ®ã§ãã‚Œã°ã¨æ€ã„ã¾ã™ã€‚\nä»Šå¹´ã¯å¼·åŒ–å­¦ç¿’ã«å¯¾ã™ã‚‹æ¥½è¦³è«–ã‚‚æ‚²è¦³è«–ã‚‚å¤šãç›®ã«ã—ãŸä¸€å¹´ã§ã—ãŸã€‚ David Silverã‚„Suttonã¯Reward is Enoughã¨ã„ã†å¼·æ°—ãªè«–æ–‡ã‚’å‡ºã—ã€çŸ¥çš„ãªã‚·ã‚¹ãƒ†ãƒ ã¯ãŠã‚ˆãå…¨ã¦å ±é…¬æœ€å¤§åŒ–ã§ä½œã‚Œã‚‹ã¨ä¸»å¼µã—ã¾ã—ãŸã€‚ ã•ã™ãŒã«å¼·æ°—ã™ãã‚‹ã¨æ€ã„ã¾ã™ãŒã€ãã®å¾ŒReward is enough for convex MDPsã‚„On the Expressivity of Markov Rewardã¨ã„ã£ãŸãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãªè«–æ–‡ãŒå‡ºã¦ããŸã®ã¯é¢ç™½ã„ã§ã™ã€‚ ã¾ãŸã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å¼·åŒ–å­¦ç¿’ãƒ»æ•™å¸«ãªã—å¼·åŒ–å­¦ç¿’ã®è«–æ–‡ãŒå¢—ãˆã¦ããŸã¨æ€ã„ã¾ã™ã€‚ ã–ã£ãã‚Šã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å¼·åŒ–å­¦ç¿’ = å¼·åŒ–å­¦ç¿’ - æ¢ç´¢ã€æ•™å¸«ãªã—å¼·åŒ–å­¦ç¿’ = å¼·åŒ–å­¦ç¿’ - å ±é…¬ã¨æ€ã£ã¦ã‚‚ã‚‰ã£ã¦å•é¡Œãªã„ã§ã—ã‚‡ã†ã€‚ ä½•ã‚’éš ãã†åƒ•ã®ä¿®å£«è«–æ–‡ã‚‚å˜ãªã‚‹ã€Œéšå±¤å‹å¼·åŒ–å­¦ç¿’ã€ã ã£ãŸã®ã§ã™ãŒã€ãƒªã‚¸ã‚§ã‚¯ãƒˆè«¸èˆ¬ã®äº‹æƒ…ã«ã‚ˆã‚Šæ•™å¸«ãªã—ã«é­”æ”¹é€ ã—ã¦å†æŠ•ç¨¿ã—ã¾ã—ãŸã€‚ Sergey Levineã«ã„ãŸã£ã¦ã¯Understanding the World Through Actionã¨ã„ã†ã‚¿ã‚¤ãƒˆãƒ«ãŒå¼·ã„è«–æ–‡ã®ä¸­ã§ã€ã€Œå¤§é‡ã«ãƒ‡ãƒ¼ã‚¿ã‚’é›†ã‚ã¦ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ•™å¸«ãªã—å¼·åŒ–å­¦ç¿’ã‚’ã™ã‚Œã°ä¸–ç•Œã‚’ç†è§£ã§ãã‚‹ï¼ˆâ‰’ä¸–ç•Œã‚’ç†è§£ã—ã¦ã„ã‚‹ã®ã¨åŒç­‰ã®ã‚·ã‚¹ãƒ†ãƒ ãŒä½œã‚Œã‚‹ï¼Ÿï¼‰ã€ã¨è¨€ã£ã¦ã„ã¾ã™ã€‚é¢ç™½ã„æ–¹å‘æ€§ã ã¨æ€ã„ã¾ã™ã€‚ ä¸€æ–¹ã§ã€ã¿ã‚“ãªå¤§å¥½ããƒ«ãƒ¼ãƒ“ãƒƒã‚¯ã‚­ãƒ¥ãƒ¼ãƒ–è«–æ–‡ã‚’å‡ºã—ãŸOpen AIã®ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ãƒãƒ¼ãƒ ã¯ã€ã€Œã¨ã‚Šã‚ãˆãšä»Šãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹é ˜åŸŸã«æ³¨åŠ›ã™ã‚‹ã€ã¨ã®ã“ã¨ã§è§£æ•£ã—ã¦ã—ã¾ã„ã¾ã—ãŸã€‚ ã“ã®ãƒ–ãƒ­ã‚°ã‚’æ›¸ã„ã¦ã„ã‚‹æœ€ä¸­ã«WebGPTã®è«–æ–‡ã‚’ç›®ã«ã—ã¾ã—ãŸãŒã€ä»Šå¾Œã¯è¨€èªãƒ¢ãƒ‡ãƒ«ï¼‹å¼·åŒ–å­¦ç¿’ã§è‰²ã€…ã‚„ã£ã¦ã„ãã®ã§ã—ã‚‡ã†ã‹ã€‚å“å·ã•ã‚“ã¯å–œã³ãã†ã§ã™ãŒã€åƒ•ãªã‚“ã‹ã¯ã“ã†ã„ã†åˆ°åº•è‡ªåˆ†ã§ã§ããªã„ã‚‚ã®ã¯ã€Œãƒ†ãƒ¬ãƒ“ã®ä¸­ã®ç ”ç©¶ã€ã¨ã„ã†æ„Ÿã˜ãŒã—ã¦ä¸€æ­©å¼•ã„ã¦ã—ã¾ã„ã¾ã™ï¼ˆæœ€è¿‘ã¯ã€ãƒ†ãƒ¬ãƒ“ã¨ã‹ãŸã¨ãˆã«ä½¿ã†ã¨å¤ã„ã®ã‹ãªâ€¦ï¼‰ã€‚ Open AIã®ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ã¯ã€Sim2Realã«ã“ã ã‚ã‚Šã™ããŸã®ã§ã¯ï¼Ÿã¨ã„ã†æ„è¦‹ã‚’æŸæ‰€ã§ãŠèãã—ã¾ã—ãŸã€‚å®Ÿéš›ãã†ãªã®ã‹ã¯çŸ¥ã‚Šã¾ã›ã‚“ãŒã€å¤§è¦æ¨¡ã«ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ã¦Sim2Realã‚’é ‘å¼µã‚‹ã®ã‹ã€å®Ÿæ©Ÿã®ãƒ‡ãƒ¼ã‚¿ã§é ‘å¼µã‚‹ã®ã‹ã¨ã„ã†ã®ã¯ã€é¢ç™½ã„è¦–ç‚¹ã§ã™ã‚ˆã­ã€‚\nOpen AIãŒä»Šã¾ã§ã»ã©å¼·åŒ–å­¦ç¿’ã«æ³¨åŠ›ã—ãªããªã£ãŸã“ã¨ã§ã€Open AI gymã‚’ã¯ã˜ã‚å¼·åŒ–å­¦ç¿’ç ”ç©¶ã§ä½¿ã‚ã‚Œã¦ããŸã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ç¾¤ã«ã‚‚ã€è‰²ã€…ã¨æƒ…å‹¢ã®å¤‰åŒ–ãŒã‚ã‚Šãã†ã§ã™ã€‚ 1. OpenAI Gymã®ãƒ¡ãƒ³ãƒ†ãƒŠãŒå¤‰ã‚ã‚Šã¾ã—ãŸã€‚ã“ã‚Œã‹ã‚‰ã¯Open AIã§ã¯ãªããƒ¡ãƒªãƒ¼ãƒ©ãƒ³ãƒ‰å¤§å­¦ã®å­¦ç”Ÿã•ã‚“ãŒãƒ¡ãƒ³ãƒ†ãƒŠã«ãªã‚‹ã‚ˆã†ã§ã™ã€‚mujoco-pyãªã©é–¢é€£ã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«ã¤ã„ã¦ã¯ç›¸å¤‰ã‚ã‚‰ãšæ”¾ç½®ã•ã‚Œã¦ã„ã¾ã™ã€‚ 2. DeepmindãŒMuJoCoã‚’è²·ã„å–ã£ã¦ç„¡æ–™ã«ã—ã¾ã—ãŸã€‚ä»Šå¾Œã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‚‚å…¬é–‹ã•ã‚Œã‚‹ã‚ˆã†ã§ã™ã€‚ 3. Googleã‹ã‚‰æ–°ã—ãbraxã¨ã„ã†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãŒå…¬é–‹ã•ã‚Œã¾ã—ãŸã€‚\nãã‚“ãªã‚ã‘ã§ã€åƒ•ã¯ã“ã‚Œã¾ã§mujoco-py + gymã§ä½œæˆã—ãŸã‚«ã‚¹ã‚¿ãƒ ç’°å¢ƒã§ãŸãã•ã‚“å®Ÿé¨“ã‚’ã‚„ã£ã¦ãã¾ã—ãŸãŒã€MuJoCoã‚’ä½¿ã†ã«ã—ã¦ã‚‚dm_controlã‚’ä½¿ã†ã¨ã‹ã€ã¯ãŸã¾ãŸbraxã«ã—ã¦ã—ã¾ã†ã¨ã‹ã€åˆ¥ã®é¸æŠè‚¢ã‚’æ¤œè¨ã—ãŸããªã£ã¦ãã¾ã—ãŸã€‚ ã“ã®ãƒ–ãƒ­ã‚°ã§ã¯ã€ã¨ã‚Šã‚ãˆãšbraxã‚’è©¦ã—ã¦ã¿ã‚ˆã†ã¨æ€ã„ã¾ã™ã€‚\n\n\n1. ã¯ã˜ã‚ã«: ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»è¬ãƒ­ãƒœãƒƒãƒˆãƒ»GPU\næœ¬é¡Œã«å…¥ã‚Šã¾ã™ãŒã€ã–ã£ãã‚Šã€å¼·åŒ–å­¦ç¿’ã¨ã¯ã€å ±é…¬ã‹ã‚‰è¡Œå‹•ã‚’å­¦ç¿’ã™ã‚‹æ çµ„ã¿ã ã¨è¨€ã†ã“ã¨ãŒã§ãã¾ã™ã€‚ ã§ã¯ä½•ã®è¡Œå‹•ã‚’å­¦ç¿’ã•ã›ãŸã„ã®ã§ã—ã‚‡ã†ã‹ã€‚ ã‚²ãƒ¼ãƒ ã®AIã ã£ãŸã‚Šã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã ã£ãŸã‚Šã€è‰²ã€…ãªé¸æŠè‚¢ãŒã‚ã‚‹ã¨æ€ã„ã¾ã™ãŒã€ã©ã†ã„ã†ã‚ã‘ã‹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ä¸Šã§å‹•ãè¬ãƒ­ãƒœãƒƒãƒˆã¨ã„ã†ã®ãŒãƒãƒ”ãƒ¥ãƒ©ãƒ¼ãªé¸æŠè‚¢ã§ã™ã€‚\nã“ã®ãƒ–ãƒ­ã‚°ã‚’ã”ã‚‰ã‚“ã®æ–¹ã®ä¸­ã«ã¯ã€ã“ã†ã„ã£ãŸãƒŠãƒŠãƒ•ã‚·ã®ã‚ˆã†ãªè¬ãƒ­ãƒœãƒƒãƒˆã®ç”»åƒã‚’ç›®ã«ã—ãŸã“ã¨ãŒã‚ã‚‹æ–¹ã‚‚å¤šã„ã®ã§ã¯ãªã„ã§ã—ã‚‡ã†ã‹ã€‚\n\n\n\nHalfCheetah\n\n\nã“ã‚Œã¯Open AI gymã®HalfCheetahã¨ã„ã†ãƒ­ãƒœãƒƒãƒˆã§ã™ã€‚è¶³ãŒ2æœ¬ãªã®ã§ãƒãƒ¼ãƒ•ãªã®ã ã¨æ€ã„ã¾ã™ãŒã€ãªã‚“ã¨ã‚‚æ®‹é…·ãªãƒãƒ¼ãƒŸãƒ³ã‚°ã§ã™ã€‚æ„›ç©ã•ã‚Œã‚‹ãŸã‚ç—…æ°—ã®ã¾ã¾å“ç¨®æ”¹è‰¯ã•ã‚Œã¦ããŸçŠ¬çŒ«ã®ã‚ˆã†ãªå“€æ„ãŒæ¼‚ã„ã¾ã™ã€‚\nMuJoCoã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã«ã€Œã“ã“ã¨ã“ã“ãŒã‚¸ãƒ§ã‚¤ãƒ³ãƒˆã§ã€å¯å‹•åŸŸã¯ã“ã†ã§ã™ã€‚åºŠã¯ç™½é»’ã§ãŠé¡˜ã„ã—ã¾ã™ã€ã¿ãŸã„ãªXMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¸¡ã™ã¨ã€ã“ã†ã„ã†ãƒ­ãƒœãƒƒãƒˆã‚’ä½œã£ã¦ãã‚Œã¾ã™ã€‚ ã‚‚ã—ãã¯ã€dm_controlãªã©ã®Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«XMLã‚’ä½œã‚‰ã›ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ ã“ã®ã‚ˆã†ãªè¬ãƒ­ãƒœãƒƒãƒˆãŒå®Ÿé¨“ã§åºƒãä½¿ã‚ã‚Œã¦ã„ã‚‹è¦å› ã¨ã—ã¦ã€ - ã¿ã‚“ãªãŒä½¿ã£ã¦ã„ã‚‹ã‹ã‚‰ - Atariãªã©ã®ã‚²ãƒ¼ãƒ ã‚ˆã‚Šé«˜é€Ÿ - ã‚¸ãƒ§ã‚¤ãƒ³ãƒˆã®é€Ÿã•ãƒ»ä½ç½®ãªã©ã®å®Œå…¨ãªå†…éƒ¨çŠ¶æ…‹ãŒæ‰‹ã«å…¥ã‚‹ - ãƒãƒ«ã‚³ãƒ•æ€§ã«ã¤ã„ã¦å¿ƒé…ã—ãªãã¦ã‚‚ã„ã„ - è‰²ã€…ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã§ãã¦ä¾¿åˆ©ã ã‹ã‚‰ - æ™®é€šã®ãƒ­ãƒœãƒƒãƒˆã‚’è¨“ç·´ã™ã‚‹ãŸã‚ã®ãƒ†ã‚¹ãƒˆã«ã¡ã‚‡ã†ã©ã„ã„ã‹ã‚‰\nãªã©ã®ç†ç”±ãŒã‚ã‚‹ã¨æ€ã„ã¾ã™ãŒã€ãªã‚“ã ã‹ã‚“ã ã¿ã‚“ãªãŒä½¿ã£ã¦ã„ã‚‹ã‹ã‚‰ã¨ã„ã†ã®ãŒå¤§ãã„æ°—ãŒã—ã¾ã™ã€‚\nã¨ã“ã‚ã§ã€ã“ã®MuJoCoã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã¨ã„ã†ã®ã¯éå¸¸ã«é«˜é€Ÿã«å‹•ä½œã™ã‚‹ã®ã§ã™ãŒã€CPUä¸Šã§ã—ã‹å‹•ä½œã—ã¾ã›ã‚“ã€‚ ä»Šæ—¥ä½¿ã‚ã‚Œã¦ã„ã‚‹æ·±å±¤å­¦ç¿’ã®ã‚³ãƒ¼ãƒ‰ã¯ã€ãã®è¨ˆç®—é‡ã®ã»ã¨ã‚“ã©ã‚’å ã‚ã‚‹è¡Œåˆ—æ¼”ç®—ãŒãƒ™ã‚¯ãƒˆãƒ«ä¸¦åˆ—åŒ–ã¨ã¨ã¦ã‚‚ç›¸æ€§ãŒã„ã„ãŸã‚ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚„ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒå¤§ãããªã‚Œã°ãªã‚‹ã»ã©GPUä¸Šã§é«˜é€Ÿã«å‹•ä½œã—ã¾ã™ã€‚ ã¨ãªã‚‹ã¨ã€GPUã§å­¦ç¿’ã‚’å›ã—ã¦ã„ã‚‹å ´åˆã€ã©ã†ã—ã¦ã‚‚CPUã‹ã‚‰GPUã«ãƒ‡ãƒ¼ã‚¿ã‚’è»¢é€ã™ã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ãŒç™ºç”Ÿã—ã€é«˜é€ŸåŒ–ã®å¦¨ã’ã«ãªã‚Šã¾ã™ã€‚ ãã“ã§ã€GPUä¸Šã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œãˆã‚‹ã‚ˆã†ã«ã—ãŸã®ãŒã€ä»Šå›ç´¹ä»‹ã™ã‚‹braxã¨ã„ã†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã§ã™ã€‚\n\n\n2. Jaxã§numpyæ¼”ç®—ã‚’é«˜é€ŸåŒ–ã—ã¦ã¿ã‚‹\nã§ã¯ã€braxã¯CUDAã‹ä½•ã‹ã§æ›¸ã‹ã‚Œã¦ã„ã‚‹ã®ã‹ãªï¼Ÿã¨æ€ã£ãŸã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã€ãªã‚“ã¨å…¨ã¦Pythonã§æ›¸ã‹ã‚Œã¦ã„ã‚‹ã®ã§ã™ã€‚ ãã®éµã¨ãªã‚‹ã®ãŒjaxã¨ã„ã†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚ ãŠã‚‚ã‚€ã‚ã«ã€ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\n! pip install jax\n\n\nãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†’é ­ã«â€™JAX is Autograd and XLAâ€™ã¨ã‚ã‚Šã¾ã™ãŒã€Jaxã¯ - Numpyæ¼”ç®—ã‚’XLAã«å¤‰æ›ã™ã‚‹ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©(Tensorflow) - jax.jit - XLAã¯Tensorflowã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¨ã—ã¦é–‹ç™ºã•ã‚ŒãŸä¸­é–“è¨€èªã§ã€GPU/TPUç”¨ã«ã™ã”ãé€Ÿã„ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã§ãã‚‹ - Numpyæ¼”ç®—ã‚’è¿½è·¡ã—ã¦å‹¾é…ã‚’è¨ˆç®—ã™ã‚‹æ©Ÿèƒ½ - jax.grad/jax.vjp ãªã©\nã®2ã¤ã®ã‚³ã‚¢æ©Ÿèƒ½ã‚’æ ¸ã¨ã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚ ã“ã®ç¯€ã§ã¯ã€ã²ã¨ã¾ãšå‰è€…ã®ã€ŒXLAã«å¤‰æ›ã™ã‚‹ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ã€ã¨ã—ã¦ã®æ©Ÿèƒ½ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã¿ã¾ã™ã€‚\nã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ã¯JITæ–¹å¼ã§å®Ÿè£…ã•ã‚Œã¦ãŠã‚Šã€ 1. jax.jitã«é–¢æ•°fã‚’æ¸¡ã™ (f_compiled = jax.jit(f)ï¼‰ 2. ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã•ã‚Œã‚‹é–¢æ•°f_compiledã‚’æœ€åˆã«å‘¼ã³å‡ºã—ãŸã¨ãã€jaxã¯Pythonã®é–¢æ•°ã‚’XLAã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã™ã‚‹ 3. 2å›ç›®ä»¥é™é–¢æ•°å‘¼ã³å‡ºã—ãŒé«˜é€Ÿã«ãªã‚‹ ã¨ã„ã†å‡¦ç†ã®æµã‚Œã«ãªã‚Šã¾ã™ã€‚\nã§ã¯ã€ã•ã£ããä½•ã‹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ é©å½“ã«å¤©äº•ã‹ã‚‰ãƒœãƒ¼ãƒ«ã‚’è½ã¨ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\nimport typing as t\n\nimport numpy as np\nfrom IPython.display import HTML, clear_output\ntry:\n    import pandas as pd\n    import seaborn as sns\n    from celluloid import Camera\n    from matplotlib import pyplot as plt\n    from matplotlib.animation import ArtistAnimation\nexcept ImportError as _e:\n    ! pip isntall pandas seaborn celluloid\n    clear_output()\n    \n    import pandas as pd\n    import seaborn as sns\n    from celluloid import Camera\n    from matplotlib import pyplot as plt\n    from matplotlib.animation import ArtistAnimation\n\nsns.set_theme(style=\"darkgrid\")\n\nArray = np.ndarray\nGRAVITY = -9.8\n\n\ndef move_balls(\n    ball_positions: Array,\n    ball_velocities: Array,\n    delta_t: float = 0.1,\n) -&gt; Array:\n    accel_x = np.zeros(ball_positions.shape[0])\n    accel_y = np.ones(ball_positions.shape[0]) * GRAVITY * delta_t  # yæ–¹å‘ã«GÎ”tåŠ é€Ÿ\n    new_velocities = np.stack((accel_x, accel_y), axis=1) + ball_velocities\n    new_positions = ball_positions + delta_t * new_velocities\n    return new_positions, new_velocities\n\n\ndef simulate_balls(\n    n_balls: int,\n    n_steps: int = 100,\n    forward: t.Callable[[Array], Array] = move_balls,\n) -&gt; t.List[Array]:\n    p = np.random.uniform(size=(n_balls, 2), low=-50, high=50.0)\n    v = np.random.randn(n_balls, 2)\n    results = [p]\n    for _ in range(n_steps):\n        p, v = forward(p, v)\n        results.append(p)\n    return results\n\n\né©å½“ã«ãƒœãƒ¼ãƒ«ã‚’20å€‹è½ã¨ã—ã¦ã¿ã¾ã™ã€‚\n\n\nCode\ndef ball_animation(balls: t.Iterable[Array]) -&gt; ArtistAnimation:\n    fig = plt.figure(figsize=(8, 8))\n    ax = fig.add_subplot()\n    ax.set_xlim(-50, 50)\n    ax.set_ylim(-50, 50)\n    camera = Camera(fig)\n    for ball_batch in balls:\n        ax.scatter(ball_batch[:, 0], ball_batch[:, 1], color=\"red\", alpha=0.7)\n        camera.snap()\n    return camera.animate()\n\n\nHTML(ball_animation(simulate_balls(20, 40)).to_jshtml())\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\nã§ã¯ã€ã“ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ã™ã‚‹ã®ã«ã€ã©ã‚Œãã‚‰ã„æ™‚é–“ãŒã‹ã‹ã‚‹ã§ã—ã‚‡ã†ã‹ã€‚ãƒœãƒ¼ãƒ«ã®æ•°ã‚’å¤‰ãˆã¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\ndef bench(\n    f: t.Callable[..., t.Any],\n    inputs: t.Iterable[t.Any],\n    number: int = 10,\n) -&gt; t.List[float]:\n    import timeit\n\n    return [timeit.Timer(lambda: f(x)).timeit(number=number) for x in inputs]\n\n\ndef bench_and_plot(f: t.Callable[..., t.Any], title: str) -&gt; pd.DataFrame:\n    inputs = [4000, 8000, 16000, 32000, 64000]\n    result = pd.DataFrame({\"x\": inputs, \"y\": bench(f, inputs)})\n    result[\"Method\"] = [title] * len(inputs)\n    ax = sns.lineplot(data=result, x=\"x\", y=\"y\")\n    ax.set_title(title)\n    ax.set_xlabel(\"Num. of balls\")\n    ax.set_ylabel(\"Time (sec.)\")\n    return result\n\n\nnumpy_result = bench_and_plot(simulate_balls, \"NumPy\")\n\n\n\n\n\nãŠãŠã‚€ã­ç·šå½¢ã«å®Ÿè¡Œæ™‚é–“ãŒå¢—ãˆã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ã“ã®ã‚³ãƒ¼ãƒ‰ã‚’ã€jaxã‚’ä½¿ã£ã¦é«˜é€ŸåŒ–ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ åŸºæœ¬çš„ã«ã¯numpyã‚’jax.numpyã«ç½®ãæ›ãˆã‚Œã°ã„ã„ã§ã™ã€‚\n\n\nCode\nimport jax\nimport jax.numpy as jnp\n\nJaxArray = jnp.DeviceArray\n\ndef move_balls_jax(\n    ball_positions: JaxArray,\n    ball_velocities: JaxArray,\n    delta_t: float = 0.1,\n) -&gt; JaxArray:\n    accel_x = jnp.zeros(ball_positions.shape[0])\n    accel_y = jnp.ones(ball_positions.shape[0]) * GRAVITY * delta_t\n    new_velocities = jnp.stack((accel_x, accel_y), axis=1) + ball_velocities\n    new_positions = ball_positions + delta_t * new_velocities\n    return new_positions, new_velocities\n\n\nã§ã¯åŒã˜ã‚ˆã†ã«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’ã¨ã£ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\njax_nojit_result = bench_and_plot(\n    lambda n: simulate_balls(n, forward=move_balls_jax),\n    \"JAX (without JIT)\",\n)\n\n\n\n\n\nè¬ã®æŒ™å‹•ã‚’è¦‹ã›ã¦ã„ã‚‹ã—ã€ã™ã”ãé…ã„ã§ã™ã­ã€‚ä»Šåº¦ã¯JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ jax.jit(f, backend=\"cpu\")ã§é–¢æ•°ã‚’CPUä¸Šã§å‹•ãXLAã‚³ãƒ¼ãƒ‰ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã§ãã¾ã™ã€‚\n\n\nCode\njax_cpu_result = bench_and_plot(\n    lambda n: simulate_balls(n, forward=jax.jit(move_balls_jax, backend=\"cpu\")),\n    \"JAX (with JIT on CPU)\",\n)\n\n\n\n\n\nã™ã”ãé€Ÿããªã‚Šã¾ã—ãŸã€‚ä»Šåº¦ã¯GPUã§ã‚„ã£ã¦ã¿ã¾ã™ã€‚\n\n\nCode\njax_gpu_result = bench_and_plot(\n    lambda n: simulate_balls(n, forward=jax.jit(move_balls_jax, backend=\"gpu\")),\n    \"JAX (with JIT for GPU)\",\n)\n\n\n\n\n\nåœ§å€’çš„ã«é€Ÿã„ã§ã™ã­ã€‚ä¸€å¿œç·šå½¢ã«å®Ÿè¡Œæ™‚é–“ãŒå¢—ãˆã¦ã¯ã„ã¾ã™ãŒâ€¦ã€‚ ãªãŠã€ä»Šå›ã¯å­¦å†…ã‚¹ãƒ‘ã‚³ãƒ³ã®NVIDIA P100 GPUã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚\n\n\nCode\nax = sns.lineplot(\n    data=pd.concat(\n        [numpy_result, jax_nojit_result, jax_cpu_result, jax_gpu_result],\n        ignore_index=True,\n    ),\n    x=\"x\",\n    y=\"y\",\n    style=\"Method\",\n    hue=\"Method\",\n)\nax.set_title(\"Ball benchmark\")\nax.set_xlabel(\"Num. of balls\")\nax.set_ylabel(\"Time (sec.)\")\nNone\n\n\n\n\n\nã“ã®ãƒœãƒ¼ãƒ«ã®æ•°ã ã¨GPUã¯ç·šå½¢ã«è¨ˆç®—æ™‚é–“ãŒå¢—ãˆã¦ã„ã‚‹ã‚ˆã†ã«è¦‹ãˆã¾ã›ã‚“ã­ã€‚ ã¾ã‚ä½•ã¯ã¨ã‚‚ã‚ã‚Œã€GPUç”¨ã«JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¦ã‚ã’ã‚‹ã¨é€Ÿãã†ã ãªã‚ã€ã¨ã„ã†æ„Ÿã˜ãŒã—ã¾ã™ã€‚\n\n\n3. Jaxã§å‹¾é…ã‚’è¨ˆç®—ã—ã¦ã¿ã‚‹\nJaxã¯å˜ã«é€Ÿã„NumPyã¨ã—ã¦ã®æ©Ÿèƒ½ã«åŠ ãˆã€è‡ªå‹•å¾®åˆ†ã«ã‚ˆã£ã¦ã€é–¢æ•°\\(f(x, y, z, ...)\\)ã®å„\\(x, y, z,...\\)ã«ã‚ˆã‚‹åå¾®åˆ†\\(\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}, \\frac{\\partial f}{\\partial z}, ...\\)ã‚’è¨ˆç®—ã™ã‚‹æ©Ÿèƒ½ã‚’æŒã£ã¦ã„ã¾ã™ã€‚ã“ã“ã§ã¯jax.gradã«ã‚ˆã‚‹å‹¾é…ã®è¨ˆç®—ã ã‘ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚\nãªã‚“ã‹ã€é©å½“ã«é–¢æ•°ã‚’æœ€é©åŒ–ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã¾ãšã¯ã€é©å½“ã«é–¢æ•°ã‚’æ±ºã‚ã¦ã¿ã¾ã™ã€‚ \\(z = x^2 + y^2 + y\\) ã«ã—ã¾ã—ãŸã€‚\n\n\nCode\ndef f(x, y):\n    return x ** 2 + y ** 2 + y\n\n\ndef plot_f(traj: t.Optional[Array] = None) -&gt; None:\n    x, y = np.meshgrid(np.linspace(-5, 5, 100), np.linspace(-5, 5, 100))\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection=\"3d\")\n    ax.plot_surface(\n        x,\n        y,\n        f(x, y),\n        cmap=sns.color_palette(\"flare\", as_cmap=True),\n        alpha=0.8,\n        linewidth=0,\n    )\n    if traj is not None:\n        ax.plot(traj[:, 0], traj[:, 1], traj[:, 2], color=\"blue\")\n    ax.set_xlabel(\"x\", fontsize=14)\n    ax.set_ylabel(\"y\", fontsize=14)\n    ax.set_zlabel(\"z\", fontsize=14, horizontalalignment=\"right\")\n    ax.set_title(\"f\", fontsize=15)\n\n\nplot_f()\n\n\n\n\n\n\\((x, y) = (5, 5)\\)ã§ã®ã“ã®é–¢æ•°ã®å‹¾é…ã‚’è¨ˆç®—ã—ã¦ã¿ã¾ã™ã€‚å‹¾é…ã‚’è¨ˆç®—ã—ã¦ã»ã—ã„å¼•æ•°ã‚’jax.grad(argnums=...)ã§æŒ‡å®šã—ã¾ã™ã€‚\n\n\nCode\njax.grad(f, argnums=(0, 1))(jnp.array(5.0), jnp.array(5.0))\n\n\n(DeviceArray(10., dtype=float32, weak_type=True),\n DeviceArray(11., dtype=float32, weak_type=True))\n\n\n\\(\\frac{\\partial z}{\\partial x}, \\frac{\\partial z}{\\partial y}\\)ã‚’è¨ˆç®—ã—ã¦ãã‚Œã¾ã—ãŸã€‚ ã›ã£ã‹ããªã®ã§ã€æœ€æ€¥é™ä¸‹æ³•ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\ndef steepest_descent(alpha: float = 0.01) -&gt; JaxArray:\n    f_grad = jax.grad(f, argnums=(0, 1))\n    x, y = jnp.array(5.0), jnp.array(5.0)\n    traj = []\n    while True:\n        traj.append((x, y, f(x, y)))\n        x_grad, y_grad = f_grad(x, y)\n        if jnp.linalg.norm(jnp.array([x_grad, y_grad])) &lt; 0.05:\n            break\n        x -= alpha * x_grad\n        y -= alpha * y_grad\n    return jnp.array(traj)\n\nplot_f(steepest_descent())\n\n\n\n\n\næœ€æ€¥é™ä¸‹æ–¹å‘ã«é€²ã‚“ã§ãã‚Œã¦ã„ã‚‹ã‚ˆã†ã«è¦‹ãˆã¾ã™ã€‚ ã¨ã“ã‚ã§ã€gradã¯ãƒˆãƒƒãƒ—ãƒ€ã‚¦ãƒ³å‹ãƒªãƒãƒ¼ã‚¹ãƒ¢ãƒ¼ãƒ‰è‡ªå‹•å¾®åˆ†ï¼ˆèª¤å·®é€†ä¼æ’­æ³•ã®é›£ã—ã„è¨€ã„æ–¹ã§ã™ï¼‰ã‚’æ¡ç”¨ã—ã¦ã„ã‚‹ã®ã§ã€ãƒªãƒãƒ¼ã‚¹ãƒ¢ãƒ¼ãƒ‰ã§Vector Jacobian Productã‚’è¨ˆç®—ã™ã‚‹vjpã¨ã„ã†é–¢æ•°ãŒä½¿ã‚ã‚Œã¦ã„ã¾ã™ã€‚ ãƒ•ã‚©ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ¢ãƒ¼ãƒ‰ã§è¨ˆç®—ã™ã‚‹jvpã¨ã„ã†é–¢æ•°ã‚‚ã‚ã‚Šã¾ã™ã€‚ ã“ã®ã‚ãŸã‚Šã®æ©Ÿèƒ½ã¯ã€ãŸã ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å­¦ç¿’ã•ã›ãŸã„ã ã‘ãªã‚‰ã»ã¨ã‚“ã©ä½¿ã„ã¾ã›ã‚“ãŒã€ä¸€å¿œã‚„ã£ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\nprimals, f_vjp = jax.vjp(f, 5.0, 5.0)\nprint(f\"VJP value: {primals.item()} grad: {[x.item() for x in f_vjp(1.0)]}\")\nvalue, grad = jax.jvp(f, (5.0, 5.0), (1.0, 1.0))\nprint(f\"JVP value: {value.item()} grad: {grad.item()}\")\n\n\nVJP value: 55.0 grad: [10.0, 11.0]\nJVP value: 55.0 grad: 21.0\n\n\nãƒ•ã‚©ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆå‹¾é…ã¨ãªã‚“ã‹ã®ãƒ™ã‚¯ãƒˆãƒ«vã¨ã®å†…ç©ãŒã§ã¦ãã¾ã™ã€‚ ã“ã®ã‚ãŸã‚Šã€è‰²ã€…ãªæ•™ç§‘æ›¸ã«æ›¸ã„ã¦ã‚ã‚‹ã¨æ€ã„ã¾ã™ãŒã€Forward modeã¨Reverse modeã®é•ã„ãªã©ã€Probabilistic Machine Learning: An Introductionã®13ç« ãŒç‰¹ã«ã‚ã‹ã‚Šã‚„ã™ã„ã¨æ€ã„ã¾ã™ã€‚èˆˆå‘³ãŒã‚ã‚‹æ–¹ã¯å‚è€ƒã«ã—ã¦ã¿ã¦ãã ã•ã„ã€‚\n\n\n4. Braxã‚’ä½¿ã£ã¦ã¿ã‚‹\nã˜ã‚ƒã‚MuJoCoã¿ãŸã„ãªç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚‚Jaxã§æ›¸ã„ã¦ã—ã¾ãˆã°å‹æ‰‹ã«GPUä¸Šã§å‹•ã„ã¦é€Ÿã„ã‚“ã˜ã‚ƒãªã„ï¼Ÿã¨ã„ã†ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã§ä½œã‚‰ã‚ŒãŸã®ãŒbraxã§ã™ã€‚ ç°¡å˜ã«ç‰¹å¾´ã‚’ã¾ã¨ã‚ã¦ã¿ã¾ã™ã€‚\n\nJaxã§è¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€jitã§é«˜é€ŸåŒ–ã§ãã‚‹\nProtocol Bufferã§ã‚·ã‚¹ãƒ†ãƒ ã‚’å®šç¾© (cf.Â MuJoCoã¯XMLï¼‰\ndataclassQPã‚’ä½¿ã£ãŸç°¡æ½”ãªçŠ¶æ…‹è¨˜è¿°\n\nQã¯æ­£æº–åº§æ¨™ã€Pã¯é‹å‹•é‡ã‚‰ã—ã„\n\nOpenAI gymé¢¨ã®Env APIã‚„Antãƒ»Halfcheetahãªã©ã®è¬ãƒ­ãƒœãƒƒãƒˆ\n\nãŠã‚‚ã‚€ã‚ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã¿ã¾ã™ã€‚\n\n\nCode\ntry:\n    import brax\nexcept ImportError:\n    !pip install git+https://github.com/google/brax.git@main\n    clear_output()\n    import brax\n\n\nã•ã£ãã¨åŒã˜ã€ãƒœãƒ¼ãƒ«ã‚’å‹•ã‹ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã•ã£ãã¯xyåº§æ¨™ã§å‹•ã‹ã—ã¾ã—ãŸãŒã€brax\n\n\nCode\ndef make_ball() -&gt; None:\n    config = brax.Config(dt=0.1, substeps=4)\n    # ãƒœãƒ¼ãƒ«ã‚’è¿½åŠ \n    ball = config.bodies.add(name=\"ball\", mass=1)\n    capsule = ball.colliders.add().capsule\n    capsule.radius = 0.5\n    # yåº§æ¨™ã«é‡åŠ›\n    config.gravity.y = GRAVITY\n    return config\n\n\ndef make_qp(p, v) -&gt; brax.QP:\n    return brax.QP(\n        pos=jnp.array([[p[0], p[1], 0.0]]),  # position\n        vel=jnp.array([[v[0], v[1], 0.0]]),  # velocity\n        rot=jnp.zeros((1,4)),  # rotation\n        ang=jnp.zeros((1, 3)),  # angular velocity\n    )\n\n\ndef simulate_one_ball_brax(n_steps: int = 100) -&gt; t.List[Array]:\n    sys = brax.System(make_ball())\n    qp = make_qp([0.0, 48.0], [1.0, 0.0])\n    results = []\n    for _ in range(n_steps):\n        qp, _ = sys.step(qp, [])\n        results.append(qp.pos[:2])\n    return results\n\n\nHTML(ball_animation(simulate_one_ball_brax(40)).to_jshtml())\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\nã“ã“ã§ã€4ã¤ã®APIã‚’ä½¿ã„ã¾ã—ãŸã€‚ - brax.Configã§ã‚·ã‚¹ãƒ†ãƒ ã‚’å®šç¾© - brax.System(config)ã§ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œæˆ - brax.QPã§åˆæœŸä½ç½®ãƒ»é€Ÿåº¦ãƒ»ã‚¢ãƒ³ã‚°ãƒ«ç­‰ã‚’ä½œæˆ - brax.System.step(qp, ...)ã§1ã‚¹ãƒ†ãƒƒãƒ—ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ãŸçµæœã‚’å–å¾—\nãƒœãƒ¼ãƒ«ãŒä¸€ã¤ã ã¨ãªã‚“ã¨ãªãç‰©è¶³ã‚Šãªã„ã§ã™ã­ã€‚å¢—ã‚„ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ãã®ãŸã‚ã«ã¯ã€jax.vmapã§sys.stepã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¾ã™ã€‚ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã€vmapã¯å¼•æ•°ã®ãƒ†ãƒ³ã‚½ãƒ«ã«å¯¾ã™ã‚‹æ¼”ç®—ã‚’axis=0ã§ãƒãƒƒãƒåŒ–ã—ã¾ã™ã€‚ ã“ã®ã‚ãŸã‚Šã¯in_axes=(1, 0, ...)ã¨ã‹ã‚„ã‚Œã°èª¿ç¯€ã§ãã¾ã™ãŒã€ä»Šå›ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§OKã§ã™ã€‚\n[make_qp(*pv) for pv in zip(p, v)]ã§ã€List[brax.QP]ã‚’ä½œã£ã¦ã„ã¾ã™ãŒã€ã“ã‚Œã‚’jax.tree_mapã§ã‚‚ã†ä¸€å›QPã«æˆ»ã—ã¦ã„ã¾ã™ã€‚\nList[QP(p=(0, 0), v(0, 0)), QP(..), ...] \nãŒ\nQP(\n    p=[(0, 0), (0.1, 0.2),. ...], \n    v=[(0, 0), (1, 2), ...],\n)\nã«å¤‰æ›ã•ã‚Œã‚‹æ„Ÿã˜ã§ã™ã€‚ ã“ã®ã‚¸ãƒ£ãƒ¼ã‚´ãƒ³ã¯ä¾¿åˆ©ãªã®ã§è¦šãˆã¦ã‚‚ã„ã„ã¨æ€ã„ã¾ã™ã€‚ ã¡ãªã¿ã«ã€treemapã®ãƒãƒ¼ãƒ‰ãŒè‘‰ã‹ã©ã†ã‹ã¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒPyTreeã‹å¦ã‹ã«ã‚ˆã‚Šã¾ã™ã€‚ ã“ã‚Œã¯ã€Œä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’Jaxã¯æš—ã«æœ¨æ§‹é€ ã ã¨ã¿ãªã—ã¾ã™ã€‚ä¸è¶³ãªã‚‰è‡ªåˆ†ã§ç™»éŒ²ã—ã¦ãã ã•ã„ã€ã¨ã„ã†è©±ãªã®ã§ã€æœ€åˆã¯é¢é£Ÿã‚‰ã†ã¨æ€ã„ã¾ã™ã€‚ ã“ã‚Œã‚’é™½ãªAPIã§ã‚„ã‚ã†ã«ã™ã‚‹ã¨Rustã‚„Scalaã«ã‚ã‚‹traitãŒå¿…è¦ãªã®ã§ã€æ‚ªã„è¨­è¨ˆã§ã¯ãªã„ã¨æ€ã„ã¾ã™ãŒã€‚ ã¨ã„ã†ã‚ã‘ã§ã€ã‚³ãƒ¼ãƒ‰ã¯ã“ã‚“ãªæ„Ÿã˜ã«ãªã‚Šã¾ã™ã€‚\n\n\nCode\ndef simulate_balls_brax(n_balls: int, n_steps: int = 100) -&gt; t.List[Array]:\n    sys = brax.System(make_ball())\n    p = np.random.uniform(size=(n_balls, 2), low=-50, high=50.0)\n    v = np.random.randn(n_balls, 2)\n    qps = [make_qp(*pv) for pv in zip(p, v)]\n    qps = jax.tree_map(lambda *args: jnp.stack(args), *qps)\n    # ã“ã“ã§\n    step_vmap = jax.jit(jax.vmap(lambda qp: sys.step(qp, [])))\n    results = []\n    for _ in range(n_steps):\n        qps, _ = step_vmap(qps)\n        results.append(qps.pos[:, 0, :2])\n    return results\n\nHTML(ball_animation(simulate_balls_brax(20, 40)).to_jshtml())\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\njitã‚’ä½¿ã‚ãªã„ã¨braxãŒãªãœã‹numpyã®é–¢æ•°ã‚’å‘¼ã¼ã†ã¨ã—ã¦ã‚¨ãƒ©ãƒ¼ã«ãªã£ãŸã®ã§ã€jitã‚‚ä½µç”¨ã—ã¦ã„ã¾ã™ã€‚\n\n\n5. Haikuã§è¬ã®ãƒ­ãƒœãƒƒãƒˆã‚’å­¦ç¿’ã•ã›ã¦ã¿ã‚‹\nã¨ã„ã†ã‚ã‘ã§ã€braxã®ä½¿ã„æ–¹ã‚’ã–ã£ã¨è¦‹ã¦ã¿ã¾ã—ãŸãŒã€æ¯å›è‡ªåˆ†ã§ãƒ­ãƒœãƒƒãƒˆã‚’è€ƒãˆã‚‹ã®ã¯å¤§å¤‰ã ã—æŸ»èª­è€…ã«ã‚‚æ–‡å¥ã‚’è¨€ã‚ã‚Œã‚‹ãªã®ã§ã€ä»Šå›ã¯è¬ãƒ­ãƒœãƒƒãƒˆã‚’å­¦ç¿’ã•ã›ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ open AI gymé¢¨ã®brax.envs.EnvãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚ä»Šå›ã¯Antã‚’è¨“ç·´ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ gym.makeã«ç›¸å½“ã™ã‚‹ã®ãŒbrax.envs.createã§ã™ã€‚ stepã®APIã¯gymã¨é•ã„å†…éƒ¨çŠ¶æ…‹ãƒ»å ±é…¬ãªã©ãŒå…¥ã£ãŸbrax.envs.Stateã¨ã„ã†ã‚¯ãƒ©ã‚¹ã‚’æ¸¡ã—ã¦æ¬¡ã®Stateã‚’å—ã‘å–ã‚‹ã¨ã„ã†ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ã‚¤ã‚¹ã§ã™ã€‚\n\n\nCode\nimport brax.envs\n\n\ndef render_html(sys: brax.System, qps: t.List[brax.QP]) -&gt; HTML:\n    import uuid\n    import brax.io.html\n\n    html = brax.io.html.render(sys, qps)\n    # A weired trick to show multiple brax viewers...\n    html = html.replace(\"brax-viewer\", f\"brax-viewer-{uuid.uuid4()}\")\n    return HTML(html)\n\n\ndef random_ant() -&gt; HTML:\n    env = brax.envs.create(env_name=\"ant\")\n    prng_key = jax.random.PRNGKey(0)\n    state = env.reset(prng_key)\n    qps = [state.qp]\n    step_jit = jax.jit(env.step)\n    for i in range(10):\n        prng_key, action_key = jax.random.split(prng_key)\n        action = jax.random.normal(action_key, shape=(env.action_size,))\n        state = step_jit(state, action)\n        qps.append(state.qp)\n    return render_html(env.sys, qps)\n\n\nrandom_ant()\n\n\n\n\n  \n    brax visualizer\n    \n  \n  \n    \n    \n    \n  \n\n\n\nãªã‚“ã‹ã€è·³ã­ã¦ã„ã¾ã™ã­ã€‚å¬‰ã—ãã†ã€‚ ã§ã¯ã€ã•ã£ããå­¦ç¿’ã•ã›ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ ä»Šå›ã¯ã€æ·±å±¤å¼·åŒ–å­¦ç¿’ã®ä»£è¡¨çš„ãªæ‰‹æ³•ã§ã‚ã‚‹PPOã‚’ä½¿ã£ã¦ã¿ã¾ã™ã€‚ æœ¬å½“ã¯SACã‚‚ç”¨æ„ã—ãŸã‹ã£ãŸã®ã§ã™ãŒã€æ™‚é–“ãŒãªã‹ã£ãŸã®ã§è«¦ã‚ã¾ã—ãŸã€‚ ã¨ã‚Šã‚ãˆãšã€ä¸‰å±¤MLPã‚’ç”¨æ„ã—ã¾ã—ã‚‡ã†ã€‚ ä¾‹ãˆã°ã€ã“ã‚“ãªæ„Ÿã˜ã®ã‚‚ã®ãŒã‚ã‚Œã°ã„ã„ã§ã™ã€‚\n\n\nCode\ndef mlp_v1(\n    observation: JaxArray,\n    w1: JaxArray,\n    b1: JaxArray,\n    w2: JaxArray,\n    b2: JaxArray,\n    w3: JaxArray,\n    b3: JaxArray,\n) -&gt; JaxArray:\n    x = jnp.dot(observation, w1) + b1\n    x = jnp.tanh(x)\n    x = jnp.dot(x, w2) + b2\n    x = jnp.tanh(x)\n    return jnp.dot(x, w3) + b3\n\n\nã“ã‚Œã‚’jitã—ã¦gradã‚’ã¨ã£ã¦Adamã‹ä½•ã‹ã§ãƒ‘ãƒ©ãƒ¡ã‚¿ã‚’æ›´æ–°ã—ã¦â€¦ã¨ã‚„ã‚Œã°MLPãŒå‹•ãã‚ã‘ã§ã™ãŒã€ãƒ‘ãƒ©ãƒ¡ã‚¿ãŒå¤šã™ãã¦ã¡ã‚‡ã£ã¨é¢å€’ã§ã™ã­ã€‚ ãã“ã§ã“ã“ã§ã¯ã€jaxã§ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’è¨“ç·´ã™ã‚‹éš›ã€ãƒ‘ãƒ©ãƒ¡ã‚¿ã®ç®¡ç†ãªã©ã‚’ã‚„ã£ã¦ãã‚Œã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã‚ã‚‹Haikuã‚’ä½¿ã£ã¦ã¿ã¾ã™ã€‚ ãªãŠã€braxå…¬å¼ã®examplesã§ã¯Flaxã‚’ä½¿ã£ã¦ã„ã¾ã™ã€‚ æ­£ç›´Haikuã‚‚Flaxã‚‚ãã“ã¾ã§å¤‰ã‚ã‚‰ãªã„ã®ã§ã™ãŒã€Flaxã®æ–¹ãŒã‚„ã‚„APIã®æŠ¼ã—ãŒå¼·ã„ï¼ˆPyTorchã§ã„ã†nn.Moduleç›¸å½“ã®ã‚‚ã®ãŒdataclassã§ãªã„ã¨ã„ã‘ãªã‹ã£ãŸã‚Šã¨ã‹ï¼‰å°è±¡ãŒã‚ã‚Šã¾ã™ã€‚ ã¾ãŸã€Haikuã¯DeepmindãŒã€Flaxã¯GoogleãŒé–‹ç™ºã—ã¦ã„ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨ãªã‚Šã¾ã™ã€‚ ã¨ã‚Šã‚ãˆãšã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\ntry:\n    import haiku as hk\n    import optax\n    import chex\n    import distrax\nexcept ImportError as e:\n    ! pip install git+https://github.com/deepmind/dm-haiku \\\n        git+https://github.com/deepmind/optax \\\n        git+https://github.com/deepmind/chex \\\n        git+https://github.com/deepmind/distrax\n    \n    import haiku as hk\n    import optax\n    import chex\n    import distrax\n    \n    clear_output()\n\n\nãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å®šç¾©ã™ã‚‹ãŸã‚ã®Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯theanoã€tensorflowã¨è‰²ã€…ã‚ã‚Šã¾ã—ãŸãŒã€æœ€è¿‘ã¯torch.nn.Moduleã‚„chainer.Linkã®ã‚ˆã†ã«ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®é‡ã¿ãƒ»forwardã®å‡ºåŠ›ãƒ»éš£æ¥ã—ã¦ã„ã‚‹ãƒãƒ¼ãƒ‰ã‚’è¨˜éŒ²ã—ãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½¿ã£ã¦ã€å‹•çš„ã«è¨ˆç®—ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰ã™ã‚‹ã‚‚ã®ãŒå¤šã„ã‹ã¨æ€ã„ã¾ã™ã€‚ ã—ã‹ã—ã€Haikuã«ã‚ˆã‚‹ãã‚Œã¯å°‘ã—ç•°ãªã‚Šã¾ã™ã€‚ãƒã‚¤ãƒ³ãƒˆã¯ã€å‹¾é…ã‚’è¨ˆç®—ã™ã‚‹éƒ¨åˆ†ã¯JaxãŒæ‹…å½“ã™ã‚‹ã®ã§ã€Haikuã¯ãŸã ã€Œãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ‘ãƒ©ãƒ¡ã‚¿ã‚’ç®¡ç†ã™ã‚‹ã ã‘ã€ã§ã„ã„ã¨ã„ã†ã“ã¨ã§ã™ã€‚ ãã®ãŸã‚ã«ã€Haikuã¯transformã¨ã„ã†APIã‚’ç”¨æ„ã—ã¦ã„ã¾ã™ã€‚ ã“ã‚Œã¯è¦‹ãŸã»ã†ãŒæ—©ã„ã§ã—ã‚‡ã†ã€‚\n\n\nCode\ndef mlp_v2(observation: JaxArray) -&gt; JaxArray:\n    w1 = hk.get_parameter(\"w1\", shape=[observation.shape[1], 3], init=jnp.ones)\n    b1 = hk.get_parameter(\"b1\", shape=[3], init=jnp.zeros)\n    w2 = hk.get_parameter(\"w2\", shape=[3, 3], init=jnp.ones)\n    b2 = hk.get_parameter(\"b2\", shape=[3], init=jnp.zeros)\n    w3 = hk.get_parameter(\"w3\", shape=[3, 2], init=jnp.ones)\n    b3 = hk.get_parameter(\"b3\", shape=[2], init=jnp.zeros)\n    x = jnp.dot(observation, w1) + b1\n    x = jnp.tanh(x)\n    x = jnp.dot(x, w2) + b2\n    x = jnp.tanh(x)\n    return jnp.dot(x, w3) + b3\n\nprng_seq = hk.PRNGSequence(0)  # ã“ã‚Œã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã«ã™ã‚‹ã®ã¯è‰¯ããªã„ã§ã™ã€‚çœŸä¼¼ã—ãªã„ã§\ninit, apply = hk.transform(mlp_v2)  # transformã™ã‚‹\n# initã¯ä¹±æ•°ã‚·ãƒ¼ãƒ‰ãƒ»ã‚¤ãƒ³ãƒ—ãƒƒãƒˆã‚’å—ã‘å–ã£ã¦ã€åˆæœŸåŒ–ã—ãŸãƒ‘ãƒ©ãƒ¡ã‚¿ã‚’è¿”ã™é–¢æ•°\nparams = init(next(prng_seq), jnp.zeros((10, 2)))\nprint(params)\n# applyã¯ãƒ‘ãƒ©ãƒ¡ã‚¿ãƒ»ä¹±æ•°ã‚·ãƒ¼ãƒ‰ãƒ»ã‚¤ãƒ³ãƒ—ãƒƒãƒˆã‚’å—ã‘å–ã£ã¦ã€å‡ºåŠ›ã‚’è¿”ã™é–¢æ•°\noutput = apply(params, next(prng_seq), jnp.zeros((10, 2)))\n\n\nFlatMap({\n  '~': FlatMap({\n         'w1': DeviceArray([[1., 1., 1.],\n                            [1., 1., 1.]], dtype=float32),\n         'b1': DeviceArray([0., 0., 0.], dtype=float32),\n         'w2': DeviceArray([[1., 1., 1.],\n                            [1., 1., 1.],\n                            [1., 1., 1.]], dtype=float32),\n         'b2': DeviceArray([0., 0., 0.], dtype=float32),\n         'w3': DeviceArray([[1., 1.],\n                            [1., 1.],\n                            [1., 1.]], dtype=float32),\n         'b3': DeviceArray([0., 0.], dtype=float32),\n       }),\n})\n\n\nã“ã‚“ãªæ„Ÿã˜ã«ãªã‚Šã¾ã™ã€‚ ã¾ã¨ã‚ã‚‹ã¨ã€ - transform(f)ã¯äºŒã¤ã®é–¢æ•°initã€applyã‚’ã‹ãˆã™ - transformã¯fã‚’ã€fã®ä¸­ã§haiku.get_parameterã‚’ä½¿ã£ã¦å‘¼ã³å‡ºã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ã‚¿ã‚’å…¥åŠ›ã¨ã™ã‚‹é–¢æ•°ã«å¤‰æ›ã™ã‚‹ - initã¯ãƒ‘ãƒ©ãƒ¡ã‚¿ã‚’åˆæœŸåŒ–ã—ã¦è¿”ã™ã€‚ãƒ‘ãƒ©ãƒ¡ã‚¿ã¯FlatMapã¨ã„ã†ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã ãŒã“ã‚Œã¯ã»ã¨ã‚“ã©dictã¨åŒã˜ - applyã¯ä¸ãˆã‚‰ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ã‚¿ã‚’ä½¿ã£ã¦æ‰€æœ›ã®è¨ˆç®—ã‚’è¡Œã† ã¨ã„ã†æ„Ÿã˜ã§ã™ã­ã€‚\nã¤ã„ã§ã«ã€ä¸Šã®ä¾‹ã§ã¯hk.PRNGSequenceã¨ã„ã†PRNGKeyã®æ›´æ–°ã‚’å‹æ‰‹ã«ã‚„ã£ã¦ãã‚Œã‚‹ã‚‚ã®ã‚’ä½¿ã£ã¦ã„ã¾ã™ã€‚\nã—ã‹ã—ã€ã“ã‚Œã§ã‚‚ã¾ã é¢å€’ã§ã™ã­ã€‚ å®Ÿéš›ã®ã¨ã“ã‚ã€ã‚ˆãä½¿ã†ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã—ã¦ã¾ã¨ã¾ã£ã¦ã„ã‚‹ã®ã§ã€ã“ã‚Œã‚’ä½¿ãˆã°ã„ã„ã§ã™ã€‚\n\n\nCode\ndef mlp_v3(output_size: int, observation: JaxArray) -&gt; JaxArray:\n    x = hk.Linear(64)(observation)\n    x = jnp.tanh(x)\n    x = hk.Linear(64)(observation)\n    x = jnp.tanh(x)\n    return hk.Linear(output_size)(observation)\n\n\nã“ã‚Œã‚’ä½¿ã£ã¦ã€PPOã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ›¸ã„ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ æ–¹ç­–ã¯æ¨™æº–åå·®ãŒçŠ¶æ…‹ã«ä¾å­˜ã—ãªã„æ­£è¦åˆ†å¸ƒã«ã—ã¾ã™ã€‚\n\n\nCode\nclass NetworkOutput(t.NamedTuple):\n    mean: JaxArray\n    stddev: JaxArray\n    value: JaxArray\n\n\ndef policy_and_value(action_size: int, observation: JaxArray) -&gt; NetworkOutput:\n    mean = mlp_v3(output_size=action_size, observation=observation)\n    value = mlp_v3(output_size=1, observation=observation)\n    logstd = hk.get_parameter(\"logstd\", (1, action_size), init=jnp.zeros)\n    stddev = jnp.ones_like(mean) * jnp.exp(logstd + 1e-8)\n    return NetworkOutput(mean, stddev, value)\n\n\nã“ã‚Œã ã‘ã§ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®é‡ã¿ã¯TruncatedNormalã§åˆæœŸåŒ–ã•ã‚Œã¾ã™ã€‚ä»Šå›ã¯å…¨éƒ¨ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã¾ã¾ã«ã—ã¾ã—ãŸã€‚\næ¬¡ã«ã€ã“ã‚Œã‚’ä½¿ã£ã¦ã€ç’°å¢ƒã¨ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒˆã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ã¿ã¾ã™ã€‚ ã„ã¾ã€braxã®åˆ©ç‚¹ã‚’æ´»ã‹ã™ãŸã‚ã«ã€ 1. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‹ã‚‰æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã‚µãƒ³ãƒ—ãƒ«ã—ã¦ 2. ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã§æ¬¡ã®çŠ¶æ…‹ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ ã¨ã„ã†éç¨‹ã‚’ã™ã¹ã¦jax.jitã®ä¸­ã§ã‚„ã‚‹ã®ãŒç†æƒ³ã§ã™ã‚ˆã­ã€‚\nã§ã™ã‹ã‚‰ã€ãŸã¨ãˆã°ã“ã‚“ãªæ„Ÿã˜ã«ã‚„ã‚Œã°ã„ã„ã§ã™ã€‚\n\n\nCode\nAction = JaxArray\n\n\ndef make_step_function(\n    env: brax.envs.Env,\n) -&gt; t.Tuple[t.Callable[..., t.Any], t.Callable[..., t.Any]]:\n    def step(state: brax.envs.State) -&gt; t.Tuple[brax.envs.State, NetworkOutput, Action]:\n        out = policy_and_value(env.action_size, state.obs)\n        policy = distrax.MultivariateNormalDiag(out.mean, out.stddev)\n        action = policy.sample(seed=hk.next_rng_key())  # transformã™ã‚‹ã¨ã“ã‚ŒãŒä½¿ãˆã¾ã™\n        state = env.step(state, jnp.tanh(action))\n        return state, out, action\n\n    init, apply = hk.transform(step)\n    return jax.jit(init), jax.jit(apply)\n\n\nã“ã“ã§ã€è¡Œå‹•ã®ã‚µãƒ³ãƒ—ãƒ«ã«ã¯distraxã¨ã„ã†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã„ã¾ã—ãŸã€‚ å¹³å‡å€¤ã«ãƒã‚¤ã‚ºã‚’ã„ã‚Œã‚‹ã ã‘ãªã®ã§ã€ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã£ã¦ã‚‚ã‚ã¾ã‚Šå¤‰ã‚ã‚‰ãªã„ã®ã§ã™ãŒâ€¦ã€‚ ã„ã¾ã€å„ã‚¸ãƒ§ã‚¤ãƒ³ãƒˆã«å¯¾ã—ã¦åŠ ãˆã‚‹åŠ›ãŒã€ãã‚Œãã‚Œç‹¬ç«‹ãªæ­£è¦åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚Œã‚‹ã¨ä»®å®šã—ã¦ã„ã‚‹ã®ã§ã€MutliVariateNormDiag(å…±åˆ†æ•£è¡Œåˆ—ãŒå¯¾è§’è¡Œåˆ—ã«ãªã‚‹å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒï¼‰ã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã—ã¦ã„ã¾ã™ã€‚ distrax.Independentã¨distrax.Normalã‚’ä½¿ã£ã¦ã‚‚åŒã˜ã“ã¨ãŒã§ãã¾ã™ã€‚ è¡Œå‹•ã¯ä¸€å¿œtanhã§\\([-1, 1]\\)ã®ç¯„å›²ã«ãªã‚‰ã—ã¦ã„ã¾ã™ã€‚\nã¡ã‚‡ã£ã¨è©¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\nant = brax.envs.create(env_name=\"ant\", batch_size=1)\ninit, step = make_step_function(ant)\ninitial_state = jax.jit(ant.reset)(next(prng_seq))\nparams = init(next(prng_seq), initial_state)\n_next_state, out, action = step(params, next(prng_seq), initial_state)\n# chexã¯ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™\nchex.assert_shape((out.mean, out.stddev, action), (1, ant.action_size))\n\n\nã¨ã„ã†ã‚ã‘ã§ç„¡äº‹ã«stepã‚’JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¦é«˜é€ŸåŒ–ã§ãã¾ã—ãŸã€‚ resetã¯ã»ã¨ã‚“ã©å‘¼ã°ãªã„ã®ã§åˆ¥ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ãªãã¦ã‚‚ã„ã„ã®ã§ã™ãŒã€jitã—ãªã„ã¨braxãŒjnp.DeviceArrayã®ã‹ã‚ã‚Šã«numpyã‚’ä½¿ã„ãŸãŒã£ã¦å°‘ã—é¢å€’ãªã®ã§jitã—ã¦ã„ã¾ã™ã€‚\nã‚ã¨ã¯PPOã‚’å®Ÿè£…ã—ã¦ã„ãã¾ã™ãŒã€æ™‚é–“ã®éƒ½åˆã§æ‰‹çŸ­ã‹ã«ã„ãã¾ã™ã€‚ ã¾ãšã¯GAEã§ã™ã­ã€‚ æ™®é€šã«æ›¸ãã¨jax.jitãŒãƒ«ãƒ¼ãƒ—ã‚¢ãƒ³ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚’è¡Œã£ã¦ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚é–“ãŒæ¿€é…ã«ãªã‚‹ã®ã§ã€jax.lax.fori_loopã¨ã„ã†é»’é­”è¡“ã‚’ä½¿ã„ã¾ã™ã€‚ ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚å®šæ•°ã¯static_argnumsã§æŒ‡å®šã—ã¾ã™ã€‚ vmapã§å„ãƒ¯ãƒ¼ã‚«ãƒ¼ç”¨ã«ä¸¦åˆ—åŒ–ã—ã¾ã™ã€‚\n\n\nCode\nimport functools\n\n@functools.partial(jax.jit, static_argnums=2)\ndef gae(\n    r_t: JaxArray,\n    discount_t: JaxArray,\n    lambda_: float,\n    values: JaxArray,\n) -&gt; chex.Array:\n    chex.assert_rank([r_t, values, discount_t], 1)\n    chex.assert_type([r_t, values, discount_t], float)\n    lambda_ = jnp.ones_like(discount_t) * lambda_\n    delta_t = r_t + discount_t * values[1:] - values[:-1]\n    n = delta_t.shape[0]\n\n    def update(i: int, advantage_t: JaxArray) -&gt; JaxArray:\n        t_ = n - i - 1\n        adv_t = delta_t[t_] + lambda_[t_] * discount_t[t_] * advantage_t[t_ + 1]\n        return jax.ops.index_update(advantage_t, t_, adv_t)\n\n    advantage_t = jax.lax.fori_loop(0, n, update, jnp.zeros(n + 1))\n    return advantage_t[:-1]\n\n\nbatched_gae = jax.vmap(gae, in_axes=(1, 1, None, 1), out_axes=1)\n\n\nãªã‚“ã‹ãƒ–ãƒ­ã‚°ã§æ›¸ãã«ã¯é»’é­”è¡“ã™ãã‚‹æ°—ã‚‚ã—ã¾ã™ãŒâ€¦ã€‚\næ¬¡ã«å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒãƒã‚’æ§‹æˆã™ã‚‹éƒ¨åˆ†ã§ã™ã­ã€‚ ã“ã‚Œã¯ã€æ™®é€šã«PyTorchã¨ã‹ã¨å¤‰ã‚ã‚‰ãªã„ã§ã™ã€‚\n\n\nCode\nimport dataclasses\n\n\n@chex.dataclass\nclass RolloutResult:\n    \"\"\"\n    Required experiences for PPO.\n    \"\"\"\n\n    observations: t.List[JaxArray]\n    actions: t.List[JaxArray] = dataclasses.field(default_factory=list)\n    rewards: t.List[JaxArray] = dataclasses.field(default_factory=list)\n    terminals: t.List[JaxArray] = dataclasses.field(default_factory=list)\n    outputs: t.List[NetworkOutput] = dataclasses.field(default_factory=list)\n\n    def append(\n        self,\n        *,\n        observation: JaxArray,\n        action: JaxArray,\n        reward: JaxArray,\n        output: NetworkOutput,\n        terminal: JaxArray,\n    ) -&gt; None:\n        self.observations.append(observation)\n        self.actions.append(action)\n        self.rewards.append(reward)\n        self.outputs.append(output)\n        self.terminals.append(terminal)\n\n    def clear(self) -&gt; None:\n        self.observations = [self.observations[-1]]\n        self.actions.clear()\n        self.rewards.clear()\n        self.outputs.clear()\n        self.terminals.clear()\n\n\nclass Batch(t.NamedTuple):\n    \"\"\"Batch for PPO, also used as minibatch by indexing.\"\"\"\n\n    observation: JaxArray\n    action: JaxArray\n    reward: JaxArray\n    advantage: JaxArray\n    value_target: JaxArray\n    log_prob: JaxArray\n\n    def __getitem__(self, idx: Array) -&gt; \"Batch\":\n        return self.__class__(\n            observation=self.observation[idx],\n            action=self.action[idx],\n            reward=self.reward[idx],\n            advantage=self.advantage[idx],\n            value_target=self.value_target[idx],\n            log_prob=self.log_prob[idx],\n        )\n\n\n@jax.jit\ndef make_batch(rollout: RolloutResult, next_value: JaxArray) -&gt; Batch:\n    action = jnp.concatenate(rollout.actions)\n    mean, stddev, value = jax.tree_map(lambda *x: jnp.concatenate(x), *rollout.outputs)\n    log_prob = distrax.MultivariateNormalDiag(mean, stddev).log_prob(action)\n    reward = jnp.stack(rollout.rewards)\n    mask = 1.0 - jnp.stack(rollout.terminals)\n    value = jnp.concatenate(\n        (value.reshape(reward.shape), next_value.reshape(1, -1)),\n        axis=0,\n    )\n    advantage = batched_gae(reward, mask * 0.99, 0.95, value)\n    value_target = advantage + value[:-1]\n    return Batch(\n        observation=jnp.concatenate(rollout.observations[:-1]),\n        action=action,\n        reward=jnp.ravel(reward),\n        advantage=jnp.ravel(advantage),\n        value_target=jnp.ravel(value_target),\n        log_prob=log_prob,\n    )\n\n\næ™®é€šã®dataclassesã¯jitã§ããªã„ã®ã§ã€chex.dataclassã‚’ä½¿ã„ã¾ã™ã€‚ ã•ã£ãå°‘ã—ã ã‘è§¦ã‚Œã¾ã—ãŸãŒã€chex.dataclassã¯ä½œæˆã—ãŸdataclassã‚’PyTreeã¨ã—ã¦jaxã«ç™»éŒ²ã—ã¦ãã‚Œã¾ã™ã€‚ å®Ÿã¯flax.struct.dataclassã¨ã„ã†ã ã„ãŸã„åŒã˜ã‚‚ã®ã‚‚ã‚ã£ã¦ã€braxã®å†…éƒ¨ã§ã¯ã“ã‚Œã‚’ä½¿ã£ã¦ã„ã‚‹ã‚ˆã†ã§ã™ã€‚ ã¾ãŸ\\(\\gamma = 0.99, \\lambda = 0.95\\)ã¨ã—ã¾ã—ãŸã€‚\nã„ã‚ˆã„ã‚ˆå­¦ç¿’ã®éƒ¨åˆ†ã§ã™ã­ã€‚ ã¾ãšã€æå¤±é–¢æ•°ã‚’jax.gradã§ãã‚‹ã‚ˆã†ã«æ›¸ãã¾ã™ã€‚\n\n\nCode\ndef ppo_loss(action_size: int, batch: Batch) -&gt; JaxArray:\n    mean, stddev, value = policy_and_value(action_size, batch.observation)\n    # Policy loss\n    policy = distrax.MultivariateNormalDiag(mean, stddev)\n    log_prob = policy.log_prob(batch.action)\n    prob_ratio = jnp.exp(log_prob - batch.log_prob)\n    clipped_ratio = jnp.clip(prob_ratio, 0.8, 1.2)\n    clipped_obj = jnp.fmin(prob_ratio * batch.advantage, clipped_ratio * batch.advantage)\n    policy_loss = -jnp.mean(clipped_obj)\n    # Value loss\n    value_loss = jnp.mean(0.5 * (value - batch.value_target) ** 2)\n    # Entropy regularization\n    entropy_mean = jnp.mean(policy.entropy(), axis=-1)\n    return policy_loss + value_loss - 0.001 * entropy_mean\n\n\n\\(\\epsilon = 0.2\\)ã§å›ºå®šã—ã¦ã„ã‚‹ã®ã§ã€\\([1 - 0.2, 1 + 0.2]\\)ã®ç¯„å›²ã§ã‚¯ãƒªãƒƒãƒ—ã—ã¾ã™ã€‚\nã§ã¯ã“ã‚Œã‚’ä½¿ã£ã¦ã€ä»Šåº¦ã¯ãƒ‘ãƒ©ãƒ¡ã‚¿ã®æ›´æ–°ã‚’å…¨éƒ¨jitã«ã¤ã£ã“ã‚“ã§ã¿ã¾ã—ã‚‡ã†ã€‚ ãƒ‘ãƒ©ãƒ¡ã‚¿ã®æ›´æ–°ã«ã¯optaxã¨ã„ã†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã„ã¾ã™ã€‚è‰²ã€…ãªSGDã®ãƒãƒªã‚¢ãƒ³ãƒˆã‚’å®Ÿè£…ã—ã¦ã„ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ãŒã€åƒ•ã¯ã»ã¨ã‚“ã©Adamã—ã‹ä½¿ã„ã¾ã›ã‚“â€¦ã€‚\n\n\nCode\nimport optax\n\n\ndef make_update_function(\n    action_size: int,\n    opt_update: optax.TransformUpdateFn,\n) -&gt; t.Callable[..., t.Any]:\n    # hk.Paramsã‚’ä½¿ã„å›ã™ã®ã§initã¯æ¨ã¦ã¦ã„ã„\n    # è¡Œå‹•ã‚’ã‚µãƒ³ãƒ—ãƒ«ã—ãªã„ã®ã§ã€without_apply_rngãŒä½¿ãˆã‚‹\n    _, loss_fn = hk.without_apply_rng(hk.transform(lambda batch: ppo_loss(action_size, batch)))\n    grad_fn = jax.grad(loss_fn)\n\n    # ã“ã“ã§jitã—ã¦ã„ã„\n    @jax.jit\n    def update(\n        params: hk.Params,\n        opt_state: optax.OptState,\n        batch: Batch,\n    ) -&gt; t.Tuple[hk.Params, Batch]:\n        grad = grad_fn(params, batch)\n        updates, new_opt_state = opt_update(grad, opt_state)\n        return optax.apply_updates(params, updates), new_opt_state\n\n    return update\n\n\nã•ã¦ã€ã“ã“ã¾ã§æ¥ãŸã‚‰ã‚ã¨ä¸€æ­©ã§ã™ã­ã€‚ æ¬¡ã«é¢å€’ã§ã™ãŒæ¬¡ã®çŠ¶æ…‹ã®valueã‚’ã¨ã£ã¦ãã¦ãƒãƒƒãƒã‚’ä½œã‚‹éƒ¨åˆ†ã‚’æ›¸ãã¾ã™ã€‚\n\n\nCode\ndef make_next_value_function(action_size: int) -&gt; Batch:\n    def next_value_fn(obs: JaxArray) -&gt; JaxArray:\n        output = policy_and_value(action_size, obs)\n        return output.value\n\n    _, next_value_fn = hk.without_apply_rng(hk.transform(next_value_fn))\n    return jax.jit(next_value_fn)\n\n\né€Ÿåº¦ã‚’æ±‚ã‚ã‚‹ãªã‚‰ã€ã“ã‚Œã¯make_batchã¨ä¸€ç·’ã«jitã—ã¦ã—ã¾ã£ã¦ã‚‚ã„ã„ã§ã™ãŒã€ã¾ã‚é¢å€’ãªã®ã§ã“ã‚Œã§ã‚‚ã„ã„ã§ã—ã‚‡ã†ã€‚\nã§ã¯ææ–™ãŒãã‚ã£ãŸã®ã§ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—ã‚’æ›¸ã„ã¦ã„ãã¾ã—ã‚‡ã†ã€‚ é¢å€’ã§ã™ãŒã€è©•ä¾¡ç”¨ã®environmentã‚‚åˆ¥ã«ä½œã‚Šã¾ã™ã€‚\n\n\nCode\ntry:\n    import tqdm\nexcept ImportError as _e:\n    ! pip install tqdm\n    import tqdm\n    clear_output()\n\n\n\n\nCode\nimport datetime\n\nfrom tqdm.notebook import trange\n\n\ndef sample_minibatch_indices(\n    n_instances: int,\n    n_minibatches: int,\n    prng_key: chex.PRNGKey,\n) -&gt; t.Iterable[JaxArray]:\n    indices = jax.random.permutation(prng_key, n_instances)\n    minibatch_size = n_instances // n_minibatches\n    for start in range(0, n_instances, minibatch_size):\n        yield indices[start : start + minibatch_size]\n\n\ndef train_ppo(\n    env_name: str = \"ant\",\n    n_workers: int = 32,\n    n_steps: int = 2048,\n    n_training_steps: int = 10000000,\n    n_optim_epochs: int = 10,\n    n_minibatches: int = 64,\n    eval_freq: int = 20,\n    eval_workers: int = 16,\n    seed: int = 0,\n) -&gt; HTML:\n    # ç’°å¢ƒã¨ã€ç’°å¢ƒã‚’å«ã‚“ã stepé–¢æ•°ã‚’ä½œã‚‹\n    env = brax.envs.create(env_name=env_name, episode_length=1000, batch_size=n_workers)\n    eval_env = brax.envs.create(\n        env_name=env_name,\n        episode_length=1000,\n        batch_size=eval_workers,\n    )\n    network_init, step = make_step_function(env)\n    _, eval_step = make_step_function(eval_env)\n    eval_reset = jax.jit(eval_env.reset)\n    # ä¹±æ•°\n    prng_seq = hk.PRNGSequence(seed)\n    # åˆæœŸçŠ¶æ…‹\n    state = jax.jit(env.reset)(rng=next(prng_seq))\n    rollout = RolloutResult(observations=[state.obs])\n    # Optimizerã¨ãƒ‘ãƒ©ãƒ¡ã‚¿ã‚’åˆæœŸåŒ–ã™ã‚‹\n    optim = optax.chain(optax.clip_by_global_norm(1.0), optax.adam(3e-4, eps=1e-4))\n    update = make_update_function(env.action_size, optim.update)\n    params = network_init(next(prng_seq), state)\n    opt_state = optim.init(params)\n    # next_value\n    next_value_fn = make_next_value_function(env.action_size)\n    n_instances = n_workers * n_steps\n\n    def evaluate(step: int) -&gt; None:\n        eval_state = eval_reset(rng=next(prng_seq))\n        return_ = jnp.zeros(eval_workers)\n        done = jnp.zeros(eval_workers, dtype=bool)\n        for _ in range(1000):\n            eval_state, _, _ = eval_step(params, next(prng_seq), eval_state)\n            return_ = return_ + eval_state.reward * (1.0 - done)\n            done = jnp.bitwise_or(done, eval_state.done.astype(bool))\n        print(f\"Step: {step} Avg. ret: {jnp.mean(return_).item()}\")\n\n    for i in trange(n_training_steps // n_instances):\n        for _ in range(n_steps):\n            state, output, action = step(params, next(prng_seq), state)\n            rollout.append(\n                observation=state.obs,\n                action=action,\n                reward=state.reward,\n                output=output,\n                terminal=state.done,\n            )\n        next_value = next_value_fn(params, state.obs)\n        batch = make_batch(rollout, next_value)\n        rollout.clear()\n        # Batchã‚’ä½œã£ãŸã®ã§ã€ãƒŸãƒ‹ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦å­¦ç¿’\n        for _ in range(n_optim_epochs):\n            for idx in sample_minibatch_indices(\n                n_instances,\n                n_minibatches,\n                next(prng_seq),\n            ):\n                minibatch = batch[idx]\n                params, opt_state = update(params, opt_state, minibatch)\n\n        # æ™‚ã€…è©•ä¾¡ã™ã‚‹\n        if (i + 1) % eval_freq == 0:\n            evaluate(i + 1)\n\n    evaluate(i + 1)\n    # Visualize\n    eval_state = eval_reset(rng=next(prng_seq))\n    qps = []\n    while eval_state.done[0] == 0.0:\n        eval_state, _, _ = eval_step(params, next(prng_seq), eval_state)\n        qps.append(jax.tree_map(lambda qp: qp[0], eval_state.qp))\n    return render_html(eval_env.sys, qps)\n\n\nstart_time = datetime.datetime.now()\nhtml = train_ppo()\nelapsed = datetime.datetime.now() - start_time\nprint(f\"Train completed after {elapsed.total_seconds() / 60:.2f} min.\")\nhtml\n\n\n\n\n\nStep: 20 Avg. ret: -286.34039306640625\nStep: 40 Avg. ret: -273.5491943359375\nStep: 60 Avg. ret: -193.0821990966797\nStep: 80 Avg. ret: -84.20954132080078\nStep: 100 Avg. ret: -45.654090881347656\nStep: 120 Avg. ret: -20.323640823364258\nStep: 140 Avg. ret: -42.74524688720703\nStep: 152 Avg. ret: -5.514527320861816\nTrain completed after 41.34 min.\n\n\n\n\n  \n    brax visualizer\n    \n  \n  \n    \n    \n    \n  \n\n\n\n100ä¸‡ã‚¹ãƒ†ãƒƒãƒ—ã®è¨“ç·´ãŒ41åˆ†ã§çµ‚ã‚ã‚Šã¾ã—ãŸã€‚é€Ÿã„ã§ã™ã­ã‚„ã£ã±ã‚Šã€‚ ãªã‚“ã‹å‰ã«è·³ã­ã™ãã¦ã„ã‚‹å¾®å¦™ãªã®ãŒãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚ºã•ã‚Œã¦ã„ã¾ã™ãŒâ€¦ã€‚\n\n\n6. ã¾ã¨ã‚\nã¨ã„ã†ã‚ã‘ã§ã€ã“ã®ãƒ–ãƒ­ã‚°ã§ã¯Jaxã€Braxã€Haikuã‚’ä½¿ã£ã¦ã€GPUã ã‘ã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ä¸Šã®ãƒ­ãƒœãƒƒãƒˆã‚’è¨“ç·´ã™ã‚‹ä¾‹ã‚’ç¤ºã—ã¾ã—ãŸã€‚ ã‹ãªã‚Šé§†ã‘è¶³ã®è§£èª¬ã«ãªã‚Šã¾ã—ãŸãŒã€ãªã‚“ã¨ãªããƒ—ãƒ­ã‚°ãƒ©ãƒ ã®çµ„ã¿æ–¹ã‚’ç†è§£ã—ã¦ã„ãŸã ã‘ãŸã®ã§ã¯ãªã„ã‹ã¨æ€ã„ã¾ã™ã€‚\nç·æ‹¬ã™ã‚‹ã¨ã€Jaxã¯ã‹ãªã‚Šåºƒã„ç¯„å›²ã®NumPyæ¼”ç®—ã‚’GPU/TPUä¸Šã§é«˜é€Ÿã«å‹•ä½œã™ã‚‹ã‚³ãƒ¼ãƒ‰ã«å¤‰æ›ã§ãã‚‹ã€éå¸¸ã«å¼·åŠ›ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚ ä»Šå›ç´¹ä»‹ã—ãŸvmapã¯ã€ä¾‹ãˆã°ä¸€ã¤ã®GPUä¸Šã§æ¼”ç®—ã‚’ä¸¦åˆ—åŒ–ã™ã‚‹æ©Ÿèƒ½ã§ã™ãŒã€ä»–ã«ã‚‚pmapã«ã‚ˆã‚‹ãƒ‡ãƒã‚¤ã‚¹ã‚’ã¾ãŸã„ã ä¸¦åˆ—åŒ–ã‚‚ã§ãã¾ã™ã€‚ ã§ã™ã‹ã‚‰ç‰¹ã«ã€ - âœ” CPUã¨GPUã®é€šä¿¡ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒæ°—ã«ãªã‚‹ã¨ã - âœ” å¤§è¦æ¨¡ã«ä¸¦åˆ—ãªã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã„ãŸã„ã¨ã\nã¯ã€JaxãŒåŠ¹æœã‚’ç™ºæ®ã™ã‚‹ã¨æ€ã„ã¾ã™ã€‚ã¾ãŸã€jax.lax.fori_loopã‚’ä½¿ã£ã¦ - âœ” Cythonã‚„C++/Rustãªã©ä»–ã®è¨€èªã‚’ä½¿ã‚ãšã«Pythonã®ãƒ«ãƒ¼ãƒ—ã‚’é«˜é€ŸåŒ–ã—ãŸã„ã¨ã\nã«ã‚‚ä½¿ãˆã¾ã™ã€‚ ä¸€æ–¹ã§ã€å˜ã«æ·±å±¤å­¦ç¿’ã‚’é«˜é€ŸåŒ–ã—ãŸã„å ´åˆã€ä¾‹ãˆã° - ğŸ”º PyTorchãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã«ãªã£ã¦ã„ã‚‹å ´åˆ\nãªã©ã¯ã€Jaxã‚„Haiku/Flaxã‚’ä½¿ã†ã“ã¨ã«ã‚ˆã‚‹é«˜é€ŸåŒ–ã®æ©æµã¯ã‚ã¾ã‚Šãªã„ã¨æ€ã„ã¾ã™ã€‚ ã†ã¾ãJitã‚’ä½¿ãˆã°Jaxã®æ–¹ãŒé€Ÿã„ã¨æ€ã„ã¾ã™ãŒã€PyTorchã®CUDAã‚³ãƒ¼ãƒ‰ã¯ã‹ãªã‚Šé€Ÿã„ã§ã™ã‹ã‚‰ã­ã€‚ã¾ãŸã€PyTorchã¨æ¯”è¼ƒã—ãŸéš›ã€ - ğŸ”º å­¦ç¿’ã‚³ã‚¹ãƒˆã«ã¤ã„ã¦ã‚‚Jaxã®æ–¹ãŒå¤§ãã„\nã®ã§ã¯ãªã„ã‹ã¨æ€ã„ã¾ã™ã€‚ ãªã®ã§ã€å€‹äººçš„ã«ã¯å­¦ç¿’ä»¥å¤–ã®éƒ¨åˆ†ã§ãƒ™ã‚¯ãƒˆãƒ«ä¸¦åˆ—åŒ–ãƒ»Jitã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã«ã‚ˆã‚‹ä¸¦åˆ—åŒ–ã®ä½™åœ°ãŒã‚ã‚‹å ´åˆã«ã€Jaxã¯ä¾¿åˆ©ã«ä½¿ãˆã‚‹ã®ã‹ãªã‚ã¨æ€ã„ã¾ã™ã€‚ ãŸã Deepmindã¯AlphaFold2ã‚’å§‹ã‚ã€å¤šãã®Jax+Haikuè£½æ·±å±¤å­¦ç¿’ã‚³ãƒ¼ãƒ‰ã‚’ãƒªãƒªãƒ¼ã‚¹ã—ã¦ã„ã¾ã™ã—ã€ä¸€å¿œèª­ã‚ã‚‹ç¨‹åº¦ã«è¦ªã—ã‚“ã§ãŠãã ã‘ã§ã‚‚ã‚ã‚‹ç¨‹åº¦ã®ãƒ¡ãƒªãƒƒãƒˆã¯ã‚ã‚‹ã¨æ€ã„ã¾ã™ã€‚\nã•ã¦ã€å†’é ­ã®å¤§è¦æ¨¡ã«ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ã¦Sim2Realã‚’é ‘å¼µã‚‹ã®ã‹ã€å®Ÿæ©Ÿã®ãƒ‡ãƒ¼ã‚¿ã§é ‘å¼µã‚‹ã®ã‹ã¨ã„ã†è©±ã«æˆ»ã‚Šã¾ã™ãŒã€ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ã‚¹ã‚±ãƒ¼ãƒ«ã•ã›ãŸã„ã®ã§ã‚ã‚Œã°å¤§è¦æ¨¡ã«ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ãŸã„ãªã‚‰Braxã®ã‚ˆã†ã«ã€Œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’Jaxã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã§ãã‚‹ã‚ˆã†ã«ä½œã‚‹ã€ã¨ã„ã†ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯é¢ç™½ã„ã¨æ€ã„ã¾ã™ã€‚åˆ†å­å‹•åŠ›å­¦è¨ˆç®—ãªã©ã€ç‰©ç†æ¼”ç®—ä»¥å¤–ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¸ã®æ´»ç”¨ã‚‚æœŸå¾…ã•ã‚Œã¾ã™ï¼ˆã¨èã„ãŸã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚åƒ•ã¯åˆ†å­å‹•åŠ›å­¦è¨ˆç®—ãŒä½•ãªã®ã‹ã‚ˆãã‚ã‹ã‚Šã¾ã›ã‚“â€¦ï¼‰ã€‚ ä¸€æ–¹ã§ã€ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãŒå¾®åˆ†å¯èƒ½ã§ã‚ã‚‹ã¨ã„ã†åˆ©ç‚¹ã‚’ã©ã†æ´»ã‹ã™ã®ã‹ã‚‚èˆˆå‘³æ·±ã„ãƒ†ãƒ¼ãƒã§ã™ã€‚åƒ•ã‚‚ä»¥å‰PFNã•ã‚“ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ã§ã€å ±é…¬ãŒå¾®åˆ†å¯èƒ½ãªã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦ã€\\(\\sum_{t = 0}^T\\frac{\\partial r_t}{\\partial \\theta}\\)ã«ã¤ã„ã¦ã®å±±ç™»ã‚Šæ³•ã§æ–¹ç­–ã‚’æ›´æ–°ã™ã‚‹ã®ã‚’è©¦ã—ãŸã“ã¨ãŒã‚ã‚‹ã®ã§ã™ãŒã€å ±é…¬ãŒé ã„ã¨ãªã‹ãªã‹é›£ã—ã„ãªã‚ã¨ã„ã†å°è±¡ã§ã—ãŸã€‚ã†ã¾ã„æ–¹æ³•ãŒã‚ã‚Œã°ã„ã„ã®ã§ã™ãŒâ€¦ã€‚æ„å¤–ã¨å‹¾é…é™ä¸‹ã ã‘ã§ãªãé€²åŒ–è¨ˆç®—ãªã©ã®ãƒ¡ã‚¿ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ã‚¯ã‚¹ã¨çµ„ã¿åˆã‚ã›ã‚‹ã¨é¢ç™½ã„ã‹ã‚‚ã—ã‚Œãªã„ã§ã™ã€‚\nã•ã¦ã€åƒ•ã¯ã‚‚ã†ä¸€ã¤ã‚¢ãƒ‰ãƒ™ãƒ³ãƒˆã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã®è¨˜äº‹ã‚’æ›¸ãäºˆå®šãŒã‚ã£ãŸã®ã§ã™ãŒã€æ™‚é–“ãŒãªã„ã®ã§ä»–ã®äººã«ä»£ã‚ã£ã¦ã‚‚ã‚‰ã†ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“â€¦ã€‚å‡ºãŸã‚‰ãã¡ã‚‰ã‚‚ã‚ˆã‚ã—ããŠé¡˜ã„ã—ã¾ã™ã€‚"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html",
    "href": "posts/understanding-what-makes-rl-difficult.html",
    "title": "ã‚ˆã‚Šè‰¯ã„å•é¡Œè¨­è¨ˆã¸å‘ã‘ã¦ï¼š ä½•ãŒå¼·åŒ–å­¦ç¿’ã‚’é›£ã—ãã™ã‚‹ã®ã‹ã‚’ç†è§£ã—ã‚ˆã†",
    "section": "",
    "text": "å¼·åŒ–å­¦ç¿’è‹¦æ‰‹ã®ä¼š Advent Calendar 2020 20æ—¥ç›® # 1. ã¯ã˜ã‚ã«\nå¼·åŒ–å­¦ç¿’ã¯ã€é€æ¬¡çš„ã«æ„æ€æ±ºå®šã™ã‚‹å•é¡Œã‚’å®šç¾©ã™ã‚‹ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚ å¾“æ¥ã¯å¤§å¤‰ã ã£ãŸ1 ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®è¨“ç·´ãŒç°¡å˜ã«ãªã£ãŸã“ã¨ã‚„ã€ Alpha GOãªã©æ·±å±¤å¼·åŒ–å­¦ç¿’(Deep RL)ã®æˆåŠŸã‚’èƒŒæ™¯ã«ã€å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ãƒ»çµŒæ¸ˆ ãªã©ã€æ§˜ã€…ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§å¼·åŒ–å­¦ç¿’ã®åˆ©ç”¨ãŒè©¦ã¿ã‚‰ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«æ€ã„ã¾ã™ã€‚ åƒ•å€‹äººã¨ã—ã¦ã‚‚ã€å¼·åŒ–å­¦ç¿’ã¯æ±ç”¨çš„ã§é¢ç™½ã„ãƒ„ãƒ¼ãƒ«ã ã¨æ€ã†ã®ã§ã€å°†æ¥çš„ã«ã¯è‰²ã€…ãªå¿œç”¨åˆ†é‡ã§åºƒãæ´»ç”¨ã•ã‚Œã‚‹ã¨ã„ã„ãªã€ã¨æ€ã„ã¾ã™ã€‚\nä¸€æ–¹ã§ã€å¼·åŒ–å­¦ç¿’ã‚’ä½•ã‹ç‰¹å®šã®å•é¡Œã«å¿œç”¨ã—ã¦ã¿ã‚ˆã†ã€ã¨ã„ã†å ´é¢ã§ã¯ã€ ãã®æ±ç”¨æ€§ã‚†ãˆã‹ãˆã£ã¦ã¨ã£ã¤ãã«ãã„ãƒ»æ‰±ã„ã¥ã‚‰ã„é¢ãŒã‚ã‚‹ã‚ˆã†ã«æ€ã„ã¾ã™ã€‚ å®Ÿéš›ã«ã€è‹¦æ‰‹ã®ä¼šãªã©ã§å¿œç”¨ç ”ç©¶ã‚’ã•ã‚Œã¦ã„ã‚‹æ–¹ã‹ã‚‰ã€ - å•é¡Œã‚’å®šç¾©ã™ã‚‹ã®ãŒãã‚‚ãã‚‚å¤§å¤‰ - è‰²ã€…ãªæ‰‹æ³•ãŒã‚ã£ã¦ã€ä½•ãŒãªã‚“ã ã‹ã‚ˆãã‚ã‹ã‚‰ãªã„\nãªã©ã®æ„è¦‹ã‚’è¦³æ¸¬ã§ãã¾ã—ãŸã€‚\nã§ã¯å¿œç”¨ç ”ç©¶ã«å¯¾ã™ã‚‹ã€Œãƒ„ãƒ¼ãƒ«ã€ã¨ã—ã¦å¼·åŒ–å­¦ç¿’ã‚’æ‰±ã†ä¸Šã§ä½•ãŒå¤§äº‹ãªã®ã ã‚ã†ã€ã¨è€ƒãˆãŸã¨ãã€ åƒ•ã¯ç°¡å˜ãªå•é¡Œã‚’è¨­è¨ˆã™ã‚‹ã“ã¨ã“ããŒå¤§äº‹ã ã¨ã„ã†ä»®èª¬ã«æ€ã„ã„ãŸã‚Šã¾ã—ãŸã€‚ ç°¡å˜ãªå•é¡Œã‚’è¨­è¨ˆã™ã‚‹ãŸã‚ã«ã¯ã€å¼·åŒ–å­¦ç¿’ã®ä¸­ã§ã‚‚ã©ã†ã„ã†å•é¡ŒãŒé›£ã—ã„ã®ã‹ã€ ã¨ã„ã†ã“ã¨ã‚’ãã¡ã‚“ã¨ç†è§£ã—ã¦ãŠãå¿…è¦ãŒã‚ã‚‹ã‚ˆã†ã«æ€ã„ã¾ã™ã€‚\nãã“ã§ã“ã®ãƒ–ãƒ­ã‚°è¨˜äº‹ã§ã¯ã€å¼·åŒ–å­¦ç¿’ã®ä¸­ã§ã‚‚ã€Œé›£ã—ã„å•é¡Œã€ãŒã©ã†ã„ã†ã‚‚ã®ãªã®ã‹ã€ ãã†ã„ã†å•é¡Œã¯ãªãœé›£ã—ã„ã®ã‹ã«ã¤ã„ã¦ã€ä¾‹ã‚’é€šã—ã¦ãªã‚‹ã¹ãç›´æ„Ÿçš„ã«èª¬æ˜ã™ã‚‹ã“ã¨ã‚’è©¦ã¿ã¾ã™ã€‚ å¼·åŒ–å­¦ç¿’ã®é›£ã—ã•ãŒã‚ã‹ã£ãŸæšã«ã¯ã€ãã£ã¨ - ãã‚‚ãã‚‚å¼·åŒ–å­¦ç¿’ã‚’ä½¿ã‚ãªã„ã¨ã„ã†é¸æŠãŒã§ãã‚‹ã—ã€ - ãªã‚‹ã¹ãç°¡å˜ã«è§£ã‘ã‚‹ã‚ˆã†ãªå¼·åŒ–å­¦ç¿’ã®å•é¡Œã‚’è¨­è¨ˆã§ãã‚‹ã—ã€ - å•é¡Œã«åˆã‚ã›ã¦æ‰‹æ³•ã‚’é¸æŠã§ãã‚‹\nã“ã¨ã§ã—ã‚‡ã†ã€‚\nè¨˜äº‹ã®æ§‹æˆã¨ã—ã¦ã€å¼·åŒ–å­¦ç¿’ã®é›£ã—ã•ã«ã¤ã„ã¦ã€Œå ´åˆåˆ†ã‘ã€ã‚’è¡Œã„ã€ - MDPã‚’è§£ãã“ã¨ã®é›£ã—ã• - ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã™ã‚‹ã“ã¨ã®é›£ã—ã•\nã¨ã„ã†2ã¤ã®è¦³ç‚¹ã‹ã‚‰æ•´ç†ã—ã¦ã„ãã¾ã™ã€‚\nå‰æçŸ¥è­˜ã«ã¤ã„ã¦ã€åˆå¿ƒè€…ã®æ–¹ã§ã‚‚èª­ã‚ã‚‹ã‚ˆã†ã«ã€ å¼·åŒ–å­¦ç¿’ã«ã¤ã„ã¦ã®çŸ¥è­˜ã«ã¤ã„ã¦ã¯ãªã‚‹ã¹ãè¨˜äº‹ã®ä¸­ã§è£œè¶³ã—ã¾ã™ã€‚ ã—ã‹ã—ã€ã™ã”ãé›‘ã«æ›¸ãã®ã§ã€è©³ç´°ã¯Reinforcement Learning: An Introduction ãªã©ã®æ•™ç§‘æ›¸ã‚’å‚ç…§ã•ã‚Œã‚‹ã¨ã„ã„ã¨æ€ã„ã¾ã™ã€‚ ã¾ãŸã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’è¦‹ãŸã»ã†ãŒã‚¤ãƒ¡ãƒ¼ã‚¸ã—ã‚„ã™ã„ï¼ˆæ–¹ã‚‚ã„ã‚‹ï¼‰ã‹ã¨æ€ã£ã¦Pythonã®ã‚³ãƒ¼ãƒ‰ä¾‹ã‚’ãŸã¾ã«å‡ºã—ã¦ã„ã¾ã™ã€‚ ã‚³ãƒ¼ãƒ‰ä¾‹ã§ã¯ã€\\(\\sum_{s} f(s) g(s, a)\\)ã®ã‚ˆã†ã«ãƒ†ãƒ³ã‚½ãƒ«ã®é©å½“ãªè»¸ã§æ›ã‘ç®—ã—ã¦è¶³ã—è¾¼ã‚€æ¼”ç®—ã« numpy.einsum ã‚’å¤šç”¨ã—ã¦ã„ã‚‹ã®ã§ã€çŸ¥ã£ã¦ã„ãŸã»ã†ãŒèª­ã¿ã‚„ã™ã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html#vpicdot-vã‚’è©•ä¾¡ã™ã‚‹é›£ã—ã•",
    "href": "posts/understanding-what-makes-rl-difficult.html#vpicdot-vã‚’è©•ä¾¡ã™ã‚‹é›£ã—ã•",
    "title": "ã‚ˆã‚Šè‰¯ã„å•é¡Œè¨­è¨ˆã¸å‘ã‘ã¦ï¼š ä½•ãŒå¼·åŒ–å­¦ç¿’ã‚’é›£ã—ãã™ã‚‹ã®ã‹ã‚’ç†è§£ã—ã‚ˆã†",
    "section": "2.1: \\(V^\\pi\\cdot V^*\\)ã‚’è©•ä¾¡ã™ã‚‹é›£ã—ã•",
    "text": "2.1: \\(V^\\pi\\cdot V^*\\)ã‚’è©•ä¾¡ã™ã‚‹é›£ã—ã•\nã§ã¯ã€ã‚‚ã†å°‘ã—é›£ã—ã„MDPã‚’è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\nmdp3 = ChainMDP(\n    [[1.0, 0.0], [0.8, 1.0], [1.0, 0.9]], [[0.0, 0.0], [0.5, 0.0], [0.0, 1.0]]\n)\n_ = mdp3.show(\"MDP3\")\n\n\n\n\n\nä»Šåº¦ã¯ã€State 1ã§å³ã«ã€State 2ã§å·¦ã«è¡Œã‘ã°è‰¯ã•ãã†ã§ã™ã€‚ \\[\n\\begin{aligned}\nV^* (1) = 0.5 + \\gamma (0.1 * V^*(1) + 0.9 * V^*(2)) \\\\\nV^* (2) = 1.0 + \\gamma (0.8 * V^*(1) + 0.2 * V^*(2))\n\\end{aligned}\n\\]\nå…ˆã»ã©ã®å•é¡Œã¨é•ã£ã¦1ã‚‚2ã‚‚å¸å¼•çŠ¶æ…‹ã§ã¯ãªã„ã®ã§ã€\\(V(1)\\)ã¨\\(V(2)\\)ãŒãŠäº’ã„ã«ä¾å­˜ã™ã‚‹é¢å€’ãª æ–¹ç¨‹å¼ãŒå‡ºã¦ãã¦ã—ã¾ã„ã¾ã—ãŸã€‚ ã“ã®ã‚ˆã†ãªãƒ«ãƒ¼ãƒ—ã®å­˜åœ¨ãŒã€å¼·åŒ–å­¦ç¿’ã‚’é›£ã—ãã—ã¦ã„ã‚‹è¦ç´ ã®ä¸€ã¤ã§ã™ã€‚\nã¨ã¯ã„ãˆã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã§æ•°å€¤çš„ã«è§£ãã®ã¯ç°¡å˜ã§ã™ã€‚ çŠ¶æ…‹\\(s\\)ã«ã„ã¦ã€ã‚ã¨\\(n\\)å›è¡Œå‹•ã§ãã‚‹æ™‚ã®ä¾¡å€¤é–¢æ•°ã‚’\\(V_n^\\pi(s)\\)ã¨æ›¸ãã¾ã™ã€‚ ä»»æ„ã®\\(s\\)ã«ã¤ã„ã¦ã€\\(V_0^\\pi(s) = 0\\)ã§ã™ï¼ˆ1å›ã‚‚è¡Œå‹•ã§ããªã„ã®ã§!ï¼‰ã€‚ \\(V_i^\\pi\\) ã‹ã‚‰ \\(V_{i + 1}^\\pi\\) ã‚’æ±‚ã‚ã‚‹ã«ã¯ã€1ã‚¹ãƒ†ãƒƒãƒ—ã ã‘å…ˆèª­ã¿ã™ã‚Œã°ã„ã„ã®ã§ã€ \\[\nV_{i + 1}^\\pi(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s) \\left( r(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s'|s, a) V_{i}^\\pi(s') \\right)\n\\] ã§è¨ˆç®—ã§ãã¾ã™ã€‚\\(\\gamma &lt; 1\\)ã«ã‚ˆã‚Šã“ã®åå¾©è¨ˆç®—ã¯åæŸã—ã€\\(V^\\pi\\) ãŒæ±‚ã¾ã‚Šã¾ã™ã€‚ å®Ÿéš›ã«ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§æ›¸ã„ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\nMAX_ITER_V_PI: int = int(1e5)\n\n\ndef v_pi(\n    r: Array2,\n    p: Array3,\n    pi: Array2,\n    gamma: float,\n    epsilon: float,\n) -&gt; Tuple[Array1, int]:\n    v = np.zeros(r.shape[0])  # VÏ€\n    r_pi = np.einsum(\"sa,sa-&gt;s\", pi, r)  # |S|, Ï€ã‚’ä½¿ã£ãŸã¨ãã«è²°ã†å ±é…¬ã®ãƒ™ã‚¯ãƒˆãƒ«\n    p_pi = np.einsum(\"saS,sa-&gt;sS\", p, pi)  # |S| x |S|, Ï€ã‚’ä½¿ã£ãŸã¨ãã®çŠ¶æ…‹é·ç§»ç¢ºç‡\n    for n_iter in range(MAX_ITER_V_PI):\n        v_next = r_pi + gamma * np.einsum(\"s,sS\", v, p_pi.T)\n        if np.all(np.absolute(v_next - v) &lt; epsilon):\n            return v_next, n_iter + 1\n        v = v_next\n    # ç†è«–çš„ã«ã¯å¿…ãšåæŸã™ã‚‹ã®ã§ã€ãƒã‚°äºˆé˜²\n    raise RuntimeError(\"Policy Evaluation did not converge &gt;_&lt;\")\n\n\npi_star = np.array([[1.0, 0.0], [1.0, 0.0], [0.0, 1.0]])\nv_star_mdp3, n_iter = v_pi(mdp3.r, mdp3.p, pi_star, gamma=0.9, epsilon=1e-4)\nprint(f\"åå¾©å›æ•°: {n_iter}\")\nprint(\" \".join([f\"V({i}): {v:.3}\" for i, v in enumerate(v_star_mdp3)]))\n\n\nåå¾©å›æ•°: 86\nV(0): 6.49 V(1): 7.21 V(2): 7.51\n\n\n86å›ã“ã®è¨ˆç®—ã‚’åå¾©ã—ãŸå¾Œã€ãªã‚“ã‹ãã‚Œã£ã½ã„æ•°å­—ãŒå‡ºã¦ãã¾ã—ãŸã€‚ ã“ã®åå¾©å›æ•°ã¯ã€ä½•ã«ã‚ˆã£ã¦æ±ºã¾ã‚‹ã®ã§ã—ã‚‡ã†ã‹ï¼Ÿ\nä»»æ„ã® \\(s\\) ã«ã¤ã„ã¦ \\(|V_{i+1}^\\pi(s) - V_i^\\pi(s)| &lt; \\epsilon\\) ãªã‚‰è¨ˆç®—çµ‚ã‚ã‚Šã€ã¨ã—ã¾ã™4ã€‚ \\(V_n^\\pi(s)\\)ã¯ã€Œã‚ã¨\\(n\\)ã‚¹ãƒ†ãƒƒãƒ—è¡Œå‹•ã§ãã‚‹æ™‚ã®çŠ¶æ…‹ä¾¡å€¤ã®æœŸå¾…å€¤ã€ãªã®ã§ã€\\(i\\) ã‚¹ãƒ†ãƒƒãƒ—ç›®ã«ã‚‚ã‚‰ã£ãŸå ±é…¬ã‚’ \\(R_i\\)ã¨ã™ã‚‹ã¨ã€ \\[\nV_n^\\pi(s) = \\mathbb{E}_{s, \\pi} \\left[ R_1 + \\gamma R_2 + \\gamma^2 R_3 + ... \\right]\n\\] ã¨æ›¸ã‘ã¾ã™ã€‚ ãªã®ã§ã€å ±é…¬ã®ç¯„å›²ãŒ\\(0 \\leq R_t &lt; R_\\textrm{max}\\)ã ã¨ä»®å®šã™ã‚‹ã¨ã€ \\(\\gamma^{k - 1} R_\\textrm{max} &lt; \\epsilon\\)ãªã‚‰ã“ã®æ•°å€¤è¨ˆç®—ãŒåæŸã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ ç°¡å˜ã®ãŸã‚\\(R_\\textrm{max}=1\\)ã¨ã—ã¦ã¿ã‚‹ã¨ã€\\(k\\)ãŒæº€ãŸã™ã¹ãæ¡ä»¶ã¯ \\[\n\\gamma^{k-1} &lt; \\epsilon\n\\Leftrightarrow\nk &lt; \\frac{\\log\\epsilon}{\\log\\gamma} + 1\n\\] ã¨ãªã‚Šã¾ã™ã€‚ ã‚³ãƒ¼ãƒ‰ã®ä¸­ã§ \\(\\gamma = 0.9, \\epsilon = 0.0001\\) ã¨ã—ãŸã®ã§ã€ãŸã‹ã ã‹89å›ã®åå¾©ã§åæŸã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ å®Ÿé¨“çµæœã§ã¯86å›ã ã£ãŸã®ã§ã€ã ã„ãŸã„åŒã˜ãã‚‰ã„ã§ã™ã­ã€‚\nã‚ˆã£ã¦ã€\\(V^\\pi\\)ã‚’åå¾©æ³•ã«ã‚ˆã‚Šè©•ä¾¡ã—ãŸæ™‚ã€ãã®åå¾©å›æ•°ã¯å ±é…¬ãƒ»\\(\\epsilon\\)ãƒ»\\(\\gamma\\)ã«ä¾å­˜ã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸã€‚ å ±é…¬ã¨\\(\\epsilon\\)ã«ã¯\\(\\log\\)ã®ã‚ªãƒ¼ãƒ€ãƒ¼ã§ã—ã‹ä¾å­˜ã—ãªã„ã®ã«å¯¾ã—ã€\\(\\gamma\\)ã«å¯¾ã—ã¦ã¯ \\(O((-\\log\\gamma)^{-1})\\)ã®ã‚ªãƒ¼ãƒ€ãƒ¼ã§ä¾å­˜ã—ã¦ã„ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ è©¦ã—ã«ã€\\(\\epsilon=0.0001\\)ã®æ™‚ã®\\(\\frac{\\log\\epsilon}{\\log\\gamma}\\)ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\n#hide_input\n_, ax = plt.subplots(1, 1)\nx = np.logspace(-0.1, -0.001, 1000)\ny = np.log(1e-4) / np.log(x)\nax.set_xlabel(\"Î³\", fontsize=16)\nax.set_ylabel(\"log(Îµ)/log(Î³)\", fontsize=16)\n_ = ax.plot(x, y, \"b-\", lw=3, alpha=0.7)\n\n\n\n\n\nã“ã®ã‚ˆã†ã«ã€\\(\\gamma\\)ãŒå¤§ãããªã‚‹ã¨ä¸€æ°—ã«åå¾©å›æ•°ãŒå¢—ãˆã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ ã¾ãŸã€ã“ã®æ•°å€¤è¨ˆç®—ãŒåæŸã—ãŸæ™‚ã€çœŸã®\\(V^\\pi\\)ã¨ã®å·®ãŒ \\[\n\\begin{aligned}\nV^\\pi(s) - V_k^\\pi(s) &= \\mathbb{E}_{s, \\pi} \\left[ \\gamma^k R_{k + 1} + \\gamma^{k + 1} R_{k + 2} ... \\right] \\\\\n&&lt; \\frac{\\gamma^k R_\\textrm{max}}{1 - \\gamma} &lt; \\frac{\\gamma \\epsilon}{1 - \\gamma}\n\\end{aligned}\n\\] ã§æŠ‘ãˆã‚‰ã‚Œã‚‹ã“ã¨ã‚‚ã‚ã‹ã‚Šã¾ã™ã€‚\næ¬¡ã¯ã€ã„ããªã‚Š\\(V^*\\)ã‚’æ±‚ã‚ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ \\(V^\\pi\\)ã‚’æ±‚ã‚ãŸæ™‚ã¨åŒã˜ã‚ˆã†ã«ã€çŠ¶æ…‹\\(s\\)ã«ã„ã¦ã€ ã‚ã¨\\(n\\)å›è¡Œå‹•ã§ãã‚‹æ™‚ã®æœ€é©ä¾¡å€¤é–¢æ•°ã‚’\\(V_n^*(s)\\)ã¨æ›¸ãã¾ã™ã€‚ å…ˆã»ã©ã¨åŒæ§˜ã«ã€\\(V_i^*\\)ã‹ã‚‰1ã‚¹ãƒ†ãƒƒãƒ—å…ˆèª­ã¿ã—ã¦\\(V^*_{i + 1}\\)ã‚’æ±‚ã‚ã¾ã™ã€‚ æ®‹ã‚Š\\(i + 1\\)ã‚¹ãƒ†ãƒƒãƒ—ã‚ã‚‹æ™‚ã€ \\(r(s, a) + \\sum_{s' \\in \\mathcal{S}} P(s'|s, a) V_i^*(s)\\) ãŒæœ€å¤§ã«ãªã‚‹ã‚ˆã†ãªè¡Œå‹•\\(a\\)ã‚’é¸ã¶ã®ãŒæœ€é©ã§ã™ã€‚ ã§ã™ã‹ã‚‰ã€\\(V^*_{i + 1}\\)ã¯ \\[\nV_{i + 1}^*(s) = \\max_a \\left( r(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s'|s, a) V_{i}^\\pi(s') \\right)\n\\] ã§æ±‚ã‚ã‚‰ã‚Œã¾ã™ã€‚ ã•ã£ãã‚ˆã‚Šç°¡å˜ãªå¼ã«ãªã£ã¦ã—ã¾ã„ã¾ã—ãŸã€‚ ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§æ›¸ã„ã¦ã¿ã¾ã™ã€‚\n\n\nCode\nMAX_ITER_VI: int = int(1e6)\n\n\ndef value_iteration(\n    r: Array2,\n    p: Array3,\n    gamma: float,\n    epsilon: float,\n) -&gt; Tuple[Array1, int]:\n    v = np.zeros(p.shape[0])\n    for n_iter in range(MAX_ITER_VI):\n        # ã“ã‚Œâ†“ã¯Q Valueã¨ã‚‚è¨€ã„ã¾ã™\n        r_plus_gamma_pv = r + gamma * np.einsum(\"saS,S-&gt;sa\", p, v)\n        v_next = r_plus_gamma_pv.max(axis=1)\n        if np.all(np.absolute(v_next - v) &lt; epsilon):\n            return v_next, n_iter + 1\n        v = v_next\n    raise RuntimeError(\"Value Iteration did not converge &gt;_&lt;\")\n\n\nv_star_mdp3_vi, n_iter = value_iteration(mdp3.r, mdp3.p, 0.9, 1e-4)\nprint(f\"åå¾©å›æ•°: {n_iter}\")\nprint(\" \".join([f\"V({i}): {v:.3}\" for i, v in enumerate(v_star_mdp3_vi)]))\n\n\nåå¾©å›æ•°: 86\nV(0): 6.49 V(1): 7.21 V(2): 7.51\n\n\nå…ˆç¨‹ã¨åŒã˜ãã€86å›ã®åå¾©ã§\\(V^*\\)ãŒæ±‚ã¾ã‚Šã¾ã—ãŸã€‚ ã“ã®åå¾©å›æ•°ã‚‚ã€å…ˆã»ã©ã®\\(V^\\pi\\)ã¨åŒã˜ã‚ˆã†ã«\\(\\gamma,\\epsilon\\)ã‚’ç”¨ã„ã¦æŠ‘ãˆã‚‰ã‚Œã¾ã™ã€‚\nã—ã‹ã—ã€\\(\\epsilon\\)ã¯äººæ‰‹ã§è¨­å®šã™ã‚‹ãƒ‘ãƒ©ãƒ¡ã‚¿ã§ã™ã€‚ æœ€é©æ–¹ç­–ãŒæ±‚ã¾ã‚Œã°\\(V^*\\)ã¯å¤§ã—ã¦æ­£ç¢ºã§ãªãã¨ã‚‚å›°ã‚‰ãªã„ã¨ã„ã†å ´åˆã¯ã€ã‚‚ã£ã¨\\(\\epsilon\\)ã‚’å¤§ããã—ã¦ã€ è¨ˆç®—ã‚’æ—©ãçµ‚ã‚ã‚‰ã›ãŸã„æ°—ãŒã—ã¾ã™ã€‚ ã§ã¯ã€ã€Œã©ã‚“ãªå ´åˆã«\\(\\epsilon\\)ã‚’å¤§ããã§ãã‚‹ã‹ã€ã‚’è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚\nç°¡å˜ã®ãŸã‚ã€ \\(Q^\\pi(s, a) = r(s, a) + \\gamma \\sum_{s' in \\mathcal{S}} P(s'|s, a) V^\\pi(s')\\) (Qã¯Qualityã®Qã‚‰ã—ã„)ã‚’å°å…¥ã—ã¾ã™ã€‚ æ®‹ã‚Š\\(k\\)ã‚¹ãƒ†ãƒƒãƒ—ã‚ã‚‹æ™‚ã®æœ€é©è¡Œå‹•ã‚’\\(a_k^* = \\textrm{argmax}_a Q_k^*(s, a)\\)ã¨ã—ã¾ã™ã€‚ ã™ã‚‹ã¨ã€\\(k+1\\)ã‚¹ãƒ†ãƒƒãƒ—ç›®ä»¥é™ã®å‰²å¼•å ±é…¬å’Œã¯ \\(\\frac{\\gamma^{k}R_\\textrm{max}}{1 -\\gamma}\\)ã§æŠ‘ãˆã‚‰ã‚Œã‚‹ã®ã§ã€ \\[\nQ_k^*(s, a_k^*) - \\max_{a \\neq a_k^*} Q_k^*(s, a) &lt; \\frac{\\gamma^k R_\\textrm{max}}{1 -\\gamma}\n\\] ãŒæˆã‚Šç«‹ã¤ãªã‚‰ã€\\(a_k^*\\)ãŒä»Šå¾Œä»–ã®è¡Œå‹•ã«é€†è»¢ã•ã‚Œã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ ãªã®ã§\\(a_k^*\\)ãŒæœ€é©ã§ã„ã„ã‚ˆã­ã€ã¯ã„ã“ã®è©±çµ‚ã‚ã‚Šã€ã¨ã„ã†ã“ã¨ã«ãªã‚Šã¾ã™ã€‚ ä»¥ä¸‹ç•¥è¨˜ã—ã¦ \\(A_\\textrm{min}^*(s, a_k) = Q_k^*(s, a_k^*) - \\max_{a \\neq a_k^*} Q_k^*(s, a)\\) ã¨æ›¸ãã¾ã™ï¼ˆä»–ã®è¡Œå‹•ã«å¯¾ã™ã‚‹ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ã®æœ€å°å€¤ã¨ã„ã†æ„å‘³ï¼‰ã€‚ ã„ã¾\\(\\gamma^{k-1} R_\\textrm{max}&lt;\\epsilon\\)ãŒçµ‚äº†æ¡ä»¶ãªã®ã§ã€ \\[\nA_\\textrm{min}^*(s, a_k) &lt; \\frac{\\epsilon\\gamma}{1 -\\gamma}\n\\Leftrightarrow\n\\frac{A_\\textrm{min}^*(s, a_k)(1 - \\gamma)}{\\gamma}&lt; \\epsilon\n\\] ãŒæˆã‚Šç«‹ã¡ã¾ã™ã€‚ ã“ã‚ŒãŒæ„å‘³ã™ã‚‹ã®ã¯ã€\\(V*\\)ã¨äºŒç•ªç›®ã«ã„ã„\\(Q^*(s, a)\\)ã¨ã®å·®ãŒå¤§ãã„ã»ã©\\(\\epsilon\\)ã‚’å¤§ããã§ãã‚‹ã¨ã„ã†ã“ã¨ã§ã™ã€‚\nã“ã“ã¾ã§ã®è­°è«–ã‹ã‚‰ã€è¨ˆç®—é‡ã®è¦³ç‚¹ã§ã¯ã€ - \\(\\gamma\\)ãŒå¤§ãã„ã»ã©MDPã‚’è§£ãã®ãŒé›£ã—ã„ - æœ€é©è§£ã¨äºŒç•ªç›®ã«ã„ã„è§£ã¨ã®å·®ãŒå°ã•ã„ã»ã©MDPã‚’è§£ãã®ãŒé›£ã—ã„\nã¨ã„ã†2ç‚¹ãŒè¨€ãˆãã†ã§ã™ã­ã€‚"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html#æ–¹ç­–æœ€é©åŒ–ã®é›£ã—ã•",
    "href": "posts/understanding-what-makes-rl-difficult.html#æ–¹ç­–æœ€é©åŒ–ã®é›£ã—ã•",
    "title": "ã‚ˆã‚Šè‰¯ã„å•é¡Œè¨­è¨ˆã¸å‘ã‘ã¦ï¼š ä½•ãŒå¼·åŒ–å­¦ç¿’ã‚’é›£ã—ãã™ã‚‹ã®ã‹ã‚’ç†è§£ã—ã‚ˆã†",
    "section": "2.2: æ–¹ç­–æœ€é©åŒ–ã®é›£ã—ã•",
    "text": "2.2: æ–¹ç­–æœ€é©åŒ–ã®é›£ã—ã•\nå‰ç¯€ã§ç”¨ã„ãŸå†å¸°çš„ãªæ•°å€¤è¨ˆç®—ã¯å‹•çš„è¨ˆç”»æ³•(DP)ã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã§ã™ã€‚ Qå­¦ç¿’ãªã©ã€å¤šãã®å¼·åŒ–å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒDPã‚’ã‚‚ã¨ã«ã—ã¦ã„ã¾ã™ã€‚ ä¸€æ–¹ã§ã€å˜ã«å¼·åŒ–å­¦ç¿’ã‚’ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹æœ€é©åŒ–ã ã¨è€ƒãˆã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ ç‰¹ã«ã€æ–¹ç­–ãƒ‘ãƒ©ãƒ¡ã‚¿\\(\\theta\\)ã‚’æœ€é©åŒ–ã—ã¦è§£ãæ–¹æ³•ã‚’æ–¹ç­–æœ€é©åŒ–ã¨å‘¼ã³ã¾ã™ã€‚\nã„ã¾ã€\\(\\pi(0|s) = \\theta(s), \\pi(1|s) = 1.0 - \\theta(s)\\)ã«ã‚ˆã£ã¦\\(\\pi\\)ã‚’ãƒ‘ãƒ©ãƒ¡ã‚¿\\(\\theta\\)ã«ã‚ˆã‚Šè¡¨ã™ã“ã¨ã«ã—ã¾ã™ ï¼ˆã“ã‚Œã‚’direct parameterizationã¨å‘¼ã³ã¾ã™ï¼‰ã€‚ ãŸã‚ã—ã«ã€å…ˆã»ã©ã®MDP3ã§\\(\\pi(0|0)=1.0\\)ã‚’å›ºå®šã—ã¦ã€\\(\\theta(1), \\theta(2)\\)ã‚’å‹•ã‹ã—ãŸæ™‚ã®\\(\\sum_{s \\in \\mathcal{S}} V^\\pi(s)\\)ã®å¤‰å‹•ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\ndef v_pi_sum_2dim(\n    r: Array2,\n    p: Array3,\n    gamma: float,\n    epsilon: float,\n    initial_pi: Array2,\n    states: Tuple[int, int],\n    n_discretization: int,\n) -&gt; Array2:\n    res = []\n    for i2 in range(n_discretization + 1):\n        p2 = (1.0 / n_discretization) * i2\n        for i1 in range(n_discretization + 1):\n            p1 = (1.0 / n_discretization) * i1\n            pi = initial_pi.copy()\n            pi[states[0]] = p1, 1 - p1\n            pi[states[1]] = p2, 1 - p2\n            res.append(v_pi(r, p, pi, gamma, epsilon)[0].sum())\n    return np.array(res).reshape(n_discretization + 1, -1)\n\n\ndef plot_piv_heatmap(\n    data: Array2,\n    xlabel: str = \"\",\n    ylabel: str = \"\",\n    title: str = \"\",\n    ax: Optional[Axes] = None,\n) -&gt; Axes:\n    from matplotlib.ticker import LinearLocator\n\n    if ax is None:\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection=\"3d\")\n    n_discr = data.shape[0]\n    x, y = np.meshgrid(np.linspace(0, 1, n_discr), np.linspace(0, 1, n_discr))\n    ax.plot_surface(x, y, data, cmap=\"inferno\", linewidth=0, antialiased=False)\n    ax.zaxis.set_major_locator(LinearLocator(10))\n    ax.zaxis.set_major_formatter('{x:.01f}')\n    ax.set_xlabel(xlabel, fontsize=14)\n    ax.set_ylabel(ylabel, fontsize=14)\n    ax.set_zlabel(\"âˆ‘VÏ€\", fontsize=14, horizontalalignment=\"right\")\n    ax.set_title(title, fontsize=15)\n    return ax\n\n\ninitial_pi = np.array([[1.0, 0.0], [0.5, 0.5], [0.5, 0.5]])\nv_pi_sums = v_pi_sum_2dim(mdp3.r, mdp3.p, 0.9, 1e-4, initial_pi, (1, 2), 20)\nax = plot_piv_heatmap(v_pi_sums, \"Î¸(1)\", \"Î¸(2)\", \"MDP3\")\n_ = ax.set_xlim(tuple(reversed(ax.get_xlim())))\n_ = ax.set_ylim(tuple(reversed(ax.get_ylim())))\n\n\n\n\n\nãªã‚“ã‹ã„ã„æ„Ÿã˜ã«å±±ã«ãªã£ã¦ã„ã¾ã™ã­ã€‚ ã“ã®å•é¡Œã®å ´åˆã¯ã€å±±ç™»ã‚Šæ³•ï¼ˆå‹¾é…ä¸Šæ˜‡æ³•ï¼‰ã§\\(\\theta\\)ã‚’æ›´æ–°ã—ã¦ã„ã‘ã°å¤§åŸŸè§£ \\(\\theta(1) = 0.0, \\theta(2) = 1.0\\)ã«ãŸã©ã‚Šç€ããã†ã§ã™5ã€‚\nã—ã‹ã—ã€\\(f(\\theta) = \\sum_{s\\in\\mathcal{S}} V^{\\pi_\\theta}(s)\\)ã¯ã€ã„ã¤ã‚‚ã“ã®ã‚ˆã†ãªæ€§è³ª ã®ã„ã„é–¢æ•°ã«ãªã£ã¦ã„ã‚‹ã®ã§ã—ã‚‡ã†ãŒï¼Ÿ çµè«–ã‹ã‚‰è¨€ã†ã¨ãã†ã§ã¯ãªã„ã§ã™ã€‚ ä¾‹ãˆã°ã€ä»¥ä¸‹ã®ã‚ˆã†ãªMDPã§ã¯ã©ã†ã§ã—ã‚‡ã†ã‹ï¼Ÿ(\\(\\gamma=0.95\\)ã«ã—ã¦ã„ã¾ã™)\n\n\nCode\nmdp4 = ChainMDP(\n    [[1.0, 0.0], [0.6, 0.9], [0.9, 0.6], [1.0, 1.0]],\n    [[0.0, 1.0], [0.0, 0.0], [0.0, 0.0], [0.9, 0.0]],\n)\nwidth, height = mdp4.figure_shape()\nfig = plt.figure(\"MDP4-pi-vis\", (width * 1.25, height))\nmdp_ax = fig.add_axes([0.42, 0.0, 1.0, 1.0])\n_ = mdp4.show(\"MDP4\", ax=mdp_ax)\npi_ax = fig.add_axes([0.0, 0.0, 0.4, 1.0], projection=\"3d\")\ninitial_pi = np.array([[0.0, 1.0], [0.5, 0.5], [0.5, 0.5], [1.0, 0.0]])\nv_pi_sums = v_pi_sum_2dim(mdp4.r, mdp4.p, 0.95, 1e-4, initial_pi, (1, 2), 24)\n_ = plot_piv_heatmap(v_pi_sums, \"Î¸(1)\", \"Î¸(2)\", ax=pi_ax)\nprint(\n    f\"f(Î¸(1) = 0.0, Î¸(2) = 0.0): {v_pi_sums[0][0]}\\n\"\n    f\"f(Î¸(1) = 0.5, Î¸(2) = 0.5): {v_pi_sums[12][12]}\\n\"\n    f\"f(Î¸(1) = 1.0, Î¸(2) = 1.0): {v_pi_sums[24][24]}\"\n)\n\n\nf(Î¸(1) = 0.0, Î¸(2) = 0.0): 74.25901721830479\nf(Î¸(1) = 0.5, Î¸(2) = 0.5): 72.01388270994806\nf(Î¸(1) = 1.0, Î¸(2) = 1.0): 70.6327625115528\n\n\n\n\n\nä¸€ç•ªå³ã ã¨æ°¸é ã«0.9ãŒã‚‚ã‚‰ãˆã¦ã€ä¸€ç•ªå·¦ã ã¨1.0ãŒã‚‚ã‚‰ãˆã‚‹ã®ã§ã€ã‚ˆã‚Šæœ€é©æ–¹ç­–ã‚’è¦‹åˆ†ã‘ã‚‹ã®ãŒé›£ã—ãã†ãªæ„Ÿã˜ãŒã—ã¾ã™ã€‚\nãƒ—ãƒ­ãƒƒãƒˆã—ã¦ã¿ã‚‹ã¨ã€\\(f(\\theta)\\)ã¯å…ˆç¨‹ã¨ã¯é€†ã«è°·ã®ã‚ˆã†ãªå½¢ã«ãªã£ã¦ã„ã¦ã€å±±ç™»ã‚Šæ³•ã§è§£ã„ã¦ã‚‚ å¿…ãšã—ã‚‚å¤§åŸŸè§£ã«åæŸã—ãªãã†ã«è¦‹ãˆã¾ã™ã€‚ ã“ã‚Œã‚’ã‚‚ã£ã¨å°‚é–€çš„ãªè¨€è‘‰ã§è¨€ã†ã¨ã€\\(f(0.0) + f(1.0) &gt; 2 * f(0.5)\\)ã‚ˆã‚Šã“ã‚Œã¯å‡¹é–¢æ•°ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ ã‚ã¾ã‚Šè©³ã—ãèª¬æ˜ã—ã¾ã›ã‚“ãŒã€å‡¹é–¢æ•°ã ã¨å±±ç™»ã‚Šæ³•ãŒå¤§åŸŸè§£ã«åæŸã™ã‚‹ãªã©å¬‰ã—ã„ç‚¹ãŒã‚ã‚‹ã®ã§ã€ ã“ã‚Œã¯æœ€é©åŒ–ã™ã‚‹ä¸Šã§å„ä»‹ãªç‰¹å¾´ã ã¨è¨€ãˆã¾ã™ã€‚\nä»¥ä¸Šã‚ˆã‚Šã€æ–¹ç­–æœ€é©åŒ–ã§å•é¡Œã‚’è§£ãæ™‚ã¯\\(\\sum_{s\\in\\mathcal{S}} V(s)\\)ãŒå‡¹é–¢æ•°ã‹ã©ã†ã‹ãŒã€ å•é¡Œã®é›£ã—ã•ã«å½±éŸ¿ã‚’ä¸ãˆãã†ã ã¨ã„ã†ã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸã€‚"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html#a-æ–¹ç­–åå¾©æ³•ã®é›£ã—ã•",
    "href": "posts/understanding-what-makes-rl-difficult.html#a-æ–¹ç­–åå¾©æ³•ã®é›£ã—ã•",
    "title": "ã‚ˆã‚Šè‰¯ã„å•é¡Œè¨­è¨ˆã¸å‘ã‘ã¦ï¼š ä½•ãŒå¼·åŒ–å­¦ç¿’ã‚’é›£ã—ãã™ã‚‹ã®ã‹ã‚’ç†è§£ã—ã‚ˆã†",
    "section": "2.A æ–¹ç­–åå¾©æ³•ã®é›£ã—ã•",
    "text": "2.A æ–¹ç­–åå¾©æ³•ã®é›£ã—ã•\n\nNote: ã“ã®ç¯€ã¯ç‰¹ã«å†…å®¹ãŒãªã„ã®ã§ã‚¢ãƒšãƒ³ãƒ‡ã‚£ã‚¯ã‚¹æ‰±ã„ã«ãªã£ã¦ã„ã¾ã™ã€‚é£›ã°ã—ã¦ã‚‚å•é¡Œã‚ã‚Šã¾ã›ã‚“ã€‚\n\nã¨ã“ã‚ã§2.1ã§\\(V^*\\)ã‚’æ±‚ã‚ãŸã¨ãã«ä½¿ã£ãŸæ‰‹æ³•ã‚’ä¾¡å€¤åå¾©æ³•ã¨è¨€ã„ã¾ã™ã€‚ ã‚‚ã†ä¸€ã¤ã€æ–¹ç­–åå¾©æ³•ã¨ã„ã†æ‰‹æ³•ã§\\(V^*\\)ã‚’æ±‚ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n\\(\\pi^*\\)ãŒæº€ãŸã™ã¹ãæ€§è³ªã«ã¤ã„ã¦è€ƒãˆã¦ã¿ã¾ã™ã€‚ \\(\\pi\\)ãŒæœ€é©ã§ã‚ã‚‹ã¨ãã€ \\[\nV^\\pi(s) \\geq \\max_{a \\in \\mathcal{A}} r(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s'|s,a) V^\\pi(s')\n\\] ãŒæˆã‚Šç«‹ã¡ã¾ã™ã€‚ ã“ã‚ŒãŒæˆã‚Šç«‹ãŸãªã„ã¨ã™ã‚‹ã¨ã€ \\[\n\\pi'(s, a) = \\begin{cases}\n1.0 &(\\textrm{if}~a = \\textrm{argmax}_a r(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s'|s,a) V^\\pi(s')) \\\\\n0.0 &(\\textrm{otherwise})\n\\end{cases}\n\\] ã®æ–¹ãŒæ€§èƒ½ãŒè‰¯ããªã‚Šã€\\(\\pi\\)ãŒæœ€é©ã§ã‚ã‚‹ã“ã¨ã¨çŸ›ç›¾ã—ã¾ã™ã€‚\nã§ã¯ã€ã“ã®æ€§è³ªãŒæˆã‚Šç«‹ã¤ã¾ã§æ–¹ç­–ã‚’æ”¹å–„ã—ç¶šã‘ã‚‹ã¨ã„ã†ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è©¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ ã•ã£ãæ›¸ã„ãŸv_pié–¢æ•°ã‚’ä½¿ã£ã¦å®Ÿè£…ã§ãã¾ã™ã€‚\n\n\nCode\nMAX_ITER_PI: int = 10000\n\ndef policy_iteration(\n    r: Array2,\n    p: Array3,\n    gamma: float,\n    epsilon: float,\n) -&gt; Tuple[Array1, Array2, int]:\n    pi = np.zeros(p.shape[:2])  # |S| x |A|\n    pi[:, 1] = 1.0  # æœ€åˆã®æ–¹ç­–ã¯æ±ºå®šçš„ãªã‚‰ãªã‚“ã§ã‚‚ã„ã„ãŒã€è¡Œå‹•1ã‚’é¸ã¶æ–¹ç­–ã«ã—ã¦ã¿ã‚‹\n    state_indices = np.arange(0, p.shape[0], dtype=np.uint)\n    for n_iter in range(MAX_ITER_PI):\n        v_pi_, _ = v_pi(r, p, pi, gamma, epsilon)\n        q_pi = r + gamma * np.einsum(\"saS,S-&gt;sa\", p, v_pi_)\n        greedy_actions = np.argmax(q_pi, axis=1)\n        pi_next = np.zeros_like(pi)\n        pi_next[state_indices, greedy_actions] = 1.0\n        # pi == pi_next ãªã‚‰åæŸ\n        if np.linalg.norm(pi - pi_next) &lt; 1.0:\n            return v_pi_, pi_next, n_iter + 1\n        pi = pi_next\n    raise RuntimeError(\"Policy Iteration did not converge &gt;_&lt;\")\n\nv_star_mdp3_vi, _, n_iter = policy_iteration(mdp3.r, mdp3.p, 0.9, 1e-4)\nprint(f\"åå¾©å›æ•°: {n_iter}\")\nprint(\" \".join([f\"V({i}): {v:.3}\" for i, v in enumerate(v_star_mdp3_vi)]))\n\n\nåå¾©å›æ•°: 2\nV(0): 6.49 V(1): 7.21 V(2): 7.51\n\n\nãªã‚“ã‹2å›åå¾©ã—ãŸã ã‘ã§æ±‚ã¾ã£ã¦ã—ã¾ã„ã¾ã—ãŸãŒâ€¦ã€‚ ã“ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯æ–¹ç­–åå¾©æ³•ã¨å‘¼ã°ã‚Œã€ãªã‚“ã‚„ã‹ã‚“ã‚„ã§æœ€é©æ–¹ç­–ã«åæŸã™ã‚‹ã“ã¨ãŒçŸ¥ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚ ã§ã¯ã€ã“ã®åå¾©å›æ•°ã¯ã€ä½•ã«ã‚ˆã£ã¦æ±ºã¾ã‚‹ã®ã§ã—ã‚‡ã†ã‹ï¼Ÿ æ–¹ç­–ã®çµ„ã¿åˆã‚ã›ã¯\\(|A|^{|S|}\\)é€šã‚Šã‚ã‚Šã¾ã™ãŒã€ä¸Šã®å®Ÿé¨“ã ã¨ãšã£ã¨é€ŸãåæŸã—ã¦ã„ã‚‹ã®ã§ã€ã‚‚ã£ã¨ã„ã„ãƒã‚¦ãƒ³ãƒ‰ãŒã‚ã‚Šãã†ã«æ€ãˆã¾ã™ã€‚ ã—ã‹ã—ã€å®Ÿéš›ã®ã¨ã“ã‚æœ€æ‚ªã‚±ãƒ¼ã‚¹ã§ã¯æŒ‡æ•°æ™‚é–“ã‹ã‹ã‚‹ã“ã¨ãŒçŸ¥ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚ ã“ã®è¨˜äº‹ã§ã¯ã€ã“ã®æ–¹ç­–åå¾©æ³•ãŒé›£ã—ããªã‚‹å ´åˆã«ã¤ã„ã¦ã‚‚è§£èª¬ã—ãŸã‹ã£ãŸã®ã§ã™ãŒã€ ç†è§£ã§ããªã‹ã£ãŸã®ã§ã€è«¦ã‚ã¾ã—ãŸã€‚ãƒ¾(ï½¡&gt;ï¹&lt;ï½¡)ï¾‰"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html#b-å‚è€ƒæ–‡çŒ®ãªã©",
    "href": "posts/understanding-what-makes-rl-difficult.html#b-å‚è€ƒæ–‡çŒ®ãªã©",
    "title": "ã‚ˆã‚Šè‰¯ã„å•é¡Œè¨­è¨ˆã¸å‘ã‘ã¦ï¼š ä½•ãŒå¼·åŒ–å­¦ç¿’ã‚’é›£ã—ãã™ã‚‹ã®ã‹ã‚’ç†è§£ã—ã‚ˆã†",
    "section": "2.B å‚è€ƒæ–‡çŒ®ãªã©",
    "text": "2.B å‚è€ƒæ–‡çŒ®ãªã©\n\nOn the Complexity of Solving Markov Decision Problems\nCournell CS 6789: Foundations of Reinforcement Learning\n\nå‚è€ƒæ–‡çŒ®ã§ã¯\\(\\frac{1}{1 - \\gamma}\\)ã§åå¾©å›æ•°ã‚’æŠ‘ãˆã¦ã„ã‚‹ã˜ã‚ƒãªã„ã‹ã€è©±ãŒé•ã†ã˜ã‚ƒãªã„ã‹ã€ã¨ã„ã†æ°—ãŒä¸€è¦‹ã—ã¦ã—ã¾ã„ã¾ã™ã€‚ ã“ã‚Œã¯æœ‰åä¸ç­‰å¼\\(\\log x \\leq x - 1\\) ã‹ã‚‰ãªã‚“ã‚„ã‹ã‚“ã‚„ã§\\(\\frac{1}{1 - \\gamma} \\geq -\\frac{1}{\\log\\gamma}\\) ã ã‹ã‚‰ã€œã¨ã„ã†æ„Ÿã˜ã§è€ƒãˆã‚Œã°ãªã‚“ã¨ã‹ãªã‚‹ã¨æ€ã„ã¾ã™ã€‚ ã“ã®ä¸ç­‰å¼ã¯\\(x=1\\)ã§ç­‰å·ãªã®ã§ã€ã‚ˆãä½¿ã†\\(\\gamma=0.99\\)ã¨ã‹ã®è¨­å®šãªã‚‰ã‹ãªã‚Šå·®ã¯è¿‘ããªã‚Šã¾ã™ã€‚"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html#å ±é…¬ãªã—æ¢æŸ»ã®å•é¡Œ",
    "href": "posts/understanding-what-makes-rl-difficult.html#å ±é…¬ãªã—æ¢æŸ»ã®å•é¡Œ",
    "title": "ã‚ˆã‚Šè‰¯ã„å•é¡Œè¨­è¨ˆã¸å‘ã‘ã¦ï¼š ä½•ãŒå¼·åŒ–å­¦ç¿’ã‚’é›£ã—ãã™ã‚‹ã®ã‹ã‚’ç†è§£ã—ã‚ˆã†",
    "section": "3.1 å ±é…¬ãªã—æ¢æŸ»ã®å•é¡Œ",
    "text": "3.1 å ±é…¬ãªã—æ¢æŸ»ã®å•é¡Œ\nã¨ã„ã†ã‚ã‘ã§ã€ã¨ã‚Šã‚ãˆãšåˆ¥ã«å­¦ç¿’ã—ãªãã¦ã„ã„ã®ã§ã€ç’°å¢ƒã‹ã‚‰æƒ…å ±ã‚’é›†ã‚ã¦ã“ã‚ˆã†ã€ã¨ã„ã†å•é¡Œã‚’è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\nfrom matplotlib.figure import Figure\nfrom matplotlib.image import FigureImage\n\n\nclass GridMDP:\n    from matplotlib.colors import ListedColormap\n\n    #: Up, Down, Left, Right\n    ACTIONS = np.array([[-1, 0], [+1, 0], [0, -1], [0, +1]])\n    #: Symbols\n    EMPTY, BLOCK, START, GOAL = range(4)\n    DESCRIPTIONS = [\"Empty\", \"Block\", \"Start\", \"Goal\"]\n    #: Colormap for visualizing the map\n    CM = ListedColormap([\"w\", \".75\", \"xkcd:leaf green\", \"xkcd:vermillion\"])\n    REWARD_COLORS = [\"xkcd:light royal blue\", \"xkcd:vermillion\"]\n    FIG_AREA = 28\n\n    # Returns PIL.Image\n    def __download_agent_image():\n        from io import BytesIO\n        from urllib import request\n\n        from PIL import Image\n\n        fd = BytesIO(\n            request.urlopen(\n                \"https://2.bp.blogspot.com/-ZwYKR5Zu28s/U6Qo2qAjsqI\"\n                + \"/AAAAAAAAhkM/HkbDZEJwvPs/s400/omocha_robot.png\"\n            ).read()\n        )\n        return Image.open(fd)\n\n    AGENT_IMAGE = __download_agent_image()\n\n    def __init__(\n        self,\n        map_array: Sequence[Sequence[int]],\n        reward_array: Optional[Sequence[Sequence[float]]] = None,\n        action_noise: float = 0.1,\n        horizon: Optional[int] = None,\n        seed: int = 123456789,\n        legend_loc: str = \"upper right\",\n    ) -&gt; None:\n        def add_padding(seq: Sequence[Sequence[T]], value: T) -&gt; list:\n            width = len(seq[0]) + 2\n            ret_list = [[value for _ in range(width)]]\n            for col in seq:\n                ret_list.append([value] + list(col) + [value])\n            ret_list.append([value for _ in range(width)])\n            return ret_list\n\n        self.map_array = np.array(add_padding(map_array, 1), dtype=np.uint8)\n        assert self.map_array.max() &lt;= 3\n        assert 0 &lt;= self.map_array.min()\n        self.rows, self.cols = self.map_array.shape\n\n        if reward_array is None:\n            self.reward_array = np.zeros((self.rows, self.cols), np.float64)\n        else:\n            self.reward_array = np.array(\n                add_padding(reward_array, 0.0), np.float64\n            )\n\n        self.action_noise = action_noise\n        self.horizon = horizon\n        self.start_positions = np.argwhere(self.map_array == self.START)\n        if len(self.start_positions) == 0:\n            raise ValueError(\"map_array needs at least one start posiiton\")\n        self.random_state = np.random.RandomState(seed)\n        _ = self.reset()\n\n        # Visualization stuffs\n        self.legend_loc = legend_loc\n        self.map_fig, self.map_ax, self.map_img = None, None, None\n        self.agent_img, self.agent_fig_img = None, None\n\n    def n_states(self) -&gt; int:\n        return np.prod(self.map_array.shape)\n\n    @staticmethod\n    def n_actions() -&gt; int:\n        return 4\n\n    def reset(self) -&gt; Array1:\n        idx = self.random_state.randint(self.start_positions.shape[0])\n        self.state = self.start_positions[idx]\n        self.n_steps = 0\n        return self.state.copy()\n\n    def state_index(self, state: Array1) -&gt; int:\n        y, x = state\n        return y * self.map_array.shape[1] + x\n\n    def _load_agent_img(self, fig_height: float) -&gt; None:\n        from io import BytesIO\n        from urllib import request\n\n        fd = BytesIO(request.urlopen(self.ROBOT).read())\n        img = Image.open(fd)\n        scale = fig_height / img.height\n        self.agent_img = img.resize((int(img.width * scale), int(img.height * scale)))\n\n    def _fig_inches(self) -&gt; Tuple[int, int]:\n        prod = self.rows * self.cols\n        scale = np.sqrt(self.FIG_AREA / prod)\n        return self.cols * scale, self.rows * scale\n\n    def _is_valid_state(self, *args) -&gt; bool:\n        if len(args) == 2:\n            y, x = args\n        else:\n            y, x = args[0]\n        return 0 &lt;= y &lt; self.rows and 0 &lt;= x &lt; self.cols\n\n    def _possible_actions(self) -&gt; Array1:\n        possible_actions = []\n        for i, act in enumerate(self.ACTIONS):\n            y, x = self.state + act\n            if self._is_valid_state(y, x) and self.map_array[y, x] != self.BLOCK:\n                possible_actions.append(i)\n        return np.array(possible_actions)\n\n    def _reward(self, next_state: Array1) -&gt; float:\n        y, x = next_state\n        return self.reward_array[y, x]\n\n    def _is_terminal(self) -&gt; bool:\n        if self.horizon is not None and self.n_steps &gt; self.horizon:\n            return True\n        y, x = self.state\n        return self.map_array[y, x] == self.GOAL\n\n    def step(self, action: int) -&gt; Tuple[Tuple[int, int], float, bool]:\n        self.n_steps += 1\n        possible_actions = self._possible_actions()\n        if self.random_state.random_sample() &lt; self.action_noise:\n            action = self.random_state.choice(possible_actions)\n\n        if action in possible_actions:\n            next_state = self.state + self.ACTIONS[action]\n        else:\n            next_state = self.state.copy()\n\n        reward = self._reward(next_state)\n        self.state = next_state\n        is_terminal = self._is_terminal()\n        return next_state.copy(), reward, is_terminal\n\n    def _draw_agent(self, fig: Figure) -&gt; FigureImage:\n        unit = self.map_img.get_window_extent().y1 / self.rows\n        y, x = self.state\n        return fig.figimage(\n            self.agent_img,\n            unit * (x + 0.3),\n            unit * (self.rows - 0.8 - y),\n        )\n\n    def _draw_rewards(self) -&gt; None:\n        for y in range(self.rows):\n            for x in range(self.cols):\n                rew = self.reward_array[y, x]\n                if abs(rew) &lt; 1e-3:\n                    continue\n                if self.map_array[y, x] == self.GOAL:\n                    color = \"w\"\n                else:\n                    color = self.REWARD_COLORS[int(rew &gt;= 0)]\n                self.map_ax.text(\n                    x + 0.1,\n                    y + 0.5,\n                    f\"{rew:+.2}\",\n                    color=color,\n                    fontsize=12,\n                )\n\n    def show(self, title: str = \"\", explicit: bool = False) -&gt; Axes:\n        if self.map_fig is None:\n            self.map_fig = plt.figure(title or \"GridMDP\", self._fig_inches())\n            ax = self.map_fig.add_axes([0, 0, 1, 1])\n            ax.set_aspect(\"equal\")\n            ax.set_xticks([])\n            ax.set_yticks([])\n            self.map_img = ax.imshow(\n                self.map_array,\n                cmap=self.CM,\n                extent=(0, self.cols, self.rows, 0),\n                vmin=0,\n                vmax=4,\n                alpha=0.6,\n            )\n            for i in range(1, 4):\n                if np.any(self.map_array == i):\n                    ax.plot([0.0], [0.0], color=self.CM(i), label=self.DESCRIPTIONS[i])\n            ax.legend(fontsize=12, loc=self.legend_loc)\n            ax.text(0.1, 0.8, title or \"GridMDP\", fontsize=16)\n            self.map_ax = ax\n\n            imw, imh = self.AGENT_IMAGE.width, self.AGENT_IMAGE.height\n            scale = (self.map_img.get_window_extent().y1 / self.rows) / imh\n            self.agent_img = self.AGENT_IMAGE.resize(\n                (int(imw * scale), int(imh * scale))\n            )\n\n            if np.linalg.norm(self.reward_array) &gt; 1e-3:\n                self._draw_rewards()\n        if self.agent_fig_img is not None:\n            self.agent_fig_img.remove()\n        self.agent_fig_img = self._draw_agent(self.map_fig)\n        if explicit:\n            from IPython.display import display\n\n            self.map_fig.canvas.draw()\n            display(self.map_fig)\n        return self.map_ax\n\n\n\n\nCode\ngrid_mdp1 = GridMDP(\n    [[0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0], \n     [0, 0, 2, 0, 0],\n     [0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0]],\n    horizon=50,\n)\n_ = grid_mdp1.show(\"GridMDP1\")\n\n\n\n\n\nGridMDPã¨é¡Œã•ã‚ŒãŸã“ã¡ã‚‰ãŒã€ä»Šå›ä½¿ç”¨ã™ã‚‹ã€Œç’°å¢ƒã€ã«ãªã‚Šã¾ã™ã€‚ ç’°å¢ƒã®ä¸­ã§è¡Œå‹•ã™ã‚‹ä¸»ä½“ã‚’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨å‘¼ã³ã¾ã™ã€‚ ä»Šå›ã¯ã€ã„ã‚‰ã™ã¨ã‚„æ§˜ã®ãƒ­ãƒœãƒƒãƒˆã®ç”»åƒã‚’ä½¿ç”¨ã•ã›ã¦ã„ãŸã ãã¾ã—ãŸã€‚ å„ãƒã‚¹ç›®ã®ä¸­ã§ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯è¡Œå‹•ã¯ä¸Šä¸‹å·¦å³ã«ç§»å‹•ã®4ç¨®é¡ã®è¡Œå‹•ã‚’é¸æŠã§ãã¾ã™ã€‚ è¡Œå‹•ã¯æ™‚ã€…å¤±æ•—ã—ã¦ã€ä¸€æ§˜ãƒ©ãƒ³ãƒ€ãƒ ãªçŠ¶æ…‹é·ç§»ãŒç™ºç”Ÿã—ã¾ã™ã€‚ ã“ã“ã§ã€å‰ç« ã§ç”¨ã„ãªã‹ã£ãŸã„ãã¤ã‹ã®æ–°ã—ã„æ¦‚å¿µã‚’å°å…¥ã—ã¾ã™ã€‚\n\nã€Œã‚¹ã‚¿ãƒ¼ãƒˆåœ°ç‚¹ã€ã®å­˜åœ¨\n\nãƒ­ãƒœãƒƒãƒˆãã‚“ã¯ã€æ±ºã‚ã‚‰ã‚ŒãŸå ´æ‰€ã‹ã‚‰è¡Œå‹•ã‚’å§‹ã‚ãªãã¦ã¯ãªã‚Šã¾ã›ã‚“ã€‚ã“ã®å•é¡Œã§ã¯ã€åˆæœŸçŠ¶æ…‹ã¯ã‚ã‚‰ã‹ã˜ã‚æ±ºã¾ã‚‰ã‚ŒãŸã„ãã¤ã‹ã®ã‚¹ã‚¿ãƒ¼ãƒˆåœ°ç‚¹ã‹ã‚‰å‡ç­‰ã«é¸ã³ã¾ã™ã€‚ç†è«–çš„ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã¯ã€åˆæœŸçŠ¶æ…‹åˆ†å¸ƒ\\(\\mu: \\mathcal{S} \\rightarrow \\mathbb{R}\\)ã¨ã—ã¦è¡¨ç¾ã™ã‚Œã°ã„ã„ã§ã™ã€‚\n\nã€Œçµ‚äº†åœ°ç‚¹ã€ã®å­˜åœ¨\n\nãƒ­ãƒœãƒƒãƒˆãã‚“ã¯ã€ã„ãã¤ã‹ã®æ±ºã‚ã‚‰ã‚ŒãŸå ´æ‰€ã«åˆ°é”ã—ãŸã‚‰å¼·åˆ¶çš„ã«ã‚¹ã‚¿ãƒ¼ãƒˆã¾ã§æˆ»ã•ã‚Œã¾ã™ã€‚\n\nã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰\n\nã‚¹ã‚¿ãƒ¼ãƒˆã‹ã‚‰çµ‚äº†ã™ã‚‹ã¾ã§ã®ä¸€é€£ã®æµã‚Œã‚’ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¨å‘¼ã³ã¾ã™ã€‚\n\nã€Œã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é•·ã•ï¼ˆãƒ›ãƒ©ã‚¤ã‚¾ãƒ³ï¼‰ã€ã®å­˜åœ¨\n\nä¸€å®šã®ã‚¿ãƒ¼ãƒ³ãŒçµŒéã—ãŸæ™‚ã€ãƒ­ãƒœãƒƒãƒˆãã‚“ã¯ã‚¹ã‚¿ãƒ¼ãƒˆåœ°ç‚¹ã¾ã§æˆ»ã•ã‚Œã¾ã™ã€‚å¼·åŒ–å­¦ç¿’ã§ã¯ã—ã°ã—ã°ã€ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’ä½•åº¦ã‚‚ãƒªã‚»ãƒƒãƒˆã—ã¦å­¦ç¿’ã—ã¾ã™ã€‚ç†è«–ã‚’å®Ÿéš›ã«è¿‘ã¥ã‘ã‚‹ãŸã‚ã€MDPã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«ã‚‚ã“ã‚ŒãŒå°å…¥ã•ã‚Œã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚\n\n\nã§ã¯ã•ã£ããã€é©å½“ã«è¡Œå‹•ã—ã¦ã‚‚ã‚‰ã„ã¾ã—ã‚‡ã†ã€‚ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«è¡Œå‹•ã•ã›ã¦ã€è¨ªå•ã—ãŸå ´æ‰€ã«è‰²ã‚’ã¤ã‘ã¦ã„ãã¾ã™ã€‚ ãªãŠã€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é•·ã•ã¯50ã¨ã—ã¾ã™ã€‚\n\n\nCode\nfrom abc import ABC, abstractmethod\nfrom typing import Callable\n\n\nclass VisitationHeatmap:\n    def __init__(\n        self,\n        map_shape: Tuple[int, int],\n        figsize: Tuple[float, float],\n        ax: Optional[Axes] = None,\n        max_visit: int = 1000,\n        title: str = \"\",\n    ) -&gt; None:\n        from matplotlib import colors as mc\n        from mpl_toolkits.axes_grid1 import make_axes_locatable\n\n        self.counter = np.zeros(map_shape, np.int64)\n        self.title = title\n        self.fig = plt.figure(self.title, figsize, facecolor=\"w\")\n        self.ax = self.fig.add_axes([0, 0, 1, 1])\n        self.ax.set_aspect(\"equal\")\n        self.ax.set_xticks([])\n        self.ax.set_yticks([])\n        r, g, b = mc.to_rgb(\"xkcd:fuchsia\")\n        cdict = {\n            \"red\": [(0.0, r, r), (1.0, r, r)],\n            \"green\": [(0.0, g, g), (1.0, g, g)],\n            \"blue\": [(0.0, b, b), (1.0, b, b)],\n            \"alpha\": [(0.0, 0.0, 0.0), (1.0, 1.0, 1.0)],\n        }\n        self.img = self.ax.imshow(\n            np.zeros(map_shape),\n            cmap=mc.LinearSegmentedColormap(\"visitation\", cdict),\n            extent=(0, map_shape[1], map_shape[0], 0),\n            vmin=0,\n            vmax=max_visit,\n        )\n\n        divider = make_axes_locatable(self.ax)\n        cax = divider.append_axes(\"right\", size=\"4%\", pad=0.1)\n        self.fig.colorbar(self.img, cax=cax, orientation=\"vertical\")\n        cax.set_ylabel(\"Num. Visitation\", rotation=0, position=(1.0, 1.1), fontsize=14)\n\n        self._update_text()\n        self.agent = None\n\n    def _update_text(self) -&gt; None:\n        self.text = self.ax.text(\n            0.1,\n            -0.5,\n            f\"{self.title} After {self.counter.sum()} steps\",\n            fontsize=16,\n        )\n\n    def _draw_agent(self, draw: Callable[[Figure], FigureImage]) -&gt; None:\n        if self.agent is not None:\n            self.agent.remove()\n        self.agent = draw(self.fig)\n\n    def visit(self, state: Array1) -&gt; int:\n        y, x = state\n        res = self.counter[y, x]\n        self.counter[y, x] += 1\n        self.img.set_data(self.counter)\n        self.text.remove()\n        self._update_text()\n        return res\n\n    def show(self) -&gt; None:\n        from IPython.display import display\n\n        display(self.fig)\n\n\ndef do_nothing(\n    _state: int,\n    _action: int,\n    _next_state: int,\n    _reward: float,\n    _is_terminal: bool,\n) -&gt; None:\n    return\n\n\ndef simulation(\n    mdp: GridMDP,\n    n: int,\n    act: Callable[[int], int],\n    learn: Callable[[int, int, int, float, bool], None] = do_nothing,\n    max_visit: Optional[int] = None,\n    vis_freq: Optional[int] = None,\n    vis_last: bool = False,\n    title: str = \"\",\n) -&gt; None:\n    visitation = VisitationHeatmap(\n        mdp.map_array.shape,\n        mdp._fig_inches(),\n        max_visit=max_visit or n // 10,\n        title=title,\n    )\n    state = mdp.reset()\n    visitation.visit(state)\n    vis_interval = n + 1 if vis_freq is None else n // vis_freq\n    for i in range(n):\n        if (i + 1) % vis_interval == 0 and (vis_last or i &lt; n - 1):\n            visitation._draw_agent(mdp._draw_agent)\n            visitation.show()\n        action = act(mdp.state_index(state))\n        next_state, reward, terminal = mdp.step(action)\n        visitation.visit(next_state)\n        learn(\n            mdp.state_index(state),\n            action,\n            mdp.state_index(next_state),\n            reward,\n            terminal,\n        )\n        if terminal:\n            state = mdp.reset()\n        else:\n            state = next_state\n    visitation._draw_agent(mdp._draw_agent)\n\n\nsimulation(grid_mdp1, 1000, lambda _: np.random.randint(4), vis_freq=2)\n\n\n\n\n\n\n\n\nãƒ©ãƒ³ãƒ€ãƒ ã«è¡Œå‹•ã•ã›ãŸã ã‘ã§ã™ãŒã€ãã‚Œãªã‚Šã«ã¾ã‚“ã¹ã‚“ãªãè‰²ãŒå¡—ã‚‰ã‚Œã¦ã„ã¦ã€ã¾ã‚ã¾ã‚ã„ã„ã®ã§ã¯ãªã„ã‹ã€ã¨ã„ã†æ°—ãŒã—ã¾ã™ã€‚ ã—ã‹ã—ã€ã‚‚ã£ã¨åºƒã„ç’°å¢ƒã§ã¯ã©ã†ã§ã—ã‚‡ã†ã‹ã€‚\n\n\nCode\ngrid_mdp2_map = [[0] * 15 for _ in range(15)]\ngrid_mdp2_map[7][7] = 2\ngrid_mdp2 = GridMDP(grid_mdp2_map, horizon=50)\n_ = grid_mdp2.show()\nrandom_state = np.random.RandomState(1)\nsimulation(\n    grid_mdp2,\n    5000,\n    lambda _: random_state.randint(4),\n    max_visit=100,\n    title=\"Random Exploration\",\n    vis_freq=None,\n)\n\n\n\n\n\n\n\n\nãªã‚“ã‹é§„ç›®ã£ã½ã„æ„Ÿã˜ã§ã™ã­ã€‚ å ´æ‰€ã«ã‚ˆã£ã¦ã¯å…¨ãè‰²ãŒã¤ã„ã¦ã„ã¾ã›ã‚“ã€‚ ç’°å¢ƒãŒåºƒã„ã¨ã€ãƒ©ãƒ³ãƒ€ãƒ ã«æ­©ãå›ã‚‹ã®ã§ã¯ã€åŠ¹ç‡ã‚ˆãæƒ…å ±ã‚’é›†ã‚ã¦ã“ã‚Œãªã„ã‚ˆã†ã§ã™ã€‚ å…·ä½“çš„ã«ã©ã®ãã‚‰ã„é›£ã—ã„ã®ã‹ã¨è¨€ã†ã¨ã€å¹³å‡ä¸€å›è¨ªå•ã™ã‚‹ã®ã«ã‹ã‹ã‚‹æ™‚é–“ãŒã€ã ã„ãŸã„ - ä¸€æ–¹é€šè¡Œã®ç›´ç·š: \\(O(|S|)\\) - äºŒæ¬¡å…ƒãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯: \\(O(|S|^2)\\)? (å‚è€ƒ: planeä¸Šã®ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ï¼‰ - æœ€æ‚ªã‚±ãƒ¼ã‚¹: \\(O(2^{|S|})\\)\nãã‚‰ã„ã«ãªã‚Šã¾ã™ã€‚ ä¸€æ–¹é€šè¡Œãªã®ã¯ãªã‚“ã¨ãªãã‚ã‹ã‚Šã¾ã™ã­ã€‚ ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ã®å ´åˆã€åŒã˜å ´æ‰€ã‚’è¡Œã£ãŸã‚ŠããŸã‚Šã§ãã‚‹ã®ã§ã€ãã®ã¶ã‚“æ™‚é–“ãŒã‹ã‹ã£ã¦ã—ã¾ã„ã¾ã™ã€‚ æœ€æ‚ªã‚±ãƒ¼ã‚¹ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«æ§‹æˆã™ã‚Œã°ã„ã„ã§ã™ã€‚\n\n\nCode\n# hide-input\nclass WorstCaseMDP(ChainMDP):\n    def __init__(self, n: int) -&gt; None:\n        self.n_states = n\n        # For plotting\n        self.circles = []\n        self.cached_ax = None\n\n    def show(self, title: str = \"\", ax: Optional[Axes] = None) -&gt; Axes:\n        # ã ã„ãŸã„ChainMDPã‹ã‚‰ã‚³ãƒ”ãƒš...\n        if self.cached_ax is not None:\n            return self.cached_ax\n\n        from matplotlib.patches import Circle\n\n        width, height = self.figure_shape()\n        circle_position = height / 2 - height / 10\n        if ax is None:\n            fig = plt.figure(title or \"ChainMDP\", (width, height))\n            ax = fig.add_axes([0, 0, 1, 1], aspect=1.0)\n        ax.set_xlim(0, width)\n        ax.set_ylim(0, height)\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n        def xi(si: int) -&gt; float:\n            return self.OFFSET + (1.0 + self.INTERVAL) * si + 0.5\n\n        self.circles = [\n            Circle((xi(i), circle_position), 0.5, fc=\"w\", ec=\"k\")\n            for i in range(self.n_states)\n        ]\n        for i in range(self.n_states):\n            x = self.OFFSET + (1.0 + self.INTERVAL) * i + 0.1\n            ax.text(x, height * 0.85, f\"State {i}\", fontsize=16)\n\n        def annon(act: int, *args, **kwargs) -&gt; None:\n            # We don't hold references to annotations (i.e., we treat them immutable)\n            a_to_b(\n                ax,\n                *args,\n                **kwargs,\n                arrowcolor=self.ACT_COLORS[act],\n                text=f\"P: 1.0\",\n                fontsize=11,\n            )\n\n        for si in range(self.n_states):\n            ax.add_patch(self.circles[si])\n            x = xi(si)\n            # Action 0:\n            y = circle_position + self.SHIFT\n            if si &lt; self.n_states - 1:\n                annon(\n                    0,\n                    (x + self.SHIFT, y),\n                    (xi(si + 1) - self.SHIFT * 1.2, y - self.SHIFT * 0.3),\n                    verticalalignment=\"center_baseline\",\n                )\n            else:\n                annon(\n                    0,\n                    (x - self.SHIFT * 1.2, y),\n                    (x + self.SHIFT * 0.5, y - self.SHIFT * 0.1),\n                    style=\"self\",\n                    verticalalignment=\"bottom\",\n                )\n            # Action 1:\n            y = circle_position - self.SHIFT\n            if si &gt; 0:\n                annon(\n                    1,\n                    (x - self.SHIFT * 1.6, y),\n                    (xi(0), y - self.SHIFT * 0.6),\n                    style=\"arc3,rad=-0.15\",\n                    verticalalignment=\"top\",\n                )\n            else:\n                annon(\n                    1,\n                    (x + self.SHIFT * 0.4, y),\n                    (x - self.SHIFT * 0.45, y + self.SHIFT * 0.1),\n                    style=\"self\",\n                    verticalalignment=\"top\",\n                )\n\n        for i in range(2):\n            ax.plot([0.0], [0.0], color=self.ACT_COLORS[i], label=f\"Action {i}\")\n        ax.legend(fontsize=11, loc=\"upper right\")\n        if len(title) &gt; 0:\n            ax.text(0.06, height * 0.9, title, fontsize=18)\n        self.cached_ax = ax\n        return ax\n\n\n_ = WorstCaseMDP(6).show()\n\n\n\n\n\nã“ã®ç’°å¢ƒã§çŠ¶æ…‹\\(0\\)ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«è¡Œå‹•ã™ã‚‹ã¨ã€å³ç«¯ã«ãŸã©ã‚Šç€ãã¾ã§ã«å¹³å‡\\(2^5\\)ãã‚‰ã„ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ãŒã‹ã‹ã‚Šã¾ã™ã€‚ ãã‚“ãªã‚“ã‚ã‚Šã‹ã‚ˆâ€¦ã£ã¦æ„Ÿã˜ã§ã™ãŒã€‚\nã“ã®çµæœã‹ã‚‰ã€æœ€æ‚ªã®å ´åˆã ã¨æŒ‡æ•°æ™‚é–“ã‹ã‹ã‚‹ã‹ã‚‰è³¢ããƒ‡ãƒ¼ã‚¿åé›†ã—ãªã„ã¨ã„ã‘ãªã„ã‚ˆã­ã€ æ€ã†ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ ãã®ä¸€æ–¹ã§ã€ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ã®ã‚ˆã†ã«é·ç§»ã®å¯¾ç§°æ€§ãŒã‚ã‚‹ç’°å¢ƒãªã‚‰ã€ ãƒ©ãƒ³ãƒ€ãƒ ã«è¡Œå‹•ã—ã¦ã‚‚ãã‚“ãªã«æ‚ªããªã„ã‚“ã˜ã‚ƒãªã„ã‹ãªã€ã¨ã‚‚æ€ãˆã¾ã™ã€‚\nã•ã¦ãã®è©±ã¯ä¸€æ—¦ãŠã„ã¦ãŠã„ã¦ã€ã‚‚ã£ã¨åŠ¹ç‡ã‚ˆããƒ‡ãƒ¼ã‚¿ã‚’é›†ã‚ã‚‹æ–¹æ³•ã‚’è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\nè¨ªå•ã—ãŸå ´æ‰€ã‚’è¦šãˆã¦ãŠã„ã¦ã€è¨ªå•ã—ã¦ã„ãªã„å ´æ‰€ã‚’å„ªå…ˆã—ã¦æ¢æŸ»ã™ã‚‹\nçŠ¶æ…‹ã¨çŠ¶æ…‹ã®é–“ã«è·é›¢ãŒå®šç¾©ã§ãã‚‹ã¨ä»®å®šã—ã¦ã€é ãã«è¡Œãã‚ˆã†ã«ã™ã‚‹\nç’°å¢ƒãŒãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰ç©ºé–“ã ã¨ä»®å®šã—ã¦SLAMã§è‡ªå·±ä½ç½®æ¨å®šã™ã‚‹\n\nãªã©ã€è‰²ã€…ãªæ–¹æ³•ãŒè€ƒãˆã‚‰ã‚Œã‚‹ã¨æ€ã„ã¾ã™ãŒã€ã“ã“ã§ã¯1ã®æ–¹æ³•ã‚’ä½¿ã£ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\nä»¥ä¸‹ã®ã‚ˆã†ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è€ƒãˆã¾ã™ã€‚ 1. é©å½“ãªæ–¹ç­–\\(\\pi_0\\)ã‹ã‚‰é–‹å§‹ã™ã‚‹ 2. çŠ¶æ…‹è¡Œå‹•è¨ªå•å›æ•°\\(n(s, a)\\)ã€çŠ¶æ…‹è¡Œå‹•æ¬¡çŠ¶æ…‹è¨ªå•å›æ•°\\(n(s, a, s')\\)ã‚’è¨˜éŒ²ã—ã¦ãŠã - ãŸã ã—ã€åˆæœŸå€¤ã¯\\(n_0(s, a) = 1.0, n_0(s, a, s) = \\frac{1}{|S|}\\)ã¨ã™ã‚‹(0é™¤ç®—é˜²æ­¢ã®ãŸã‚) 3. ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãŒçµ‚ã‚ã£ãŸã¨ãã€ä»¥ä¸‹ã®ã‚ˆã†ã«æ–¹ç­–ã‚’æ›´æ–°ã™ã‚‹ 1. çŠ¶æ…‹é·ç§»é–¢æ•°ã®æ¨å®šå€¤\\(\\hat{P}(s'|s, a) = \\frac{n(s, a, s')}{n(s, a}\\)ã€ç–‘ä¼¼å ±é…¬\\(r_k(s, a)=\\frac{1}{n(s, a)}\\)ã€é©å½“ãª\\(\\gamma\\)ã‹ã‚‰æˆã‚‹MDP\\(\\mathcal{M}_k\\)ã‚’è§£ã 2. \\(\\mathcal{M}_k\\)ã®æœ€é©ä¾¡å€¤é–¢æ•°\\(V^*_k,Q^*_k\\)ã‹ã‚‰ä»¥ä¸‹ã®ã‚ˆã†ã«æ–¹ç­–\\(pi_{k+1}\\)ã‚’æ§‹æˆã™ã‚‹ - \\(V^*_k(s) &lt; \\frac{1}{|S|}\\sum_{s'\\in\\mathcal{S}}V^*_k(s')\\) ãªã‚‰ \\(\\pi_{k+1}(s)\\)ã¯\\(Q^*_k\\)ã«åŸºã¥ãè²ªæ¬²è¡Œå‹• - ãã‚Œä»¥å¤–ã®å ´åˆã€\\(\\pi_{k+1}(s)\\)ã¯ä¸€æ§˜ãƒ©ãƒ³ãƒ€ãƒ ãªè¡Œå‹•ã‚’ã¨ã‚‹ (=æ–¹ç­–ã‚’ç·©å’Œã™ã‚‹)\nç–‘ä¼¼å ±é…¬\\(r_k=\\frac{1}{n(s, a)}\\)ã‚’ä½¿ç”¨ã—ã¦ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã™ã‚‹ã®ãŒã€æœ€ã‚‚é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã§ã™ã€‚ ã“ã®å€¤ã¯ã€ä¸€åº¦ã‚‚çŠ¶æ…‹è¡Œå‹•ãƒšã‚¢\\((s,a)\\)ã‚’çµŒé¨“ã—ã¦ã„ãªã„ãªã‚‰\\(1\\)ã€ä¸€åº¦çµŒé¨“ã—ãŸã‚‰\\(1/2\\)ã€2å›çµŒé¨“ã—ãŸã‚‰\\(1/3\\)ã®ã‚ˆã†ã«æ¸›è¡°ã—ã¾ã™ã€‚ ã“ã‚Œã‚’å ±é…¬ã¨ã™ã‚‹MDPã‚’è§£ãã“ã¨ã§ã€ã‚ã¾ã‚ŠçµŒé¨“ã—ã¦ã„ãªã„çŠ¶æ…‹è¡Œå‹•ãƒšã‚¢ã‚’ã¨ã‚ã†ã¨ã™ã‚‹æ–¹ç­–ãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚ å®Œå…¨ãªè²ªæ¬²æ–¹ç­–ã§ã¯ãªãç·©å’Œã‚’ã„ã‚Œã¦ã„ã‚‹ã®ã¯ã€é«˜ã„å ±é…¬ã®çŠ¶æ…‹ã‚’ãƒ«ãƒ¼ãƒ—ã—ãªã„ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã§ã™ã€‚ ã§ã¯ã€ã‚„ã£ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\nclass RewardFreeExplore:\n    def __init__(\n        self,\n        n_states: int,\n        n_actions: int,\n        seed: int = 1,\n        gamma: float = 0.95,\n    ) -&gt; None:\n        self.sa_count = np.ones((n_states, n_actions))\n        self.sas_count = np.ones((n_states, n_actions, n_states)) / n_states\n        self.pi = np.ones((n_states, n_actions)) / n_actions\n        self.random_state = np.random.RandomState(seed)\n        self.n_states, self.n_actions = n_states, n_actions\n        self.n_updates = 0\n        self.gamma = gamma\n        self.pi_indices = np.arange(n_states, dtype=np.uint32)\n\n    def learn(\n        self,\n        state: int,\n        action: int,\n        next_state: int,\n        reward: float,\n        is_terminal: bool,\n    ) -&gt; None:\n        # è¨ªå•è¨˜éŒ²ã‚’æ›´æ–°ã™ã‚‹\n        self.sa_count[state, action] += 1\n        if is_terminal:\n            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãŒçµ‚ã‚ã£ãŸã‚‰ã€Value Iterationã‚’è§£ã„ã¦æ–¹ç­–ã‚’æ›´æ–°ã™ã‚‹\n            r = 1.0 / self.sa_count\n            p = self.sas_count / np.expand_dims(self.sa_count, axis=-1)\n            v, _n_iter = value_iteration(r, p, self.gamma, 1e-2)\n            v_is_larger_than_mean = v &gt; v.mean()\n            q = r + self.gamma * np.einsum(\"saS,S-&gt;sa\", p, v)\n            self.pi.fill(0.0)\n            for state in range(self.n_states):\n                # VãŒå¤§ãã„å ´æ‰€ã§ã¯æ–¹ç­–ã‚’ç·©å’Œã™ã‚‹\n                if v_is_larger_than_mean[state]:\n                    self.pi[state] = 1.0 / self.n_actions\n                # ãã†ã§ãªã„å ´åˆã¯è²ªæ¬²\n                else:\n                    self.pi[state][q[state].argmax()] = 1.0\n            self.n_updates += 1\n        else:\n            self.sas_count[state, action, next_state] += 1\n\n    def act(self, state: int) -&gt; int:\n        return self.random_state.choice(self.n_actions, p=self.pi[state])\n\n\nagent = RewardFreeExplore(grid_mdp2.n_states(), grid_mdp2.n_actions())\nsimulation(\n    grid_mdp2,\n    5000,\n    agent.act,\n    learn=agent.learn,\n    max_visit=100,\n    title=\"Strategic Exploration\",\n    vis_freq=None,\n)\n\n\n\n\n\nã•ã£ãã‚ˆã‚Šã‚‚æº€éãªãã€è‰²ã€…ãªçŠ¶æ…‹ã‚’è¨ªå•ã—ã¦ãã‚Œã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã­ã€‚"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html#å ±é…¬ã‚ã‚Šæ¢æŸ»",
    "href": "posts/understanding-what-makes-rl-difficult.html#å ±é…¬ã‚ã‚Šæ¢æŸ»",
    "title": "ã‚ˆã‚Šè‰¯ã„å•é¡Œè¨­è¨ˆã¸å‘ã‘ã¦ï¼š ä½•ãŒå¼·åŒ–å­¦ç¿’ã‚’é›£ã—ãã™ã‚‹ã®ã‹ã‚’ç†è§£ã—ã‚ˆã†",
    "section": "3.2 å ±é…¬ã‚ã‚Šæ¢æŸ»",
    "text": "3.2 å ±é…¬ã‚ã‚Šæ¢æŸ»\næ¬¡ã¯ã€ç’°å¢ƒã‹ã‚‰å ±é…¬ãŒä¸ãˆã‚‰ã‚Œã‚‹ã®ã§ã€ãªã‚‹ã¹ãæ—©ãå­¦ç¿’ã‚’çµ‚ã‚ã‚‰ã›ãŸã„ã€ã¨ã„ã†å•é¡Œã‚’è€ƒãˆã¾ã™ã€‚ è¨ªå•ã—ã¦ã„ãªã„å ´æ‰€ã«ç©æ¥µçš„ã«ã„ã‘ã°ã„ã„ã€ã¨ã„ã†æ–¹é‡ã¯ã•ã£ãã¨å¤‰ã‚ã‚Šã¾ã›ã‚“ã€‚ ä¸€æ–¹ã§ã€ã‚ã¾ã‚Šã«å ±é…¬ãŒã‚‚ã‚‰ãˆãªã•ãã†ãªçŠ¶æ…‹ã¯ã¨ã£ã¨ã¨è«¦ã‚ã‚‹ã“ã¨ã‚’ã—ãªãã¦ã¯ã„ã‘ãªã„ç‚¹ãŒç•°ãªã£ã¦ã„ã¾ã™ã€‚\nä¾‹ãˆã°ã€å ±é…¬ã®æ¨å®šå€¤ã‚’\\(\\hat{r}(s, a)\\)ã¨ã™ã‚‹ã¨ãã€å…ˆã»ã©ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ç–‘ä¼¼å ±é…¬ã‚’ \\(r_k(s, a)=\\hat{r}(s, a)+\\frac{\\beta}{\\sqrt{n(s, a)}}\\)ã¨ã™ã‚Œã°ã„ã„ã§ã™ã€‚ ã“ã‚Œã‚’ã€å˜ã«\\(r_k(s, a)=\\hat{r}(s, a)\\)ã¨ã™ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ (Approximate Value Iteration)ã¨æ¯”è¼ƒã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ ã“ã¡ã‚‰ã¯ã€ç¢ºç‡\\(\\epsilon\\)ã§ä¸€æ§˜åˆ†å¸ƒã‹ã‚‰è¡Œå‹•ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€\\(1-\\epsilon\\)ã§\\(Q^*_k\\)ãŒä¸€ç•ªå¤§ãã„è¡Œå‹•ã‚’é¸æŠã™ã‚‹ã¨ã„ã†è¡Œå‹•æ–¹ç­–ã‚’ä½¿ã£ã¦ã¿ã¾ã—ã‚‡ã†ï¼ˆ\\(\\epsilon\\)-Greedyã¨è¨€ã„ã¾ã™ï¼‰ã€‚ ä»Šå›ã¯\\(\\epsilon=0.9 \\rightarrow 0.4\\)ã¨ã—ã¾ã™ã€‚\n\n\nCode\ngrid_mdp3 = GridMDP(\n    [[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n    [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9.0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0, 0]],\n    horizon=100,\n    legend_loc=\"lower right\"\n)\n_ = grid_mdp3.show(\"GridMDP3\")\n\n\n\n\n\nã¾ãšã€ã“ã¡ã‚‰ã®ç’°å¢ƒã§å®Ÿé¨“ã—ã¦ã¿ã¾ã™ã€‚ç´ ç›´ã«\\(0.1\\)ã®å ±é…¬â†’\\(9.0\\)ã®å ±é…¬ã‚’ç›®æŒ‡ã›ã°ã„ã„æ„Ÿã˜ã§ã™ã€‚ ã¾ãŸã€\\(\\gamma=0.99\\)ã¨ã—ã¾ã™ã€‚\n\n\nCode\nclass EpsgApproxVI:\n    def __init__(\n        self,\n        n_states: int,\n        n_actions: int,\n        seed: int = 1,\n        gamma: float = 0.99,\n        epsilon: float = 0.9,\n        epsilon_delta: float = 0.0001,\n    ) -&gt; None:\n        self.sa_count = np.ones((n_states, n_actions))\n        self.sas_count = np.ones((n_states, n_actions, n_states)) / n_states\n        self.r_sum = np.zeros((n_states, n_actions))\n        self.pi = np.ones((n_states, n_actions)) / n_actions\n        self.random_state = np.random.RandomState(seed)\n        self.n_states, self.n_actions = n_states, n_actions\n        self.n_updates = 0\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.epsilon_delta = epsilon_delta\n        self.pi_indices = np.arange(n_states, dtype=np.uint32)\n\n    def learn(\n        self,\n        state: int,\n        action: int,\n        next_state: int,\n        reward: float,\n        is_terminal: bool,\n    ) -&gt; None:\n        self.sa_count[state, action] += 1\n        self.r_sum[state, action] += reward\n        if is_terminal:\n            r = self.r_sum / self.sa_count\n            p = self.sas_count / np.expand_dims(self.sa_count, axis=-1)\n            v, _n_iter = value_iteration(r, p, self.gamma, 1e-2)\n            q = r + self.gamma * np.einsum(\"saS,S-&gt;sa\", p, v)\n            self.pi.fill(0)\n            for state in range(self.n_states):\n                self.pi[state][q[state].argmax()] = 1.0\n            self.n_updates += 1\n        else:\n            self.sas_count[state, action, next_state] += 1\n\n    def act(self, state: int) -&gt; int:\n        if self.random_state.rand() &lt; self.epsilon:\n            self.epsilon -= self.epsilon_delta\n            return self.random_state.choice(self.n_actions)\n        else:\n            return self.random_state.choice(self.n_actions, p=self.pi[state])\n\n\nclass MBIB_EB:\n    def __init__(\n        self,\n        n_states: int,\n        n_actions: int,\n        seed: int = 1,\n        gamma: float = 0.99,\n        beta: float = 0.1,\n    ) -&gt; None:\n        self.sa_count = np.ones((n_states, n_actions))\n        self.sas_count = np.ones((n_states, n_actions, n_states)) / n_states\n        self.r_sum = np.zeros((n_states, n_actions))\n        self.pi = np.ones((n_states, n_actions)) / n_actions\n        self.random_state = np.random.RandomState(seed)\n        self.n_states, self.n_actions = n_states, n_actions\n        self.n_updates = 0\n        self.gamma = gamma\n        self.beta = beta\n        self.pi_indices = np.arange(n_states, dtype=np.uint32)\n    \n    def learn(\n        self,\n        state: int,\n        action: int,\n        next_state: int,\n        reward: float,\n        is_terminal: bool,\n    ) -&gt; None:\n        self.sa_count[state, action] += 1\n        self.r_sum[state, action] += reward\n        if is_terminal:\n            r = self.r_sum / self.sa_count + self.beta / np.sqrt(self.sa_count)\n            p = self.sas_count / np.expand_dims(self.sa_count, axis=-1)\n            v, _n_iter = value_iteration(r, p, self.gamma, 1e-2)\n            v_is_larger_than_mean = v &gt; v.mean()\n            q = r + self.gamma * np.einsum(\"saS,S-&gt;sa\", p, v)\n            self.pi.fill(0.0)\n            for state in range(self.n_states):\n                if v_is_larger_than_mean[state]:\n                    self.pi[state] = 1.0 / self.n_actions\n                else:\n                    self.pi[state][q[state].argmax()] = 1.0\n            self.n_updates += 1\n        else:\n            self.sas_count[state, action, next_state] += 1\n\n    def act(self, state: int) -&gt; int:\n        return self.random_state.choice(self.n_actions, p=self.pi[state])\n\n\nepsg_vi = EpsgApproxVI(grid_mdp3.n_states(), grid_mdp3.n_actions())\nsimulation(\n    grid_mdp3,\n    10000,\n    epsg_vi.act,\n    learn=epsg_vi.learn,\n    max_visit=100,\n    title=\"Îµ-Greedy\",\n    vis_freq=None,\n)\nmbib_eb = MBIB_EB(grid_mdp3.n_states(), grid_mdp3.n_actions())\nsimulation(\n    grid_mdp3,\n    10000,\n    mbib_eb.act,\n    learn=mbib_eb.learn,\n    max_visit=100,\n    title=\"Startegic Exploration\",\n    vis_freq=None,\n)\n\n\n\n\n\n\n\n\nä¸¡æ–¹ã¨ã‚‚ã€ã„ã„æ„Ÿã˜ã«æ¢æŸ»ã—ã¦ãã‚Œã¦ã„ã‚‹ã‚ˆã†ã«è¦‹ãˆã¾ã™ã€‚ \\(\\epsilon\\)-Greedyã®æ–¹ãŒã€\\(R_t=9.0\\)ãŒã‚‚ã‚‰ãˆã‚‹ã‚´ãƒ¼ãƒ«ã®å‘¨è¾ºã‚’å¤šãæ¢æŸ»ã—ã¦ã„ã¦ã€ è‰¯ã•ãã†ã«è¦‹ãˆã¾ã™ã€‚ ä¸€æ–¹ã§ã€ã‚‚ã†å°‘ã—æ„åœ°æ‚ªãªç’°å¢ƒã®å ´åˆã¯ã©ã†ã§ã—ã‚‡ã†ã‹ï¼Ÿ\n\n\nCode\ngrid_mdp4 = GridMDP(\n    [[0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 3],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n    [[0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 9.0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n    horizon=100,\n    legend_loc=\"lower right\"\n)\n_ = grid_mdp4.show(\"GridMDP3\")\n\n\n\n\n\nã“ã®ç’°å¢ƒã§ã¯ã€\\(+5.0\\)ã¨ã‹ã„ã†é‚ªé­”ãã•ã„å ±é…¬ãŒã‚ã‚Šã¾ã™ã€‚ ã—ã‹ã‚‚ã“ã“ã¯ã‚´ãƒ¼ãƒ«ãªã®ã§ã€ã“ã“ã«è¡Œãã¨ã¾ãŸãƒªã‚»ãƒƒãƒˆã—ã¦ã‚„ã‚Šç›´ã—ã§ã™ã€‚ ã“ã“ã‚’ç›®æŒ‡ã™ã‚ˆã†ã«å­¦ç¿’ã—ã¦ã—ã¾ã†ã¨ã€ãªã‹ãªã‹\\(+9.0\\)ã®æ–¹ã«è¡Œãã®ã¯å³ã—ãã†ã«è¦‹ãˆã¾ã™ã€‚ å®Ÿé¨“ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\nepsg_vi = EpsgApproxVI(grid_mdp4.n_states(), grid_mdp4.n_actions())\nsimulation(\n    grid_mdp4,\n    10000,\n    epsg_vi.act,\n    learn=epsg_vi.learn,\n    max_visit=100,\n    title=\"Îµ-Greedy\",\n    vis_freq=None,\n)\nmbib_eb = MBIB_EB(grid_mdp4.n_states(), grid_mdp4.n_actions())\nsimulation(\n    grid_mdp4,\n    10000,\n    mbib_eb.act,\n    learn=mbib_eb.learn,\n    max_visit=100,\n    title=\"Startegic Exploration\",\n    vis_freq=None,\n)\n\n\n\n\n\n\n\n\näºˆæƒ³é€šã‚Šã€\\(\\epsilon\\)-Greedyã®æ–¹ã¯å³ä¸Šã°ã‹ã‚Šè¡Œã£ã¦ã—ã¾ã£ã¦ã‚¤ãƒã‚¤ãƒãªæ„Ÿã˜ã«ãªã‚Šã¾ã—ãŸã€‚\nä»¥ä¸Šã®çµæœã‹ã‚‰ã€ - é‚ªé­”ãŒãªãé·ç§»é–¢æ•°ãŒå¯¾ç§°ãªçŠ¶æ…‹ç©ºé–“ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ã®ã‚ˆã†ã«è€ƒãˆã‚‰ã‚Œã‚‹ã‚‚ã®ï¼‰ã§ã¯ã€ã‚ã‚Šã‚ã„ç°¡å˜ã«ãƒ‡ãƒ¼ã‚¿åé›†ãŒã§ãã‚‹ - é‚ªé­”ãªå ±é…¬ãŒãªã„ç’°å¢ƒã§ã¯ã€ã‚ã‚Šã‚ã„ç°¡å˜ã«ãƒ‡ãƒ¼ã‚¿åé›†ãŒã§ãã‚‹\nã¨ã„ã†2ç‚¹ãŒè¨€ãˆã‚‹ã‹ã¨æ€ã„ã¾ã™ã€‚ ãƒ¯ãƒ¼ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’è€ƒãˆã‚‹ã¨æ¢æŸ»ãŒé›£ã—ã„ã®ã‚‚äº‹å®Ÿã§ã™ãŒã€å®Ÿç”¨ä¸Šã¯é›£ã—ã„ã‚±ãƒ¼ã‚¹ã‚’è€ƒãˆã‚‹ã‚ˆã‚Šé‚ªé­”ãªå ±é…¬ã‚’æ’é™¤ã™ã‚‹ ã“ã¨ã‚’è€ƒãˆã‚‹ã®ãŒé‡è¦ã§ã™ã€‚"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html#a-å‚è€ƒæ–‡çŒ®ãªã©",
    "href": "posts/understanding-what-makes-rl-difficult.html#a-å‚è€ƒæ–‡çŒ®ãªã©",
    "title": "ã‚ˆã‚Šè‰¯ã„å•é¡Œè¨­è¨ˆã¸å‘ã‘ã¦ï¼š ä½•ãŒå¼·åŒ–å­¦ç¿’ã‚’é›£ã—ãã™ã‚‹ã®ã‹ã‚’ç†è§£ã—ã‚ˆã†",
    "section": "3.A å‚è€ƒæ–‡çŒ®ãªã©",
    "text": "3.A å‚è€ƒæ–‡çŒ®ãªã©\n\nOn the Sample Complexity of Reinforcement Learning\nReward-Free Exploration for Reinforcement Learning\nSample Complexity Bounds of Exploration\nAn analysis of model-based Interval Estimation for Markov Decision Processes\n\n3.1ã§ç´¹ä»‹ã—ãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ä¸€å¿œ2.ã®æ–‡çŒ®ã‚’å‚è€ƒã«ã—ã¦ã„ã¾ã™ãŒã€åƒ•ãŒã•ã£ãé©å½“ã«è€ƒãˆãŸ(ã¯ï¼Ÿ)ã‚‚ã®ã§ã™ã€‚ ç†è«–ä¿è¨¼ãŒã‚ã‚‹ã‹ã¯ã‚ã‚„ã—ã„ã¨æ€ã„ã¾ã™ã€‚ 3.2ã®ã‚„ã¤ã¯MBIB-EB(4.)ã«ä¼¼ã¦ã„ã¾ã™ãŒã€æ–¹ç­–ã®ç·©å’ŒãŒå…¥ã£ã¦ã„ã‚‹ç‚¹ãŒé•ã„ã¾ã™ã€‚ ç·©å’Œã‚‚åƒ•ãŒé©å½“ã«è€ƒãˆãŸã‚‚ã®ãªã®ã§ã™ãŒã€å…¥ã‚ŒãŸæ–¹ãŒæ€§èƒ½ãŒè‰¯ã‹ã£ãŸã®ã§å…¥ã‚Œã¦ã¿ã¾ã—ãŸã€‚ è‰¯ã„å­ã®çš†ã•ã‚“ã¯çœŸä¼¼ã—ãªã„ã§ãã ã•ã„ã€‚"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html#footnotes",
    "href": "posts/understanding-what-makes-rl-difficult.html#footnotes",
    "title": "ã‚ˆã‚Šè‰¯ã„å•é¡Œè¨­è¨ˆã¸å‘ã‘ã¦ï¼š ä½•ãŒå¼·åŒ–å­¦ç¿’ã‚’é›£ã—ãã™ã‚‹ã®ã‹ã‚’ç†è§£ã—ã‚ˆã†",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSuttonã‚‚èª¤å·®é€†ä¼æ’­ã‚’ä½¿ã†ã®ã«ã¯ãƒˆãƒªãƒƒã‚­ãƒ¼ãªå·¥å¤«ãŒå¿…è¦ã ã¨è¨€ã£ã¦ã„ã¾ã™ã€‚â†©ï¸\nã“ã®å ±é…¬é–¢æ•°ã¯æœ€ã‚‚ç°¡å˜ãªå®šç¾©ã§ã™ã€‚ä»–ã«\\(r: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow \\mathbb{R}\\)(é·ç§»å…ˆã«ä¾å­˜)ã€\\(r: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{E}[R_{s, a}]\\)ï¼ˆç¢ºç‡çš„ï¼‰ãŒã‚ã‚Šã¾ã™ã€‚â†©ï¸\nåˆæœŸçŠ¶æ…‹åˆ†å¸ƒ(é›‘ã«è¨€ã†ã¨ã€ã‚¹ã‚¿ãƒ¼ãƒˆåœ°ç‚¹ã®åˆ†å¸ƒ)ã‚’\\(\\mu(s)\\)ã¨ã™ã‚‹ã¨ã€$ {s} (s)V(s)$ ãŒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç²å¾—ã™ã‚‹å‰²å¼•å ±é…¬å’Œã®æœŸå¾…å€¤ã§ã™ã€‚\\(V_\\pi(s)\\) ãŒæœ€å¤§ãªã‚‰ã“ã‚Œã‚‚æœ€å¤§ã«ãªã‚Šã¾ã™ã€‚â†©ï¸\nã“ã®\\(\\epsilon\\)ã¯\\(\\epsilon\\)-Optimal Policyã®\\(\\epsilon\\)ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚â†©ï¸\nã“ã®è¨˜äº‹ã§ã¯å‹¾é…ã®å°å‡ºã«ã¤ã„ã¦ã¯ä¸€åˆ‡è§¦ã‚Œãªã„ã®ã§ã€åˆ¥é€”è³‡æ–™ãªã©ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚â†©ï¸"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "RLog2: RL blog 2",
    "section": "",
    "text": "Blog posts on reinforcement learning and other technical stuff with some code"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RLog2",
    "section": "",
    "text": "AttentionãŒä½•ã‚’ã‚„ã£ã¦ã„ã‚‹ã®ã‹ç†è§£ã—ã‚ˆã†\n\n\n\n\n\n\n\nja\n\n\nNLP\n\n\ndeep\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2025\n\n\n\n\n\n\n  \n\n\n\n\nExercise 5.12 Racetrack from the Reinforcement Learning textbook\n\n\n\n\n\n\n\nen\n\n\nRL\n\n\nbasic\n\n\n\n\n\n\n\n\n\n\n\nJul 3, 2022\n\n\n\n\n\n\n  \n\n\n\n\nJaxãƒ»Braxãƒ»Haikuã§GPUå¼•ãã“ã‚‚ã‚Šå­¦ç¿’\n\n\n\n\n\n\n\nja\n\n\nRL\n\n\ndeep\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2022\n\n\n\n\n\n\n  \n\n\n\n\nã‚ˆã‚Šè‰¯ã„å•é¡Œè¨­è¨ˆã¸å‘ã‘ã¦ï¼š ä½•ãŒå¼·åŒ–å­¦ç¿’ã‚’é›£ã—ãã™ã‚‹ã®ã‹ã‚’ç†è§£ã—ã‚ˆã†\n\n\n\n\n\n\n\nja\n\n\nRL\n\n\nbasic\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2021\n\n\n\n\n\n\nNo matching items"
  }
]