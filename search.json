[
  {
    "objectID": "posts/fast_2d_physics_in_jax_en.html",
    "href": "posts/fast_2d_physics_in_jax_en.html",
    "title": "Implement fast 2D physics simulation with Jax",
    "section": "",
    "text": "â€» This article is translated from Japanese version with some improvements on code.\nMaking simulation fast utilizing the power of GPU/TPU is a hot topic in the reinforcement learning community. LLM and RLHF things are getting â€˜the thingâ€™ in the community, though. Anyway, it is quite fun to speed up simulation. NVIDIA IsaacSym is quite solid, but if you want to speed up the entire RL pipeline, Jax-based libraries such as brax is useful.\nYou donâ€™t know Jax? Itâ€™s just a NumPy-like library, but it can be compiled into a fast vectorized machine code that works really fast on GPU and TPU. Itâ€™s even fast on CPUs. I have written a blog post (Japanese only) before, but the current version of brax is farther improved. It allows you to choose a more accurate method, and it will be tightly integrated with MuJoCo XLA.\nHowever, recently, I wanted a simple 2-D physics simulation, and tried brax. My impression was that not only brax is a complete overkill for my usecase, but also its API is quite difficult to set up a new environment. Loading MJCF is easy, but that was the only easy way to set up the stuff. I wanted to make my environment with some lines of Python code like game-physics engines such as pymunk. So, anyway, I tried to make my own."
  },
  {
    "objectID": "posts/fast_2d_physics_in_jax_en.html#check-contacts",
    "href": "posts/fast_2d_physics_in_jax_en.html#check-contacts",
    "title": "Implement fast 2D physics simulation with Jax",
    "section": "Check contacts",
    "text": "Check contacts\nNext, letâ€™s implement collision detection. Itâ€™s pretty easy because we have only circles now. We just need to store some information such as location and overwrap to resolve collisions later.\nThe other thing I need to care is vectorization. Say, this is a naive Python code to check contacts.\nfor i in range(N):\n    for j in range(i + 1, N):\n        check_contact(objects[i], objects[j])\nIt has \\(O(N^2)\\) loop! However, you might be smart enough to point out that each operation in this loop has no dependency, or the order of computation doesnâ€™t matter. Here, jax.vmap is for you to vectorize those operations. Then, letâ€™s unrole this loop by hand, make pairs, and then call check_contact. Like,\nobject_1, object_2 = make_pair(objects)\njax.vmap(check_contact)(object1, object2)\nHow to make pairs? There are various ways, what I found the simplest is triu_indices, which is a function that produces the indices of upper triangular matrix with some offsets.\n\n\nCode\nfrom typing import Any, Callable\n\nAxis = Sequence[int] | int\n\n\ndef normalize(x: jax.Array, axis: Axis | None = None) -&gt; tuple[jax.Array, jax.Array]:\n    norm = jnp.linalg.norm(x, axis=axis)\n    n = x / jnp.clip(norm, a_min=1e-6)\n    return n, norm\n\n\n@chex.dataclass\nclass Contact(PyTreeOps):\n    pos: jax.Array\n    normal: jax.Array\n    penetration: jax.Array\n    elasticity: jax.Array\n    friction: jax.Array\n\n    def contact_dim(self) -&gt; int:\n        return self.pos.shape[1]\n\n@jax.vmap\ndef _circle_to_circle_impl(\n    a: Circle,\n    b: Circle,\n    a_pos: Position,\n    b_pos: Position,\n    isactive: jax.Array,\n) -&gt; Contact:\n    a2b_normal, dist = normalize(b_pos.xy - a_pos.xy)\n    penetration = a.radius + b.radius - dist\n    a_contact = a_pos.xy + a2b_normal * a.radius\n    b_contact = b_pos.xy - a2b_normal * b.radius\n    pos = (a_contact + b_contact) * 0.5\n    # Filter penetration\n    penetration = jnp.where(isactive, penetration, jnp.ones_like(penetration) * -1)\n    return Contact(\n        pos=pos,\n        normal=a2b_normal,\n        penetration=penetration,\n        elasticity=(a.elasticity + b.elasticity) * 0.5,\n        friction=(a.friction + b.friction) * 0.5,\n    )\n\n\ndef check_circle_to_circle(\n    space: Space,\n    position: Position,\n    is_active: jax.Array,\n) -&gt; tuple[Contact, Circle, Circle]:\n    idx1, idx2 = jnp.triu_indices(is_active.shape[0], 1)\n    circle1, circle2 = space.circle.get_slice(idx1), space.circle.get_slice(idx2)\n    pos1, pos2 = position.get_slice(idx1), position.get_slice(idx2)\n    is_active = jnp.logical_and(is_active[idx1], is_active[idx2])\n    contacts = _circle_to_circle_impl(circle1, circle2, pos1, pos2, is_active)\n    return contacts, circle1, circle2\n\n\n\n\nCode\nimport seaborn as sns\nfrom matplotlib.patches import Arrow\n\nN = 5\npalette = sns.color_palette(\"husl\", N)\n\n\ncircles = Circle(\n    mass=jnp.ones(N),\n    radius=jnp.ones(N),\n    moment=jnp.ones(N) * 0.5,\n    elasticity=jnp.ones(N) * 0.5,\n    friction=jnp.ones(N) * 0.2,\n    rgba=jnp.array([p + (1.0,) for p in palette]),\n)\nspace = Space(gravity=jnp.array([0.0, -9.8]), circle=circles)\np = Position(\n    angle=jnp.zeros(N),\n    xy=jnp.array([[-3, 4.0], [0.0, 2.0], [5.0, 3], [-3, 1], [2, 0]]),\n)\nv_xy = jnp.concatenate((jnp.zeros((N - 2, 2)), jnp.array([[0, 10.0], [-2.0, 8.0]])))\nv = Velocity(angle=jnp.zeros(N), xy=v_xy)\nf = Force(angle=jnp.zeros(N), xy=jnp.zeros((N, 2)))\nstate = State(p=p, v=v, f=f, is_active=jnp.ones(N, dtype=bool))\npositions = [state.p]\ncontact_list = []\nfor i in range(10):\n    state = update_velocity(space, circles, state)\n    state = update_position(space, state)\n    positions.append(state.p)\n    contacts, _, _ = check_circle_to_circle(space, state.p, state.is_active)\n    total_index = 0\n    for j in range(N):\n        for k in range(j + 1, N):\n            if contacts.penetration[total_index] &gt; 0:\n                contact_list.append(contacts.get_slice(total_index))\n            total_index += 1\nfig, ax = plt.subplots()\nax.set_aspect(\"equal\", adjustable=\"box\")\nax.set_xlim((-5, 5))\nax.set_ylim((-5, 5))\nvisualize_balls(ax, space.circle, positions)\nfor contact in contact_list:\n    arrow = Arrow(*contact.pos, *contact.normal, width=0.2, color=\"r\")\n    ax.add_patch(arrow)\nfig\n\n\n\n\n\n\n\n\n\nObjects are overlapping since I havenâ€™t implemented resolving collision, but the dected collision looks correct."
  },
  {
    "objectID": "posts/fast_2d_physics_in_jax_en.html#what-to-do-after-collision-detection",
    "href": "posts/fast_2d_physics_in_jax_en.html#what-to-do-after-collision-detection",
    "title": "Implement fast 2D physics simulation with Jax",
    "section": "What to do after collision detection",
    "text": "What to do after collision detection\nIâ€™m not quite familiar with physics actually, but in the real world, collisions are automatically resolved by each small molecule. But in the game physics engine, we want to get results faster by enforcing physically acurate constraints, such as:\n\nSpeed changes by collision\nNo overlap between objects\n\nSo, Iâ€™m just solving these constraints. There are many ways to solve these, but here, I employ a common approach called Sequential Impulse, which is used in a famous physics engines including Chipmunk and Box2D. Other approaches includes recently trending position-based dynamics, but sequatinal impulse caught my eyes because of its simplicity.\nAnyway, let me explain how sequential impulse method resolves collision. First, letâ€™s assume a collision model in which collision produces impulse, and let the generated impulse be \\(\\mathbf{p}\\). I donâ€™t use angular velocity here for simplicity. Let the velocity and mass of object 1 be \\(\\mathbf{v}_1\\) and \\(m_1\\), and the velocity and mass of object 2 be \\(\\mathbf{v}_2\\) and \\(m_2\\), respectively, then the velocity after the impulse occurs is\n\\[\n\\begin{align*}\n\\mathbf{v}_1 = \\mathbf{v}_1^{\\mathrm{old}} - \\mathbf{p} / m_1 \\\\\n\\mathbf{v}_2 = \\mathbf{v}_2^{\\mathrm{old}} + \\mathbf{p} / m_2\n\\end{align*}\n\\]\nHere, because the direction of \\(\\mathbf{p}\\) is the normal vector of collision \\(\\mathbf{n}\\), we can write that \\(\\mathbf{p} = p\\mathbf{n}\\). Hence, we just need to compute \\(p\\). Let the relative velocity at the collision point be \\(\\Delta \\mathbf{v} = \\mathbf{v}_2 - \\mathbf{v}_1\\). Here combining the avove two equalities with \\(\\Delta \\mathbf{v} \\cdot n = 0\\), \\(p = \\frac{-\\Delta \\mathbf{v}^{\\mathrm{old}}\\cdot \\mathbf{n}}{\\frac{1}{m_1} + \\frac{1}{m_2}}\\). Angle makes this a bit more complicated, but the core equation is just like this.\nã“ã†ã—ã¦è¨ˆç®—ã—ãŸã‚¤ãƒ³ãƒ‘ãƒ«ã‚¹ã‚’å…¨ã¦ã®è¡çªã«å¯¾ã—ã¦é©ç”¨ã—ã€ã‚¤ãƒ³ãƒ‘ãƒ«ã‚¹ãŒå°ã•ããªã‚‹ã¾ã§ç¹°ã‚Šè¿”ã—ã¾ã™ã€‚ã—ã‹ã—ã€ã“ã®æ‰‹æ³•ã¯ç‰©ä½“ã®ã‚ã‚Šã“ã¿ã‚’è€ƒæ…®ã—ã¦ã„ãªã„ã®ã§ã€ã“ã‚Œã ã‘ã ã¨ç‰©ä½“ãŒã‚ã‚Šã“ã‚“ã ã¾ã¾ã«ãªã£ã¦ã—ã¾ã†ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚ ã‚ã‚Šè¾¼ã¿ã‚’æ¸›ã‚‰ã™ãŸã‚ã®æ‰‹æ³•ã¯ã„ãã¤ã‹ã‚ã‚Šã¾ã™ãŒã€ä¸»ã«\n\nã©ã®ãã‚‰ã„ã‚ã‚Šè¾¼ã‚“ã§ã„ã‚‹ã‹ã«å¿œã˜ã¦ãƒã‚¤ã‚¢ã‚¹é€Ÿåº¦\\(v_\\mathrm{bias} = \\frac{\\beta}{\\Delta t}\\max(0, \\delta - \\delta_\\mathrm{slop})\\)(\\(\\delta\\)ã¯ã‚ã‚Šã“ã¿ã®é•·ã•ã€\\(\\delta_\\mathrm{slop}\\)ã¯è¨±å®¹ã•ã‚Œã‚‹ã‚ã‚Šã“ã¿ã®é•·ã•)ã‚’åŠ ãˆ\\(p = \\frac{-\\Delta \\mathbf{v}^{\\mathrm{old}}\\cdot \\mathbf{n} + v_\\mathrm{bias}}{\\frac{1}{m_1} + \\frac{1}{m_2}}\\)ã¨ã™ã‚‹ (Baumegarte)\né€Ÿåº¦ã‚’æ›´æ–°ã—ãŸå¾Œã«ã‚‚ã†ä¸€å›Positionã«é–¢ã™ã‚‹åˆ¶ç´„ã‚’è§£ã„ã¦æ“¬ä¼¼çš„ãªé€Ÿåº¦ã‚’åŠ ãˆã‚‹ (Nonlinear Gauss Seidel, NGS)\n\nã¨ã„ã†2ç¨®é¡ã®æ‰‹æ³•ãŒã‚ã‚Šã¾ã™ã€‚å…ˆç¨‹ç´¹ä»‹ã—ãŸBox2Dã®è³‡æ–™ã‚„Chipmunk2Dã§ã¯1ãŒã€ç¾åœ¨ã®Box2Dã§ã¯2ãŒä½¿ã‚ã‚Œã¦ã„ã¾ã™ã€‚è©³ã—ãã¯Box2D 3.0ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚ ä»Šå›ã¯è‹¥å¹²é«˜é€Ÿãª1ã®æ‰‹æ³•ã‚’å®Ÿè£…ã—ã‚ˆã†ã‹ã¨æ€ã£ãŸã®ã§ã™ãŒã€ã“ã®æ–¹æ³•ã ã¨2ã¤ã®ç‰©ä½“ãŒåŒã˜æ–¹å‘ã«é€²ã‚“ã§ã„ã‚‹æ™‚ã¯ã‚ã‚Šã“ã¿ã‚’è§£æ¶ˆã§ããªã„ã®ã§ã€çµå±€2ã®æ‰‹æ³•ã‚’å®Ÿè£…ã—ã¾ã—ãŸã€‚å…·ä½“çš„ã«ã‚½ãƒ«ãƒã®å®Ÿè£…ã¨ã—ã¦ã¯ã€\n\nè¡çªã«ã‚ˆã‚Šç™ºç”Ÿã™ã‚‹ã‚¤ãƒ³ãƒ‘ãƒ«ã‚¹ã«é–¢ã™ã‚‹åˆ¶ç´„ã‚’è§£ã\nå¼¾æ€§ã«ã‚ˆã‚Šç™ºç”Ÿã™ã‚‹ã‚¤ãƒ³ãƒ‘ãƒ«ã‚¹ã‚’åŠ ãˆã‚‹\nä½ç½®ã«é–¢ã™ã‚‹åˆ¶ç´„ã‚’è§£ã\n\nã¨ã„ã†3ã¤ã®ã‚¹ãƒ†ãƒƒãƒ—ã«åˆ†ã‘ã¦å®Ÿè£…ã™ã‚Œã°ã„ã„ã§ã™ã€‚\nã¾ãŸã€ã•ã£ãä¸¦åˆ—åŒ–ã®ãŸã‚æ‰‹å‹•ã§å…¨ãƒšã‚¢ã«å¯¾ã™ã‚‹ãƒ«ãƒ¼ãƒ—ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ«ã—ã¾ã—ãŸãŒã€Sequential Impulseã®å®Ÿè£…ã§ã‚‚ã“ã‚ŒãŒä½¿ãˆã¾ã™ã€‚ãŸã ã—ã€ã‚¤ãƒ³ãƒ‘ãƒ«ã‚¹ã‚’åŠ ãˆãŸå¾Œã®é€Ÿåº¦ã®æ›´æ–°ã¯ã€v_update[i][j]ã«iç•ªç›®ã®ç‰©ä½“ã¨jç•ªç›®ã®ç‰©ä½“ã®è¡çªã«ã‚ˆã‚Šç”Ÿã˜ã‚‹iç•ªç›®ã®ç‰©ä½“ã®é€Ÿåº¦å¤‰åŒ–ãŒå…¥ã£ã¦ã„ã‚‹ã¨ã—ã¦ã€\nfor i in range(N):\n    for j in range(i + 1, N):\n        obj[i].velocity += v_update[i][j]\n        obj[j].velocity += v_update[j][i]\nã®ã‚ˆã†ã«å„è¡çªã«ã‚ˆã‚Šç”Ÿã˜ãŸé€Ÿåº¦å¤‰åŒ–ã‚’ç‰©ä½“ã«ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã‚‚ã„ã¡ã„ã¡ãƒ«ãƒ¼ãƒ—ã§æ›¸ãã¨é…ããªã£ã¦ã—ã¾ã†ã®ã§ã™ãŒã€ã•ã£ãã®generate_self_pairsã§\\(0, 1, 2, ..., N - 1\\)ã®ãƒšã‚¢ã‚’ç”Ÿæˆã—ã¦ãŠã„ã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ã™ã‚‹ã¨ãƒ«ãƒ¼ãƒ—ãªã—ã§æ›¸ã‘ã¾ã™ã€‚ç´°ã‹ãè¨€ã†ã¨ã€generate_self_pairsã§ã¯ãƒ«ãƒ¼ãƒ—ãŒä½¿ã‚ã‚Œã¦ã„ã‚‹ã®ã§ã™ãŒã€jax.jitã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ãŸæ™‚ã«è¨ˆç®—çµæœãŒã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã‚‹ã¯ãšãªã®ã§æ°—ã«ã—ãªãã¦ã‚‚ã„ã„ã§ã™ã€‚\nã¨ã„ã†ã‚ã‘ã§å®Ÿè£…ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚å®Ÿè£…ã¯åŸºæœ¬çš„ã«Box2d-Liteã¨é–‹ç™ºä¸­ã®æœ€æ–°ç‰ˆã§ã‚ã‚‹Box2D 3.0ã‚’å‚è€ƒã«ã—ã¾ã—ãŸã€‚ã¾ãŸã€Box2Dä½œè€…ã®Erin Cattoæ°ã«ã‚ˆã‚‹è¬›æ¼”ã®å†…å®¹ã‚’Typescriptã§å®Ÿè£…ã—ãŸãƒªãƒã‚¸ãƒˆãƒªãŒã‚ã£ãŸã®ã§ã“ã‚Œã‚‚å‚è€ƒã«ã—ã¾ã—ãŸã€‚\n\n\nCode\nimport functools\n\n\n@chex.dataclass\nclass ContactHelper:\n    tangent: jax.Array\n    mass_normal: jax.Array\n    mass_tangent: jax.Array\n    v_bias: jax.Array\n    bounce: jax.Array\n    r1: jax.Array\n    r2: jax.Array\n    inv_mass1: jax.Array\n    inv_mass2: jax.Array\n    inv_moment1: jax.Array\n    inv_moment2: jax.Array\n    local_anchor1: jax.Array\n    local_anchor2: jax.Array\n    allow_bounce: jax.Array\n\n\n@chex.dataclass\nclass VelocitySolver:\n    v1: Velocity\n    v2: Velocity\n    pn: jax.Array\n    pt: jax.Array\n    contact: jax.Array\n\n    def update(self, new_contact: jax.Array) -&gt; Self:\n        continuing_contact = jnp.logical_and(self.contact, new_contact)\n        pn = jnp.where(continuing_contact, self.pn, jnp.zeros_like(self.pn))\n        pt = jnp.where(continuing_contact, self.pt, jnp.zeros_like(self.pt))\n        return self.replace(pn=pn, pt=pt, contact=new_contact)\n\n\ndef init_solver(n: int) -&gt; VelocitySolver:\n    return VelocitySolver(\n        v1=Velocity.zeros(n),\n        v2=Velocity.zeros(n),\n        pn=jnp.zeros(n),\n        pt=jnp.zeros(n),\n        contact=jnp.zeros(n, dtype=bool),\n    )\n\n\ndef _pv_gather(\n    p1: _PositionLike,\n    p2: _PositionLike,\n    orig: _PositionLike,\n) -&gt; _PositionLike:\n    indices = jnp.arange(len(orig.angle))\n    outer, inner = generate_self_pairs(indices)\n    p1_xy = jnp.zeros_like(orig.xy).at[outer].add(p1.xy)\n    p1_angle = jnp.zeros_like(orig.angle).at[outer].add(p1.angle)\n    p2_xy = jnp.zeros_like(orig.xy).at[inner].add(p2.xy)\n    p2_angle = jnp.zeros_like(orig.angle).at[inner].add(p2.angle)\n    return p1.__class__(xy=p1_xy + p2_xy, angle=p1_angle + p2_angle)\n\n\ndef _vmap_dot(xy1: jax.Array, xy2: jax.Array) -&gt; jax.Array:\n    \"\"\"Dot product between nested vectors\"\"\"\n    chex.assert_equal_shape((xy1, xy2))\n    orig_shape = xy1.shape\n    a = xy1.reshape(-1, orig_shape[-1])\n    b = xy2.reshape(-1, orig_shape[-1])\n    return jax.vmap(jnp.dot, in_axes=(0, 0))(a, b).reshape(*orig_shape[:-1])\n\n\ndef _sv_cross(s: jax.Array, v: jax.Array) -&gt; jax.Array:\n    \"\"\"Cross product with scalar and vector\"\"\"\n    x, y = _get_xy(v)\n    return jnp.stack((y * -s, x * s), axis=-1)\n\n\ndef _dv2from1(v1: Velocity, r1: jax.Array, v2: Velocity, r2: jax.Array) -&gt; jax.Array:\n    \"\"\"Compute relative veclotiy from v2/r2 to v1/r1\"\"\"\n    rel_v1 = v1.xy + _sv_cross(v1.angle, r1)\n    rel_v2 = v2.xy + _sv_cross(v2.angle, r2)\n    return rel_v2 - rel_v1\n\n\ndef _effective_mass(\n    inv_mass: jax.Array,\n    inv_moment: jax.Array,\n    r: jax.Array,\n    n: jax.Array,\n) -&gt; jax.Array:\n    rn2 = jnp.cross(r, n) ** 2\n    return inv_mass + inv_moment * rn2\n\n\ndef init_contact_helper(\n    space: Space,\n    contact: Contact,\n    a: Shape,\n    b: Shape,\n    p1: Position,\n    p2: Position,\n    v1: Velocity,\n    v2: Velocity,\n) -&gt; ContactHelper:\n    r1 = contact.pos - p1.xy\n    r2 = contact.pos - p2.xy\n\n    inv_mass1, inv_mass2 = a.inv_mass(), b.inv_mass()\n    inv_moment1, inv_moment2 = a.inv_moment(), b.inv_moment()\n    kn1 = _effective_mass(inv_mass1, inv_moment1, r1, contact.normal)\n    kn2 = _effective_mass(inv_mass2, inv_moment2, r2, contact.normal)\n    nx, ny = _get_xy(contact.normal)\n    tangent = jnp.stack((-ny, nx), axis=-1)\n    kt1 = _effective_mass(inv_mass1, inv_moment1, r1, tangent)\n    kt2 = _effective_mass(inv_mass2, inv_moment2, r2, tangent)\n    clipped_p = jnp.clip(space.allowed_penetration - contact.penetration, a_max=0.0)\n    v_bias = -space.bias_factor / space.dt * clipped_p\n    # k_normal, k_tangent, and v_bias should have (N(N-1)/2, N_contacts) shape\n    chex.assert_equal_shape((contact.friction, kn1, kn2, kt1, kt2, v_bias))\n    # Compute elasiticity * relative_vel\n    dv = _dv2from1(v1, r1, v2, r2)\n    vn = _vmap_dot(dv, contact.normal)\n    return ContactHelper(\n        tangent=tangent,\n        mass_normal=1 / (kn1 + kn2),\n        mass_tangent=1 / (kt1 + kt2),\n        v_bias=v_bias,\n        bounce=vn * contact.elasticity,\n        r1=r1,\n        r2=r2,\n        inv_mass1=inv_mass1,\n        inv_mass2=inv_mass2,\n        inv_moment1=inv_moment1,\n        inv_moment2=inv_moment2,\n        local_anchor1=p1.inv_rotate(r1),\n        local_anchor2=p2.inv_rotate(r2),\n        allow_bounce=vn &lt;= -space.bounce_threshold,\n    )\n\n\n@jax.vmap\ndef apply_initial_impulse(\n    contact: Contact,\n    helper: ContactHelper,\n    solver: VelocitySolver,\n) -&gt; VelocitySolver:\n    \"\"\"Warm starting by applying initial impulse\"\"\"\n    p = helper.tangent * solver.pt + contact.normal * solver.pn\n    v1 = solver.v1 - Velocity(\n        angle=helper.inv_moment1 * jnp.cross(helper.r1, p),\n        xy=p * helper.inv_mass1,\n    )\n    v2 = solver.v2 + Velocity(\n        angle=helper.inv_moment2 * jnp.cross(helper.r2, p),\n        xy=p * helper.inv_mass2,\n    )\n    return solver.replace(v1=v1, v2=v2)\n\n\n@jax.vmap\ndef apply_velocity_normal(\n    contact: Contact,\n    helper: ContactHelper,\n    solver: VelocitySolver,\n) -&gt; VelocitySolver:\n    \"\"\"\n    Apply velocity constraints to the solver.\n    Suppose that each shape has (N_contact, 1) or (N_contact, 2).\n    \"\"\"\n    # Relative veclocity (from shape2 to shape1)\n    dv = _dv2from1(solver.v1, helper.r1, solver.v2, helper.r2)\n    vt = jnp.dot(dv, helper.tangent)\n    dpt = -helper.mass_tangent * vt\n    # Clamp friction impulse\n    max_pt = contact.friction * solver.pn\n    pt = jnp.clip(solver.pt + dpt, a_min=-max_pt, a_max=max_pt)\n    dpt_clamped = helper.tangent * (pt - solver.pt)\n    # Velocity update by contact tangent\n    dvt1 = Velocity(\n        angle=-helper.inv_moment1 * jnp.cross(helper.r1, dpt_clamped),\n        xy=-dpt_clamped * helper.inv_mass1,\n    )\n    dvt2 = Velocity(\n        angle=helper.inv_moment2 * jnp.cross(helper.r2, dpt_clamped),\n        xy=dpt_clamped * helper.inv_mass2,\n    )\n    # Compute Relative velocity again\n    dv = _dv2from1(solver.v1 + dvt1, helper.r1, solver.v2 + dvt2, helper.r2)\n    vn = _vmap_dot(dv, contact.normal)\n    dpn = helper.mass_normal * (-vn + helper.v_bias)\n    # Accumulate and clamp impulse\n    pn = jnp.clip(solver.pn + dpn, a_min=0.0)\n    dpn_clamped = contact.normal * (pn - solver.pn)\n    # Velocity update by contact normal\n    dvn1 = Velocity(\n        angle=-helper.inv_moment1 * jnp.cross(helper.r1, dpn_clamped),\n        xy=-dpn_clamped * helper.inv_mass1,\n    )\n    dvn2 = Velocity(\n        angle=helper.inv_moment2 * jnp.cross(helper.r2, dpn_clamped),\n        xy=dpn_clamped * helper.inv_mass2,\n    )\n    # Filter dv\n    dv1, dv2 = jax.tree_map(\n        lambda x: jnp.where(solver.contact, x, jnp.zeros_like(x)),\n        (dvn1 + dvt1, dvn2 + dvt2),\n    )\n    # Summing up dv per each contact pair\n    return VelocitySolver(\n        v1=dv1,\n        v2=dv2,\n        pn=pn,\n        pt=pt,\n        contact=solver.contact,\n    )\n\n\n@jax.vmap\ndef apply_bounce(\n    contact: Contact,\n    helper: ContactHelper,\n    solver: VelocitySolver,\n) -&gt; tuple[Velocity, Velocity]:\n    \"\"\"\n    Apply bounce (resititution).\n    Suppose that each shape has (N_contact, 1) or (N_contact, 2).\n    \"\"\"\n    # Relative veclocity (from shape2 to shape1)\n    dv = _dv2from1(solver.v1, helper.r1, solver.v2, helper.r2)\n    vn = jnp.dot(dv, contact.normal)\n    pn = -helper.mass_normal * (vn + helper.bounce)\n    dpn = contact.normal * pn\n    # Velocity update by contact normal\n    dv1 = Velocity(\n        angle=-helper.inv_moment1 * jnp.cross(helper.r1, dpn),\n        xy=-dpn * helper.inv_mass1,\n    )\n    dv2 = Velocity(\n        angle=helper.inv_moment2 * jnp.cross(helper.r2, dpn),\n        xy=dpn * helper.inv_mass2,\n    )\n    # Filter dv\n    allow_bounce = jnp.logical_and(solver.contact, helper.allow_bounce)\n    return jax.tree_map(\n        lambda x: jnp.where(allow_bounce, x, jnp.zeros_like(x)),\n        (dv1, dv2),\n    )\n\n\n@chex.dataclass\nclass PositionSolver:\n    p1: Position\n    p2: Position\n    contact: jax.Array\n    min_separation: jax.Array\n\n\n@functools.partial(jax.vmap, in_axes=(None, None, None, 0, 0, 0))\ndef correct_position(\n    bias_factor: float | jax.Array,\n    linear_slop: float | jax.Array,\n    max_linear_correction: float | jax.Array,\n    contact: Contact,\n    helper: ContactHelper,\n    solver: PositionSolver,\n) -&gt; PositionSolver:\n    \"\"\"\n    Correct positions to remove penetration.\n    Suppose that each shape in contact and helper has (N_contact, 1) or (N_contact, 2).\n    p1 and p2 should have xy: (1, 2) angle (1, 1) shape\n    \"\"\"\n    # (N_contact, 2)\n    r1 = solver.p1.rotate(helper.local_anchor1)\n    r2 = solver.p2.rotate(helper.local_anchor2)\n    ga2_ga1 = r2 - r1 + solver.p2.xy - solver.p1.xy\n    separation = jnp.dot(ga2_ga1, contact.normal) - contact.penetration\n    c = jnp.clip(\n        bias_factor * (separation + linear_slop),\n        a_min=-max_linear_correction,\n        a_max=0.0,\n    )\n    kn1 = _effective_mass(helper.inv_mass1, helper.inv_moment1, r1, contact.normal)\n    kn2 = _effective_mass(helper.inv_mass2, helper.inv_moment2, r2, contact.normal)\n    k_normal = kn1 + kn2\n    impulse = jnp.where(k_normal &gt; 0.0, -c / k_normal, jnp.zeros_like(c))\n    pn = impulse * contact.normal\n    p1 = Position(\n        angle=-helper.inv_moment1 * jnp.cross(r1, pn),\n        xy=-pn * helper.inv_mass1,\n    )\n    p2 = Position(\n        angle=helper.inv_moment2 * jnp.cross(r2, pn),\n        xy=pn * helper.inv_mass2,\n    )\n    min_sep = jnp.fmin(solver.min_separation, separation)\n    # Filter separation\n    p1, p2 = jax.tree_map(\n        lambda x: jnp.where(solver.contact, x, jnp.zeros_like(x)),\n        (p1, p2),\n    )\n    return solver.replace(p1=p1, p2=p2, min_separation=min_sep)\n\n\ndef fake_fori_loop(start, end, step, initial):\n    \"\"\"For debugging. Just replace jax.lax.fori_loop with this.\"\"\"\n    state = initial\n    for i in range(start, end):\n        state = step(i, state)\n    return state\n\n\ndef apply_seq_impulses(\n    space: Space,\n    solver: VelocitySolver,\n    p: Position,\n    v: Velocity,\n    contact: Contact,\n    a: Shape,\n    b: Shape,\n) -&gt; tuple[Velocity, Position, VelocitySolver]:\n    \"\"\"Resolve collisions by Sequential Impulse method\"\"\"\n    p1, p2 = tree_map2(generate_self_pairs, p)\n    v1, v2 = tree_map2(generate_self_pairs, v)\n    helper = init_contact_helper(space, contact, a, b, p1, p2, v1, v2)\n    solver = apply_initial_impulse(\n        contact,\n        helper,\n        solver.replace(v1=v1, v2=v2),\n    )\n\n    def vstep(\n        _n_iter: int,\n        vs: tuple[Velocity, VelocitySolver],\n    ) -&gt; tuple[Velocity, VelocitySolver]:\n        v_i, solver_i = vs\n        solver_i1 = apply_velocity_normal(contact, helper, solver_i)\n        v_i1 = _pv_gather(solver_i1.v1, solver_i1.v2, v_i) + v_i\n        v1, v2 = tree_map2(generate_self_pairs, v_i1)\n        return v_i1, solver_i1.replace(v1=v1, v2=v2)\n\n    v, solver = jax.lax.fori_loop(0, space.n_velocity_iter, vstep, (v, solver))\n    rest_v1, rest_v2 = apply_bounce(contact, helper, solver)\n    v = _pv_gather(rest_v1, rest_v2, v) + v\n\n    def pstep(\n        _n_iter: int,\n        ps: tuple[Position, PositionSolver],\n    ) -&gt; tuple[Position, PositionSolver]:\n        p_i, solver_i = ps\n        solver_i1 = correct_position(\n            space.bias_factor,\n            space.linear_slop,\n            space.max_linear_correction,\n            contact,\n            helper,\n            solver_i,\n        )\n        p_i1 = _pv_gather(solver_i1.p1, solver_i1.p2, p_i) + p_i\n        p1, p2 = tree_map2(generate_self_pairs, p_i1)\n        return p_i1, solver_i1.replace(p1=p1, p2=p2)\n\n    pos_solver = PositionSolver(\n        p1=p1,\n        p2=p2,\n        contact=solver.contact,\n        min_separation=jnp.zeros_like(p1.angle),\n    )\n    p, pos_solver = jax.lax.fori_loop(0, space.n_position_iter, pstep, (p, pos_solver))\n    return v, p, solver\n\n\nãªã‹ãªã‹è¤‡é›‘ã«ãªã‚Šã¾ã—ãŸãŒã€å®Ÿè£…ã§ãã¾ã—ãŸã€‚è¡çªã•ã›ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\nfrom celluloid import Camera\nfrom IPython.display import HTML\n\n\ndef animate_balls(\n    fig,\n    ax: Axes,\n    circles: Circle,\n    positions: Iterable[Position],\n) -&gt; HTML:\n    pos = list(positions)\n    camera = Camera(fig)\n    circle_list = circles.tolist()\n    for pi in pos:\n        for pij, circle in zip(pi.tolist(), circle_list):\n            circle_patch = CirclePatch(\n                xy=pij.xy,\n                radius=circle.radius,\n                fill=False,\n                color=circle.rgba.tolist(),\n            )\n            ax.add_patch(circle_patch)\n        camera.snap()\n    return HTML(camera.animate().to_jshtml())\n\n\n\n\nCode\nspace = Space(gravity=jnp.array([0.0, -9.8]), dt=0.04, bias_factor=0.2, circle=circles)\nstate = State(p=p, v=v, f=f, is_active=jnp.ones(N, dtype=bool))\npositions = [state.p]\nsolver = init_solver(N * (N - 1) // 2)\n\n\n@jax.jit\ndef step(state: State, solver: VelocitySolver) -&gt; tuple[State, VelocitySolver]:\n    state = update_velocity(space, space.circle, state)\n    contacts, c1, c2 = check_circle_to_circle(space, state.p, state.is_active)\n    v, p, solver = apply_seq_impulses(\n        space,\n        solver.update(contacts.penetration &gt;= 0),\n        state.p,\n        state.v,\n        contacts,\n        c1,\n        c2,\n    )\n    return update_position(space, state.replace(v=v, p=p)), solver\n\n\nfor i in range(30):\n    state, solver = step(state, solver)\n    positions.append(state.p)\nfig, ax = plt.subplots()\nax.set_aspect(\"equal\", adjustable=\"box\")\nax.set_xlim((-10, 10))\nax.set_ylim((-10, 10))\nanimate_balls(fig, ax, space.circle, positions)\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\næœ€åˆã®è¡çªã§è‹¥å¹²ã®ã‚ã‚Šã“ã¿ãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ãŒã€ä¸€å¿œå¤§ä¸ˆå¤«ãã†ã§ã™ã­ã€‚"
  },
  {
    "objectID": "posts/fast_2d_physics_in_jax_en.html#ç·šåˆ†",
    "href": "posts/fast_2d_physics_in_jax_en.html#ç·šåˆ†",
    "title": "Implement fast 2D physics simulation with Jax",
    "section": "ç·šåˆ†",
    "text": "ç·šåˆ†\nå††åŒå£«ã®è¡çªãŒå®Ÿè£…ã§ããŸã¨ã“ã‚ã§ã€æ¬¡ã¯å‡¸å¤šè§’å½¢ã®å®Ÿè£…â€¦ã¨è¨€ã„ãŸã„ã¨ã“ã‚ã§ã™ãŒã€ãã‚Œã¯å¾Œå›ã—ã«ã—ã¦ã€ã‚²ãƒ¼ãƒ ã«ã¯æ¬ ã‹ã›ãªã„ã€Œç·šåˆ†ã€ã‚’å®Ÿè£…ã—ã¦ã¿ã¾ã™ã€‚ç¾å®Ÿä¸–ç•Œã«ã¯ãã‚“ãªã‚‚ã®å­˜åœ¨ã—ãªã„ã®ã§ã™ãŒã€ã‚²ãƒ¼ãƒ ã‚„ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ä¸–ç•Œã§ã¯ã€ã©ã†ã—ã¦ã‚‚åœ°é¢ã‚„æŸµã¨ã„ã£ãŸå¢ƒç•Œã‚’è¡¨ç¾ã™ã‚‹å¿…è¦ãŒç”Ÿã˜ã¦ãã¾ã™ã€‚ã“ã†ã„ã£ãŸã‚‚ã®ã‚’ä¾‹ãˆã°ã€Œã‚ã¡ã‚ƒãã¡ã‚ƒé‡ã„é•·æ–¹å½¢ã€ã¨ã—ã¦è¡¨ç¾ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ãŒã€ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’çµ„ã‚€ãƒ¦ãƒ¼ã‚¶ãƒ¼å´ã‹ã‚‰ã™ã‚‹ã¨ã„ã¡ã„ã¡é•·æ–¹å½¢ã®å¤§ãã•ã ã£ãŸã‚Šã‚’å®šç¾©ã™ã‚‹ã®ã¯é¢å€’ãªã®ã§ã€ã€Œç„¡é™ã®è³ªé‡ã‚’æŒã¤ç·šã€ã¨ã—ã¦æ‰±ãˆãŸã»ã†ãŒæ¥½ã§ã™ã‚ˆã­ã€‚ã¨ã„ã†ã‚ã‘ã§ç·šåˆ†ã‚’å®Ÿè£…ã—ã¦ã¿ã¾ã™ã€‚ç·šåˆ†ã®è¡çªåˆ¤å®šã¯è–¬ã®ã‚«ãƒ—ã‚»ãƒ«ğŸ’Šã®ã‚ˆã†ã«ä¸¡ç«¯ãŒä¸¸ããªã£ã¦ã„ã‚‹ã‚„ã¤(Box2Dã ã¨ã‚«ãƒ—ã‚»ãƒ«ã¨å‘¼ã°ã‚Œã¦ã„ã‚‹ã®ã§ã‚«ãƒ—ã‚»ãƒ«ã¨å‘¼ã³ã¾ã™)ã¨å®Ÿè£…ãŒã»ã¼åŒã˜ãªã®ã§ã€ã‚«ãƒ—ã‚»ãƒ«ã‚‚ä¸€ç·’ã«å®Ÿè£…ã—ã¦ã—ã¾ã„ã¾ã™ã€‚ãŸã ã‚«ãƒ—ã‚»ãƒ«åŒå£«ã®è¡çªã¯é¢å€’ãªã®ã§ã€ã¨ã‚Šã‚ãˆãšå††ã¨ã‚«ãƒ—ã‚»ãƒ«ã ã‘å®Ÿè£…ã—ã¾ã—ã‚‡ã†ã€‚æ–°ã—ã„å›³å½¢ã‚’åŠ ãˆãŸã®ã§ã€Spaceã‚‚ä½œã‚Šç›´ã™å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã¨ã‚Šã‚ãˆãšdataclassã«å…¨ã‚·ã‚§ã‚¤ãƒ—ã‚’ã¤ã£ã“ã‚“ã§ãŠã„ã¦ã€å„ã‚·ã‚§ã‚¤ãƒ—ã®çµ„ã¿åˆã‚ã›ã”ã¨ã«è¡çªåˆ¤å®šã‚’è¡Œã„ã€è¡çªè§£æ±ºã®ã¨ãã¯jnp.concatenateã§å…¨éƒ¨ãã£ã¤ã‘ã¦ã‹ã‚‰vmapã§ä¸€åº¦ã«ã‚„ã‚‹ã¨ã„ã†å®Ÿè£…æ–¹é‡ã«ã—ã¾ã—ãŸã€‚ã•ã£ãã®ãƒšã‚¢ã«å¯¾ã™ã‚‹ãƒ«ãƒ¼ãƒ—ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ«ã™ã‚‹ã¨ã“ã‚ã¯ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒšã‚¢ã‚’æŒã£ã¦ãŠã„ã¦é©å½“ãªã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’è¶³ã—ã¦ãŠã‘ã°ãã®ã¾ã¾ä½¿ãˆã¾ã™ã€‚\n\n\nCode\nfrom matplotlib.patches import Rectangle\n\n@chex.dataclass\nclass Capsule(Shape):\n    length: jax.Array\n    radius: jax.Array\n\n\n@chex.dataclass\nclass Segment(Shape):\n    length: jax.Array\n\n    def to_capsule(self) -&gt; Capsule:\n        return Capsule(\n            mass=self.mass,\n            moment=self.moment,\n            elasticity=self.elasticity,\n            friction=self.friction,\n            rgba=self.rgba,\n            length=self.length,\n            radius=jnp.zeros_like(self.length),\n        )\n\n\ndef _length_to_points(length: jax.Array) -&gt; tuple[jax.Array, jax.Array]:\n    a = jnp.stack((length * -0.5, length * 0.0), axis=-1)\n    b = jnp.stack((length * 0.5, length * 0.0), axis=-1)\n    return a, b\n\n\n@jax.vmap\ndef _capsule_to_circle_impl(\n    a: Capsule,\n    b: Circle,\n    a_pos: Position,\n    b_pos: Position,\n    isactive: jax.Array,\n) -&gt; Contact:\n    # Move b_pos to capsule's coordinates\n    pb = a_pos.inv_transform(b_pos.xy)\n    p1, p2 = _length_to_points(a.length)\n    edge = p2 - p1\n    s1 = jnp.dot(pb - p1, edge)\n    s2 = jnp.dot(p2 - pb, edge)\n    in_segment = jnp.logical_and(s1 &gt;= 0.0, s2 &gt;= 0.0)\n    ee = jnp.sum(jnp.square(edge), axis=-1, keepdims=True)\n    # Closest point\n    # s1 &lt; 0: pb is left to the capsule\n    # s2 &lt; 0: pb is right to the capsule\n    # else: pb is in between capsule\n    pa = jax.lax.select(\n        in_segment,\n        p1 + edge * s1 / ee,\n        jax.lax.select(s1 &lt; 0.0, p1, p2),\n    )\n    a2b_normal, dist = normalize(pb - pa)\n    penetration = a.radius + b.radius - dist\n    a_contact = pa + a2b_normal * a.radius\n    b_contact = pb - a2b_normal * b.radius\n    pos = a_pos.transform((a_contact + b_contact) * 0.5)\n    xy_zeros = jnp.zeros_like(b_pos.xy)\n    a2b_normal_rotated = a_pos.replace(xy=xy_zeros).transform(a2b_normal)\n    # Filter penetration\n    penetration = jnp.where(isactive, penetration, jnp.ones_like(penetration) * -1)\n    return Contact(\n        pos=pos,\n        normal=a2b_normal_rotated,\n        penetration=penetration,\n        elasticity=(a.elasticity + b.elasticity) * 0.5,\n        friction=(a.friction + b.friction) * 0.5,\n    )\n\n\n@chex.dataclass\nclass ShapeDict:\n    circle: Circle | None = None\n    segment: Segment | None = None\n    capsule: Capsule | None = None\n    \n    def concat(self) -&gt; Shape:\n        shapes = [s.to_shape() for s in self.values() if s is not None]\n        return jax.tree_map(lambda *args: jnp.concatenate(args, axis=0), *shapes)\n\n\n@chex.dataclass\nclass StateDict:\n    circle: State | None = None\n    segment: State | None = None\n    capsule: State | None = None\n\n    def concat(self) -&gt; None:\n        states = [s for s in self.values() if s is not None]\n        return jax.tree_map(lambda *args: jnp.concatenate(args, axis=0), *states)\n\n    def offset(self, key: str) -&gt; int:\n        total = 0\n        for k, state in self.items():\n            if k == key:\n                return total\n            if state is not None:\n                total += state.p.batch_size()\n        raise RuntimeError(\"Unreachable\")\n        \n    def _get(self, name: str, state: State) -&gt; State | None:\n        if self[name] is None:\n            return None\n        else:\n            start = self.offset(name)\n            end = start + self[name].p.batch_size()\n            return state.get_slice(jnp.arange(start, end))\n        \n    def update(self, statec: State) -&gt; Self:\n        circle = self._get(\"circle\", statec)\n        segment = self._get(\"segment\", statec)\n        capsule = self._get(\"capsule\", statec)\n        return self.__class__(circle=circle, segment=segment, capsule=capsule)\n\n\nContactFn = Callable[[StateDict], tuple[Contact, Shape, Shape]]\n\n\ndef _pair_outer(x: jax.Array, reps: int) -&gt; jax.Array:\n    return jnp.repeat(x, reps, axis=0, total_repeat_length=x.shape[0] * reps)\n\n\ndef _pair_inner(x: jax.Array, reps: int) -&gt; jax.Array:\n    return jnp.tile(x, (reps,) + (1,) * (x.ndim - 1))\n\n\ndef generate_pairs(x: jax.Array, y: jax.Array) -&gt; tuple[jax.Array, jax.Array]:\n    \"\"\"Returns two arrays that iterate over all combination of elements in x and y\"\"\"\n    xlen, ylen = x.shape[0], y.shape[0]\n    return _pair_outer(x, ylen), _pair_inner(y, xlen)\n\n\ndef _circle_to_circle(\n    shaped: ShapeDict,\n    stated: StateDict,\n) -&gt; tuple[Contact, Circle, Circle]:\n    circle1, circle2 = tree_map2(generate_self_pairs, shaped.circle)\n    pos1, pos2 = tree_map2(generate_self_pairs, stated.circle.p)\n    is_active = jnp.logical_and(*generate_self_pairs(stated.circle.is_active))\n    contacts = _circle_to_circle_impl(\n        circle1,\n        circle2,\n        pos1,\n        pos2,\n        is_active,\n    )\n    return contacts, circle1, circle2\n\n\ndef _capsule_to_circle(\n    shaped: ShapeDict,\n    stated: StateDict,\n) -&gt; tuple[Contact, Capsule, Circle]:\n    capsule = jax.tree_map(\n        functools.partial(_pair_outer, reps=shaped.circle.mass.shape[0]),\n        shaped.capsule,\n    )\n    circle = jax.tree_map(\n        functools.partial(_pair_inner, reps=shaped.capsule.mass.shape[0]),\n        shaped.circle,\n    )\n    pos1, pos2 = tree_map2(generate_pairs, stated.capsule.p, stated.circle.p)\n    is_active = jnp.logical_and(\n        *generate_pairs(stated.capsule.is_active, stated.circle.is_active)\n    )\n    contacts = _capsule_to_circle_impl(\n        capsule,\n        circle,\n        pos1,\n        pos2,\n        is_active,\n    )\n    return contacts, capsule, circle\n\n\ndef _segment_to_circle(\n    shaped: ShapeDict,\n    stated: StateDict,\n) -&gt; tuple[Contact, Segment, Circle]:\n    segment = jax.tree_map(\n        functools.partial(_pair_outer, reps=shaped.circle.mass.shape[0]),\n        shaped.segment,\n    )\n    circle = jax.tree_map(\n        functools.partial(_pair_inner, reps=shaped.segment.mass.shape[0]),\n        shaped.circle,\n    )\n    pos1, pos2 = tree_map2(generate_pairs, stated.segment.p, stated.circle.p)\n    is_active = jnp.logical_and(\n        *generate_pairs(stated.segment.is_active, stated.circle.is_active)\n    )\n    contacts = _capsule_to_circle_impl(\n        segment.to_capsule(),\n        circle,\n        pos1,\n        pos2,\n        is_active,\n    )\n    return contacts, segment, circle\n\n\n_CONTACT_FUNCTIONS = {\n    (\"circle\", \"circle\"): _circle_to_circle,\n    (\"capsule\", \"circle\"): _capsule_to_circle,\n    (\"segment\", \"circle\"): _segment_to_circle,\n}\n\n\n@chex.dataclass\nclass ContactWithMetadata:\n    contact: Contact\n    shape1: Shape\n    shape2: Shape\n    outer_index: jax.Array\n    inner_index: jax.Array\n\n    def gather_p_or_v(\n        self,\n        outer: _PositionLike,\n        inner: _PositionLike,\n        orig: _PositionLike,\n    ) -&gt; _PositionLike:\n        xy_outer = jnp.zeros_like(orig.xy).at[self.outer_index].add(outer.xy)\n        angle_outer = jnp.zeros_like(orig.angle).at[self.outer_index].add(outer.angle)\n        xy_inner = jnp.zeros_like(orig.xy).at[self.inner_index].add(inner.xy)\n        angle_inner = jnp.zeros_like(orig.angle).at[self.inner_index].add(inner.angle)\n        return orig.__class__(angle=angle_outer + angle_inner, xy=xy_outer + xy_inner)\n\n\n@chex.dataclass\nclass ExtendedSpace:\n    gravity: jax.Array\n    shaped: ShapeDict\n    dt: jax.Array | float = 0.1\n    linear_damping: jax.Array | float = 0.95\n    angular_damping: jax.Array | float = 0.95\n    bias_factor: jax.Array | float = 0.2\n    n_velocity_iter: int = 8\n    n_position_iter: int = 2\n    linear_slop: jax.Array | float = 0.005\n    max_linear_correction: jax.Array | float = 0.2\n    allowed_penetration: jax.Array | float = 0.005\n    bounce_threshold: float = 1.0\n\n    def check_contacts(self, stated: StateDict) -&gt; ContactWithMetadata:\n        contacts = []\n        for (n1, n2), fn in _CONTACT_FUNCTIONS.items():\n            if stated[n1] is not None and stated[n2] is not None:\n                contact, shape1, shape2 = fn(self.shaped, stated)\n                len1, len2 = stated[n1].p.batch_size(), stated[n2].p.batch_size()\n                offset1, offset2 = stated.offset(n1), stated.offset(n2)\n                if n1 == n2:\n                    outer_index, inner_index = generate_self_pairs(jnp.arange(len1))\n                else:\n                    outer_index, inner_index = generate_pairs(\n                        jnp.arange(len1),\n                        jnp.arange(len2),\n                    )\n                contact_with_meta = ContactWithMetadata(\n                    contact=contact,\n                    shape1=shape1.to_shape(),\n                    shape2=shape2.to_shape(),\n                    outer_index=outer_index + offset1,\n                    inner_index=inner_index + offset2,\n                )\n                contacts.append(contact_with_meta)\n        return jax.tree_map(lambda *args: jnp.concatenate(args, axis=0), *contacts)\n    \n    def n_possible_contacts(self) -&gt; int:\n        n = 0\n        for n1, n2 in _CONTACT_FUNCTIONS.keys():\n            if self.shaped[n1] is not None and self.shaped[n2] is not None:\n                len1, len2 = len(self.shaped[n1].mass), len(self.shaped[n2].mass)\n                if n1 == n2:\n                    n += len1 * (len1 - 1) // 2\n                else:\n                    n += len1 * len2\n        return n\n\n\ndef animate_balls_and_segments(\n    fig,\n    ax: Axes,\n    circles: Circle,\n    segments: Segment,\n    c_pos: Iterable[Position],\n    s_pos: Position,\n) -&gt; HTML:\n    camera = Camera(fig)\n    circle_list = circles.tolist()\n    # Lower left\n    segment_ll = s_pos.transform(\n        jnp.stack((-segments.length * 0.5, jnp.zeros_like(segments.length)), axis=1)\n    )\n    for pi in c_pos:\n        for pij, circle in zip(pi.tolist(), circle_list):\n            circle_patch = CirclePatch(\n                xy=pij.xy,\n                radius=circle.radius,\n                fill=False,\n                color=circle.rgba.tolist(),\n            )\n            ax.add_patch(circle_patch)\n        for ll, pj, segment in zip(segment_ll, s_pos.tolist(), segments.tolist()):\n            rect_patch = Rectangle(\n                xy=ll,\n                width=segment.length,\n                angle=(pj.angle / jnp.pi).item() * 180,\n                height=0.1,\n            )\n            ax.add_patch(rect_patch)\n        camera.snap()\n    return HTML(camera.animate().to_jshtml())\n\ndef solve_constraints(\n    space: Space,\n    solver: VelocitySolver,\n    p: Position,\n    v: Velocity,\n    contact_with_meta: ContactWithMetadata,\n) -&gt; tuple[Velocity, Position, VelocitySolver]:\n    \"\"\"Resolve collisions by Sequential Impulse method\"\"\"\n    outer, inner = contact_with_meta.outer_index, contact_with_meta.inner_index\n\n    def get_pairs(p_or_v: _PositionLike) -&gt; tuple[_PositionLike, _PositionLike]:\n        return p_or_v.get_slice(outer), p_or_v.get_slice(inner)\n\n    p1, p2 = get_pairs(p)\n    v1, v2 = get_pairs(v)\n    helper = init_contact_helper(\n        space,\n        contact_with_meta.contact,\n        contact_with_meta.shape1,\n        contact_with_meta.shape2,\n        p1,\n        p2,\n        v1,\n        v2,\n    )\n    # Warm up the velocity solver\n    solver = apply_initial_impulse(\n        contact_with_meta.contact,\n        helper,\n        solver.replace(v1=v1, v2=v2),\n    )\n\n    def vstep(\n        _n_iter: int,\n        vs: tuple[Velocity, VelocitySolver],\n    ) -&gt; tuple[Velocity, VelocitySolver]:\n        v_i, solver_i = vs\n        solver_i1 = apply_velocity_normal(contact_with_meta.contact, helper, solver_i)\n        v_i1 = contact_with_meta.gather_p_or_v(solver_i1.v1, solver_i1.v2, v_i) + v_i\n        v1, v2 = get_pairs(v_i1)\n        return v_i1, solver_i1.replace(v1=v1, v2=v2)\n\n    v, solver = jax.lax.fori_loop(0, space.n_velocity_iter, vstep, (v, solver))\n    bv1, bv2 = apply_bounce(contact_with_meta.contact, helper, solver)\n    v = contact_with_meta.gather_p_or_v(bv1, bv2, v) + v\n\n    def pstep(\n        _n_iter: int,\n        ps: tuple[Position, PositionSolver],\n    ) -&gt; tuple[Position, PositionSolver]:\n        p_i, solver_i = ps\n        solver_i1 = correct_position(\n            space.bias_factor,\n            space.linear_slop,\n            space.max_linear_correction,\n            contact_with_meta.contact,\n            helper,\n            solver_i,\n        )\n        p_i1 = contact_with_meta.gather_p_or_v(solver_i1.p1, solver_i1.p2, p_i) + p_i\n        p1, p2 = get_pairs(p_i1)\n        return p_i1, solver_i1.replace(p1=p1, p2=p2)\n\n    pos_solver = PositionSolver(\n        p1=p1,\n        p2=p2,\n        contact=solver.contact,\n        min_separation=jnp.zeros_like(p1.angle),\n    )\n    p, pos_solver = jax.lax.fori_loop(0, space.n_position_iter, pstep, (p, pos_solver))\n    return v, p, solver\n\n\ndef dont_solve_constraints(\n    _space: Space,\n    solver: VelocitySolver,\n    p: Position,\n    v: Velocity,\n    _contact_with_meta: ContactWithMetadata,\n) -&gt; tuple[Velocity, Position, VelocitySolver]:\n    return v, p, solver\n\n\nN_SEG = 3\nsegments = Segment(\n    mass=jnp.ones(N_SEG) * jnp.inf,\n    moment=jnp.ones(N_SEG) * jnp.inf,\n    elasticity=jnp.ones(N_SEG) * 0.5,\n    friction=jnp.ones(N_SEG) * 1.0,\n    rgba=jnp.ones((N_SEG, 4)),\n    length=jnp.array([4 * jnp.sqrt(2), 4, 4 * jnp.sqrt(2)]),\n)\ncpos = jnp.array([[2, 2], [4, 3], [3, 6], [6, 5], [5, 7]], dtype=jnp.float32)\nstated = StateDict(\n    circle=State(\n        p=Position(xy=cpos, angle=jnp.zeros(N)),\n        v=Velocity.zeros(N),\n        f=Force.zeros(N),\n        is_active=jnp.array([True, True, True, True, True]),\n    ),\n    segment=State(\n        p=Position(\n            xy=jnp.array([[-2.0, 2.0], [2, 0], [6, 2]], dtype=jnp.float32),\n            angle=jnp.array([jnp.pi * 1.75, 0, jnp.pi * 0.25]),\n        ),\n        v=Velocity.zeros(N_SEG),\n        f=Force.zeros(N_SEG),\n        is_active=jnp.ones(N_SEG, dtype=bool),\n    ),\n)\nspace = ExtendedSpace(\n    gravity=jnp.array([0.0, -9.8]),\n    linear_damping=1.0,\n    angular_damping=1.0,\n    dt=0.04,\n    bias_factor=0.2,\n    n_velocity_iter=6,\n    shaped=ShapeDict(circle=circles, segment=segments),\n)\n\n\n@jax.jit\ndef step(stated: StateDict, solver: VelocitySolver) -&gt; StateDict:\n    state = update_velocity(space, space.shaped.concat(), stated.concat())\n    contact_with_meta = space.check_contacts(stated.update(state))\n    # Check there's any penetration\n    contacts = contact_with_meta.contact.penetration &gt;= 0\n    v, p, solver = jax.lax.cond(\n        jnp.any(contacts),\n        solve_constraints,\n        dont_solve_constraints,\n        space,\n        solver.update(contacts),\n        state.p,\n        state.v,\n        contact_with_meta,\n    )\n    statec = update_position(space, state.replace(v=v, p=p))\n    return stated.update(statec)\n\n\nã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã—ã¦ã¿ã¾ã™ã€‚\n\n\nCode\npositions = [stated[\"circle\"].p]\nsolver = init_solver(space.n_possible_contacts())\n\nfor i in range(50):\n    stated = step(stated, solver)\n    positions.append(stated[\"circle\"].p)\nfig, ax = plt.subplots()\nax.set_aspect(\"equal\", adjustable=\"box\")\nax.set_xlim((-10, 10))\nax.set_ylim((0, 10))\nanimate_balls_and_segments(\n    fig,\n    ax,\n    space.shaped[\"circle\"],\n    space.shaped[\"segment\"],\n    positions,\n    stated[\"segment\"].p,\n)\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nã¡ã‚‡ã£ã¨ã‚ã‚Šã“ã‚“ã§ã„ã¾ã™ãŒã¾ã‚ä¸€å¿œè¨ˆç®—ã§ãã¦ã¯ã„ãã†ã§ã™ã€‚ä¸€å¿œç°¡å˜ã«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã—ã¦ã¿ã¾ã™ã€‚\n\n\nCode\n%timeit step(stated, solver)\n\n\nãƒœãƒ¼ãƒ«ã®æ•°ãŒã¾ã å°‘ãªã„ã§ã™ãŒã€1ã‚¹ãƒ†ãƒƒãƒ—ã‚ãŸã‚Šç´„700ãƒã‚¤ã‚¯ãƒ­ç§’ã¨ã„ã†ã“ã¨ã§ã€ã‹ãªã‚Šé«˜é€Ÿã«ã§ããŸã®ã§ã¯ãªã„ã§ã—ã‚‡ã†ã‹ã€‚"
  },
  {
    "objectID": "posts/sandb-exercise-racetrack.html",
    "href": "posts/sandb-exercise-racetrack.html",
    "title": "Exercise 5.12 Racetrack from the Reinforcement Learning textbook",
    "section": "",
    "text": "Here I demonstrate the execise 5.12 of the textbook Reinforcement Learning: An Introduction by Richard Sutton and Andrew G. Barto, using both of planning method and Monte Carlo method. Basic knowledge of Python (&gt;= 3.7) and NumPy are assumed. Some konwledge of matplotlib and Python typing library also helps.\nContact: yuji.kanagawa@oist.jp"
  },
  {
    "objectID": "posts/sandb-exercise-racetrack.html#modeling-the-problem-in-code",
    "href": "posts/sandb-exercise-racetrack.html#modeling-the-problem-in-code",
    "title": "Exercise 5.12 Racetrack from the Reinforcement Learning textbook",
    "section": "Modeling the problem in code",
    "text": "Modeling the problem in code\nLetâ€™s start from writing the problem in code. What are important in this phase? Here, Iâ€™d like to emphasize the importantness of looking back at the definition of the environment. I.e., in reinforcement learning (RL), environments are modelled by Markov decision process (MDP), consisting of states, actions, transition function, and reward function. So first letâ€™s check the definition of states and actions in the problem statement. Itâ€™s (somehow) not very straightforward, but we can find &gt; In our simplified racetrack, the car is at one of a discrete set of grid positions, the cells in the diagram. The velocity is also discrete, a number of grid cells moved horizontally and vertically per time step.\nSo, a state consists of position and velocity of the car (a.k.a. agent). What about actions?\n\nThe actions are increments to the velocity components. Each may be changed by +1, âˆ’1, or 0 in each step, for a total of nine (\\(3 \\times 3\\)) actions.\n\nSo there are 9 actions for each direction (â†“â†™â†â†–â†‘â†—â†’â†˜ or no acceleration). Here, we can also notice that the total number of states is given by \\(\\textrm{Num. positions} \\times \\textrm{Num. choices of velocity}\\). And the texbook also says &gt; Both velocity components are restricted to be nonnegative and less than 5, and they cannot both be zero except at the starting line.\nSo there are 24 possible velocity at the non-starting positions:\n\n\nCode\nimport itertools\nlist(itertools.product(range(5), range(5)))[1:]\n\n\n[(0, 1),\n (0, 2),\n (0, 3),\n (0, 4),\n (1, 0),\n (1, 1),\n (1, 2),\n (1, 3),\n (1, 4),\n (2, 0),\n (2, 1),\n (2, 2),\n (2, 3),\n (2, 4),\n (3, 0),\n (3, 1),\n (3, 2),\n (3, 3),\n (3, 4),\n (4, 0),\n (4, 1),\n (4, 2),\n (4, 3),\n (4, 4)]\n\n\nAgain, number of states is given by (roughly) \\(24 \\times \\textrm{Num. positions}\\) and number of actions is \\(9\\). Sounds not very easy problem with many positions.\nSo, letâ€™s start the coding from representing the state and actions. There are multiple ways, but I prefer to NumPy array for representing everything.\nLetâ€™s consider a ASCII representation of the map (or track) like this:\n\n\nCode\nSMALL_TRACK = \"\"\"\n###      F\n##       F\n##       F\n#      ###\n#      ###\n#      ###\n#SSSSS####\n\"\"\"\n\n\nHere, S denotes a starting positiona, F denotes a finishing position, # denotes a wall, and  denotes a road. We have this track as a 2D NumPy Array, and encode agentâ€™s position as an index of this array.\n\n\nCode\nimport numpy as np\n\ndef ascii_to_array(ascii_track: str) -&gt; np.ndarray:\n    \"\"\"Convert the ascii (string) map to a NumPy array.\"\"\"\n\n    lines = [line for line in ascii_track.split(\"\\n\") if len(line) &gt; 0]\n    byte_lines = [list(bytes(line, encoding=\"utf-8\")) for line in lines]\n    return np.array(byte_lines, dtype=np.uint8)\n\ntrack = ascii_to_array(SMALL_TRACK)\nprint(track)\nposition = np.array([0, 0])\ntrack[tuple(position)] == int.from_bytes(b'#', \"big\")\n\n\n[[35 35 35 32 32 32 32 32 32 70]\n [35 35 32 32 32 32 32 32 32 70]\n [35 35 32 32 32 32 32 32 32 70]\n [35 32 32 32 32 32 32 35 35 35]\n [35 32 32 32 32 32 32 35 35 35]\n [35 32 32 32 32 32 32 35 35 35]\n [35 83 83 83 83 83 35 35 35 35]]\n\n\nTrue\n\n\nThen, agentâ€™s velocity and acceleration are also naturally represented by an array. And, we represent an action as an index of an array consisting of all possible acceleration vetors:\n\n\nCode\nnp.array(list(itertools.product([-1, 0, 1], [-1, 0, 1])))\n\n\narray([[-1, -1],\n       [-1,  0],\n       [-1,  1],\n       [ 0, -1],\n       [ 0,  0],\n       [ 0,  1],\n       [ 1, -1],\n       [ 1,  0],\n       [ 1,  1]])\n\n\nThe next step is to represent a transition function as a black box simulator. Note that we will visit another representation by transition matrix, but implementing the simulator is easier. Basically, the simulator should take an agentâ€™s action and current state, and then return the next state. Letâ€™s call this function step. However, letâ€™s make it return some other things to make the implementation easier. Reward function sounds fairly easy to implement given the agentâ€™s position. &gt; The rewards are âˆ’1 for each step until the car crosses the finish line.\nAlso, we have to handle the termination of the episode. &gt; Each episode begins in one of the randomly selected start states with both velocity components zero and ends when the car crosses the finish line.\nSo the resulting step function should return a tuple (state, reward, termination). The below cell contains my implementation of the simulator with matplotlib visualization. The step function is so complicated to handle the case where the agent goes through a wall, so readers are encouraged to just run their eyes through.\n\n\nCode\n# collapse-hide\n\nfrom typing import List, NamedTuple, Optional, Tuple\n\nfrom IPython.display import display\nfrom matplotlib import pyplot as plt\nfrom matplotlib.axes import Axes\nfrom matplotlib.colors import ListedColormap\n\n\nclass State(NamedTuple):\n    position: np.ndarray\n    velocity: np.ndarray\n\n\nclass RacetrackEnv:\n    \"\"\"Racetrack environment\"\"\"\n\n    EMPTY = int.from_bytes(b\" \", \"big\")\n    WALL = int.from_bytes(b\"#\", \"big\")\n    START = int.from_bytes(b\"S\", \"big\")\n    FINISH = int.from_bytes(b\"F\", \"big\")\n\n    def __init__(\n        self,\n        ascii_track: str,\n        noise_prob: float = 0.1,\n        seed: int = 0,\n    ) -&gt; None:\n        self._track = ascii_to_array(ascii_track)\n        self._max_height, self._max_width = self._track.shape\n        self._noise_prob = noise_prob\n        self._actions = np.array(list(itertools.product([-1, 0, 1], [-1, 0, 1])))\n        self._no_accel = 4\n        self._random_state = np.random.RandomState(seed=seed)\n        self._start_positions = np.argwhere(self._track == self.START)\n        self._state_indices = None\n        self._ax = None\n        self._agent_fig = None\n        self._arrow_fig = None\n        \n    def state_index(self, state: State) -&gt; int:\n        \"\"\"Returns a state index\"\"\"\n        (y, x), (vy, vx) = state\n        return y * self._max_width * 25 + x * 25 + vy * 5 + vx\n        \n\n    def _all_passed_positions(\n        self,\n        start: np.ndarray,\n        velocity: np.ndarray,\n    ) -&gt; Tuple[List[np.ndarray], bool]:\n        \"\"\"\n        List all positions that the agent passes over.\n        Here we assume that the y-directional velocity is already flipped by -1.\n        \"\"\"\n\n        maxv = np.max(np.abs(velocity))\n        if maxv == 0:\n            return [start], False\n        one_step_vector = velocity / maxv\n        pos = start + 0.0\n        traj = []\n        for i in range(maxv):\n            pos = pos + one_step_vector\n            ceiled = np.ceil(pos).astype(int)\n            if self._is_out(ceiled):\n                return traj, True\n            traj.append(ceiled)\n        # To prevent numerical issue\n        traj[-1] = start + velocity\n        return traj, False\n\n    def _is_out(self, position: np.ndarray) -&gt; bool:\n        \"\"\"Returns whether the given position is out of the map.\"\"\"\n        y, x = position\n        return y &lt; 0 or x &gt;= self._max_width\n\n    def step(self, state: State, action: int) -&gt; Tuple[State, float, bool]:\n        \"\"\"\n        Taking the current state and an agents' action, returns the next state,\n        reward and a boolean flag that indicates that the current episode terminates.\n        \"\"\"\n        position, velocity = state\n        if self._random_state.rand() &lt; self._noise_prob:\n            accel = self._actions[self._no_accel]\n        else:\n            accel = self._actions[action]\n        # velocity is clipped so that only â†‘â†’ directions are possible\n        next_velocity = np.clip(velocity + accel, a_min=0, a_max=4)\n        # If both of velocity is 0, cancel the acceleration\n        if np.sum(next_velocity) == 0:\n            next_velocity = velocity\n        # List up trajectory. y_velocity is flipped to adjust the coordinate system.\n        traj, went_out = self._all_passed_positions(\n            position,\n            next_velocity * np.array([-1, 1]),\n        )\n        passed_wall, passed_finish = False, False\n        for track in map(lambda pos: self._track[tuple(pos)], traj):\n            passed_wall |= track == self.WALL\n            passed_finish |= track == self.FINISH\n        if not passed_wall and passed_finish:  # Goal!\n            return State(traj[-1], next_velocity), 0, True\n        elif passed_wall or went_out:  # Crasshed to the wall or run outside\n            return self.reset(), -1.0, False\n        else:\n            return State(traj[-1], next_velocity), -1, False\n\n    def reset(self) -&gt; State:\n        \"\"\"Randomly assigns a start position of the agent.\"\"\"\n        n_starts = len(self._start_positions)\n        initial_pos_idx = self._random_state.choice(n_starts)\n        initial_pos = self._start_positions[initial_pos_idx]\n        initial_velocity = np.array([0, 0])\n        return State(initial_pos, initial_velocity)\n\n    def render(\n        self,\n        state: Optional[State] = None,\n        movie: bool = False,\n        ax: Optional[Axes] = None,\n    ) -&gt; Axes:\n        \"\"\"Render the map and (optinally) the agents' position and velocity.\"\"\"\n        if self._ax is None or ax is not None:\n            if ax is None:\n                _, ax = plt.subplots(1, 1, figsize=(8, 8))\n            ax.set_aspect(\"equal\")\n            ax.set_xticks([])\n            ax.set_yticks([])\n            # Map the track to one of [0, 1, 2, 3] to that simple colormap works\n            map_array = np.zeros_like(track)\n            symbols = [self.EMPTY, self.WALL, self.START, self.FINISH]\n            for i in range(track.shape[0]):\n                for j in range(track.shape[1]):\n                    map_array[i, j] = symbols.index(self._track[i, j])\n            cm = ListedColormap(\n                [\"w\", \".75\", \"xkcd:reddish orange\", \"xkcd:kermit green\"]\n            )\n            map_img = ax.imshow(\n                map_array,\n                cmap=cm,\n                vmin=0,\n                vmax=4,\n                alpha=0.8,\n            )\n            if ax.get_legend() is None:\n                descriptions = [\"Empty\", \"Wall\", \"Start\", \"Finish\"]\n                for i in range(1, 4):\n                    if np.any(map_array == i):\n                        ax.plot([0.0], [0.0], color=cm(i), label=descriptions[i])\n                ax.legend(fontsize=12, loc=\"lower right\")\n            self._ax = ax\n        if state is not None:\n            if not movie and self._agent_fig is not None:\n                self._agent_fig.remove()\n            if not movie and self._arrow_fig is not None:\n                self._arrow_fig.remove()\n            pos, vel = state\n            self._agent_fig = self._ax.plot(pos[1], pos[0], \"k^\", markersize=20)[0]\n            # Show velocity\n            self._arrow_fig = self._ax.annotate(\n                \"\",\n                xy=(pos[1], pos[0] + 0.2),\n                xycoords=\"data\",\n                xytext=(pos[1] - vel[1], pos[0] + vel[0] + 0.2),\n                textcoords=\"data\",\n                arrowprops={\"color\": \"xkcd:blueberry\", \"alpha\": 0.6, \"width\": 2},\n            )\n        return self._ax\n\n\nsmalltrack = RacetrackEnv(SMALL_TRACK)\nstate = smalltrack.reset()\nprint(state)\ndisplay(smalltrack.render(state=state).get_figure())\nnext_state, reward, termination = smalltrack.step(state, 7)\nprint(next_state)\nsmalltrack.render(state=next_state)\n\n\nState(position=array([6, 5]), velocity=array([0, 0]))\n\n\n\n\n\n\n\n\n\nState(position=array([5, 5]), velocity=array([1, 0]))\n\n\n\n\n\n\n\n\n\nNote that the vertical velocity is negative, so that we can simply represent the coordinate by an array index."
  },
  {
    "objectID": "posts/sandb-exercise-racetrack.html#solve-a-small-problem-by-dynamic-programming",
    "href": "posts/sandb-exercise-racetrack.html#solve-a-small-problem-by-dynamic-programming",
    "title": "Exercise 5.12 Racetrack from the Reinforcement Learning textbook",
    "section": "Solve a small problem by dynamic programming",
    "text": "Solve a small problem by dynamic programming\nOK, now we have a simulator, so letâ€™s solve the problem! However, before stepping into reinforcement learning, itâ€™s better to compute an optimal policy in a small problem for sanity check. To do so, we need a transition matrix p, which is a \\(|\\mathcal{S}| \\times |\\mathcal{A}| \\times |\\mathcal{S}|\\) NumPy array where p[i][j][k] is the probability of transiting from i to k when action j is taken. Also, we need a \\(|\\mathcal{S}| \\times |\\mathcal{A}| \\times |\\mathcal{S}|\\) reward matrix r. Note that this representation is not general as \\(R_t\\) can be stochastic, but since the only stochasticity of rewards is the noise to actions in this problem, this notion suffices. It is often not very straightforward to get p and r from the problem definition, but basically we need to give 0-indexd indices to each state (0, 1, 2, ...) and fill the array. Here, I index each state by \\(\\textrm{Idx(S)} = y \\times 25 \\times W + x \\times 25 + vy \\times 5 + vx\\), where \\((x, y)\\) is a position, \\((vx, vy\\)) is a velocity, and \\(W\\) is the width of the map.\n\n\nCode\n# collapse-hide\nfrom typing import Iterable\n\n\ndef get_p_and_r(env: RacetrackEnv) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Taking RacetrackEnv, returns the transition probability p and reward fucntion r of the env.\"\"\"\n    n_states = env._max_height * env._max_width * 25\n    n_actions = len(env._actions)\n    p = np.zeros((n_states, n_actions, n_states))\n    r = np.ones((n_states, n_actions, n_states)) * -1\n    noise = env._noise_prob\n\n    def state_prob(*indices):\n        \"\"\"Returns a |S| length zero-initialized array where specified elements are filled\"\"\"\n        prob = 1.0 / len(indices)\n        res = np.zeros(n_states)\n        for idx in indices:\n            res[idx] = prob\n        return res\n\n    # List up all states and memonize starting states\n    states = []\n    starting_states = []\n    for y in range(env._max_height):\n        for x in range(env._max_width):\n            track = env._track[y][x]\n            for y_velocity in range(5):\n                for x_velocity in range(5):\n                    state = State(np.array([y, x]), np.array([y_velocity, x_velocity]))\n                    states.append(state)\n                    if track == env.START:\n                        starting_states.append(env.state_index(state))\n\n    for state in states:\n        position, velocity = state\n        i = env.state_index(state)\n        track = env._track[tuple(position)]\n        # At a terminating state or unreachable, the agent cannot move\n        if (\n            track == env.FINISH\n            or track == env.WALL\n            or (np.sum(velocity) == 0 and track != env.START)\n        ):\n            r[i] = 0\n            p[i, :] = state_prob(i)\n        # Start or empty states\n        else:\n            # First, compute next state probs without noise\n            next_state_prob = []\n            for j, action in enumerate(env._actions):\n                next_velocity = np.clip(velocity + action, a_min=0, a_max=4)\n                if np.sum(next_velocity) == 0:\n                    next_velocity = velocity\n                traj, went_out = env._all_passed_positions(\n                    position,\n                    next_velocity * np.array([-1, 1]),\n                )\n                passed_wall, passed_finish = False, False\n                for track in map(lambda pos: env._track[tuple(pos)], traj):\n                    passed_wall |= track == env.WALL\n                    passed_finish |= track == env.FINISH\n                if passed_wall or (went_out and not passed_finish):\n                    #  Run outside or crasheed to the wall\n                    next_state_prob.append(state_prob(*starting_states))\n                else:\n                    next_state_idx = env.state_index(State(traj[-1], next_velocity))\n                    next_state_prob.append(state_prob(next_state_idx))\n                    if passed_finish:\n                        r[i, j, next_state_idx] = 0.0\n            # Then linearly mix the transition probs with noise\n            for j in range(n_actions):\n                p[i][j] = (\n                    noise * next_state_prob[env._no_accel]\n                    + (1.0 - noise) * next_state_prob[j]\n                )\n\n    return p, r\n\n\nThen, letâ€™s compute the optimal Q value by value iteration. So far, we only learned dynamic programming with discount factor \\(\\gamma\\), so letâ€™s use \\(\\gamma =0.95\\) that is sufficiently large for this small problem. \\(\\epsilon = 0.000001\\) is used as a convergence threshold. Letâ€™s show the elapsed time and the required number of iteration.\n\n\nCode\nimport datetime\n\n\nclass ValueIterationResult(NamedTuple):\n    q: np.ndarray\n    v: np.ndarray\n    elapsed: datetime.timedelta\n    n_iterations: int\n\n\ndef value_iteration(\n    p: np.ndarray,\n    r: np.ndarray,\n    discount: float,\n    epsilon: float = 1e-6,\n) -&gt; ValueIterationResult:\n    n_states, n_actions, _ = p.shape\n    q = np.zeros((n_states, n_actions))\n    v = np.zeros(n_states)\n    n_iterations = 0\n    start = datetime.datetime.now()\n    while True:\n        n_iterations += 1\n        v_old = v.copy()\n        for s in range(n_states):\n            # Q(s, a) = âˆ‘ p(s, a, s') * (r(s, a, s') + Î³ v(s')\n            for a in range(n_actions):\n                q[s, a] = np.dot(p[s, a], r[s, a] + discount * v)\n            # V(s) = max_a Q(s, a)\n            v[s] = np.max(q[s])\n        if np.linalg.norm(v - v_old, ord=np.inf) &lt; epsilon:\n            break\n    return ValueIterationResult(q, v, datetime.datetime.now() - start, n_iterations)\n\n\np, r = get_p_and_r(smalltrack)\nvi_result = value_iteration(p, r, discount=0.95)\nprint(f\"Elapsed: {vi_result.elapsed.total_seconds()} n_iter: {vi_result.n_iterations}\")\n\n\nElapsed: 4.235676 n_iter: 10\n\n\nIt took longer that a second on my laptop, even for this small problem. Some technical notes on value iteration (\\(R_\\textrm{max} = 1.0\\) is assumed for simplicity): - Each iteration takes \\(O(|\\mathcal{S}| ^ 2 |\\mathcal{A}|)\\) time - The required iteration number is bounded by \\(\\frac{\\log \\epsilon}{\\gamma - 1}\\) - In our case, \\(\\frac{\\log \\epsilon}{\\gamma - 1} \\approx 270\\), so our computation converged quicker than theory - Thus the total computation time is bounded by \\(O(|\\mathcal{S}| ^ 2 |\\mathcal{A}|\\frac{\\log \\epsilon}{\\gamma - 1})\\) - For a convergence threshold \\(\\epsilon\\), \\(\\max |V(s) - V^*(s)| &lt; \\frac{\\gamma\\epsilon}{1 - \\gamma}\\) is guranteed - In our case, \\(\\frac{\\gamma\\epsilon}{1 - \\gamma} \\approx 0.00002\\) - This is called relative error\nLetâ€™s visualize the V value and an optimal trajectory. celluloid) is used for making an animation.\n\n\nCode\nfrom typing import Callable\n\nfrom IPython.display import HTML\n\ntry:\n    from celluloid import Camera\nexcept ImportError as _e:\n    ! pip install celluloid --user\n    from celluloid import Camera\n\nPolicy = Callable[[int], int]\n\n\ndef smalltrack_optimal_policy(state_idx: int) -&gt; int:\n    return np.argmax(vi_result.q[state_idx])\n\n\ndef show_rollout(\n    env: RacetrackEnv,\n    policy: Policy,\n    v: np.ndarray = vi_result.v,\n    title: Optional[str] = None,\n) -&gt; HTML:\n    state = env.reset()\n    prev_termination = False\n    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n    camera = Camera(fig)\n    initial = True\n    while True:\n        env.render(state=state, movie=True, ax=ax)\n        state_idx = env.state_index(state)\n        ax.text(3, 0.5, f\"V(s): {v[state_idx]:02}\", c=\"red\")\n        camera.snap()\n        if prev_termination:\n            break\n        state, _, prev_termination = env.step(state, policy(state_idx))\n    if title is not None:\n        ax.text(3, 0.1, title, c=\"k\")\n    return HTML(camera.animate(interval=1000).to_jshtml())\n\n\nshow_rollout(smalltrack, smalltrack_optimal_policy)\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nThe computed optimal policy and value seems correct."
  },
  {
    "objectID": "posts/sandb-exercise-racetrack.html#monte-carlo-prediction",
    "href": "posts/sandb-exercise-racetrack.html#monte-carlo-prediction",
    "title": "Exercise 5.12 Racetrack from the Reinforcement Learning textbook",
    "section": "Monte-Carlo prediction",
    "text": "Monte-Carlo prediction\nThen letâ€™s try â€˜reinforcement learningâ€™. First, I implemeted â€˜First visit Monte-Carlo predictionâ€™, which evaluates a (Markovian) policy \\(\\pi\\) by doing a simulation multiple times, and calculates the average of received returns. Here, I evaluate the optimal policy \\(\\pi^*\\) obtained by value iteration.\n\n\nCode\n# collapse-hide\n\nfrom typing import Union\n\n\ndef first_visit_mc_prediction(\n    policy: Policy,\n    env: RacetrackEnv,\n    n_episodes: int,\n    discount: float = 0.95,\n    record_all_values: bool = False,\n) -&gt; Tuple[np.ndarray, List[np.ndarray]]:\n    \"\"\"Predict value function corresponding to the policy by First-visit MC prediction\"\"\"\n    n_states = env._max_width * env._max_height * 25\n    v = np.zeros(n_states)\n    all_values = []\n    # Note that we have to have a list of returns for each state!\n    # So the maximum memory usage would be Num.States x Num.Episodes\n    all_returns = [[] for _ in range(n_states)]\n    for i in range(n_episodes):\n        if record_all_values:\n            all_values.append(v.copy())\n        state = env.reset()\n        visited_states = [env.state_index(state)]\n        received_rewards = []\n        # Rollout the policy until the episode ends\n        while True:\n            # Sample an action from the policy\n            action = policy(env.state_index(state))\n            # Step the simulator\n            state, reward, termination = env.step(state, action)\n            visited_states.append(env.state_index(state))\n            received_rewards.append(reward)\n            if termination:\n                break\n        # Compute return\n        traj_len = len(received_rewards)\n        returns = np.zeros(traj_len)\n        # Gt = Rt when t = T\n        returns[-1] = received_rewards[-1]\n        # Iterating from T - 2, T - 1, ..., to 0\n        for t in reversed(range(traj_len - 1)):\n            # Gt = Rt + Î³Gt+1\n            returns[t] = received_rewards[t] + discount * returns[t + 1]\n        updated = set()\n        # Update the value\n        for i, state in enumerate(visited_states[: -1]):\n            # If the state is already visited, skip it\n            if state in updated:\n                continue\n            updated.add(state)\n            all_returns[state].append(returns[i].item())\n            # V(St) â† average(Returns(St))\n            v[state] = np.mean(all_returns[state])\n    return v, all_values\n\n\nv, all_values = first_visit_mc_prediction(\n    smalltrack_optimal_policy,\n    smalltrack,\n    1000,\n    record_all_values=True,\n)\nvalue_diff = []\nfor i, mc_value in enumerate(all_values + [v]):\n    value_diff.append(np.mean(np.abs(mc_value - vi_result.v)))\nplt.plot(value_diff)\nplt.xlabel(\"Num. Episodes\")\nplt.ylabel(\"Avg. |V* - V|\")\n\n\nText(0, 0.5, 'Avg. |V* - V|')\n\n\n\n\n\n\n\n\n\nIt looks like that the difference between \\(V^*\\) and the value function estimated by MC prediction converges after 600 steps, but itâ€™s still larger than \\(0\\), because \\(\\pi^*\\) doesnâ€™t visit all states. Letâ€™s plot the difference between value functions only on starting states.\n\n\nCode\nstart_states = []\nfor x in range(1, 6):\n    idx = smalltrack.state_index(State(position=np.array([6, x]), velocity=np.array([0, 0])))\n    start_states.append(idx)\nstart_values = []\nfor i, mc_value in enumerate(all_values + [v]):\n    start_values.append(np.mean(np.abs(mc_value - vi_result.v)[start_states]))\nplt.plot(start_values)\nplt.xlabel(\"Num. Episodes\")\nplt.ylabel(\"|V*(start) - V(start)| only on possible states\")\nplt.ylim((0.0, 0.5))\n\n\n\n\n\n\n\n\n\nHere, we can confirm that the estimated value certainly converged close to 0.0, while fractuating a bit. Note that the magnitude of fractuation is larger than the relative error we allowed for value iteration (\\(0.00002\\)), implying the difficulty of convergence."
  },
  {
    "objectID": "posts/sandb-exercise-racetrack.html#monte-carlo-control",
    "href": "posts/sandb-exercise-racetrack.html#monte-carlo-control",
    "title": "Exercise 5.12 Racetrack from the Reinforcement Learning textbook",
    "section": "Monte-Carlo Control",
    "text": "Monte-Carlo Control\nNow we successfully estimate \\(V^\\pi\\) using Monte Carlo method, so then letâ€™s try to learn a sub-optimal \\(\\pi\\) directly using Monte Carlo method. In the textbook, three methods are introduced: - Monte Carlo ES (Exploring Starts) - On-policy first visit Monte Carlo Control - Off-policy first visit Monte Carlo Control\nHere, letâ€™s try all three methods and compare the resulting value functions. However, we cannot naively implement the pseudo code in the textbook, due to a â€˜loopâ€™ problem. Since the car that crashed into the wall is returned to a starting point, the episode length can be infinitte depending on a policy. As a remedy for this problem, I limit the length of the episode as \\(H\\). Supposing that we ignore the future rewards smaller than \\(\\epsilon\\), how to set \\(H\\)? Just by solving \\(\\gamma^H R &lt; \\epsilon\\), we get \\(H &gt; \\frac{\\log \\epsilon}{\\log \\gamma}\\), which is about \\(270\\) in case \\(\\gamma = 0.95\\) and \\(\\epsilon = 0.000001\\).\nBelow are the implementation of three methods. A few notes about implementation: - Monte Carlo ES requires a set of all possible states, which is implemented in valid_states function. - For On-Policy MC, \\(\\epsilon\\) is decreased from 0.5 to 0.01 - We can use arbitary policy in Off-Policy MC, but I used the same \\(\\epsilon\\)-soft policy as On-Policy MC.\n\n\nCode\n# collapse-hide\n\ndef valid_states(env: RacetrackEnv) -&gt; List[State]:\n    states = []\n    for y in range(env._max_height):\n        for x in range(env._max_width):\n            track = env._track[y][x]\n            if track == env.WALL:\n                continue\n            for y_velocity in range(5):\n                for x_velocity in range(5):\n                    state = State(np.array([y, x]), np.array([y_velocity, x_velocity]))\n                    if track != env.START and (x_velocity &gt; 0 or y_velocity &gt; 0):\n                        states.append(state)\n    return states\n\n\ndef mc_es(\n    env: RacetrackEnv,\n    n_episodes: int,\n    discount: float = 0.95,\n    record_all_values: bool = False,\n    seed: int = 999,\n) -&gt; Tuple[np.ndarray, List[np.ndarray]]:\n    \"\"\"Monte-Carlo Control with Exploring Starts\"\"\"\n    n_states = env._max_width * env._max_height * 25\n    n_actions = len(env._actions)\n    random_state = np.random.RandomState(seed=seed)\n    q = np.zeros((n_states, n_actions))\n    pi = random_state.randint(9, size=n_states)\n    all_values = []\n    all_returns = [[[] for _ in range(n_actions)] for _ in range(n_states)]\n    possible_starts = valid_states(env)\n    max_episode_length = int(np.ceil(np.log(1e-6) / np.log(discount)))\n    for i in range(n_episodes):\n        if record_all_values:\n            all_values.append(q.copy())\n        state = possible_starts[random_state.choice(len(possible_starts))]\n        visited_states = [env.state_index(state)]\n        taken_actions = []\n        received_rewards = []\n        initial = True\n        for _ in range(max_episode_length):\n            if initial:\n                # Randomly sample the first action\n                action = random_state.randint(9)\n                initial = False\n            else:\n                # Take an action following the policy\n                action = pi[env.state_index(state)]\n            taken_actions.append(action)\n            # Step the simulator\n            state, reward, termination = env.step(state, action)\n            visited_states.append(env.state_index(state))\n            received_rewards.append(reward)\n            if termination:\n                break\n        # Compute return\n        traj_len = len(received_rewards)\n        returns = np.zeros(traj_len)\n        # Gt = Rt when t = T\n        returns[-1] = received_rewards[-1]\n        # Iterating from T - 2, T - 1, ..., to 0\n        for t in reversed(range(traj_len - 1)):\n            # Gt = Rt + Î³Gt+1\n            returns[t] = received_rewards[t] + discount * returns[t + 1]\n        updated = set()\n        # Update the value\n        for i, (state, action) in enumerate(zip(visited_states[:-1], taken_actions)):\n            # If the state is already visited, skip it\n            if (state, action) in updated:\n                continue\n            updated.add((state, action))\n            all_returns[state][action].append(returns[i].item())\n            # Q(St, At) â† average(Returns(St, At))\n            q[state, action] = np.mean(all_returns[state][action])\n            pi[state] = np.argmax(q[state])\n    return q, all_values\n\n\ndef on_policy_fist_visit_mc(\n    env: RacetrackEnv,\n    n_episodes: int,\n    discount: float = 0.95,\n    epsilon: float = 0.1,\n    epsilon_final: float = 0.1,\n    record_all_values: bool = False,\n    seed: int = 999,\n) -&gt; Tuple[np.ndarray, List[np.ndarray]]:\n    \"\"\"On-policy first visit Monte-Carlo\"\"\"\n\n    n_states = env._max_width * env._max_height * 25\n    n_actions = len(env._actions)\n    random_state = np.random.RandomState(seed=seed)\n    q = np.zeros((n_states, n_actions))\n    pi = random_state.randint(9, size=n_states)\n    all_values = []\n    all_returns = [[[] for _ in range(n_actions)] for _ in range(n_states)]\n    possible_starts = valid_states(env)\n    max_episode_length = int(np.ceil(np.log(1e-6) / np.log(discount)))\n    epsilon_decay = (epsilon - epsilon_final) / n_episodes\n    for i in range(n_episodes):\n        if record_all_values:\n            all_values.append(q.copy())\n        state = env.reset()\n        visited_states = [env.state_index(state)]\n        taken_actions = []\n        received_rewards = []\n        for _ in range(max_episode_length):\n            # Îµ-soft policy\n            if random_state.rand() &lt; epsilon:\n                action = random_state.randint(9)\n            else:\n                action = pi[env.state_index(state)]\n            taken_actions.append(action)\n            # Step the simulator\n            state, reward, termination = env.step(state, action)\n            visited_states.append(env.state_index(state))\n            received_rewards.append(reward)\n            if termination:\n                break\n        # Below code is the same as mc_es\n        # Compute return\n        traj_len = len(received_rewards)\n        returns = np.zeros(traj_len)\n        # Gt = Rt when t = T\n        returns[-1] = received_rewards[-1]\n        # Iterating from T - 2, T - 1, ..., to 0\n        for t in reversed(range(traj_len - 1)):\n            # Gt = Rt + Î³Gt+1\n            returns[t] = received_rewards[t] + discount * returns[t + 1]\n        updated = set()\n        # Update the value\n        for i, (state, action) in enumerate(zip(visited_states[:-1], taken_actions)):\n            # If the state is already visited, skip it\n            if (state, action) in updated:\n                continue\n            updated.add((state, action))\n            all_returns[state][action].append(returns[i].item())\n            # Q(St, At) â† average(Returns(St, At))\n            q[state, action] = np.mean(all_returns[state][action])\n            pi[state] = np.argmax(q[state])\n        epsilon -= epsilon_decay\n    return q, all_values\n\n\ndef off_policy_mc(\n    env: RacetrackEnv,\n    n_episodes: int,\n    discount: float = 0.95,\n    record_all_values: bool = False,\n    epsilon: float = 0.1,\n    epsilon_final: float = 0.1,\n    seed: int = 999,\n) -&gt; Tuple[np.ndarray, List[np.ndarray]]:\n    \"\"\"Off-policy MC control\"\"\"\n\n    n_states = env._max_width * env._max_height * 25\n    n_actions = len(env._actions)\n    random_state = np.random.RandomState(seed=seed)\n    q = np.zeros((n_states, n_actions))\n    c = np.zeros_like(q)\n    pi = np.argmax(q, axis=1)\n    all_values = []\n    possible_starts = valid_states(env)\n    max_episode_length = int(np.ceil(np.log(1e-6) / np.log(discount)))\n    epsilon_decay = (epsilon - epsilon_final) / n_episodes\n    for i in range(n_episodes):\n        if record_all_values:\n            all_values.append(q.copy())\n        state = env.reset()\n        visited_states = [env.state_index(state)]\n        taken_actions = []\n        received_rewards = []\n        acted_optimally = []\n        for _ in range(max_episode_length):\n            # Îµ-soft policy\n            if random_state.rand() &lt; epsilon:\n                action = random_state.randint(9)\n            else:\n                action = pi[env.state_index(state)]\n            acted_optimally.append(action == pi[env.state_index(state)])\n            taken_actions.append(action)\n            # Step the simulator\n            state, reward, termination = env.step(state, action)\n            visited_states.append(env.state_index(state))\n            received_rewards.append(reward)\n            if termination:\n                break\n        g = 0\n        w = 1.0\n        for i, (state, action) in enumerate(zip(visited_states[:-1], taken_actions)):\n            g = discount * g + received_rewards[i]\n            c[state, action] += w\n            q[state, action] += w / c[state, action] * (g - q[state, action])\n            pi[state] = np.argmax(q[state])\n            if action == pi[state]:\n                break\n            else:\n                if acted_optimally[i]:\n                    w *= 1.0 - epsilon + epsilon / n_actions\n                else:\n                    w *= epsilon / n_actions\n        epsilon -= epsilon_decay\n    return q, all_values\n\n\nmces_result = mc_es(smalltrack, 3000, record_all_values=True)\non_mc_result = on_policy_fist_visit_mc(\n    smalltrack,\n    3000,\n    epsilon=0.5,\n    epsilon_final=0.01,\n    record_all_values=True,\n)\noff_mc_result = off_policy_mc(\n    smalltrack,\n    3000,\n    epsilon=0.5,\n    epsilon_final=0.01,\n    record_all_values=True,\n)\n\n\nLetâ€™s plot the results. Here I plotted the difference from the optimal value function and the number of states that the policy choices the optimal action.\n\n\nCode\ndef value_diff(q_values: List[np.ndarray]) -&gt; List[float]:\n    diffs = []\n    for i, q in enumerate(q_values):\n        diff = np.abs(np.max(q, axis=-1) - vi_result.v)[start_states]\n        diffs.append(np.mean(diff))\n    return diffs\n\n\ndef n_optimal_actions(q_values: List[np.ndarray]) -&gt; List[int]:\n    n_optimal = []\n    optimal_actions = np.argmax(vi_result.q, axis=-1)\n    for i, q in enumerate(q_values):\n        greedy = np.argmax(q, axis=-1)\n        n_optimal.append(np.sum(greedy == optimal_actions))\n    return n_optimal\n\n\n_, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\nmc_es_value_diff = value_diff([mces_result[0]] + mces_result[1])\non_mc_value_diff = value_diff([on_mc_result[0]] + on_mc_result[1])\noff_mc_value_diff = value_diff([off_mc_result[0]] + off_mc_result[1])\nax1.plot(mc_es_value_diff, label=\"MC-ES\")\nax1.plot(on_mc_value_diff, label=\"On-Policy\")\nax1.plot(off_mc_value_diff, label=\"Off-Policy\")\nax1.set_xlabel(\"Num. Episodes\")\nax1.set_ylabel(\"Avg. |V* - V|\")\nax1.set_title(\"Diff. from V*\")\nmc_es_nopt = n_optimal_actions([mces_result[0]] + mces_result[1])\non_mc_nopt =  n_optimal_actions([on_mc_result[0]] + on_mc_result[1])\noff_mc_nopt =  n_optimal_actions([off_mc_result[0]] + off_mc_result[1])\nax1.legend(fontsize=12, loc=\"upper right\")\nax2.plot(mc_es_nopt, label=\"MC-ES\")\nax2.plot(on_mc_nopt, label=\"On-Policy\")\nax2.plot(off_mc_nopt, label=\"Off-Policy\")\nax2.set_xlabel(\"Num. Episodes\")\nax2.set_ylabel(\"Num. Optimal Actions\")\nax2.legend(fontsize=12, loc=\"upper right\")\nax2.set_title(\"Num. of optimal actions\")\n\n\nText(0.5, 1.0, 'Num. of optimal actions')\n\n\n\n\n\n\n\n\n\nSome observations from the results: - On-Policy MC converges to the optimal policy the fastest, though the convergence of its value function is the slowest - MC-ES struggles to distinguish optimal and non-optimal actions at some states, probably because of the lack of exploration during an episode. - Compared to MC-ES and On-Policy MC, the peak of value differences of Off-Policy MC is much milder. - A randomly initialized policy is often caught in a loop and cannot reach to the goal. The value of such a policy is really small (\\(-1  -1 * 0.95 - 1 * 0.95^2 - ... \\approx -20\\)). However, Off-Policy MC uses important sampling to decay the rewards by uncertain actions, resulting the smaller value differences.\nHere I visualized sampled plays from all three methods. On-Policy MC looks the most efficient.\n\n\nCode\nfor q, name in zip([mces_result[0], on_mc_result[0], off_mc_result[0]], [\"MC-ES\", \"On-Policy\", \"Off-Policy\"]):\n    display(show_rollout(smalltrack, lambda i: np.argmax(q[i]), np.argmax(q, axis=-1), name))\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html",
    "href": "posts/understanding-what-makes-rl-difficult.html",
    "title": "ã‚ˆã‚Šè‰¯ã„å•é¡Œè¨­è¨ˆã¸å‘ã‘ã¦ï¼š ä½•ãŒå¼·åŒ–å­¦ç¿’ã‚’é›£ã—ãã™ã‚‹ã®ã‹ã‚’ç†è§£ã—ã‚ˆã†",
    "section": "",
    "text": "å¼·åŒ–å­¦ç¿’è‹¦æ‰‹ã®ä¼š Advent Calendar 2020 20æ—¥ç›® # 1. ã¯ã˜ã‚ã«\nå¼·åŒ–å­¦ç¿’ã¯ã€é€æ¬¡çš„ã«æ„æ€æ±ºå®šã™ã‚‹å•é¡Œã‚’å®šç¾©ã™ã‚‹ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚ å¾“æ¥ã¯å¤§å¤‰ã ã£ãŸ1 ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®è¨“ç·´ãŒç°¡å˜ã«ãªã£ãŸã“ã¨ã‚„ã€ Alpha GOãªã©æ·±å±¤å¼·åŒ–å­¦ç¿’(Deep RL)ã®æˆåŠŸã‚’èƒŒæ™¯ã«ã€å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ãƒ»çµŒæ¸ˆ ãªã©ã€æ§˜ã€…ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§å¼·åŒ–å­¦ç¿’ã®åˆ©ç”¨ãŒè©¦ã¿ã‚‰ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«æ€ã„ã¾ã™ã€‚ åƒ•å€‹äººã¨ã—ã¦ã‚‚ã€å¼·åŒ–å­¦ç¿’ã¯æ±ç”¨çš„ã§é¢ç™½ã„ãƒ„ãƒ¼ãƒ«ã ã¨æ€ã†ã®ã§ã€å°†æ¥çš„ã«ã¯è‰²ã€…ãªå¿œç”¨åˆ†é‡ã§åºƒãæ´»ç”¨ã•ã‚Œã‚‹ã¨ã„ã„ãªã€ã¨æ€ã„ã¾ã™ã€‚\nä¸€æ–¹ã§ã€å¼·åŒ–å­¦ç¿’ã‚’ä½•ã‹ç‰¹å®šã®å•é¡Œã«å¿œç”¨ã—ã¦ã¿ã‚ˆã†ã€ã¨ã„ã†å ´é¢ã§ã¯ã€ ãã®æ±ç”¨æ€§ã‚†ãˆã‹ãˆã£ã¦ã¨ã£ã¤ãã«ãã„ãƒ»æ‰±ã„ã¥ã‚‰ã„é¢ãŒã‚ã‚‹ã‚ˆã†ã«æ€ã„ã¾ã™ã€‚ å®Ÿéš›ã«ã€è‹¦æ‰‹ã®ä¼šãªã©ã§å¿œç”¨ç ”ç©¶ã‚’ã•ã‚Œã¦ã„ã‚‹æ–¹ã‹ã‚‰ã€ - å•é¡Œã‚’å®šç¾©ã™ã‚‹ã®ãŒãã‚‚ãã‚‚å¤§å¤‰ - è‰²ã€…ãªæ‰‹æ³•ãŒã‚ã£ã¦ã€ä½•ãŒãªã‚“ã ã‹ã‚ˆãã‚ã‹ã‚‰ãªã„\nãªã©ã®æ„è¦‹ã‚’è¦³æ¸¬ã§ãã¾ã—ãŸã€‚\nã§ã¯å¿œç”¨ç ”ç©¶ã«å¯¾ã™ã‚‹ã€Œãƒ„ãƒ¼ãƒ«ã€ã¨ã—ã¦å¼·åŒ–å­¦ç¿’ã‚’æ‰±ã†ä¸Šã§ä½•ãŒå¤§äº‹ãªã®ã ã‚ã†ã€ã¨è€ƒãˆãŸã¨ãã€ åƒ•ã¯ç°¡å˜ãªå•é¡Œã‚’è¨­è¨ˆã™ã‚‹ã“ã¨ã“ããŒå¤§äº‹ã ã¨ã„ã†ä»®èª¬ã«æ€ã„ã„ãŸã‚Šã¾ã—ãŸã€‚ ç°¡å˜ãªå•é¡Œã‚’è¨­è¨ˆã™ã‚‹ãŸã‚ã«ã¯ã€å¼·åŒ–å­¦ç¿’ã®ä¸­ã§ã‚‚ã©ã†ã„ã†å•é¡ŒãŒé›£ã—ã„ã®ã‹ã€ ã¨ã„ã†ã“ã¨ã‚’ãã¡ã‚“ã¨ç†è§£ã—ã¦ãŠãå¿…è¦ãŒã‚ã‚‹ã‚ˆã†ã«æ€ã„ã¾ã™ã€‚\nãã“ã§ã“ã®ãƒ–ãƒ­ã‚°è¨˜äº‹ã§ã¯ã€å¼·åŒ–å­¦ç¿’ã®ä¸­ã§ã‚‚ã€Œé›£ã—ã„å•é¡Œã€ãŒã©ã†ã„ã†ã‚‚ã®ãªã®ã‹ã€ ãã†ã„ã†å•é¡Œã¯ãªãœé›£ã—ã„ã®ã‹ã«ã¤ã„ã¦ã€ä¾‹ã‚’é€šã—ã¦ãªã‚‹ã¹ãç›´æ„Ÿçš„ã«èª¬æ˜ã™ã‚‹ã“ã¨ã‚’è©¦ã¿ã¾ã™ã€‚ å¼·åŒ–å­¦ç¿’ã®é›£ã—ã•ãŒã‚ã‹ã£ãŸæšã«ã¯ã€ãã£ã¨ - ãã‚‚ãã‚‚å¼·åŒ–å­¦ç¿’ã‚’ä½¿ã‚ãªã„ã¨ã„ã†é¸æŠãŒã§ãã‚‹ã—ã€ - ãªã‚‹ã¹ãç°¡å˜ã«è§£ã‘ã‚‹ã‚ˆã†ãªå¼·åŒ–å­¦ç¿’ã®å•é¡Œã‚’è¨­è¨ˆã§ãã‚‹ã—ã€ - å•é¡Œã«åˆã‚ã›ã¦æ‰‹æ³•ã‚’é¸æŠã§ãã‚‹\nã“ã¨ã§ã—ã‚‡ã†ã€‚\nè¨˜äº‹ã®æ§‹æˆã¨ã—ã¦ã€å¼·åŒ–å­¦ç¿’ã®é›£ã—ã•ã«ã¤ã„ã¦ã€Œå ´åˆåˆ†ã‘ã€ã‚’è¡Œã„ã€ - MDPã‚’è§£ãã“ã¨ã®é›£ã—ã• - ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã™ã‚‹ã“ã¨ã®é›£ã—ã•\nã¨ã„ã†2ã¤ã®è¦³ç‚¹ã‹ã‚‰æ•´ç†ã—ã¦ã„ãã¾ã™ã€‚\nå‰æçŸ¥è­˜ã«ã¤ã„ã¦ã€åˆå¿ƒè€…ã®æ–¹ã§ã‚‚èª­ã‚ã‚‹ã‚ˆã†ã«ã€ å¼·åŒ–å­¦ç¿’ã«ã¤ã„ã¦ã®çŸ¥è­˜ã«ã¤ã„ã¦ã¯ãªã‚‹ã¹ãè¨˜äº‹ã®ä¸­ã§è£œè¶³ã—ã¾ã™ã€‚ ã—ã‹ã—ã€ã™ã”ãé›‘ã«æ›¸ãã®ã§ã€è©³ç´°ã¯Reinforcement Learning: An Introduction ãªã©ã®æ•™ç§‘æ›¸ã‚’å‚ç…§ã•ã‚Œã‚‹ã¨ã„ã„ã¨æ€ã„ã¾ã™ã€‚ ã¾ãŸã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’è¦‹ãŸã»ã†ãŒã‚¤ãƒ¡ãƒ¼ã‚¸ã—ã‚„ã™ã„ï¼ˆæ–¹ã‚‚ã„ã‚‹ï¼‰ã‹ã¨æ€ã£ã¦Pythonã®ã‚³ãƒ¼ãƒ‰ä¾‹ã‚’ãŸã¾ã«å‡ºã—ã¦ã„ã¾ã™ã€‚ ã‚³ãƒ¼ãƒ‰ä¾‹ã§ã¯ã€\\(\\sum_{s} f(s) g(s, a)\\)ã®ã‚ˆã†ã«ãƒ†ãƒ³ã‚½ãƒ«ã®é©å½“ãªè»¸ã§æ›ã‘ç®—ã—ã¦è¶³ã—è¾¼ã‚€æ¼”ç®—ã« numpy.einsum ã‚’å¤šç”¨ã—ã¦ã„ã‚‹ã®ã§ã€çŸ¥ã£ã¦ã„ãŸã»ã†ãŒèª­ã¿ã‚„ã™ã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html#vpicdot-vã‚’è©•ä¾¡ã™ã‚‹é›£ã—ã•",
    "href": "posts/understanding-what-makes-rl-difficult.html#vpicdot-vã‚’è©•ä¾¡ã™ã‚‹é›£ã—ã•",
    "title": "ã‚ˆã‚Šè‰¯ã„å•é¡Œè¨­è¨ˆã¸å‘ã‘ã¦ï¼š ä½•ãŒå¼·åŒ–å­¦ç¿’ã‚’é›£ã—ãã™ã‚‹ã®ã‹ã‚’ç†è§£ã—ã‚ˆã†",
    "section": "2.1: \\(V^\\pi\\cdot V^*\\)ã‚’è©•ä¾¡ã™ã‚‹é›£ã—ã•",
    "text": "2.1: \\(V^\\pi\\cdot V^*\\)ã‚’è©•ä¾¡ã™ã‚‹é›£ã—ã•\nã§ã¯ã€ã‚‚ã†å°‘ã—é›£ã—ã„MDPã‚’è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\nmdp3 = ChainMDP(\n    [[1.0, 0.0], [0.8, 1.0], [1.0, 0.9]], [[0.0, 0.0], [0.5, 0.0], [0.0, 1.0]]\n)\n_ = mdp3.show(\"MDP3\")\n\n\n\n\n\n\n\n\n\nä»Šåº¦ã¯ã€State 1ã§å³ã«ã€State 2ã§å·¦ã«è¡Œã‘ã°è‰¯ã•ãã†ã§ã™ã€‚ \\[\n\\begin{aligned}\nV^* (1) = 0.5 + \\gamma (0.1 * V^*(1) + 0.9 * V^*(2)) \\\\\nV^* (2) = 1.0 + \\gamma (0.8 * V^*(1) + 0.2 * V^*(2))\n\\end{aligned}\n\\]\nå…ˆã»ã©ã®å•é¡Œã¨é•ã£ã¦1ã‚‚2ã‚‚å¸å¼•çŠ¶æ…‹ã§ã¯ãªã„ã®ã§ã€\\(V(1)\\)ã¨\\(V(2)\\)ãŒãŠäº’ã„ã«ä¾å­˜ã™ã‚‹é¢å€’ãª æ–¹ç¨‹å¼ãŒå‡ºã¦ãã¦ã—ã¾ã„ã¾ã—ãŸã€‚ ã“ã®ã‚ˆã†ãªãƒ«ãƒ¼ãƒ—ã®å­˜åœ¨ãŒã€å¼·åŒ–å­¦ç¿’ã‚’é›£ã—ãã—ã¦ã„ã‚‹è¦ç´ ã®ä¸€ã¤ã§ã™ã€‚\nã¨ã¯ã„ãˆã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã§æ•°å€¤çš„ã«è§£ãã®ã¯ç°¡å˜ã§ã™ã€‚ çŠ¶æ…‹\\(s\\)ã«ã„ã¦ã€ã‚ã¨\\(n\\)å›è¡Œå‹•ã§ãã‚‹æ™‚ã®ä¾¡å€¤é–¢æ•°ã‚’\\(V_n^\\pi(s)\\)ã¨æ›¸ãã¾ã™ã€‚ ä»»æ„ã®\\(s\\)ã«ã¤ã„ã¦ã€\\(V_0^\\pi(s) = 0\\)ã§ã™ï¼ˆ1å›ã‚‚è¡Œå‹•ã§ããªã„ã®ã§!ï¼‰ã€‚ \\(V_i^\\pi\\) ã‹ã‚‰ \\(V_{i + 1}^\\pi\\) ã‚’æ±‚ã‚ã‚‹ã«ã¯ã€1ã‚¹ãƒ†ãƒƒãƒ—ã ã‘å…ˆèª­ã¿ã™ã‚Œã°ã„ã„ã®ã§ã€ \\[\nV_{i + 1}^\\pi(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s) \\left( r(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s'|s, a) V_{i}^\\pi(s') \\right)\n\\] ã§è¨ˆç®—ã§ãã¾ã™ã€‚\\(\\gamma &lt; 1\\)ã«ã‚ˆã‚Šã“ã®åå¾©è¨ˆç®—ã¯åæŸã—ã€\\(V^\\pi\\) ãŒæ±‚ã¾ã‚Šã¾ã™ã€‚ å®Ÿéš›ã«ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§æ›¸ã„ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\nMAX_ITER_V_PI: int = int(1e5)\n\n\ndef v_pi(\n    r: Array2,\n    p: Array3,\n    pi: Array2,\n    gamma: float,\n    epsilon: float,\n) -&gt; Tuple[Array1, int]:\n    v = np.zeros(r.shape[0])  # VÏ€\n    r_pi = np.einsum(\"sa,sa-&gt;s\", pi, r)  # |S|, Ï€ã‚’ä½¿ã£ãŸã¨ãã«è²°ã†å ±é…¬ã®ãƒ™ã‚¯ãƒˆãƒ«\n    p_pi = np.einsum(\"saS,sa-&gt;sS\", p, pi)  # |S| x |S|, Ï€ã‚’ä½¿ã£ãŸã¨ãã®çŠ¶æ…‹é·ç§»ç¢ºç‡\n    for n_iter in range(MAX_ITER_V_PI):\n        v_next = r_pi + gamma * np.einsum(\"s,sS\", v, p_pi.T)\n        if np.all(np.absolute(v_next - v) &lt; epsilon):\n            return v_next, n_iter + 1\n        v = v_next\n    # ç†è«–çš„ã«ã¯å¿…ãšåæŸã™ã‚‹ã®ã§ã€ãƒã‚°äºˆé˜²\n    raise RuntimeError(\"Policy Evaluation did not converge &gt;_&lt;\")\n\n\npi_star = np.array([[1.0, 0.0], [1.0, 0.0], [0.0, 1.0]])\nv_star_mdp3, n_iter = v_pi(mdp3.r, mdp3.p, pi_star, gamma=0.9, epsilon=1e-4)\nprint(f\"åå¾©å›æ•°: {n_iter}\")\nprint(\" \".join([f\"V({i}): {v:.3}\" for i, v in enumerate(v_star_mdp3)]))\n\n\nåå¾©å›æ•°: 86\nV(0): 6.49 V(1): 7.21 V(2): 7.51\n\n\n86å›ã“ã®è¨ˆç®—ã‚’åå¾©ã—ãŸå¾Œã€ãªã‚“ã‹ãã‚Œã£ã½ã„æ•°å­—ãŒå‡ºã¦ãã¾ã—ãŸã€‚ ã“ã®åå¾©å›æ•°ã¯ã€ä½•ã«ã‚ˆã£ã¦æ±ºã¾ã‚‹ã®ã§ã—ã‚‡ã†ã‹ï¼Ÿ\nä»»æ„ã® \\(s\\) ã«ã¤ã„ã¦ \\(|V_{i+1}^\\pi(s) - V_i^\\pi(s)| &lt; \\epsilon\\) ãªã‚‰è¨ˆç®—çµ‚ã‚ã‚Šã€ã¨ã—ã¾ã™4ã€‚ \\(V_n^\\pi(s)\\)ã¯ã€Œã‚ã¨\\(n\\)ã‚¹ãƒ†ãƒƒãƒ—è¡Œå‹•ã§ãã‚‹æ™‚ã®çŠ¶æ…‹ä¾¡å€¤ã®æœŸå¾…å€¤ã€ãªã®ã§ã€\\(i\\) ã‚¹ãƒ†ãƒƒãƒ—ç›®ã«ã‚‚ã‚‰ã£ãŸå ±é…¬ã‚’ \\(R_i\\)ã¨ã™ã‚‹ã¨ã€ \\[\nV_n^\\pi(s) = \\mathbb{E}_{s, \\pi} \\left[ R_1 + \\gamma R_2 + \\gamma^2 R_3 + ... \\right]\n\\] ã¨æ›¸ã‘ã¾ã™ã€‚ ãªã®ã§ã€å ±é…¬ã®ç¯„å›²ãŒ\\(0 \\leq R_t &lt; R_\\textrm{max}\\)ã ã¨ä»®å®šã™ã‚‹ã¨ã€ \\(\\gamma^{k - 1} R_\\textrm{max} &lt; \\epsilon\\)ãªã‚‰ã“ã®æ•°å€¤è¨ˆç®—ãŒåæŸã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ ç°¡å˜ã®ãŸã‚\\(R_\\textrm{max}=1\\)ã¨ã—ã¦ã¿ã‚‹ã¨ã€\\(k\\)ãŒæº€ãŸã™ã¹ãæ¡ä»¶ã¯ \\[\n\\gamma^{k-1} &lt; \\epsilon\n\\Leftrightarrow\nk &lt; \\frac{\\log\\epsilon}{\\log\\gamma} + 1\n\\] ã¨ãªã‚Šã¾ã™ã€‚ ã‚³ãƒ¼ãƒ‰ã®ä¸­ã§ \\(\\gamma = 0.9, \\epsilon = 0.0001\\) ã¨ã—ãŸã®ã§ã€ãŸã‹ã ã‹89å›ã®åå¾©ã§åæŸã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ å®Ÿé¨“çµæœã§ã¯86å›ã ã£ãŸã®ã§ã€ã ã„ãŸã„åŒã˜ãã‚‰ã„ã§ã™ã­ã€‚\nã‚ˆã£ã¦ã€\\(V^\\pi\\)ã‚’åå¾©æ³•ã«ã‚ˆã‚Šè©•ä¾¡ã—ãŸæ™‚ã€ãã®åå¾©å›æ•°ã¯å ±é…¬ãƒ»\\(\\epsilon\\)ãƒ»\\(\\gamma\\)ã«ä¾å­˜ã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸã€‚ å ±é…¬ã¨\\(\\epsilon\\)ã«ã¯\\(\\log\\)ã®ã‚ªãƒ¼ãƒ€ãƒ¼ã§ã—ã‹ä¾å­˜ã—ãªã„ã®ã«å¯¾ã—ã€\\(\\gamma\\)ã«å¯¾ã—ã¦ã¯ \\(O((-\\log\\gamma)^{-1})\\)ã®ã‚ªãƒ¼ãƒ€ãƒ¼ã§ä¾å­˜ã—ã¦ã„ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ è©¦ã—ã«ã€\\(\\epsilon=0.0001\\)ã®æ™‚ã®\\(\\frac{\\log\\epsilon}{\\log\\gamma}\\)ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\n#hide_input\n_, ax = plt.subplots(1, 1)\nx = np.logspace(-0.1, -0.001, 1000)\ny = np.log(1e-4) / np.log(x)\nax.set_xlabel(\"Î³\", fontsize=16)\nax.set_ylabel(\"log(Îµ)/log(Î³)\", fontsize=16)\n_ = ax.plot(x, y, \"b-\", lw=3, alpha=0.7)\n\n\n\n\n\n\n\n\n\nã“ã®ã‚ˆã†ã«ã€\\(\\gamma\\)ãŒå¤§ãããªã‚‹ã¨ä¸€æ°—ã«åå¾©å›æ•°ãŒå¢—ãˆã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ ã¾ãŸã€ã“ã®æ•°å€¤è¨ˆç®—ãŒåæŸã—ãŸæ™‚ã€çœŸã®\\(V^\\pi\\)ã¨ã®å·®ãŒ \\[\n\\begin{aligned}\nV^\\pi(s) - V_k^\\pi(s) &= \\mathbb{E}_{s, \\pi} \\left[ \\gamma^k R_{k + 1} + \\gamma^{k + 1} R_{k + 2} ... \\right] \\\\\n&&lt; \\frac{\\gamma^k R_\\textrm{max}}{1 - \\gamma} &lt; \\frac{\\gamma \\epsilon}{1 - \\gamma}\n\\end{aligned}\n\\] ã§æŠ‘ãˆã‚‰ã‚Œã‚‹ã“ã¨ã‚‚ã‚ã‹ã‚Šã¾ã™ã€‚\næ¬¡ã¯ã€ã„ããªã‚Š\\(V^*\\)ã‚’æ±‚ã‚ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ \\(V^\\pi\\)ã‚’æ±‚ã‚ãŸæ™‚ã¨åŒã˜ã‚ˆã†ã«ã€çŠ¶æ…‹\\(s\\)ã«ã„ã¦ã€ ã‚ã¨\\(n\\)å›è¡Œå‹•ã§ãã‚‹æ™‚ã®æœ€é©ä¾¡å€¤é–¢æ•°ã‚’\\(V_n^*(s)\\)ã¨æ›¸ãã¾ã™ã€‚ å…ˆã»ã©ã¨åŒæ§˜ã«ã€\\(V_i^*\\)ã‹ã‚‰1ã‚¹ãƒ†ãƒƒãƒ—å…ˆèª­ã¿ã—ã¦\\(V^*_{i + 1}\\)ã‚’æ±‚ã‚ã¾ã™ã€‚ æ®‹ã‚Š\\(i + 1\\)ã‚¹ãƒ†ãƒƒãƒ—ã‚ã‚‹æ™‚ã€ \\(r(s, a) + \\sum_{s' \\in \\mathcal{S}} P(s'|s, a) V_i^*(s)\\) ãŒæœ€å¤§ã«ãªã‚‹ã‚ˆã†ãªè¡Œå‹•\\(a\\)ã‚’é¸ã¶ã®ãŒæœ€é©ã§ã™ã€‚ ã§ã™ã‹ã‚‰ã€\\(V^*_{i + 1}\\)ã¯ \\[\nV_{i + 1}^*(s) = \\max_a \\left( r(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s'|s, a) V_{i}^\\pi(s') \\right)\n\\] ã§æ±‚ã‚ã‚‰ã‚Œã¾ã™ã€‚ ã•ã£ãã‚ˆã‚Šç°¡å˜ãªå¼ã«ãªã£ã¦ã—ã¾ã„ã¾ã—ãŸã€‚ ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§æ›¸ã„ã¦ã¿ã¾ã™ã€‚\n\n\nCode\nMAX_ITER_VI: int = int(1e6)\n\n\ndef value_iteration(\n    r: Array2,\n    p: Array3,\n    gamma: float,\n    epsilon: float,\n) -&gt; Tuple[Array1, int]:\n    v = np.zeros(p.shape[0])\n    for n_iter in range(MAX_ITER_VI):\n        # ã“ã‚Œâ†“ã¯Q Valueã¨ã‚‚è¨€ã„ã¾ã™\n        r_plus_gamma_pv = r + gamma * np.einsum(\"saS,S-&gt;sa\", p, v)\n        v_next = r_plus_gamma_pv.max(axis=1)\n        if np.all(np.absolute(v_next - v) &lt; epsilon):\n            return v_next, n_iter + 1\n        v = v_next\n    raise RuntimeError(\"Value Iteration did not converge &gt;_&lt;\")\n\n\nv_star_mdp3_vi, n_iter = value_iteration(mdp3.r, mdp3.p, 0.9, 1e-4)\nprint(f\"åå¾©å›æ•°: {n_iter}\")\nprint(\" \".join([f\"V({i}): {v:.3}\" for i, v in enumerate(v_star_mdp3_vi)]))\n\n\nåå¾©å›æ•°: 86\nV(0): 6.49 V(1): 7.21 V(2): 7.51\n\n\nå…ˆç¨‹ã¨åŒã˜ãã€86å›ã®åå¾©ã§\\(V^*\\)ãŒæ±‚ã¾ã‚Šã¾ã—ãŸã€‚ ã“ã®åå¾©å›æ•°ã‚‚ã€å…ˆã»ã©ã®\\(V^\\pi\\)ã¨åŒã˜ã‚ˆã†ã«\\(\\gamma,\\epsilon\\)ã‚’ç”¨ã„ã¦æŠ‘ãˆã‚‰ã‚Œã¾ã™ã€‚\nã—ã‹ã—ã€\\(\\epsilon\\)ã¯äººæ‰‹ã§è¨­å®šã™ã‚‹ãƒ‘ãƒ©ãƒ¡ã‚¿ã§ã™ã€‚ æœ€é©æ–¹ç­–ãŒæ±‚ã¾ã‚Œã°\\(V^*\\)ã¯å¤§ã—ã¦æ­£ç¢ºã§ãªãã¨ã‚‚å›°ã‚‰ãªã„ã¨ã„ã†å ´åˆã¯ã€ã‚‚ã£ã¨\\(\\epsilon\\)ã‚’å¤§ããã—ã¦ã€ è¨ˆç®—ã‚’æ—©ãçµ‚ã‚ã‚‰ã›ãŸã„æ°—ãŒã—ã¾ã™ã€‚ ã§ã¯ã€ã€Œã©ã‚“ãªå ´åˆã«\\(\\epsilon\\)ã‚’å¤§ããã§ãã‚‹ã‹ã€ã‚’è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚\nç°¡å˜ã®ãŸã‚ã€ \\(Q^\\pi(s, a) = r(s, a) + \\gamma \\sum_{s' in \\mathcal{S}} P(s'|s, a) V^\\pi(s')\\) (Qã¯Qualityã®Qã‚‰ã—ã„)ã‚’å°å…¥ã—ã¾ã™ã€‚ æ®‹ã‚Š\\(k\\)ã‚¹ãƒ†ãƒƒãƒ—ã‚ã‚‹æ™‚ã®æœ€é©è¡Œå‹•ã‚’\\(a_k^* = \\textrm{argmax}_a Q_k^*(s, a)\\)ã¨ã—ã¾ã™ã€‚ ã™ã‚‹ã¨ã€\\(k+1\\)ã‚¹ãƒ†ãƒƒãƒ—ç›®ä»¥é™ã®å‰²å¼•å ±é…¬å’Œã¯ \\(\\frac{\\gamma^{k}R_\\textrm{max}}{1 -\\gamma}\\)ã§æŠ‘ãˆã‚‰ã‚Œã‚‹ã®ã§ã€ \\[\nQ_k^*(s, a_k^*) - \\max_{a \\neq a_k^*} Q_k^*(s, a) &lt; \\frac{\\gamma^k R_\\textrm{max}}{1 -\\gamma}\n\\] ãŒæˆã‚Šç«‹ã¤ãªã‚‰ã€\\(a_k^*\\)ãŒä»Šå¾Œä»–ã®è¡Œå‹•ã«é€†è»¢ã•ã‚Œã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ ãªã®ã§\\(a_k^*\\)ãŒæœ€é©ã§ã„ã„ã‚ˆã­ã€ã¯ã„ã“ã®è©±çµ‚ã‚ã‚Šã€ã¨ã„ã†ã“ã¨ã«ãªã‚Šã¾ã™ã€‚ ä»¥ä¸‹ç•¥è¨˜ã—ã¦ \\(A_\\textrm{min}^*(s, a_k) = Q_k^*(s, a_k^*) - \\max_{a \\neq a_k^*} Q_k^*(s, a)\\) ã¨æ›¸ãã¾ã™ï¼ˆä»–ã®è¡Œå‹•ã«å¯¾ã™ã‚‹ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ã®æœ€å°å€¤ã¨ã„ã†æ„å‘³ï¼‰ã€‚ ã„ã¾\\(\\gamma^{k-1} R_\\textrm{max}&lt;\\epsilon\\)ãŒçµ‚äº†æ¡ä»¶ãªã®ã§ã€ \\[\nA_\\textrm{min}^*(s, a_k) &lt; \\frac{\\epsilon\\gamma}{1 -\\gamma}\n\\Leftrightarrow\n\\frac{A_\\textrm{min}^*(s, a_k)(1 - \\gamma)}{\\gamma}&lt; \\epsilon\n\\] ãŒæˆã‚Šç«‹ã¡ã¾ã™ã€‚ ã“ã‚ŒãŒæ„å‘³ã™ã‚‹ã®ã¯ã€\\(V*\\)ã¨äºŒç•ªç›®ã«ã„ã„\\(Q^*(s, a)\\)ã¨ã®å·®ãŒå¤§ãã„ã»ã©\\(\\epsilon\\)ã‚’å¤§ããã§ãã‚‹ã¨ã„ã†ã“ã¨ã§ã™ã€‚\nã“ã“ã¾ã§ã®è­°è«–ã‹ã‚‰ã€è¨ˆç®—é‡ã®è¦³ç‚¹ã§ã¯ã€ - \\(\\gamma\\)ãŒå¤§ãã„ã»ã©MDPã‚’è§£ãã®ãŒé›£ã—ã„ - æœ€é©è§£ã¨äºŒç•ªç›®ã«ã„ã„è§£ã¨ã®å·®ãŒå°ã•ã„ã»ã©MDPã‚’è§£ãã®ãŒé›£ã—ã„\nã¨ã„ã†2ç‚¹ãŒè¨€ãˆãã†ã§ã™ã­ã€‚"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html#æ–¹ç­–æœ€é©åŒ–ã®é›£ã—ã•",
    "href": "posts/understanding-what-makes-rl-difficult.html#æ–¹ç­–æœ€é©åŒ–ã®é›£ã—ã•",
    "title": "ã‚ˆã‚Šè‰¯ã„å•é¡Œè¨­è¨ˆã¸å‘ã‘ã¦ï¼š ä½•ãŒå¼·åŒ–å­¦ç¿’ã‚’é›£ã—ãã™ã‚‹ã®ã‹ã‚’ç†è§£ã—ã‚ˆã†",
    "section": "2.2: æ–¹ç­–æœ€é©åŒ–ã®é›£ã—ã•",
    "text": "2.2: æ–¹ç­–æœ€é©åŒ–ã®é›£ã—ã•\nå‰ç¯€ã§ç”¨ã„ãŸå†å¸°çš„ãªæ•°å€¤è¨ˆç®—ã¯å‹•çš„è¨ˆç”»æ³•(DP)ã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã§ã™ã€‚ Qå­¦ç¿’ãªã©ã€å¤šãã®å¼·åŒ–å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒDPã‚’ã‚‚ã¨ã«ã—ã¦ã„ã¾ã™ã€‚ ä¸€æ–¹ã§ã€å˜ã«å¼·åŒ–å­¦ç¿’ã‚’ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹æœ€é©åŒ–ã ã¨è€ƒãˆã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ ç‰¹ã«ã€æ–¹ç­–ãƒ‘ãƒ©ãƒ¡ã‚¿\\(\\theta\\)ã‚’æœ€é©åŒ–ã—ã¦è§£ãæ–¹æ³•ã‚’æ–¹ç­–æœ€é©åŒ–ã¨å‘¼ã³ã¾ã™ã€‚\nã„ã¾ã€\\(\\pi(0|s) = \\theta(s), \\pi(1|s) = 1.0 - \\theta(s)\\)ã«ã‚ˆã£ã¦\\(\\pi\\)ã‚’ãƒ‘ãƒ©ãƒ¡ã‚¿\\(\\theta\\)ã«ã‚ˆã‚Šè¡¨ã™ã“ã¨ã«ã—ã¾ã™ ï¼ˆã“ã‚Œã‚’direct parameterizationã¨å‘¼ã³ã¾ã™ï¼‰ã€‚ ãŸã‚ã—ã«ã€å…ˆã»ã©ã®MDP3ã§\\(\\pi(0|0)=1.0\\)ã‚’å›ºå®šã—ã¦ã€\\(\\theta(1), \\theta(2)\\)ã‚’å‹•ã‹ã—ãŸæ™‚ã®\\(\\sum_{s \\in \\mathcal{S}} V^\\pi(s)\\)ã®å¤‰å‹•ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\ndef v_pi_sum_2dim(\n    r: Array2,\n    p: Array3,\n    gamma: float,\n    epsilon: float,\n    initial_pi: Array2,\n    states: Tuple[int, int],\n    n_discretization: int,\n) -&gt; Array2:\n    res = []\n    for i2 in range(n_discretization + 1):\n        p2 = (1.0 / n_discretization) * i2\n        for i1 in range(n_discretization + 1):\n            p1 = (1.0 / n_discretization) * i1\n            pi = initial_pi.copy()\n            pi[states[0]] = p1, 1 - p1\n            pi[states[1]] = p2, 1 - p2\n            res.append(v_pi(r, p, pi, gamma, epsilon)[0].sum())\n    return np.array(res).reshape(n_discretization + 1, -1)\n\n\ndef plot_piv_heatmap(\n    data: Array2,\n    xlabel: str = \"\",\n    ylabel: str = \"\",\n    title: str = \"\",\n    ax: Optional[Axes] = None,\n) -&gt; Axes:\n    from matplotlib.ticker import LinearLocator\n\n    if ax is None:\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection=\"3d\")\n    n_discr = data.shape[0]\n    x, y = np.meshgrid(np.linspace(0, 1, n_discr), np.linspace(0, 1, n_discr))\n    ax.plot_surface(x, y, data, cmap=\"inferno\", linewidth=0, antialiased=False)\n    ax.zaxis.set_major_locator(LinearLocator(10))\n    ax.zaxis.set_major_formatter('{x:.01f}')\n    ax.set_xlabel(xlabel, fontsize=14)\n    ax.set_ylabel(ylabel, fontsize=14)\n    ax.set_zlabel(\"âˆ‘VÏ€\", fontsize=14, horizontalalignment=\"right\")\n    ax.set_title(title, fontsize=15)\n    return ax\n\n\ninitial_pi = np.array([[1.0, 0.0], [0.5, 0.5], [0.5, 0.5]])\nv_pi_sums = v_pi_sum_2dim(mdp3.r, mdp3.p, 0.9, 1e-4, initial_pi, (1, 2), 20)\nax = plot_piv_heatmap(v_pi_sums, \"Î¸(1)\", \"Î¸(2)\", \"MDP3\")\n_ = ax.set_xlim(tuple(reversed(ax.get_xlim())))\n_ = ax.set_ylim(tuple(reversed(ax.get_ylim())))\n\n\n\n\n\n\n\n\n\nãªã‚“ã‹ã„ã„æ„Ÿã˜ã«å±±ã«ãªã£ã¦ã„ã¾ã™ã­ã€‚ ã“ã®å•é¡Œã®å ´åˆã¯ã€å±±ç™»ã‚Šæ³•ï¼ˆå‹¾é…ä¸Šæ˜‡æ³•ï¼‰ã§\\(\\theta\\)ã‚’æ›´æ–°ã—ã¦ã„ã‘ã°å¤§åŸŸè§£ \\(\\theta(1) = 0.0, \\theta(2) = 1.0\\)ã«ãŸã©ã‚Šç€ããã†ã§ã™5ã€‚\nã—ã‹ã—ã€\\(f(\\theta) = \\sum_{s\\in\\mathcal{S}} V^{\\pi_\\theta}(s)\\)ã¯ã€ã„ã¤ã‚‚ã“ã®ã‚ˆã†ãªæ€§è³ª ã®ã„ã„é–¢æ•°ã«ãªã£ã¦ã„ã‚‹ã®ã§ã—ã‚‡ã†ãŒï¼Ÿ çµè«–ã‹ã‚‰è¨€ã†ã¨ãã†ã§ã¯ãªã„ã§ã™ã€‚ ä¾‹ãˆã°ã€ä»¥ä¸‹ã®ã‚ˆã†ãªMDPã§ã¯ã©ã†ã§ã—ã‚‡ã†ã‹ï¼Ÿ(\\(\\gamma=0.95\\)ã«ã—ã¦ã„ã¾ã™)\n\n\nCode\nmdp4 = ChainMDP(\n    [[1.0, 0.0], [0.6, 0.9], [0.9, 0.6], [1.0, 1.0]],\n    [[0.0, 1.0], [0.0, 0.0], [0.0, 0.0], [0.9, 0.0]],\n)\nwidth, height = mdp4.figure_shape()\nfig = plt.figure(\"MDP4-pi-vis\", (width * 1.25, height))\nmdp_ax = fig.add_axes([0.42, 0.0, 1.0, 1.0])\n_ = mdp4.show(\"MDP4\", ax=mdp_ax)\npi_ax = fig.add_axes([0.0, 0.0, 0.4, 1.0], projection=\"3d\")\ninitial_pi = np.array([[0.0, 1.0], [0.5, 0.5], [0.5, 0.5], [1.0, 0.0]])\nv_pi_sums = v_pi_sum_2dim(mdp4.r, mdp4.p, 0.95, 1e-4, initial_pi, (1, 2), 24)\n_ = plot_piv_heatmap(v_pi_sums, \"Î¸(1)\", \"Î¸(2)\", ax=pi_ax)\nprint(\n    f\"f(Î¸(1) = 0.0, Î¸(2) = 0.0): {v_pi_sums[0][0]}\\n\"\n    f\"f(Î¸(1) = 0.5, Î¸(2) = 0.5): {v_pi_sums[12][12]}\\n\"\n    f\"f(Î¸(1) = 1.0, Î¸(2) = 1.0): {v_pi_sums[24][24]}\"\n)\n\n\nf(Î¸(1) = 0.0, Î¸(2) = 0.0): 74.25901721830479\nf(Î¸(1) = 0.5, Î¸(2) = 0.5): 72.01388270994806\nf(Î¸(1) = 1.0, Î¸(2) = 1.0): 70.6327625115528\n\n\n\n\n\n\n\n\n\nä¸€ç•ªå³ã ã¨æ°¸é ã«0.9ãŒã‚‚ã‚‰ãˆã¦ã€ä¸€ç•ªå·¦ã ã¨1.0ãŒã‚‚ã‚‰ãˆã‚‹ã®ã§ã€ã‚ˆã‚Šæœ€é©æ–¹ç­–ã‚’è¦‹åˆ†ã‘ã‚‹ã®ãŒé›£ã—ãã†ãªæ„Ÿã˜ãŒã—ã¾ã™ã€‚\nãƒ—ãƒ­ãƒƒãƒˆã—ã¦ã¿ã‚‹ã¨ã€\\(f(\\theta)\\)ã¯å…ˆç¨‹ã¨ã¯é€†ã«è°·ã®ã‚ˆã†ãªå½¢ã«ãªã£ã¦ã„ã¦ã€å±±ç™»ã‚Šæ³•ã§è§£ã„ã¦ã‚‚ å¿…ãšã—ã‚‚å¤§åŸŸè§£ã«åæŸã—ãªãã†ã«è¦‹ãˆã¾ã™ã€‚ ã“ã‚Œã‚’ã‚‚ã£ã¨å°‚é–€çš„ãªè¨€è‘‰ã§è¨€ã†ã¨ã€\\(f(0.0) + f(1.0) &gt; 2 * f(0.5)\\)ã‚ˆã‚Šã“ã‚Œã¯å‡¹é–¢æ•°ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ ã‚ã¾ã‚Šè©³ã—ãèª¬æ˜ã—ã¾ã›ã‚“ãŒã€å‡¹é–¢æ•°ã ã¨å±±ç™»ã‚Šæ³•ãŒå¤§åŸŸè§£ã«åæŸã™ã‚‹ãªã©å¬‰ã—ã„ç‚¹ãŒã‚ã‚‹ã®ã§ã€ ã“ã‚Œã¯æœ€é©åŒ–ã™ã‚‹ä¸Šã§å„ä»‹ãªç‰¹å¾´ã ã¨è¨€ãˆã¾ã™ã€‚\nä»¥ä¸Šã‚ˆã‚Šã€æ–¹ç­–æœ€é©åŒ–ã§å•é¡Œã‚’è§£ãæ™‚ã¯\\(\\sum_{s\\in\\mathcal{S}} V(s)\\)ãŒå‡¹é–¢æ•°ã‹ã©ã†ã‹ãŒã€ å•é¡Œã®é›£ã—ã•ã«å½±éŸ¿ã‚’ä¸ãˆãã†ã ã¨ã„ã†ã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸã€‚"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html#a-æ–¹ç­–åå¾©æ³•ã®é›£ã—ã•",
    "href": "posts/understanding-what-makes-rl-difficult.html#a-æ–¹ç­–åå¾©æ³•ã®é›£ã—ã•",
    "title": "ã‚ˆã‚Šè‰¯ã„å•é¡Œè¨­è¨ˆã¸å‘ã‘ã¦ï¼š ä½•ãŒå¼·åŒ–å­¦ç¿’ã‚’é›£ã—ãã™ã‚‹ã®ã‹ã‚’ç†è§£ã—ã‚ˆã†",
    "section": "2.A æ–¹ç­–åå¾©æ³•ã®é›£ã—ã•",
    "text": "2.A æ–¹ç­–åå¾©æ³•ã®é›£ã—ã•\n\nNote: ã“ã®ç¯€ã¯ç‰¹ã«å†…å®¹ãŒãªã„ã®ã§ã‚¢ãƒšãƒ³ãƒ‡ã‚£ã‚¯ã‚¹æ‰±ã„ã«ãªã£ã¦ã„ã¾ã™ã€‚é£›ã°ã—ã¦ã‚‚å•é¡Œã‚ã‚Šã¾ã›ã‚“ã€‚\n\nã¨ã“ã‚ã§2.1ã§\\(V^*\\)ã‚’æ±‚ã‚ãŸã¨ãã«ä½¿ã£ãŸæ‰‹æ³•ã‚’ä¾¡å€¤åå¾©æ³•ã¨è¨€ã„ã¾ã™ã€‚ ã‚‚ã†ä¸€ã¤ã€æ–¹ç­–åå¾©æ³•ã¨ã„ã†æ‰‹æ³•ã§\\(V^*\\)ã‚’æ±‚ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n\\(\\pi^*\\)ãŒæº€ãŸã™ã¹ãæ€§è³ªã«ã¤ã„ã¦è€ƒãˆã¦ã¿ã¾ã™ã€‚ \\(\\pi\\)ãŒæœ€é©ã§ã‚ã‚‹ã¨ãã€ \\[\nV^\\pi(s) \\geq \\max_{a \\in \\mathcal{A}} r(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s'|s,a) V^\\pi(s')\n\\] ãŒæˆã‚Šç«‹ã¡ã¾ã™ã€‚ ã“ã‚ŒãŒæˆã‚Šç«‹ãŸãªã„ã¨ã™ã‚‹ã¨ã€ \\[\n\\pi'(s, a) = \\begin{cases}\n1.0 &(\\textrm{if}~a = \\textrm{argmax}_a r(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s'|s,a) V^\\pi(s')) \\\\\n0.0 &(\\textrm{otherwise})\n\\end{cases}\n\\] ã®æ–¹ãŒæ€§èƒ½ãŒè‰¯ããªã‚Šã€\\(\\pi\\)ãŒæœ€é©ã§ã‚ã‚‹ã“ã¨ã¨çŸ›ç›¾ã—ã¾ã™ã€‚\nã§ã¯ã€ã“ã®æ€§è³ªãŒæˆã‚Šç«‹ã¤ã¾ã§æ–¹ç­–ã‚’æ”¹å–„ã—ç¶šã‘ã‚‹ã¨ã„ã†ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è©¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ ã•ã£ãæ›¸ã„ãŸv_pié–¢æ•°ã‚’ä½¿ã£ã¦å®Ÿè£…ã§ãã¾ã™ã€‚\n\n\nCode\nMAX_ITER_PI: int = 10000\n\ndef policy_iteration(\n    r: Array2,\n    p: Array3,\n    gamma: float,\n    epsilon: float,\n) -&gt; Tuple[Array1, Array2, int]:\n    pi = np.zeros(p.shape[:2])  # |S| x |A|\n    pi[:, 1] = 1.0  # æœ€åˆã®æ–¹ç­–ã¯æ±ºå®šçš„ãªã‚‰ãªã‚“ã§ã‚‚ã„ã„ãŒã€è¡Œå‹•1ã‚’é¸ã¶æ–¹ç­–ã«ã—ã¦ã¿ã‚‹\n    state_indices = np.arange(0, p.shape[0], dtype=np.uint)\n    for n_iter in range(MAX_ITER_PI):\n        v_pi_, _ = v_pi(r, p, pi, gamma, epsilon)\n        q_pi = r + gamma * np.einsum(\"saS,S-&gt;sa\", p, v_pi_)\n        greedy_actions = np.argmax(q_pi, axis=1)\n        pi_next = np.zeros_like(pi)\n        pi_next[state_indices, greedy_actions] = 1.0\n        # pi == pi_next ãªã‚‰åæŸ\n        if np.linalg.norm(pi - pi_next) &lt; 1.0:\n            return v_pi_, pi_next, n_iter + 1\n        pi = pi_next\n    raise RuntimeError(\"Policy Iteration did not converge &gt;_&lt;\")\n\nv_star_mdp3_vi, _, n_iter = policy_iteration(mdp3.r, mdp3.p, 0.9, 1e-4)\nprint(f\"åå¾©å›æ•°: {n_iter}\")\nprint(\" \".join([f\"V({i}): {v:.3}\" for i, v in enumerate(v_star_mdp3_vi)]))\n\n\nåå¾©å›æ•°: 2\nV(0): 6.49 V(1): 7.21 V(2): 7.51\n\n\nãªã‚“ã‹2å›åå¾©ã—ãŸã ã‘ã§æ±‚ã¾ã£ã¦ã—ã¾ã„ã¾ã—ãŸãŒâ€¦ã€‚ ã“ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯æ–¹ç­–åå¾©æ³•ã¨å‘¼ã°ã‚Œã€ãªã‚“ã‚„ã‹ã‚“ã‚„ã§æœ€é©æ–¹ç­–ã«åæŸã™ã‚‹ã“ã¨ãŒçŸ¥ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚ ã§ã¯ã€ã“ã®åå¾©å›æ•°ã¯ã€ä½•ã«ã‚ˆã£ã¦æ±ºã¾ã‚‹ã®ã§ã—ã‚‡ã†ã‹ï¼Ÿ æ–¹ç­–ã®çµ„ã¿åˆã‚ã›ã¯\\(|A|^{|S|}\\)é€šã‚Šã‚ã‚Šã¾ã™ãŒã€ä¸Šã®å®Ÿé¨“ã ã¨ãšã£ã¨é€ŸãåæŸã—ã¦ã„ã‚‹ã®ã§ã€ã‚‚ã£ã¨ã„ã„ãƒã‚¦ãƒ³ãƒ‰ãŒã‚ã‚Šãã†ã«æ€ãˆã¾ã™ã€‚ ã—ã‹ã—ã€å®Ÿéš›ã®ã¨ã“ã‚æœ€æ‚ªã‚±ãƒ¼ã‚¹ã§ã¯æŒ‡æ•°æ™‚é–“ã‹ã‹ã‚‹ã“ã¨ãŒçŸ¥ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚ ã“ã®è¨˜äº‹ã§ã¯ã€ã“ã®æ–¹ç­–åå¾©æ³•ãŒé›£ã—ããªã‚‹å ´åˆã«ã¤ã„ã¦ã‚‚è§£èª¬ã—ãŸã‹ã£ãŸã®ã§ã™ãŒã€ ç†è§£ã§ããªã‹ã£ãŸã®ã§ã€è«¦ã‚ã¾ã—ãŸã€‚ãƒ¾(ï½¡&gt;ï¹&lt;ï½¡)ï¾‰"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html#b-å‚è€ƒæ–‡çŒ®ãªã©",
    "href": "posts/understanding-what-makes-rl-difficult.html#b-å‚è€ƒæ–‡çŒ®ãªã©",
    "title": "ã‚ˆã‚Šè‰¯ã„å•é¡Œè¨­è¨ˆã¸å‘ã‘ã¦ï¼š ä½•ãŒå¼·åŒ–å­¦ç¿’ã‚’é›£ã—ãã™ã‚‹ã®ã‹ã‚’ç†è§£ã—ã‚ˆã†",
    "section": "2.B å‚è€ƒæ–‡çŒ®ãªã©",
    "text": "2.B å‚è€ƒæ–‡çŒ®ãªã©\n\nOn the Complexity of Solving Markov Decision Problems\nCournell CS 6789: Foundations of Reinforcement Learning\n\nå‚è€ƒæ–‡çŒ®ã§ã¯\\(\\frac{1}{1 - \\gamma}\\)ã§åå¾©å›æ•°ã‚’æŠ‘ãˆã¦ã„ã‚‹ã˜ã‚ƒãªã„ã‹ã€è©±ãŒé•ã†ã˜ã‚ƒãªã„ã‹ã€ã¨ã„ã†æ°—ãŒä¸€è¦‹ã—ã¦ã—ã¾ã„ã¾ã™ã€‚ ã“ã‚Œã¯æœ‰åä¸ç­‰å¼\\(\\log x \\leq x - 1\\) ã‹ã‚‰ãªã‚“ã‚„ã‹ã‚“ã‚„ã§\\(\\frac{1}{1 - \\gamma} \\geq -\\frac{1}{\\log\\gamma}\\) ã ã‹ã‚‰ã€œã¨ã„ã†æ„Ÿã˜ã§è€ƒãˆã‚Œã°ãªã‚“ã¨ã‹ãªã‚‹ã¨æ€ã„ã¾ã™ã€‚ ã“ã®ä¸ç­‰å¼ã¯\\(x=1\\)ã§ç­‰å·ãªã®ã§ã€ã‚ˆãä½¿ã†\\(\\gamma=0.99\\)ã¨ã‹ã®è¨­å®šãªã‚‰ã‹ãªã‚Šå·®ã¯è¿‘ããªã‚Šã¾ã™ã€‚"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html#å ±é…¬ãªã—æ¢æŸ»ã®å•é¡Œ",
    "href": "posts/understanding-what-makes-rl-difficult.html#å ±é…¬ãªã—æ¢æŸ»ã®å•é¡Œ",
    "title": "ã‚ˆã‚Šè‰¯ã„å•é¡Œè¨­è¨ˆã¸å‘ã‘ã¦ï¼š ä½•ãŒå¼·åŒ–å­¦ç¿’ã‚’é›£ã—ãã™ã‚‹ã®ã‹ã‚’ç†è§£ã—ã‚ˆã†",
    "section": "3.1 å ±é…¬ãªã—æ¢æŸ»ã®å•é¡Œ",
    "text": "3.1 å ±é…¬ãªã—æ¢æŸ»ã®å•é¡Œ\nã¨ã„ã†ã‚ã‘ã§ã€ã¨ã‚Šã‚ãˆãšåˆ¥ã«å­¦ç¿’ã—ãªãã¦ã„ã„ã®ã§ã€ç’°å¢ƒã‹ã‚‰æƒ…å ±ã‚’é›†ã‚ã¦ã“ã‚ˆã†ã€ã¨ã„ã†å•é¡Œã‚’è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\nfrom matplotlib.figure import Figure\nfrom matplotlib.image import FigureImage\n\n\nclass GridMDP:\n    from matplotlib.colors import ListedColormap\n\n    #: Up, Down, Left, Right\n    ACTIONS = np.array([[-1, 0], [+1, 0], [0, -1], [0, +1]])\n    #: Symbols\n    EMPTY, BLOCK, START, GOAL = range(4)\n    DESCRIPTIONS = [\"Empty\", \"Block\", \"Start\", \"Goal\"]\n    #: Colormap for visualizing the map\n    CM = ListedColormap([\"w\", \".75\", \"xkcd:leaf green\", \"xkcd:vermillion\"])\n    REWARD_COLORS = [\"xkcd:light royal blue\", \"xkcd:vermillion\"]\n    FIG_AREA = 28\n\n    # Returns PIL.Image\n    def __download_agent_image():\n        from io import BytesIO\n        from urllib import request\n\n        from PIL import Image\n\n        fd = BytesIO(\n            request.urlopen(\n                \"https://2.bp.blogspot.com/-ZwYKR5Zu28s/U6Qo2qAjsqI\"\n                + \"/AAAAAAAAhkM/HkbDZEJwvPs/s400/omocha_robot.png\"\n            ).read()\n        )\n        return Image.open(fd)\n\n    AGENT_IMAGE = __download_agent_image()\n\n    def __init__(\n        self,\n        map_array: Sequence[Sequence[int]],\n        reward_array: Optional[Sequence[Sequence[float]]] = None,\n        action_noise: float = 0.1,\n        horizon: Optional[int] = None,\n        seed: int = 123456789,\n        legend_loc: str = \"upper right\",\n    ) -&gt; None:\n        def add_padding(seq: Sequence[Sequence[T]], value: T) -&gt; list:\n            width = len(seq[0]) + 2\n            ret_list = [[value for _ in range(width)]]\n            for col in seq:\n                ret_list.append([value] + list(col) + [value])\n            ret_list.append([value for _ in range(width)])\n            return ret_list\n\n        self.map_array = np.array(add_padding(map_array, 1), dtype=np.uint8)\n        assert self.map_array.max() &lt;= 3\n        assert 0 &lt;= self.map_array.min()\n        self.rows, self.cols = self.map_array.shape\n\n        if reward_array is None:\n            self.reward_array = np.zeros((self.rows, self.cols), np.float64)\n        else:\n            self.reward_array = np.array(\n                add_padding(reward_array, 0.0), np.float64\n            )\n\n        self.action_noise = action_noise\n        self.horizon = horizon\n        self.start_positions = np.argwhere(self.map_array == self.START)\n        if len(self.start_positions) == 0:\n            raise ValueError(\"map_array needs at least one start posiiton\")\n        self.random_state = np.random.RandomState(seed)\n        _ = self.reset()\n\n        # Visualization stuffs\n        self.legend_loc = legend_loc\n        self.map_fig, self.map_ax, self.map_img = None, None, None\n        self.agent_img, self.agent_fig_img = None, None\n\n    def n_states(self) -&gt; int:\n        return np.prod(self.map_array.shape)\n\n    @staticmethod\n    def n_actions() -&gt; int:\n        return 4\n\n    def reset(self) -&gt; Array1:\n        idx = self.random_state.randint(self.start_positions.shape[0])\n        self.state = self.start_positions[idx]\n        self.n_steps = 0\n        return self.state.copy()\n\n    def state_index(self, state: Array1) -&gt; int:\n        y, x = state\n        return y * self.map_array.shape[1] + x\n\n    def _load_agent_img(self, fig_height: float) -&gt; None:\n        from io import BytesIO\n        from urllib import request\n\n        fd = BytesIO(request.urlopen(self.ROBOT).read())\n        img = Image.open(fd)\n        scale = fig_height / img.height\n        self.agent_img = img.resize((int(img.width * scale), int(img.height * scale)))\n\n    def _fig_inches(self) -&gt; Tuple[int, int]:\n        prod = self.rows * self.cols\n        scale = np.sqrt(self.FIG_AREA / prod)\n        return self.cols * scale, self.rows * scale\n\n    def _is_valid_state(self, *args) -&gt; bool:\n        if len(args) == 2:\n            y, x = args\n        else:\n            y, x = args[0]\n        return 0 &lt;= y &lt; self.rows and 0 &lt;= x &lt; self.cols\n\n    def _possible_actions(self) -&gt; Array1:\n        possible_actions = []\n        for i, act in enumerate(self.ACTIONS):\n            y, x = self.state + act\n            if self._is_valid_state(y, x) and self.map_array[y, x] != self.BLOCK:\n                possible_actions.append(i)\n        return np.array(possible_actions)\n\n    def _reward(self, next_state: Array1) -&gt; float:\n        y, x = next_state\n        return self.reward_array[y, x]\n\n    def _is_terminal(self) -&gt; bool:\n        if self.horizon is not None and self.n_steps &gt; self.horizon:\n            return True\n        y, x = self.state\n        return self.map_array[y, x] == self.GOAL\n\n    def step(self, action: int) -&gt; Tuple[Tuple[int, int], float, bool]:\n        self.n_steps += 1\n        possible_actions = self._possible_actions()\n        if self.random_state.random_sample() &lt; self.action_noise:\n            action = self.random_state.choice(possible_actions)\n\n        if action in possible_actions:\n            next_state = self.state + self.ACTIONS[action]\n        else:\n            next_state = self.state.copy()\n\n        reward = self._reward(next_state)\n        self.state = next_state\n        is_terminal = self._is_terminal()\n        return next_state.copy(), reward, is_terminal\n\n    def _draw_agent(self, fig: Figure) -&gt; FigureImage:\n        unit = self.map_img.get_window_extent().y1 / self.rows\n        y, x = self.state\n        return fig.figimage(\n            self.agent_img,\n            unit * (x + 0.3),\n            unit * (self.rows - 0.8 - y),\n        )\n\n    def _draw_rewards(self) -&gt; None:\n        for y in range(self.rows):\n            for x in range(self.cols):\n                rew = self.reward_array[y, x]\n                if abs(rew) &lt; 1e-3:\n                    continue\n                if self.map_array[y, x] == self.GOAL:\n                    color = \"w\"\n                else:\n                    color = self.REWARD_COLORS[int(rew &gt;= 0)]\n                self.map_ax.text(\n                    x + 0.1,\n                    y + 0.5,\n                    f\"{rew:+.2}\",\n                    color=color,\n                    fontsize=12,\n                )\n\n    def show(self, title: str = \"\", explicit: bool = False) -&gt; Axes:\n        if self.map_fig is None:\n            self.map_fig = plt.figure(title or \"GridMDP\", self._fig_inches())\n            ax = self.map_fig.add_axes([0, 0, 1, 1])\n            ax.set_aspect(\"equal\")\n            ax.set_xticks([])\n            ax.set_yticks([])\n            self.map_img = ax.imshow(\n                self.map_array,\n                cmap=self.CM,\n                extent=(0, self.cols, self.rows, 0),\n                vmin=0,\n                vmax=4,\n                alpha=0.6,\n            )\n            for i in range(1, 4):\n                if np.any(self.map_array == i):\n                    ax.plot([0.0], [0.0], color=self.CM(i), label=self.DESCRIPTIONS[i])\n            ax.legend(fontsize=12, loc=self.legend_loc)\n            ax.text(0.1, 0.8, title or \"GridMDP\", fontsize=16)\n            self.map_ax = ax\n\n            imw, imh = self.AGENT_IMAGE.width, self.AGENT_IMAGE.height\n            scale = (self.map_img.get_window_extent().y1 / self.rows) / imh\n            self.agent_img = self.AGENT_IMAGE.resize(\n                (int(imw * scale), int(imh * scale))\n            )\n\n            if np.linalg.norm(self.reward_array) &gt; 1e-3:\n                self._draw_rewards()\n        if self.agent_fig_img is not None:\n            self.agent_fig_img.remove()\n        self.agent_fig_img = self._draw_agent(self.map_fig)\n        if explicit:\n            from IPython.display import display\n\n            self.map_fig.canvas.draw()\n            display(self.map_fig)\n        return self.map_ax\n\n\n\n\nCode\ngrid_mdp1 = GridMDP(\n    [[0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0], \n     [0, 0, 2, 0, 0],\n     [0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0]],\n    horizon=50,\n)\n_ = grid_mdp1.show(\"GridMDP1\")\n\n\n\n\n\n\n\n\n\nGridMDPã¨é¡Œã•ã‚ŒãŸã“ã¡ã‚‰ãŒã€ä»Šå›ä½¿ç”¨ã™ã‚‹ã€Œç’°å¢ƒã€ã«ãªã‚Šã¾ã™ã€‚ ç’°å¢ƒã®ä¸­ã§è¡Œå‹•ã™ã‚‹ä¸»ä½“ã‚’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨å‘¼ã³ã¾ã™ã€‚ ä»Šå›ã¯ã€ã„ã‚‰ã™ã¨ã‚„æ§˜ã®ãƒ­ãƒœãƒƒãƒˆã®ç”»åƒã‚’ä½¿ç”¨ã•ã›ã¦ã„ãŸã ãã¾ã—ãŸã€‚ å„ãƒã‚¹ç›®ã®ä¸­ã§ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯è¡Œå‹•ã¯ä¸Šä¸‹å·¦å³ã«ç§»å‹•ã®4ç¨®é¡ã®è¡Œå‹•ã‚’é¸æŠã§ãã¾ã™ã€‚ è¡Œå‹•ã¯æ™‚ã€…å¤±æ•—ã—ã¦ã€ä¸€æ§˜ãƒ©ãƒ³ãƒ€ãƒ ãªçŠ¶æ…‹é·ç§»ãŒç™ºç”Ÿã—ã¾ã™ã€‚ ã“ã“ã§ã€å‰ç« ã§ç”¨ã„ãªã‹ã£ãŸã„ãã¤ã‹ã®æ–°ã—ã„æ¦‚å¿µã‚’å°å…¥ã—ã¾ã™ã€‚\n\nã€Œã‚¹ã‚¿ãƒ¼ãƒˆåœ°ç‚¹ã€ã®å­˜åœ¨\n\nãƒ­ãƒœãƒƒãƒˆãã‚“ã¯ã€æ±ºã‚ã‚‰ã‚ŒãŸå ´æ‰€ã‹ã‚‰è¡Œå‹•ã‚’å§‹ã‚ãªãã¦ã¯ãªã‚Šã¾ã›ã‚“ã€‚ã“ã®å•é¡Œã§ã¯ã€åˆæœŸçŠ¶æ…‹ã¯ã‚ã‚‰ã‹ã˜ã‚æ±ºã¾ã‚‰ã‚ŒãŸã„ãã¤ã‹ã®ã‚¹ã‚¿ãƒ¼ãƒˆåœ°ç‚¹ã‹ã‚‰å‡ç­‰ã«é¸ã³ã¾ã™ã€‚ç†è«–çš„ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã¯ã€åˆæœŸçŠ¶æ…‹åˆ†å¸ƒ\\(\\mu: \\mathcal{S} \\rightarrow \\mathbb{R}\\)ã¨ã—ã¦è¡¨ç¾ã™ã‚Œã°ã„ã„ã§ã™ã€‚\n\nã€Œçµ‚äº†åœ°ç‚¹ã€ã®å­˜åœ¨\n\nãƒ­ãƒœãƒƒãƒˆãã‚“ã¯ã€ã„ãã¤ã‹ã®æ±ºã‚ã‚‰ã‚ŒãŸå ´æ‰€ã«åˆ°é”ã—ãŸã‚‰å¼·åˆ¶çš„ã«ã‚¹ã‚¿ãƒ¼ãƒˆã¾ã§æˆ»ã•ã‚Œã¾ã™ã€‚\n\nã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰\n\nã‚¹ã‚¿ãƒ¼ãƒˆã‹ã‚‰çµ‚äº†ã™ã‚‹ã¾ã§ã®ä¸€é€£ã®æµã‚Œã‚’ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¨å‘¼ã³ã¾ã™ã€‚\n\nã€Œã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é•·ã•ï¼ˆãƒ›ãƒ©ã‚¤ã‚¾ãƒ³ï¼‰ã€ã®å­˜åœ¨\n\nä¸€å®šã®ã‚¿ãƒ¼ãƒ³ãŒçµŒéã—ãŸæ™‚ã€ãƒ­ãƒœãƒƒãƒˆãã‚“ã¯ã‚¹ã‚¿ãƒ¼ãƒˆåœ°ç‚¹ã¾ã§æˆ»ã•ã‚Œã¾ã™ã€‚å¼·åŒ–å­¦ç¿’ã§ã¯ã—ã°ã—ã°ã€ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’ä½•åº¦ã‚‚ãƒªã‚»ãƒƒãƒˆã—ã¦å­¦ç¿’ã—ã¾ã™ã€‚ç†è«–ã‚’å®Ÿéš›ã«è¿‘ã¥ã‘ã‚‹ãŸã‚ã€MDPã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«ã‚‚ã“ã‚ŒãŒå°å…¥ã•ã‚Œã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚\n\n\nã§ã¯ã•ã£ããã€é©å½“ã«è¡Œå‹•ã—ã¦ã‚‚ã‚‰ã„ã¾ã—ã‚‡ã†ã€‚ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«è¡Œå‹•ã•ã›ã¦ã€è¨ªå•ã—ãŸå ´æ‰€ã«è‰²ã‚’ã¤ã‘ã¦ã„ãã¾ã™ã€‚ ãªãŠã€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é•·ã•ã¯50ã¨ã—ã¾ã™ã€‚\n\n\nCode\nfrom abc import ABC, abstractmethod\nfrom typing import Callable\n\n\nclass VisitationHeatmap:\n    def __init__(\n        self,\n        map_shape: Tuple[int, int],\n        figsize: Tuple[float, float],\n        ax: Optional[Axes] = None,\n        max_visit: int = 1000,\n        title: str = \"\",\n    ) -&gt; None:\n        from matplotlib import colors as mc\n        from mpl_toolkits.axes_grid1 import make_axes_locatable\n\n        self.counter = np.zeros(map_shape, np.int64)\n        self.title = title\n        self.fig = plt.figure(self.title, figsize, facecolor=\"w\")\n        self.ax = self.fig.add_axes([0, 0, 1, 1])\n        self.ax.set_aspect(\"equal\")\n        self.ax.set_xticks([])\n        self.ax.set_yticks([])\n        r, g, b = mc.to_rgb(\"xkcd:fuchsia\")\n        cdict = {\n            \"red\": [(0.0, r, r), (1.0, r, r)],\n            \"green\": [(0.0, g, g), (1.0, g, g)],\n            \"blue\": [(0.0, b, b), (1.0, b, b)],\n            \"alpha\": [(0.0, 0.0, 0.0), (1.0, 1.0, 1.0)],\n        }\n        self.img = self.ax.imshow(\n            np.zeros(map_shape),\n            cmap=mc.LinearSegmentedColormap(\"visitation\", cdict),\n            extent=(0, map_shape[1], map_shape[0], 0),\n            vmin=0,\n            vmax=max_visit,\n        )\n\n        divider = make_axes_locatable(self.ax)\n        cax = divider.append_axes(\"right\", size=\"4%\", pad=0.1)\n        self.fig.colorbar(self.img, cax=cax, orientation=\"vertical\")\n        cax.set_ylabel(\"Num. Visitation\", rotation=0, position=(1.0, 1.1), fontsize=14)\n\n        self._update_text()\n        self.agent = None\n\n    def _update_text(self) -&gt; None:\n        self.text = self.ax.text(\n            0.1,\n            -0.5,\n            f\"{self.title} After {self.counter.sum()} steps\",\n            fontsize=16,\n        )\n\n    def _draw_agent(self, draw: Callable[[Figure], FigureImage]) -&gt; None:\n        if self.agent is not None:\n            self.agent.remove()\n        self.agent = draw(self.fig)\n\n    def visit(self, state: Array1) -&gt; int:\n        y, x = state\n        res = self.counter[y, x]\n        self.counter[y, x] += 1\n        self.img.set_data(self.counter)\n        self.text.remove()\n        self._update_text()\n        return res\n\n    def show(self) -&gt; None:\n        from IPython.display import display\n\n        display(self.fig)\n\n\ndef do_nothing(\n    _state: int,\n    _action: int,\n    _next_state: int,\n    _reward: float,\n    _is_terminal: bool,\n) -&gt; None:\n    return\n\n\ndef simulation(\n    mdp: GridMDP,\n    n: int,\n    act: Callable[[int], int],\n    learn: Callable[[int, int, int, float, bool], None] = do_nothing,\n    max_visit: Optional[int] = None,\n    vis_freq: Optional[int] = None,\n    vis_last: bool = False,\n    title: str = \"\",\n) -&gt; None:\n    visitation = VisitationHeatmap(\n        mdp.map_array.shape,\n        mdp._fig_inches(),\n        max_visit=max_visit or n // 10,\n        title=title,\n    )\n    state = mdp.reset()\n    visitation.visit(state)\n    vis_interval = n + 1 if vis_freq is None else n // vis_freq\n    for i in range(n):\n        if (i + 1) % vis_interval == 0 and (vis_last or i &lt; n - 1):\n            visitation._draw_agent(mdp._draw_agent)\n            visitation.show()\n        action = act(mdp.state_index(state))\n        next_state, reward, terminal = mdp.step(action)\n        visitation.visit(next_state)\n        learn(\n            mdp.state_index(state),\n            action,\n            mdp.state_index(next_state),\n            reward,\n            terminal,\n        )\n        if terminal:\n            state = mdp.reset()\n        else:\n            state = next_state\n    visitation._draw_agent(mdp._draw_agent)\n\n\nsimulation(grid_mdp1, 1000, lambda _: np.random.randint(4), vis_freq=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nãƒ©ãƒ³ãƒ€ãƒ ã«è¡Œå‹•ã•ã›ãŸã ã‘ã§ã™ãŒã€ãã‚Œãªã‚Šã«ã¾ã‚“ã¹ã‚“ãªãè‰²ãŒå¡—ã‚‰ã‚Œã¦ã„ã¦ã€ã¾ã‚ã¾ã‚ã„ã„ã®ã§ã¯ãªã„ã‹ã€ã¨ã„ã†æ°—ãŒã—ã¾ã™ã€‚ ã—ã‹ã—ã€ã‚‚ã£ã¨åºƒã„ç’°å¢ƒã§ã¯ã©ã†ã§ã—ã‚‡ã†ã‹ã€‚\n\n\nCode\ngrid_mdp2_map = [[0] * 15 for _ in range(15)]\ngrid_mdp2_map[7][7] = 2\ngrid_mdp2 = GridMDP(grid_mdp2_map, horizon=50)\n_ = grid_mdp2.show()\nrandom_state = np.random.RandomState(1)\nsimulation(\n    grid_mdp2,\n    5000,\n    lambda _: random_state.randint(4),\n    max_visit=100,\n    title=\"Random Exploration\",\n    vis_freq=None,\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nãªã‚“ã‹é§„ç›®ã£ã½ã„æ„Ÿã˜ã§ã™ã­ã€‚ å ´æ‰€ã«ã‚ˆã£ã¦ã¯å…¨ãè‰²ãŒã¤ã„ã¦ã„ã¾ã›ã‚“ã€‚ ç’°å¢ƒãŒåºƒã„ã¨ã€ãƒ©ãƒ³ãƒ€ãƒ ã«æ­©ãå›ã‚‹ã®ã§ã¯ã€åŠ¹ç‡ã‚ˆãæƒ…å ±ã‚’é›†ã‚ã¦ã“ã‚Œãªã„ã‚ˆã†ã§ã™ã€‚ å…·ä½“çš„ã«ã©ã®ãã‚‰ã„é›£ã—ã„ã®ã‹ã¨è¨€ã†ã¨ã€å¹³å‡ä¸€å›è¨ªå•ã™ã‚‹ã®ã«ã‹ã‹ã‚‹æ™‚é–“ãŒã€ã ã„ãŸã„ - ä¸€æ–¹é€šè¡Œã®ç›´ç·š: \\(O(|S|)\\) - äºŒæ¬¡å…ƒãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯: \\(O(|S|^2)\\)? (å‚è€ƒ: planeä¸Šã®ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ï¼‰ - æœ€æ‚ªã‚±ãƒ¼ã‚¹: \\(O(2^{|S|})\\)\nãã‚‰ã„ã«ãªã‚Šã¾ã™ã€‚ ä¸€æ–¹é€šè¡Œãªã®ã¯ãªã‚“ã¨ãªãã‚ã‹ã‚Šã¾ã™ã­ã€‚ ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ã®å ´åˆã€åŒã˜å ´æ‰€ã‚’è¡Œã£ãŸã‚ŠããŸã‚Šã§ãã‚‹ã®ã§ã€ãã®ã¶ã‚“æ™‚é–“ãŒã‹ã‹ã£ã¦ã—ã¾ã„ã¾ã™ã€‚ æœ€æ‚ªã‚±ãƒ¼ã‚¹ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«æ§‹æˆã™ã‚Œã°ã„ã„ã§ã™ã€‚\n\n\nCode\n# hide-input\nclass WorstCaseMDP(ChainMDP):\n    def __init__(self, n: int) -&gt; None:\n        self.n_states = n\n        # For plotting\n        self.circles = []\n        self.cached_ax = None\n\n    def show(self, title: str = \"\", ax: Optional[Axes] = None) -&gt; Axes:\n        # ã ã„ãŸã„ChainMDPã‹ã‚‰ã‚³ãƒ”ãƒš...\n        if self.cached_ax is not None:\n            return self.cached_ax\n\n        from matplotlib.patches import Circle\n\n        width, height = self.figure_shape()\n        circle_position = height / 2 - height / 10\n        if ax is None:\n            fig = plt.figure(title or \"ChainMDP\", (width, height))\n            ax = fig.add_axes([0, 0, 1, 1], aspect=1.0)\n        ax.set_xlim(0, width)\n        ax.set_ylim(0, height)\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n        def xi(si: int) -&gt; float:\n            return self.OFFSET + (1.0 + self.INTERVAL) * si + 0.5\n\n        self.circles = [\n            Circle((xi(i), circle_position), 0.5, fc=\"w\", ec=\"k\")\n            for i in range(self.n_states)\n        ]\n        for i in range(self.n_states):\n            x = self.OFFSET + (1.0 + self.INTERVAL) * i + 0.1\n            ax.text(x, height * 0.85, f\"State {i}\", fontsize=16)\n\n        def annon(act: int, *args, **kwargs) -&gt; None:\n            # We don't hold references to annotations (i.e., we treat them immutable)\n            a_to_b(\n                ax,\n                *args,\n                **kwargs,\n                arrowcolor=self.ACT_COLORS[act],\n                text=f\"P: 1.0\",\n                fontsize=11,\n            )\n\n        for si in range(self.n_states):\n            ax.add_patch(self.circles[si])\n            x = xi(si)\n            # Action 0:\n            y = circle_position + self.SHIFT\n            if si &lt; self.n_states - 1:\n                annon(\n                    0,\n                    (x + self.SHIFT, y),\n                    (xi(si + 1) - self.SHIFT * 1.2, y - self.SHIFT * 0.3),\n                    verticalalignment=\"center_baseline\",\n                )\n            else:\n                annon(\n                    0,\n                    (x - self.SHIFT * 1.2, y),\n                    (x + self.SHIFT * 0.5, y - self.SHIFT * 0.1),\n                    style=\"self\",\n                    verticalalignment=\"bottom\",\n                )\n            # Action 1:\n            y = circle_position - self.SHIFT\n            if si &gt; 0:\n                annon(\n                    1,\n                    (x - self.SHIFT * 1.6, y),\n                    (xi(0), y - self.SHIFT * 0.6),\n                    style=\"arc3,rad=-0.15\",\n                    verticalalignment=\"top\",\n                )\n            else:\n                annon(\n                    1,\n                    (x + self.SHIFT * 0.4, y),\n                    (x - self.SHIFT * 0.45, y + self.SHIFT * 0.1),\n                    style=\"self\",\n                    verticalalignment=\"top\",\n                )\n\n        for i in range(2):\n            ax.plot([0.0], [0.0], color=self.ACT_COLORS[i], label=f\"Action {i}\")\n        ax.legend(fontsize=11, loc=\"upper right\")\n        if len(title) &gt; 0:\n            ax.text(0.06, height * 0.9, title, fontsize=18)\n        self.cached_ax = ax\n        return ax\n\n\n_ = WorstCaseMDP(6).show()\n\n\n\n\n\n\n\n\n\nã“ã®ç’°å¢ƒã§çŠ¶æ…‹\\(0\\)ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«è¡Œå‹•ã™ã‚‹ã¨ã€å³ç«¯ã«ãŸã©ã‚Šç€ãã¾ã§ã«å¹³å‡\\(2^5\\)ãã‚‰ã„ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ãŒã‹ã‹ã‚Šã¾ã™ã€‚ ãã‚“ãªã‚“ã‚ã‚Šã‹ã‚ˆâ€¦ã£ã¦æ„Ÿã˜ã§ã™ãŒã€‚\nã“ã®çµæœã‹ã‚‰ã€æœ€æ‚ªã®å ´åˆã ã¨æŒ‡æ•°æ™‚é–“ã‹ã‹ã‚‹ã‹ã‚‰è³¢ããƒ‡ãƒ¼ã‚¿åé›†ã—ãªã„ã¨ã„ã‘ãªã„ã‚ˆã­ã€ æ€ã†ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ ãã®ä¸€æ–¹ã§ã€ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ã®ã‚ˆã†ã«é·ç§»ã®å¯¾ç§°æ€§ãŒã‚ã‚‹ç’°å¢ƒãªã‚‰ã€ ãƒ©ãƒ³ãƒ€ãƒ ã«è¡Œå‹•ã—ã¦ã‚‚ãã‚“ãªã«æ‚ªããªã„ã‚“ã˜ã‚ƒãªã„ã‹ãªã€ã¨ã‚‚æ€ãˆã¾ã™ã€‚\nã•ã¦ãã®è©±ã¯ä¸€æ—¦ãŠã„ã¦ãŠã„ã¦ã€ã‚‚ã£ã¨åŠ¹ç‡ã‚ˆããƒ‡ãƒ¼ã‚¿ã‚’é›†ã‚ã‚‹æ–¹æ³•ã‚’è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\nè¨ªå•ã—ãŸå ´æ‰€ã‚’è¦šãˆã¦ãŠã„ã¦ã€è¨ªå•ã—ã¦ã„ãªã„å ´æ‰€ã‚’å„ªå…ˆã—ã¦æ¢æŸ»ã™ã‚‹\nçŠ¶æ…‹ã¨çŠ¶æ…‹ã®é–“ã«è·é›¢ãŒå®šç¾©ã§ãã‚‹ã¨ä»®å®šã—ã¦ã€é ãã«è¡Œãã‚ˆã†ã«ã™ã‚‹\nç’°å¢ƒãŒãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰ç©ºé–“ã ã¨ä»®å®šã—ã¦SLAMã§è‡ªå·±ä½ç½®æ¨å®šã™ã‚‹\n\nãªã©ã€è‰²ã€…ãªæ–¹æ³•ãŒè€ƒãˆã‚‰ã‚Œã‚‹ã¨æ€ã„ã¾ã™ãŒã€ã“ã“ã§ã¯1ã®æ–¹æ³•ã‚’ä½¿ã£ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\nä»¥ä¸‹ã®ã‚ˆã†ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è€ƒãˆã¾ã™ã€‚ 1. é©å½“ãªæ–¹ç­–\\(\\pi_0\\)ã‹ã‚‰é–‹å§‹ã™ã‚‹ 2. çŠ¶æ…‹è¡Œå‹•è¨ªå•å›æ•°\\(n(s, a)\\)ã€çŠ¶æ…‹è¡Œå‹•æ¬¡çŠ¶æ…‹è¨ªå•å›æ•°\\(n(s, a, s')\\)ã‚’è¨˜éŒ²ã—ã¦ãŠã - ãŸã ã—ã€åˆæœŸå€¤ã¯\\(n_0(s, a) = 1.0, n_0(s, a, s) = \\frac{1}{|S|}\\)ã¨ã™ã‚‹(0é™¤ç®—é˜²æ­¢ã®ãŸã‚) 3. ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãŒçµ‚ã‚ã£ãŸã¨ãã€ä»¥ä¸‹ã®ã‚ˆã†ã«æ–¹ç­–ã‚’æ›´æ–°ã™ã‚‹ 1. çŠ¶æ…‹é·ç§»é–¢æ•°ã®æ¨å®šå€¤\\(\\hat{P}(s'|s, a) = \\frac{n(s, a, s')}{n(s, a}\\)ã€ç–‘ä¼¼å ±é…¬\\(r_k(s, a)=\\frac{1}{n(s, a)}\\)ã€é©å½“ãª\\(\\gamma\\)ã‹ã‚‰æˆã‚‹MDP\\(\\mathcal{M}_k\\)ã‚’è§£ã 2. \\(\\mathcal{M}_k\\)ã®æœ€é©ä¾¡å€¤é–¢æ•°\\(V^*_k,Q^*_k\\)ã‹ã‚‰ä»¥ä¸‹ã®ã‚ˆã†ã«æ–¹ç­–\\(pi_{k+1}\\)ã‚’æ§‹æˆã™ã‚‹ - \\(V^*_k(s) &lt; \\frac{1}{|S|}\\sum_{s'\\in\\mathcal{S}}V^*_k(s')\\) ãªã‚‰ \\(\\pi_{k+1}(s)\\)ã¯\\(Q^*_k\\)ã«åŸºã¥ãè²ªæ¬²è¡Œå‹• - ãã‚Œä»¥å¤–ã®å ´åˆã€\\(\\pi_{k+1}(s)\\)ã¯ä¸€æ§˜ãƒ©ãƒ³ãƒ€ãƒ ãªè¡Œå‹•ã‚’ã¨ã‚‹ (=æ–¹ç­–ã‚’ç·©å’Œã™ã‚‹)\nç–‘ä¼¼å ±é…¬\\(r_k=\\frac{1}{n(s, a)}\\)ã‚’ä½¿ç”¨ã—ã¦ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã™ã‚‹ã®ãŒã€æœ€ã‚‚é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã§ã™ã€‚ ã“ã®å€¤ã¯ã€ä¸€åº¦ã‚‚çŠ¶æ…‹è¡Œå‹•ãƒšã‚¢\\((s,a)\\)ã‚’çµŒé¨“ã—ã¦ã„ãªã„ãªã‚‰\\(1\\)ã€ä¸€åº¦çµŒé¨“ã—ãŸã‚‰\\(1/2\\)ã€2å›çµŒé¨“ã—ãŸã‚‰\\(1/3\\)ã®ã‚ˆã†ã«æ¸›è¡°ã—ã¾ã™ã€‚ ã“ã‚Œã‚’å ±é…¬ã¨ã™ã‚‹MDPã‚’è§£ãã“ã¨ã§ã€ã‚ã¾ã‚ŠçµŒé¨“ã—ã¦ã„ãªã„çŠ¶æ…‹è¡Œå‹•ãƒšã‚¢ã‚’ã¨ã‚ã†ã¨ã™ã‚‹æ–¹ç­–ãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚ å®Œå…¨ãªè²ªæ¬²æ–¹ç­–ã§ã¯ãªãç·©å’Œã‚’ã„ã‚Œã¦ã„ã‚‹ã®ã¯ã€é«˜ã„å ±é…¬ã®çŠ¶æ…‹ã‚’ãƒ«ãƒ¼ãƒ—ã—ãªã„ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã§ã™ã€‚ ã§ã¯ã€ã‚„ã£ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\nclass RewardFreeExplore:\n    def __init__(\n        self,\n        n_states: int,\n        n_actions: int,\n        seed: int = 1,\n        gamma: float = 0.95,\n    ) -&gt; None:\n        self.sa_count = np.ones((n_states, n_actions))\n        self.sas_count = np.ones((n_states, n_actions, n_states)) / n_states\n        self.pi = np.ones((n_states, n_actions)) / n_actions\n        self.random_state = np.random.RandomState(seed)\n        self.n_states, self.n_actions = n_states, n_actions\n        self.n_updates = 0\n        self.gamma = gamma\n        self.pi_indices = np.arange(n_states, dtype=np.uint32)\n\n    def learn(\n        self,\n        state: int,\n        action: int,\n        next_state: int,\n        reward: float,\n        is_terminal: bool,\n    ) -&gt; None:\n        # è¨ªå•è¨˜éŒ²ã‚’æ›´æ–°ã™ã‚‹\n        self.sa_count[state, action] += 1\n        if is_terminal:\n            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãŒçµ‚ã‚ã£ãŸã‚‰ã€Value Iterationã‚’è§£ã„ã¦æ–¹ç­–ã‚’æ›´æ–°ã™ã‚‹\n            r = 1.0 / self.sa_count\n            p = self.sas_count / np.expand_dims(self.sa_count, axis=-1)\n            v, _n_iter = value_iteration(r, p, self.gamma, 1e-2)\n            v_is_larger_than_mean = v &gt; v.mean()\n            q = r + self.gamma * np.einsum(\"saS,S-&gt;sa\", p, v)\n            self.pi.fill(0.0)\n            for state in range(self.n_states):\n                # VãŒå¤§ãã„å ´æ‰€ã§ã¯æ–¹ç­–ã‚’ç·©å’Œã™ã‚‹\n                if v_is_larger_than_mean[state]:\n                    self.pi[state] = 1.0 / self.n_actions\n                # ãã†ã§ãªã„å ´åˆã¯è²ªæ¬²\n                else:\n                    self.pi[state][q[state].argmax()] = 1.0\n            self.n_updates += 1\n        else:\n            self.sas_count[state, action, next_state] += 1\n\n    def act(self, state: int) -&gt; int:\n        return self.random_state.choice(self.n_actions, p=self.pi[state])\n\n\nagent = RewardFreeExplore(grid_mdp2.n_states(), grid_mdp2.n_actions())\nsimulation(\n    grid_mdp2,\n    5000,\n    agent.act,\n    learn=agent.learn,\n    max_visit=100,\n    title=\"Strategic Exploration\",\n    vis_freq=None,\n)\n\n\n\n\n\n\n\n\n\nã•ã£ãã‚ˆã‚Šã‚‚æº€éãªãã€è‰²ã€…ãªçŠ¶æ…‹ã‚’è¨ªå•ã—ã¦ãã‚Œã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã­ã€‚"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html#å ±é…¬ã‚ã‚Šæ¢æŸ»",
    "href": "posts/understanding-what-makes-rl-difficult.html#å ±é…¬ã‚ã‚Šæ¢æŸ»",
    "title": "ã‚ˆã‚Šè‰¯ã„å•é¡Œè¨­è¨ˆã¸å‘ã‘ã¦ï¼š ä½•ãŒå¼·åŒ–å­¦ç¿’ã‚’é›£ã—ãã™ã‚‹ã®ã‹ã‚’ç†è§£ã—ã‚ˆã†",
    "section": "3.2 å ±é…¬ã‚ã‚Šæ¢æŸ»",
    "text": "3.2 å ±é…¬ã‚ã‚Šæ¢æŸ»\næ¬¡ã¯ã€ç’°å¢ƒã‹ã‚‰å ±é…¬ãŒä¸ãˆã‚‰ã‚Œã‚‹ã®ã§ã€ãªã‚‹ã¹ãæ—©ãå­¦ç¿’ã‚’çµ‚ã‚ã‚‰ã›ãŸã„ã€ã¨ã„ã†å•é¡Œã‚’è€ƒãˆã¾ã™ã€‚ è¨ªå•ã—ã¦ã„ãªã„å ´æ‰€ã«ç©æ¥µçš„ã«ã„ã‘ã°ã„ã„ã€ã¨ã„ã†æ–¹é‡ã¯ã•ã£ãã¨å¤‰ã‚ã‚Šã¾ã›ã‚“ã€‚ ä¸€æ–¹ã§ã€ã‚ã¾ã‚Šã«å ±é…¬ãŒã‚‚ã‚‰ãˆãªã•ãã†ãªçŠ¶æ…‹ã¯ã¨ã£ã¨ã¨è«¦ã‚ã‚‹ã“ã¨ã‚’ã—ãªãã¦ã¯ã„ã‘ãªã„ç‚¹ãŒç•°ãªã£ã¦ã„ã¾ã™ã€‚\nä¾‹ãˆã°ã€å ±é…¬ã®æ¨å®šå€¤ã‚’\\(\\hat{r}(s, a)\\)ã¨ã™ã‚‹ã¨ãã€å…ˆã»ã©ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ç–‘ä¼¼å ±é…¬ã‚’ \\(r_k(s, a)=\\hat{r}(s, a)+\\frac{\\beta}{\\sqrt{n(s, a)}}\\)ã¨ã™ã‚Œã°ã„ã„ã§ã™ã€‚ ã“ã‚Œã‚’ã€å˜ã«\\(r_k(s, a)=\\hat{r}(s, a)\\)ã¨ã™ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ (Approximate Value Iteration)ã¨æ¯”è¼ƒã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ ã“ã¡ã‚‰ã¯ã€ç¢ºç‡\\(\\epsilon\\)ã§ä¸€æ§˜åˆ†å¸ƒã‹ã‚‰è¡Œå‹•ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€\\(1-\\epsilon\\)ã§\\(Q^*_k\\)ãŒä¸€ç•ªå¤§ãã„è¡Œå‹•ã‚’é¸æŠã™ã‚‹ã¨ã„ã†è¡Œå‹•æ–¹ç­–ã‚’ä½¿ã£ã¦ã¿ã¾ã—ã‚‡ã†ï¼ˆ\\(\\epsilon\\)-Greedyã¨è¨€ã„ã¾ã™ï¼‰ã€‚ ä»Šå›ã¯\\(\\epsilon=0.9 \\rightarrow 0.4\\)ã¨ã—ã¾ã™ã€‚\n\n\nCode\ngrid_mdp3 = GridMDP(\n    [[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n    [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9.0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0, 0]],\n    horizon=100,\n    legend_loc=\"lower right\"\n)\n_ = grid_mdp3.show(\"GridMDP3\")\n\n\n\n\n\n\n\n\n\nã¾ãšã€ã“ã¡ã‚‰ã®ç’°å¢ƒã§å®Ÿé¨“ã—ã¦ã¿ã¾ã™ã€‚ç´ ç›´ã«\\(0.1\\)ã®å ±é…¬â†’\\(9.0\\)ã®å ±é…¬ã‚’ç›®æŒ‡ã›ã°ã„ã„æ„Ÿã˜ã§ã™ã€‚ ã¾ãŸã€\\(\\gamma=0.99\\)ã¨ã—ã¾ã™ã€‚\n\n\nCode\nclass EpsgApproxVI:\n    def __init__(\n        self,\n        n_states: int,\n        n_actions: int,\n        seed: int = 1,\n        gamma: float = 0.99,\n        epsilon: float = 0.9,\n        epsilon_delta: float = 0.0001,\n    ) -&gt; None:\n        self.sa_count = np.ones((n_states, n_actions))\n        self.sas_count = np.ones((n_states, n_actions, n_states)) / n_states\n        self.r_sum = np.zeros((n_states, n_actions))\n        self.pi = np.ones((n_states, n_actions)) / n_actions\n        self.random_state = np.random.RandomState(seed)\n        self.n_states, self.n_actions = n_states, n_actions\n        self.n_updates = 0\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.epsilon_delta = epsilon_delta\n        self.pi_indices = np.arange(n_states, dtype=np.uint32)\n\n    def learn(\n        self,\n        state: int,\n        action: int,\n        next_state: int,\n        reward: float,\n        is_terminal: bool,\n    ) -&gt; None:\n        self.sa_count[state, action] += 1\n        self.r_sum[state, action] += reward\n        if is_terminal:\n            r = self.r_sum / self.sa_count\n            p = self.sas_count / np.expand_dims(self.sa_count, axis=-1)\n            v, _n_iter = value_iteration(r, p, self.gamma, 1e-2)\n            q = r + self.gamma * np.einsum(\"saS,S-&gt;sa\", p, v)\n            self.pi.fill(0)\n            for state in range(self.n_states):\n                self.pi[state][q[state].argmax()] = 1.0\n            self.n_updates += 1\n        else:\n            self.sas_count[state, action, next_state] += 1\n\n    def act(self, state: int) -&gt; int:\n        if self.random_state.rand() &lt; self.epsilon:\n            self.epsilon -= self.epsilon_delta\n            return self.random_state.choice(self.n_actions)\n        else:\n            return self.random_state.choice(self.n_actions, p=self.pi[state])\n\n\nclass MBIB_EB:\n    def __init__(\n        self,\n        n_states: int,\n        n_actions: int,\n        seed: int = 1,\n        gamma: float = 0.99,\n        beta: float = 0.1,\n    ) -&gt; None:\n        self.sa_count = np.ones((n_states, n_actions))\n        self.sas_count = np.ones((n_states, n_actions, n_states)) / n_states\n        self.r_sum = np.zeros((n_states, n_actions))\n        self.pi = np.ones((n_states, n_actions)) / n_actions\n        self.random_state = np.random.RandomState(seed)\n        self.n_states, self.n_actions = n_states, n_actions\n        self.n_updates = 0\n        self.gamma = gamma\n        self.beta = beta\n        self.pi_indices = np.arange(n_states, dtype=np.uint32)\n    \n    def learn(\n        self,\n        state: int,\n        action: int,\n        next_state: int,\n        reward: float,\n        is_terminal: bool,\n    ) -&gt; None:\n        self.sa_count[state, action] += 1\n        self.r_sum[state, action] += reward\n        if is_terminal:\n            r = self.r_sum / self.sa_count + self.beta / np.sqrt(self.sa_count)\n            p = self.sas_count / np.expand_dims(self.sa_count, axis=-1)\n            v, _n_iter = value_iteration(r, p, self.gamma, 1e-2)\n            v_is_larger_than_mean = v &gt; v.mean()\n            q = r + self.gamma * np.einsum(\"saS,S-&gt;sa\", p, v)\n            self.pi.fill(0.0)\n            for state in range(self.n_states):\n                if v_is_larger_than_mean[state]:\n                    self.pi[state] = 1.0 / self.n_actions\n                else:\n                    self.pi[state][q[state].argmax()] = 1.0\n            self.n_updates += 1\n        else:\n            self.sas_count[state, action, next_state] += 1\n\n    def act(self, state: int) -&gt; int:\n        return self.random_state.choice(self.n_actions, p=self.pi[state])\n\n\nepsg_vi = EpsgApproxVI(grid_mdp3.n_states(), grid_mdp3.n_actions())\nsimulation(\n    grid_mdp3,\n    10000,\n    epsg_vi.act,\n    learn=epsg_vi.learn,\n    max_visit=100,\n    title=\"Îµ-Greedy\",\n    vis_freq=None,\n)\nmbib_eb = MBIB_EB(grid_mdp3.n_states(), grid_mdp3.n_actions())\nsimulation(\n    grid_mdp3,\n    10000,\n    mbib_eb.act,\n    learn=mbib_eb.learn,\n    max_visit=100,\n    title=\"Startegic Exploration\",\n    vis_freq=None,\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nä¸¡æ–¹ã¨ã‚‚ã€ã„ã„æ„Ÿã˜ã«æ¢æŸ»ã—ã¦ãã‚Œã¦ã„ã‚‹ã‚ˆã†ã«è¦‹ãˆã¾ã™ã€‚ \\(\\epsilon\\)-Greedyã®æ–¹ãŒã€\\(R_t=9.0\\)ãŒã‚‚ã‚‰ãˆã‚‹ã‚´ãƒ¼ãƒ«ã®å‘¨è¾ºã‚’å¤šãæ¢æŸ»ã—ã¦ã„ã¦ã€ è‰¯ã•ãã†ã«è¦‹ãˆã¾ã™ã€‚ ä¸€æ–¹ã§ã€ã‚‚ã†å°‘ã—æ„åœ°æ‚ªãªç’°å¢ƒã®å ´åˆã¯ã©ã†ã§ã—ã‚‡ã†ã‹ï¼Ÿ\n\n\nCode\ngrid_mdp4 = GridMDP(\n    [[0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 3],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n    [[0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 9.0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n    horizon=100,\n    legend_loc=\"lower right\"\n)\n_ = grid_mdp4.show(\"GridMDP3\")\n\n\n\n\n\n\n\n\n\nã“ã®ç’°å¢ƒã§ã¯ã€\\(+5.0\\)ã¨ã‹ã„ã†é‚ªé­”ãã•ã„å ±é…¬ãŒã‚ã‚Šã¾ã™ã€‚ ã—ã‹ã‚‚ã“ã“ã¯ã‚´ãƒ¼ãƒ«ãªã®ã§ã€ã“ã“ã«è¡Œãã¨ã¾ãŸãƒªã‚»ãƒƒãƒˆã—ã¦ã‚„ã‚Šç›´ã—ã§ã™ã€‚ ã“ã“ã‚’ç›®æŒ‡ã™ã‚ˆã†ã«å­¦ç¿’ã—ã¦ã—ã¾ã†ã¨ã€ãªã‹ãªã‹\\(+9.0\\)ã®æ–¹ã«è¡Œãã®ã¯å³ã—ãã†ã«è¦‹ãˆã¾ã™ã€‚ å®Ÿé¨“ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\nepsg_vi = EpsgApproxVI(grid_mdp4.n_states(), grid_mdp4.n_actions())\nsimulation(\n    grid_mdp4,\n    10000,\n    epsg_vi.act,\n    learn=epsg_vi.learn,\n    max_visit=100,\n    title=\"Îµ-Greedy\",\n    vis_freq=None,\n)\nmbib_eb = MBIB_EB(grid_mdp4.n_states(), grid_mdp4.n_actions())\nsimulation(\n    grid_mdp4,\n    10000,\n    mbib_eb.act,\n    learn=mbib_eb.learn,\n    max_visit=100,\n    title=\"Startegic Exploration\",\n    vis_freq=None,\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\näºˆæƒ³é€šã‚Šã€\\(\\epsilon\\)-Greedyã®æ–¹ã¯å³ä¸Šã°ã‹ã‚Šè¡Œã£ã¦ã—ã¾ã£ã¦ã‚¤ãƒã‚¤ãƒãªæ„Ÿã˜ã«ãªã‚Šã¾ã—ãŸã€‚\nä»¥ä¸Šã®çµæœã‹ã‚‰ã€ - é‚ªé­”ãŒãªãé·ç§»é–¢æ•°ãŒå¯¾ç§°ãªçŠ¶æ…‹ç©ºé–“ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ã®ã‚ˆã†ã«è€ƒãˆã‚‰ã‚Œã‚‹ã‚‚ã®ï¼‰ã§ã¯ã€ã‚ã‚Šã‚ã„ç°¡å˜ã«ãƒ‡ãƒ¼ã‚¿åé›†ãŒã§ãã‚‹ - é‚ªé­”ãªå ±é…¬ãŒãªã„ç’°å¢ƒã§ã¯ã€ã‚ã‚Šã‚ã„ç°¡å˜ã«ãƒ‡ãƒ¼ã‚¿åé›†ãŒã§ãã‚‹\nã¨ã„ã†2ç‚¹ãŒè¨€ãˆã‚‹ã‹ã¨æ€ã„ã¾ã™ã€‚ ãƒ¯ãƒ¼ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’è€ƒãˆã‚‹ã¨æ¢æŸ»ãŒé›£ã—ã„ã®ã‚‚äº‹å®Ÿã§ã™ãŒã€å®Ÿç”¨ä¸Šã¯é›£ã—ã„ã‚±ãƒ¼ã‚¹ã‚’è€ƒãˆã‚‹ã‚ˆã‚Šé‚ªé­”ãªå ±é…¬ã‚’æ’é™¤ã™ã‚‹ ã“ã¨ã‚’è€ƒãˆã‚‹ã®ãŒé‡è¦ã§ã™ã€‚"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html#a-å‚è€ƒæ–‡çŒ®ãªã©",
    "href": "posts/understanding-what-makes-rl-difficult.html#a-å‚è€ƒæ–‡çŒ®ãªã©",
    "title": "ã‚ˆã‚Šè‰¯ã„å•é¡Œè¨­è¨ˆã¸å‘ã‘ã¦ï¼š ä½•ãŒå¼·åŒ–å­¦ç¿’ã‚’é›£ã—ãã™ã‚‹ã®ã‹ã‚’ç†è§£ã—ã‚ˆã†",
    "section": "3.A å‚è€ƒæ–‡çŒ®ãªã©",
    "text": "3.A å‚è€ƒæ–‡çŒ®ãªã©\n\nOn the Sample Complexity of Reinforcement Learning\nReward-Free Exploration for Reinforcement Learning\nSample Complexity Bounds of Exploration\nAn analysis of model-based Interval Estimation for Markov Decision Processes\n\n3.1ã§ç´¹ä»‹ã—ãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ä¸€å¿œ2.ã®æ–‡çŒ®ã‚’å‚è€ƒã«ã—ã¦ã„ã¾ã™ãŒã€åƒ•ãŒã•ã£ãé©å½“ã«è€ƒãˆãŸ(ã¯ï¼Ÿ)ã‚‚ã®ã§ã™ã€‚ ç†è«–ä¿è¨¼ãŒã‚ã‚‹ã‹ã¯ã‚ã‚„ã—ã„ã¨æ€ã„ã¾ã™ã€‚ 3.2ã®ã‚„ã¤ã¯MBIB-EB(4.)ã«ä¼¼ã¦ã„ã¾ã™ãŒã€æ–¹ç­–ã®ç·©å’ŒãŒå…¥ã£ã¦ã„ã‚‹ç‚¹ãŒé•ã„ã¾ã™ã€‚ ç·©å’Œã‚‚åƒ•ãŒé©å½“ã«è€ƒãˆãŸã‚‚ã®ãªã®ã§ã™ãŒã€å…¥ã‚ŒãŸæ–¹ãŒæ€§èƒ½ãŒè‰¯ã‹ã£ãŸã®ã§å…¥ã‚Œã¦ã¿ã¾ã—ãŸã€‚ è‰¯ã„å­ã®çš†ã•ã‚“ã¯çœŸä¼¼ã—ãªã„ã§ãã ã•ã„ã€‚"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html#footnotes",
    "href": "posts/understanding-what-makes-rl-difficult.html#footnotes",
    "title": "ã‚ˆã‚Šè‰¯ã„å•é¡Œè¨­è¨ˆã¸å‘ã‘ã¦ï¼š ä½•ãŒå¼·åŒ–å­¦ç¿’ã‚’é›£ã—ãã™ã‚‹ã®ã‹ã‚’ç†è§£ã—ã‚ˆã†",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSuttonã‚‚èª¤å·®é€†ä¼æ’­ã‚’ä½¿ã†ã®ã«ã¯ãƒˆãƒªãƒƒã‚­ãƒ¼ãªå·¥å¤«ãŒå¿…è¦ã ã¨è¨€ã£ã¦ã„ã¾ã™ã€‚â†©ï¸\nã“ã®å ±é…¬é–¢æ•°ã¯æœ€ã‚‚ç°¡å˜ãªå®šç¾©ã§ã™ã€‚ä»–ã«\\(r: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow \\mathbb{R}\\)(é·ç§»å…ˆã«ä¾å­˜)ã€\\(r: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{E}[R_{s, a}]\\)ï¼ˆç¢ºç‡çš„ï¼‰ãŒã‚ã‚Šã¾ã™ã€‚â†©ï¸\nåˆæœŸçŠ¶æ…‹åˆ†å¸ƒ(é›‘ã«è¨€ã†ã¨ã€ã‚¹ã‚¿ãƒ¼ãƒˆåœ°ç‚¹ã®åˆ†å¸ƒ)ã‚’\\(\\mu(s)\\)ã¨ã™ã‚‹ã¨ã€$ {s} (s)V(s)$ ãŒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç²å¾—ã™ã‚‹å‰²å¼•å ±é…¬å’Œã®æœŸå¾…å€¤ã§ã™ã€‚\\(V_\\pi(s)\\) ãŒæœ€å¤§ãªã‚‰ã“ã‚Œã‚‚æœ€å¤§ã«ãªã‚Šã¾ã™ã€‚â†©ï¸\nã“ã®\\(\\epsilon\\)ã¯\\(\\epsilon\\)-Optimal Policyã®\\(\\epsilon\\)ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚â†©ï¸\nã“ã®è¨˜äº‹ã§ã¯å‹¾é…ã®å°å‡ºã«ã¤ã„ã¦ã¯ä¸€åˆ‡è§¦ã‚Œãªã„ã®ã§ã€åˆ¥é€”è³‡æ–™ãªã©ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚â†©ï¸"
  },
  {
    "objectID": "posts/jax-brax-haiku.html",
    "href": "posts/jax-brax-haiku.html",
    "title": "Jaxãƒ»Braxãƒ»Haikuã§GPUå¼•ãã“ã‚‚ã‚Šå­¦ç¿’",
    "section": "",
    "text": "å¼·åŒ–å­¦ç¿’è‹¥æ‰‹ã®ä¼š Advent Calendar 2021 18æ—¥ç›®\n\n\n0. æ™‚å‹¢ã®ã‚ã„ã•ã¤ã¨ã‹\n\nNote: ã“ã®ãƒ–ãƒ­ã‚°ã¯å¼·åŒ–å­¦ç¿’è‹¥æ‰‹ã®ä¼š Advent Calendar 2021 18æ—¥ç›®ã®è¨˜äº‹ã¨ã—ã¦æ›¸ã‹ã‚Œã¾ã—ãŸ\n\nã“ã‚“ã«ã¡ã¯ã€‚ ã‚³ãƒ­ãƒŠç¦ã‚‚çµ‚ã‚ã‚ŠãŒè¦‹ãˆã¤ã¤ã‚ã‚‹ï¼ˆã¨æ€ã£ãŸã‚‰ã‚ªãƒŸã‚¯ãƒ­ãƒ³æ ªãŒâ€¦ï¼‰2021å¹´ã‚‚ã‚ã¨ã‚ãšã‹ã€‚å¯’ã•ã‚‚å³ã—ããªã£ã¦ãã¾ã—ãŸãŒã€çš†ã•ã‚“å¦‚ä½•ãŠéã”ã—ã§ã—ã‚‡ã†ã‹ã€‚ ã¨ã¯è¨€ã£ãŸã‚‚ã®ã®ã€åƒ•ã¯æ²–ç¸„ã«ã„ã‚‹ã®ã§ã€ãã‚Œã»ã©å¯’ãã¯ãªã„ã®ã§ã™ãŒâ€¦ã€‚\nè‹¥æ‰‹ã®ä¼šã®ã‚¢ãƒ‰ãƒ™ãƒ³ãƒˆã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã¨ã„ã†ã“ã¨ã§ã€å›½å†…ã§ã®ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®æ´»å‹•ã«ã¤ã„ã¦ã€æœ€åˆã«ç·æ‹¬ã—ã¦ã¿ãŸã„ã¨æ€ã„ã¾ã™ã€‚\n\nè‹¥æ‰‹ã®ä¼šã§ã¯æ¨¡å€£å­¦ç¿’ã®å‹‰å¼·ä¼šã‚’ã—ã¾ã—ãŸãŒã€çµå±€2å›ã—ã‹ç¶šãã¾ã›ã‚“ã§ã—ãŸã€‚\nè‹¦æ‰‹ã®ä¼šã®ã‚‚ãã‚‚ãä¼šã¯ãƒ•ãƒªã‚¹ãƒ“ãƒ¼ã‚„ãƒãƒ©ã‚½ãƒ³ã®ç·´ç¿’ã¨ãƒãƒƒãƒ†ã‚£ãƒ³ã‚°ã—ã¦ã‚„ã‚‰ãªããªã£ã¦ã—ã¾ã„ã¾ã—ãŸãŒã€æœ€è¿‘æ—¥ç¨‹ã‚’å¤‰ãˆã¦ã€ç«æ›œæ—¥ã®å¤œã«å§‹ã‚ã¾ã—ãŸã€‚æš‡ãªæ–¹ä¸€ç·’ã«ã‚‚ãã‚‚ãã—ã¾ã—ã‚‡ã†ã€‚\nå¼·åŒ–å­¦ç¿’ã®è¬›ç¾©è³‡æ–™ã®ç¿»è¨³ã‚’ã—ã¦ã„ã¾ã™ã€‚é›£ã—ã„ã§ã™ãŒã€ã‘ã£ã“ã†å‹‰å¼·ã«ãªã‚Šã¾ã™ã€‚æœ‰é™ã‚µãƒ³ãƒ—ãƒ«ã§ã®ãƒã‚¦ãƒ³ãƒ‰ã‚’ã€åˆã‚ã¦å‹‰å¼·ã—ã¾ã—ãŸã€‚èˆˆå‘³ãŒã‚ã‚‹æ–¹ã¯ã€ãœã²ä¸€ç·’ã«ã‚„ã‚Šã¾ã—ã‚‡ã†ã€‚\næ˜¨å¹´ã®ãƒ–ãƒ­ã‚°ã‚’æ›¸ã„ã¦ã‹ã‚‰ã¯ã‚„ä¸€å¹´ã€ã›ã£ã‹ãå°‚ç”¨ã®ãƒ–ãƒ­ã‚°ã‚’ä½œã£ãŸã®ã§ä»Šå¹´ã‚‚ã„ã„æ„Ÿã˜ã«matplotlibèŠ¸å¼·åŒ–å­¦ç¿’ã®è¨˜äº‹ã‚’æ›¸ã„ã¦ã„ããŸã„ã¨æ€ã£ã¦ã„ã¾ã—ãŸãŒã€çµå±€ä½•ã‚‚æ›¸ãã¾ã›ã‚“ã§ã—ãŸã€‚\n\næœ€è¿‘ã¯äººå·¥é€²åŒ–ã‚„äººå·¥ç”Ÿå‘½ã®ç ”ç©¶ã‚‚å§‹ã‚ãŸã®ã§ã€ã‚‚ã¯ã‚„ã€Œå¼·åŒ–å­¦ç¿’ã®äººã€ã¨åä¹—ã£ã¦ã„ã„ã®ã‹ã‚ˆãã‚ã‹ã‚Šã¾ã›ã‚“ãŒã€ä»Šå¾Œã‚‚å›½å†…ã®ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã«ä½•ã‹è²¢çŒ®ã§ãã‚Œã°ã¨æ€ã„ã¾ã™ã€‚\nä»Šå¹´ã¯å¼·åŒ–å­¦ç¿’ã«å¯¾ã™ã‚‹æ¥½è¦³è«–ã‚‚æ‚²è¦³è«–ã‚‚å¤šãç›®ã«ã—ãŸä¸€å¹´ã§ã—ãŸã€‚ David Silverã‚„Suttonã¯Reward is Enoughã¨ã„ã†å¼·æ°—ãªè«–æ–‡ã‚’å‡ºã—ã€çŸ¥çš„ãªã‚·ã‚¹ãƒ†ãƒ ã¯ãŠã‚ˆãå…¨ã¦å ±é…¬æœ€å¤§åŒ–ã§ä½œã‚Œã‚‹ã¨ä¸»å¼µã—ã¾ã—ãŸã€‚ ã•ã™ãŒã«å¼·æ°—ã™ãã‚‹ã¨æ€ã„ã¾ã™ãŒã€ãã®å¾ŒReward is enough for convex MDPsã‚„On the Expressivity of Markov Rewardã¨ã„ã£ãŸãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãªè«–æ–‡ãŒå‡ºã¦ããŸã®ã¯é¢ç™½ã„ã§ã™ã€‚ ã¾ãŸã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å¼·åŒ–å­¦ç¿’ãƒ»æ•™å¸«ãªã—å¼·åŒ–å­¦ç¿’ã®è«–æ–‡ãŒå¢—ãˆã¦ããŸã¨æ€ã„ã¾ã™ã€‚ ã–ã£ãã‚Šã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å¼·åŒ–å­¦ç¿’ = å¼·åŒ–å­¦ç¿’ - æ¢ç´¢ã€æ•™å¸«ãªã—å¼·åŒ–å­¦ç¿’ = å¼·åŒ–å­¦ç¿’ - å ±é…¬ã¨æ€ã£ã¦ã‚‚ã‚‰ã£ã¦å•é¡Œãªã„ã§ã—ã‚‡ã†ã€‚ ä½•ã‚’éš ãã†åƒ•ã®ä¿®å£«è«–æ–‡ã‚‚å˜ãªã‚‹ã€Œéšå±¤å‹å¼·åŒ–å­¦ç¿’ã€ã ã£ãŸã®ã§ã™ãŒã€ãƒªã‚¸ã‚§ã‚¯ãƒˆè«¸èˆ¬ã®äº‹æƒ…ã«ã‚ˆã‚Šæ•™å¸«ãªã—ã«é­”æ”¹é€ ã—ã¦å†æŠ•ç¨¿ã—ã¾ã—ãŸã€‚ Sergey Levineã«ã„ãŸã£ã¦ã¯Understanding the World Through Actionã¨ã„ã†ã‚¿ã‚¤ãƒˆãƒ«ãŒå¼·ã„è«–æ–‡ã®ä¸­ã§ã€ã€Œå¤§é‡ã«ãƒ‡ãƒ¼ã‚¿ã‚’é›†ã‚ã¦ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ•™å¸«ãªã—å¼·åŒ–å­¦ç¿’ã‚’ã™ã‚Œã°ä¸–ç•Œã‚’ç†è§£ã§ãã‚‹ï¼ˆâ‰’ä¸–ç•Œã‚’ç†è§£ã—ã¦ã„ã‚‹ã®ã¨åŒç­‰ã®ã‚·ã‚¹ãƒ†ãƒ ãŒä½œã‚Œã‚‹ï¼Ÿï¼‰ã€ã¨è¨€ã£ã¦ã„ã¾ã™ã€‚é¢ç™½ã„æ–¹å‘æ€§ã ã¨æ€ã„ã¾ã™ã€‚ ä¸€æ–¹ã§ã€ã¿ã‚“ãªå¤§å¥½ããƒ«ãƒ¼ãƒ“ãƒƒã‚¯ã‚­ãƒ¥ãƒ¼ãƒ–è«–æ–‡ã‚’å‡ºã—ãŸOpen AIã®ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ãƒãƒ¼ãƒ ã¯ã€ã€Œã¨ã‚Šã‚ãˆãšä»Šãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹é ˜åŸŸã«æ³¨åŠ›ã™ã‚‹ã€ã¨ã®ã“ã¨ã§è§£æ•£ã—ã¦ã—ã¾ã„ã¾ã—ãŸã€‚ ã“ã®ãƒ–ãƒ­ã‚°ã‚’æ›¸ã„ã¦ã„ã‚‹æœ€ä¸­ã«WebGPTã®è«–æ–‡ã‚’ç›®ã«ã—ã¾ã—ãŸãŒã€ä»Šå¾Œã¯è¨€èªãƒ¢ãƒ‡ãƒ«ï¼‹å¼·åŒ–å­¦ç¿’ã§è‰²ã€…ã‚„ã£ã¦ã„ãã®ã§ã—ã‚‡ã†ã‹ã€‚å“å·ã•ã‚“ã¯å–œã³ãã†ã§ã™ãŒã€åƒ•ãªã‚“ã‹ã¯ã“ã†ã„ã†åˆ°åº•è‡ªåˆ†ã§ã§ããªã„ã‚‚ã®ã¯ã€Œãƒ†ãƒ¬ãƒ“ã®ä¸­ã®ç ”ç©¶ã€ã¨ã„ã†æ„Ÿã˜ãŒã—ã¦ä¸€æ­©å¼•ã„ã¦ã—ã¾ã„ã¾ã™ï¼ˆæœ€è¿‘ã¯ã€ãƒ†ãƒ¬ãƒ“ã¨ã‹ãŸã¨ãˆã«ä½¿ã†ã¨å¤ã„ã®ã‹ãªâ€¦ï¼‰ã€‚ Open AIã®ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ã¯ã€Sim2Realã«ã“ã ã‚ã‚Šã™ããŸã®ã§ã¯ï¼Ÿã¨ã„ã†æ„è¦‹ã‚’æŸæ‰€ã§ãŠèãã—ã¾ã—ãŸã€‚å®Ÿéš›ãã†ãªã®ã‹ã¯çŸ¥ã‚Šã¾ã›ã‚“ãŒã€å¤§è¦æ¨¡ã«ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ã¦Sim2Realã‚’é ‘å¼µã‚‹ã®ã‹ã€å®Ÿæ©Ÿã®ãƒ‡ãƒ¼ã‚¿ã§é ‘å¼µã‚‹ã®ã‹ã¨ã„ã†ã®ã¯ã€é¢ç™½ã„è¦–ç‚¹ã§ã™ã‚ˆã­ã€‚\nOpen AIãŒä»Šã¾ã§ã»ã©å¼·åŒ–å­¦ç¿’ã«æ³¨åŠ›ã—ãªããªã£ãŸã“ã¨ã§ã€Open AI gymã‚’ã¯ã˜ã‚å¼·åŒ–å­¦ç¿’ç ”ç©¶ã§ä½¿ã‚ã‚Œã¦ããŸã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ç¾¤ã«ã‚‚ã€è‰²ã€…ã¨æƒ…å‹¢ã®å¤‰åŒ–ãŒã‚ã‚Šãã†ã§ã™ã€‚ 1. OpenAI Gymã®ãƒ¡ãƒ³ãƒ†ãƒŠãŒå¤‰ã‚ã‚Šã¾ã—ãŸã€‚ã“ã‚Œã‹ã‚‰ã¯Open AIã§ã¯ãªããƒ¡ãƒªãƒ¼ãƒ©ãƒ³ãƒ‰å¤§å­¦ã®å­¦ç”Ÿã•ã‚“ãŒãƒ¡ãƒ³ãƒ†ãƒŠã«ãªã‚‹ã‚ˆã†ã§ã™ã€‚mujoco-pyãªã©é–¢é€£ã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«ã¤ã„ã¦ã¯ç›¸å¤‰ã‚ã‚‰ãšæ”¾ç½®ã•ã‚Œã¦ã„ã¾ã™ã€‚ 2. DeepmindãŒMuJoCoã‚’è²·ã„å–ã£ã¦ç„¡æ–™ã«ã—ã¾ã—ãŸã€‚ä»Šå¾Œã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‚‚å…¬é–‹ã•ã‚Œã‚‹ã‚ˆã†ã§ã™ã€‚ 3. Googleã‹ã‚‰æ–°ã—ãbraxã¨ã„ã†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãŒå…¬é–‹ã•ã‚Œã¾ã—ãŸã€‚\nãã‚“ãªã‚ã‘ã§ã€åƒ•ã¯ã“ã‚Œã¾ã§mujoco-py + gymã§ä½œæˆã—ãŸã‚«ã‚¹ã‚¿ãƒ ç’°å¢ƒã§ãŸãã•ã‚“å®Ÿé¨“ã‚’ã‚„ã£ã¦ãã¾ã—ãŸãŒã€MuJoCoã‚’ä½¿ã†ã«ã—ã¦ã‚‚dm_controlã‚’ä½¿ã†ã¨ã‹ã€ã¯ãŸã¾ãŸbraxã«ã—ã¦ã—ã¾ã†ã¨ã‹ã€åˆ¥ã®é¸æŠè‚¢ã‚’æ¤œè¨ã—ãŸããªã£ã¦ãã¾ã—ãŸã€‚ ã“ã®ãƒ–ãƒ­ã‚°ã§ã¯ã€ã¨ã‚Šã‚ãˆãšbraxã‚’è©¦ã—ã¦ã¿ã‚ˆã†ã¨æ€ã„ã¾ã™ã€‚\n\n\n1. ã¯ã˜ã‚ã«: ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»è¬ãƒ­ãƒœãƒƒãƒˆãƒ»GPU\næœ¬é¡Œã«å…¥ã‚Šã¾ã™ãŒã€ã–ã£ãã‚Šã€å¼·åŒ–å­¦ç¿’ã¨ã¯ã€å ±é…¬ã‹ã‚‰è¡Œå‹•ã‚’å­¦ç¿’ã™ã‚‹æ çµ„ã¿ã ã¨è¨€ã†ã“ã¨ãŒã§ãã¾ã™ã€‚ ã§ã¯ä½•ã®è¡Œå‹•ã‚’å­¦ç¿’ã•ã›ãŸã„ã®ã§ã—ã‚‡ã†ã‹ã€‚ ã‚²ãƒ¼ãƒ ã®AIã ã£ãŸã‚Šã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã ã£ãŸã‚Šã€è‰²ã€…ãªé¸æŠè‚¢ãŒã‚ã‚‹ã¨æ€ã„ã¾ã™ãŒã€ã©ã†ã„ã†ã‚ã‘ã‹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ä¸Šã§å‹•ãè¬ãƒ­ãƒœãƒƒãƒˆã¨ã„ã†ã®ãŒãƒãƒ”ãƒ¥ãƒ©ãƒ¼ãªé¸æŠè‚¢ã§ã™ã€‚\nã“ã®ãƒ–ãƒ­ã‚°ã‚’ã”ã‚‰ã‚“ã®æ–¹ã®ä¸­ã«ã¯ã€ã“ã†ã„ã£ãŸãƒŠãƒŠãƒ•ã‚·ã®ã‚ˆã†ãªè¬ãƒ­ãƒœãƒƒãƒˆã®ç”»åƒã‚’ç›®ã«ã—ãŸã“ã¨ãŒã‚ã‚‹æ–¹ã‚‚å¤šã„ã®ã§ã¯ãªã„ã§ã—ã‚‡ã†ã‹ã€‚\n\n\n\nHalfCheetah\n\n\nã“ã‚Œã¯Open AI gymã®HalfCheetahã¨ã„ã†ãƒ­ãƒœãƒƒãƒˆã§ã™ã€‚è¶³ãŒ2æœ¬ãªã®ã§ãƒãƒ¼ãƒ•ãªã®ã ã¨æ€ã„ã¾ã™ãŒã€ãªã‚“ã¨ã‚‚æ®‹é…·ãªãƒãƒ¼ãƒŸãƒ³ã‚°ã§ã™ã€‚æ„›ç©ã•ã‚Œã‚‹ãŸã‚ç—…æ°—ã®ã¾ã¾å“ç¨®æ”¹è‰¯ã•ã‚Œã¦ããŸçŠ¬çŒ«ã®ã‚ˆã†ãªå“€æ„ãŒæ¼‚ã„ã¾ã™ã€‚\nMuJoCoã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã«ã€Œã“ã“ã¨ã“ã“ãŒã‚¸ãƒ§ã‚¤ãƒ³ãƒˆã§ã€å¯å‹•åŸŸã¯ã“ã†ã§ã™ã€‚åºŠã¯ç™½é»’ã§ãŠé¡˜ã„ã—ã¾ã™ã€ã¿ãŸã„ãªXMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¸¡ã™ã¨ã€ã“ã†ã„ã†ãƒ­ãƒœãƒƒãƒˆã‚’ä½œã£ã¦ãã‚Œã¾ã™ã€‚ ã‚‚ã—ãã¯ã€dm_controlãªã©ã®Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«XMLã‚’ä½œã‚‰ã›ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ ã“ã®ã‚ˆã†ãªè¬ãƒ­ãƒœãƒƒãƒˆãŒå®Ÿé¨“ã§åºƒãä½¿ã‚ã‚Œã¦ã„ã‚‹è¦å› ã¨ã—ã¦ã€ - ã¿ã‚“ãªãŒä½¿ã£ã¦ã„ã‚‹ã‹ã‚‰ - Atariãªã©ã®ã‚²ãƒ¼ãƒ ã‚ˆã‚Šé«˜é€Ÿ - ã‚¸ãƒ§ã‚¤ãƒ³ãƒˆã®é€Ÿã•ãƒ»ä½ç½®ãªã©ã®å®Œå…¨ãªå†…éƒ¨çŠ¶æ…‹ãŒæ‰‹ã«å…¥ã‚‹ - ãƒãƒ«ã‚³ãƒ•æ€§ã«ã¤ã„ã¦å¿ƒé…ã—ãªãã¦ã‚‚ã„ã„ - è‰²ã€…ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã§ãã¦ä¾¿åˆ©ã ã‹ã‚‰ - æ™®é€šã®ãƒ­ãƒœãƒƒãƒˆã‚’è¨“ç·´ã™ã‚‹ãŸã‚ã®ãƒ†ã‚¹ãƒˆã«ã¡ã‚‡ã†ã©ã„ã„ã‹ã‚‰\nãªã©ã®ç†ç”±ãŒã‚ã‚‹ã¨æ€ã„ã¾ã™ãŒã€ãªã‚“ã ã‹ã‚“ã ã¿ã‚“ãªãŒä½¿ã£ã¦ã„ã‚‹ã‹ã‚‰ã¨ã„ã†ã®ãŒå¤§ãã„æ°—ãŒã—ã¾ã™ã€‚\nã¨ã“ã‚ã§ã€ã“ã®MuJoCoã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã¨ã„ã†ã®ã¯éå¸¸ã«é«˜é€Ÿã«å‹•ä½œã™ã‚‹ã®ã§ã™ãŒã€CPUä¸Šã§ã—ã‹å‹•ä½œã—ã¾ã›ã‚“ã€‚ ä»Šæ—¥ä½¿ã‚ã‚Œã¦ã„ã‚‹æ·±å±¤å­¦ç¿’ã®ã‚³ãƒ¼ãƒ‰ã¯ã€ãã®è¨ˆç®—é‡ã®ã»ã¨ã‚“ã©ã‚’å ã‚ã‚‹è¡Œåˆ—æ¼”ç®—ãŒãƒ™ã‚¯ãƒˆãƒ«ä¸¦åˆ—åŒ–ã¨ã¨ã¦ã‚‚ç›¸æ€§ãŒã„ã„ãŸã‚ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚„ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒå¤§ãããªã‚Œã°ãªã‚‹ã»ã©GPUä¸Šã§é«˜é€Ÿã«å‹•ä½œã—ã¾ã™ã€‚ ã¨ãªã‚‹ã¨ã€GPUã§å­¦ç¿’ã‚’å›ã—ã¦ã„ã‚‹å ´åˆã€ã©ã†ã—ã¦ã‚‚CPUã‹ã‚‰GPUã«ãƒ‡ãƒ¼ã‚¿ã‚’è»¢é€ã™ã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ãŒç™ºç”Ÿã—ã€é«˜é€ŸåŒ–ã®å¦¨ã’ã«ãªã‚Šã¾ã™ã€‚ ãã“ã§ã€GPUä¸Šã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œãˆã‚‹ã‚ˆã†ã«ã—ãŸã®ãŒã€ä»Šå›ç´¹ä»‹ã™ã‚‹braxã¨ã„ã†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã§ã™ã€‚\n\n\n2. Jaxã§numpyæ¼”ç®—ã‚’é«˜é€ŸåŒ–ã—ã¦ã¿ã‚‹\nã§ã¯ã€braxã¯CUDAã‹ä½•ã‹ã§æ›¸ã‹ã‚Œã¦ã„ã‚‹ã®ã‹ãªï¼Ÿã¨æ€ã£ãŸã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã€ãªã‚“ã¨å…¨ã¦Pythonã§æ›¸ã‹ã‚Œã¦ã„ã‚‹ã®ã§ã™ã€‚ ãã®éµã¨ãªã‚‹ã®ãŒjaxã¨ã„ã†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚ ãŠã‚‚ã‚€ã‚ã«ã€ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\n! pip install jax\n\n\nãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†’é ­ã«â€™JAX is Autograd and XLAâ€™ã¨ã‚ã‚Šã¾ã™ãŒã€Jaxã¯ - Numpyæ¼”ç®—ã‚’XLAã«å¤‰æ›ã™ã‚‹ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©(Tensorflow) - jax.jit - XLAã¯Tensorflowã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¨ã—ã¦é–‹ç™ºã•ã‚ŒãŸä¸­é–“è¨€èªã§ã€GPU/TPUç”¨ã«ã™ã”ãé€Ÿã„ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã§ãã‚‹ - Numpyæ¼”ç®—ã‚’è¿½è·¡ã—ã¦å‹¾é…ã‚’è¨ˆç®—ã™ã‚‹æ©Ÿèƒ½ - jax.grad/jax.vjp ãªã©\nã®2ã¤ã®ã‚³ã‚¢æ©Ÿèƒ½ã‚’æ ¸ã¨ã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚ ã“ã®ç¯€ã§ã¯ã€ã²ã¨ã¾ãšå‰è€…ã®ã€ŒXLAã«å¤‰æ›ã™ã‚‹ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ã€ã¨ã—ã¦ã®æ©Ÿèƒ½ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã¿ã¾ã™ã€‚\nã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ã¯JITæ–¹å¼ã§å®Ÿè£…ã•ã‚Œã¦ãŠã‚Šã€ 1. jax.jitã«é–¢æ•°fã‚’æ¸¡ã™ (f_compiled = jax.jit(f)ï¼‰ 2. ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã•ã‚Œã‚‹é–¢æ•°f_compiledã‚’æœ€åˆã«å‘¼ã³å‡ºã—ãŸã¨ãã€jaxã¯Pythonã®é–¢æ•°ã‚’XLAã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã™ã‚‹ 3. 2å›ç›®ä»¥é™é–¢æ•°å‘¼ã³å‡ºã—ãŒé«˜é€Ÿã«ãªã‚‹ ã¨ã„ã†å‡¦ç†ã®æµã‚Œã«ãªã‚Šã¾ã™ã€‚\nã§ã¯ã€ã•ã£ããä½•ã‹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ é©å½“ã«å¤©äº•ã‹ã‚‰ãƒœãƒ¼ãƒ«ã‚’è½ã¨ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\nimport typing as t\n\nimport numpy as np\nfrom IPython.display import HTML, clear_output\ntry:\n    import pandas as pd\n    import seaborn as sns\n    from celluloid import Camera\n    from matplotlib import pyplot as plt\n    from matplotlib.animation import ArtistAnimation\nexcept ImportError as _e:\n    ! pip isntall pandas seaborn celluloid\n    clear_output()\n    \n    import pandas as pd\n    import seaborn as sns\n    from celluloid import Camera\n    from matplotlib import pyplot as plt\n    from matplotlib.animation import ArtistAnimation\n\nsns.set_theme(style=\"darkgrid\")\n\nArray = np.ndarray\nGRAVITY = -9.8\n\n\ndef move_balls(\n    ball_positions: Array,\n    ball_velocities: Array,\n    delta_t: float = 0.1,\n) -&gt; Array:\n    accel_x = np.zeros(ball_positions.shape[0])\n    accel_y = np.ones(ball_positions.shape[0]) * GRAVITY * delta_t  # yæ–¹å‘ã«GÎ”tåŠ é€Ÿ\n    new_velocities = np.stack((accel_x, accel_y), axis=1) + ball_velocities\n    new_positions = ball_positions + delta_t * new_velocities\n    return new_positions, new_velocities\n\n\ndef simulate_balls(\n    n_balls: int,\n    n_steps: int = 100,\n    forward: t.Callable[[Array], Array] = move_balls,\n) -&gt; t.List[Array]:\n    p = np.random.uniform(size=(n_balls, 2), low=-50, high=50.0)\n    v = np.random.randn(n_balls, 2)\n    results = [p]\n    for _ in range(n_steps):\n        p, v = forward(p, v)\n        results.append(p)\n    return results\n\n\né©å½“ã«ãƒœãƒ¼ãƒ«ã‚’20å€‹è½ã¨ã—ã¦ã¿ã¾ã™ã€‚\n\n\nCode\ndef ball_animation(balls: t.Iterable[Array]) -&gt; ArtistAnimation:\n    fig = plt.figure(figsize=(8, 8))\n    ax = fig.add_subplot()\n    ax.set_xlim(-50, 50)\n    ax.set_ylim(-50, 50)\n    camera = Camera(fig)\n    for ball_batch in balls:\n        ax.scatter(ball_batch[:, 0], ball_batch[:, 1], color=\"red\", alpha=0.7)\n        camera.snap()\n    return camera.animate()\n\n\nHTML(ball_animation(simulate_balls(20, 40)).to_jshtml())\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nã§ã¯ã€ã“ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ã™ã‚‹ã®ã«ã€ã©ã‚Œãã‚‰ã„æ™‚é–“ãŒã‹ã‹ã‚‹ã§ã—ã‚‡ã†ã‹ã€‚ãƒœãƒ¼ãƒ«ã®æ•°ã‚’å¤‰ãˆã¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\ndef bench(\n    f: t.Callable[..., t.Any],\n    inputs: t.Iterable[t.Any],\n    number: int = 10,\n) -&gt; t.List[float]:\n    import timeit\n\n    return [timeit.Timer(lambda: f(x)).timeit(number=number) for x in inputs]\n\n\ndef bench_and_plot(f: t.Callable[..., t.Any], title: str) -&gt; pd.DataFrame:\n    inputs = [4000, 8000, 16000, 32000, 64000]\n    result = pd.DataFrame({\"x\": inputs, \"y\": bench(f, inputs)})\n    result[\"Method\"] = [title] * len(inputs)\n    ax = sns.lineplot(data=result, x=\"x\", y=\"y\")\n    ax.set_title(title)\n    ax.set_xlabel(\"Num. of balls\")\n    ax.set_ylabel(\"Time (sec.)\")\n    return result\n\n\nnumpy_result = bench_and_plot(simulate_balls, \"NumPy\")\n\n\n\n\n\n\n\n\n\nãŠãŠã‚€ã­ç·šå½¢ã«å®Ÿè¡Œæ™‚é–“ãŒå¢—ãˆã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ã“ã®ã‚³ãƒ¼ãƒ‰ã‚’ã€jaxã‚’ä½¿ã£ã¦é«˜é€ŸåŒ–ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ åŸºæœ¬çš„ã«ã¯numpyã‚’jax.numpyã«ç½®ãæ›ãˆã‚Œã°ã„ã„ã§ã™ã€‚\n\n\nCode\nimport jax\nimport jax.numpy as jnp\n\nJaxArray = jnp.DeviceArray\n\ndef move_balls_jax(\n    ball_positions: JaxArray,\n    ball_velocities: JaxArray,\n    delta_t: float = 0.1,\n) -&gt; JaxArray:\n    accel_x = jnp.zeros(ball_positions.shape[0])\n    accel_y = jnp.ones(ball_positions.shape[0]) * GRAVITY * delta_t\n    new_velocities = jnp.stack((accel_x, accel_y), axis=1) + ball_velocities\n    new_positions = ball_positions + delta_t * new_velocities\n    return new_positions, new_velocities\n\n\nã§ã¯åŒã˜ã‚ˆã†ã«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’ã¨ã£ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\njax_nojit_result = bench_and_plot(\n    lambda n: simulate_balls(n, forward=move_balls_jax),\n    \"JAX (without JIT)\",\n)\n\n\n\n\n\n\n\n\n\nè¬ã®æŒ™å‹•ã‚’è¦‹ã›ã¦ã„ã‚‹ã—ã€ã™ã”ãé…ã„ã§ã™ã­ã€‚ä»Šåº¦ã¯JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ jax.jit(f, backend=\"cpu\")ã§é–¢æ•°ã‚’CPUä¸Šã§å‹•ãXLAã‚³ãƒ¼ãƒ‰ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã§ãã¾ã™ã€‚\n\n\nCode\njax_cpu_result = bench_and_plot(\n    lambda n: simulate_balls(n, forward=jax.jit(move_balls_jax, backend=\"cpu\")),\n    \"JAX (with JIT on CPU)\",\n)\n\n\n\n\n\n\n\n\n\nã™ã”ãé€Ÿããªã‚Šã¾ã—ãŸã€‚ä»Šåº¦ã¯GPUã§ã‚„ã£ã¦ã¿ã¾ã™ã€‚\n\n\nCode\njax_gpu_result = bench_and_plot(\n    lambda n: simulate_balls(n, forward=jax.jit(move_balls_jax, backend=\"gpu\")),\n    \"JAX (with JIT for GPU)\",\n)\n\n\n\n\n\n\n\n\n\nåœ§å€’çš„ã«é€Ÿã„ã§ã™ã­ã€‚ä¸€å¿œç·šå½¢ã«å®Ÿè¡Œæ™‚é–“ãŒå¢—ãˆã¦ã¯ã„ã¾ã™ãŒâ€¦ã€‚ ãªãŠã€ä»Šå›ã¯å­¦å†…ã‚¹ãƒ‘ã‚³ãƒ³ã®NVIDIA P100 GPUã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚\n\n\nCode\nax = sns.lineplot(\n    data=pd.concat(\n        [numpy_result, jax_nojit_result, jax_cpu_result, jax_gpu_result],\n        ignore_index=True,\n    ),\n    x=\"x\",\n    y=\"y\",\n    style=\"Method\",\n    hue=\"Method\",\n)\nax.set_title(\"Ball benchmark\")\nax.set_xlabel(\"Num. of balls\")\nax.set_ylabel(\"Time (sec.)\")\nNone\n\n\n\n\n\n\n\n\n\nã“ã®ãƒœãƒ¼ãƒ«ã®æ•°ã ã¨GPUã¯ç·šå½¢ã«è¨ˆç®—æ™‚é–“ãŒå¢—ãˆã¦ã„ã‚‹ã‚ˆã†ã«è¦‹ãˆã¾ã›ã‚“ã­ã€‚ ã¾ã‚ä½•ã¯ã¨ã‚‚ã‚ã‚Œã€GPUç”¨ã«JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¦ã‚ã’ã‚‹ã¨é€Ÿãã†ã ãªã‚ã€ã¨ã„ã†æ„Ÿã˜ãŒã—ã¾ã™ã€‚\n\n\n3. Jaxã§å‹¾é…ã‚’è¨ˆç®—ã—ã¦ã¿ã‚‹\nJaxã¯å˜ã«é€Ÿã„NumPyã¨ã—ã¦ã®æ©Ÿèƒ½ã«åŠ ãˆã€è‡ªå‹•å¾®åˆ†ã«ã‚ˆã£ã¦ã€é–¢æ•°\\(f(x, y, z, ...)\\)ã®å„\\(x, y, z,...\\)ã«ã‚ˆã‚‹åå¾®åˆ†\\(\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}, \\frac{\\partial f}{\\partial z}, ...\\)ã‚’è¨ˆç®—ã™ã‚‹æ©Ÿèƒ½ã‚’æŒã£ã¦ã„ã¾ã™ã€‚ã“ã“ã§ã¯jax.gradã«ã‚ˆã‚‹å‹¾é…ã®è¨ˆç®—ã ã‘ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚\nãªã‚“ã‹ã€é©å½“ã«é–¢æ•°ã‚’æœ€é©åŒ–ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã¾ãšã¯ã€é©å½“ã«é–¢æ•°ã‚’æ±ºã‚ã¦ã¿ã¾ã™ã€‚ \\(z = x^2 + y^2 + y\\) ã«ã—ã¾ã—ãŸã€‚\n\n\nCode\ndef f(x, y):\n    return x ** 2 + y ** 2 + y\n\n\ndef plot_f(traj: t.Optional[Array] = None) -&gt; None:\n    x, y = np.meshgrid(np.linspace(-5, 5, 100), np.linspace(-5, 5, 100))\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection=\"3d\")\n    ax.plot_surface(\n        x,\n        y,\n        f(x, y),\n        cmap=sns.color_palette(\"flare\", as_cmap=True),\n        alpha=0.8,\n        linewidth=0,\n    )\n    if traj is not None:\n        ax.plot(traj[:, 0], traj[:, 1], traj[:, 2], color=\"blue\")\n    ax.set_xlabel(\"x\", fontsize=14)\n    ax.set_ylabel(\"y\", fontsize=14)\n    ax.set_zlabel(\"z\", fontsize=14, horizontalalignment=\"right\")\n    ax.set_title(\"f\", fontsize=15)\n\n\nplot_f()\n\n\n\n\n\n\n\n\n\n\\((x, y) = (5, 5)\\)ã§ã®ã“ã®é–¢æ•°ã®å‹¾é…ã‚’è¨ˆç®—ã—ã¦ã¿ã¾ã™ã€‚å‹¾é…ã‚’è¨ˆç®—ã—ã¦ã»ã—ã„å¼•æ•°ã‚’jax.grad(argnums=...)ã§æŒ‡å®šã—ã¾ã™ã€‚\n\n\nCode\njax.grad(f, argnums=(0, 1))(jnp.array(5.0), jnp.array(5.0))\n\n\n(DeviceArray(10., dtype=float32, weak_type=True),\n DeviceArray(11., dtype=float32, weak_type=True))\n\n\n\\(\\frac{\\partial z}{\\partial x}, \\frac{\\partial z}{\\partial y}\\)ã‚’è¨ˆç®—ã—ã¦ãã‚Œã¾ã—ãŸã€‚ ã›ã£ã‹ããªã®ã§ã€æœ€æ€¥é™ä¸‹æ³•ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\ndef steepest_descent(alpha: float = 0.01) -&gt; JaxArray:\n    f_grad = jax.grad(f, argnums=(0, 1))\n    x, y = jnp.array(5.0), jnp.array(5.0)\n    traj = []\n    while True:\n        traj.append((x, y, f(x, y)))\n        x_grad, y_grad = f_grad(x, y)\n        if jnp.linalg.norm(jnp.array([x_grad, y_grad])) &lt; 0.05:\n            break\n        x -= alpha * x_grad\n        y -= alpha * y_grad\n    return jnp.array(traj)\n\nplot_f(steepest_descent())\n\n\n\n\n\n\n\n\n\næœ€æ€¥é™ä¸‹æ–¹å‘ã«é€²ã‚“ã§ãã‚Œã¦ã„ã‚‹ã‚ˆã†ã«è¦‹ãˆã¾ã™ã€‚ ã¨ã“ã‚ã§ã€gradã¯ãƒˆãƒƒãƒ—ãƒ€ã‚¦ãƒ³å‹ãƒªãƒãƒ¼ã‚¹ãƒ¢ãƒ¼ãƒ‰è‡ªå‹•å¾®åˆ†ï¼ˆèª¤å·®é€†ä¼æ’­æ³•ã®é›£ã—ã„è¨€ã„æ–¹ã§ã™ï¼‰ã‚’æ¡ç”¨ã—ã¦ã„ã‚‹ã®ã§ã€ãƒªãƒãƒ¼ã‚¹ãƒ¢ãƒ¼ãƒ‰ã§Vector Jacobian Productã‚’è¨ˆç®—ã™ã‚‹vjpã¨ã„ã†é–¢æ•°ãŒä½¿ã‚ã‚Œã¦ã„ã¾ã™ã€‚ ãƒ•ã‚©ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ¢ãƒ¼ãƒ‰ã§è¨ˆç®—ã™ã‚‹jvpã¨ã„ã†é–¢æ•°ã‚‚ã‚ã‚Šã¾ã™ã€‚ ã“ã®ã‚ãŸã‚Šã®æ©Ÿèƒ½ã¯ã€ãŸã ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å­¦ç¿’ã•ã›ãŸã„ã ã‘ãªã‚‰ã»ã¨ã‚“ã©ä½¿ã„ã¾ã›ã‚“ãŒã€ä¸€å¿œã‚„ã£ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\nprimals, f_vjp = jax.vjp(f, 5.0, 5.0)\nprint(f\"VJP value: {primals.item()} grad: {[x.item() for x in f_vjp(1.0)]}\")\nvalue, grad = jax.jvp(f, (5.0, 5.0), (1.0, 1.0))\nprint(f\"JVP value: {value.item()} grad: {grad.item()}\")\n\n\nVJP value: 55.0 grad: [10.0, 11.0]\nJVP value: 55.0 grad: 21.0\n\n\nãƒ•ã‚©ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆå‹¾é…ã¨ãªã‚“ã‹ã®ãƒ™ã‚¯ãƒˆãƒ«vã¨ã®å†…ç©ãŒã§ã¦ãã¾ã™ã€‚ ã“ã®ã‚ãŸã‚Šã€è‰²ã€…ãªæ•™ç§‘æ›¸ã«æ›¸ã„ã¦ã‚ã‚‹ã¨æ€ã„ã¾ã™ãŒã€Forward modeã¨Reverse modeã®é•ã„ãªã©ã€Probabilistic Machine Learning: An Introductionã®13ç« ãŒç‰¹ã«ã‚ã‹ã‚Šã‚„ã™ã„ã¨æ€ã„ã¾ã™ã€‚èˆˆå‘³ãŒã‚ã‚‹æ–¹ã¯å‚è€ƒã«ã—ã¦ã¿ã¦ãã ã•ã„ã€‚\n\n\n4. Braxã‚’ä½¿ã£ã¦ã¿ã‚‹\nã˜ã‚ƒã‚MuJoCoã¿ãŸã„ãªç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚‚Jaxã§æ›¸ã„ã¦ã—ã¾ãˆã°å‹æ‰‹ã«GPUä¸Šã§å‹•ã„ã¦é€Ÿã„ã‚“ã˜ã‚ƒãªã„ï¼Ÿã¨ã„ã†ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã§ä½œã‚‰ã‚ŒãŸã®ãŒbraxã§ã™ã€‚ ç°¡å˜ã«ç‰¹å¾´ã‚’ã¾ã¨ã‚ã¦ã¿ã¾ã™ã€‚\n\nJaxã§è¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€jitã§é«˜é€ŸåŒ–ã§ãã‚‹\nProtocol Bufferã§ã‚·ã‚¹ãƒ†ãƒ ã‚’å®šç¾© (cf.Â MuJoCoã¯XMLï¼‰\ndataclassQPã‚’ä½¿ã£ãŸç°¡æ½”ãªçŠ¶æ…‹è¨˜è¿°\n\nQã¯æ­£æº–åº§æ¨™ã€Pã¯é‹å‹•é‡ã‚‰ã—ã„\n\nOpenAI gymé¢¨ã®Env APIã‚„Antãƒ»Halfcheetahãªã©ã®è¬ãƒ­ãƒœãƒƒãƒˆ\n\nãŠã‚‚ã‚€ã‚ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã¿ã¾ã™ã€‚\n\n\nCode\ntry:\n    import brax\nexcept ImportError:\n    !pip install git+https://github.com/google/brax.git@main\n    clear_output()\n    import brax\n\n\nã•ã£ãã¨åŒã˜ã€ãƒœãƒ¼ãƒ«ã‚’å‹•ã‹ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã•ã£ãã¯xyåº§æ¨™ã§å‹•ã‹ã—ã¾ã—ãŸãŒã€brax\n\n\nCode\ndef make_ball() -&gt; None:\n    config = brax.Config(dt=0.1, substeps=4)\n    # ãƒœãƒ¼ãƒ«ã‚’è¿½åŠ \n    ball = config.bodies.add(name=\"ball\", mass=1)\n    capsule = ball.colliders.add().capsule\n    capsule.radius = 0.5\n    # yåº§æ¨™ã«é‡åŠ›\n    config.gravity.y = GRAVITY\n    return config\n\n\ndef make_qp(p, v) -&gt; brax.QP:\n    return brax.QP(\n        pos=jnp.array([[p[0], p[1], 0.0]]),  # position\n        vel=jnp.array([[v[0], v[1], 0.0]]),  # velocity\n        rot=jnp.zeros((1,4)),  # rotation\n        ang=jnp.zeros((1, 3)),  # angular velocity\n    )\n\n\ndef simulate_one_ball_brax(n_steps: int = 100) -&gt; t.List[Array]:\n    sys = brax.System(make_ball())\n    qp = make_qp([0.0, 48.0], [1.0, 0.0])\n    results = []\n    for _ in range(n_steps):\n        qp, _ = sys.step(qp, [])\n        results.append(qp.pos[:2])\n    return results\n\n\nHTML(ball_animation(simulate_one_ball_brax(40)).to_jshtml())\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nã“ã“ã§ã€4ã¤ã®APIã‚’ä½¿ã„ã¾ã—ãŸã€‚ - brax.Configã§ã‚·ã‚¹ãƒ†ãƒ ã‚’å®šç¾© - brax.System(config)ã§ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œæˆ - brax.QPã§åˆæœŸä½ç½®ãƒ»é€Ÿåº¦ãƒ»ã‚¢ãƒ³ã‚°ãƒ«ç­‰ã‚’ä½œæˆ - brax.System.step(qp, ...)ã§1ã‚¹ãƒ†ãƒƒãƒ—ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ãŸçµæœã‚’å–å¾—\nãƒœãƒ¼ãƒ«ãŒä¸€ã¤ã ã¨ãªã‚“ã¨ãªãç‰©è¶³ã‚Šãªã„ã§ã™ã­ã€‚å¢—ã‚„ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ãã®ãŸã‚ã«ã¯ã€jax.vmapã§sys.stepã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¾ã™ã€‚ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã€vmapã¯å¼•æ•°ã®ãƒ†ãƒ³ã‚½ãƒ«ã«å¯¾ã™ã‚‹æ¼”ç®—ã‚’axis=0ã§ãƒãƒƒãƒåŒ–ã—ã¾ã™ã€‚ ã“ã®ã‚ãŸã‚Šã¯in_axes=(1, 0, ...)ã¨ã‹ã‚„ã‚Œã°èª¿ç¯€ã§ãã¾ã™ãŒã€ä»Šå›ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§OKã§ã™ã€‚\n[make_qp(*pv) for pv in zip(p, v)]ã§ã€List[brax.QP]ã‚’ä½œã£ã¦ã„ã¾ã™ãŒã€ã“ã‚Œã‚’jax.tree_mapã§ã‚‚ã†ä¸€å›QPã«æˆ»ã—ã¦ã„ã¾ã™ã€‚\nList[QP(p=(0, 0), v(0, 0)), QP(..), ...] \nãŒ\nQP(\n    p=[(0, 0), (0.1, 0.2),. ...], \n    v=[(0, 0), (1, 2), ...],\n)\nã«å¤‰æ›ã•ã‚Œã‚‹æ„Ÿã˜ã§ã™ã€‚ ã“ã®ã‚¸ãƒ£ãƒ¼ã‚´ãƒ³ã¯ä¾¿åˆ©ãªã®ã§è¦šãˆã¦ã‚‚ã„ã„ã¨æ€ã„ã¾ã™ã€‚ ã¡ãªã¿ã«ã€treemapã®ãƒãƒ¼ãƒ‰ãŒè‘‰ã‹ã©ã†ã‹ã¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒPyTreeã‹å¦ã‹ã«ã‚ˆã‚Šã¾ã™ã€‚ ã“ã‚Œã¯ã€Œä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’Jaxã¯æš—ã«æœ¨æ§‹é€ ã ã¨ã¿ãªã—ã¾ã™ã€‚ä¸è¶³ãªã‚‰è‡ªåˆ†ã§ç™»éŒ²ã—ã¦ãã ã•ã„ã€ã¨ã„ã†è©±ãªã®ã§ã€æœ€åˆã¯é¢é£Ÿã‚‰ã†ã¨æ€ã„ã¾ã™ã€‚ ã“ã‚Œã‚’é™½ãªAPIã§ã‚„ã‚ã†ã«ã™ã‚‹ã¨Rustã‚„Scalaã«ã‚ã‚‹traitãŒå¿…è¦ãªã®ã§ã€æ‚ªã„è¨­è¨ˆã§ã¯ãªã„ã¨æ€ã„ã¾ã™ãŒã€‚ ã¨ã„ã†ã‚ã‘ã§ã€ã‚³ãƒ¼ãƒ‰ã¯ã“ã‚“ãªæ„Ÿã˜ã«ãªã‚Šã¾ã™ã€‚\n\n\nCode\ndef simulate_balls_brax(n_balls: int, n_steps: int = 100) -&gt; t.List[Array]:\n    sys = brax.System(make_ball())\n    p = np.random.uniform(size=(n_balls, 2), low=-50, high=50.0)\n    v = np.random.randn(n_balls, 2)\n    qps = [make_qp(*pv) for pv in zip(p, v)]\n    qps = jax.tree_map(lambda *args: jnp.stack(args), *qps)\n    # ã“ã“ã§\n    step_vmap = jax.jit(jax.vmap(lambda qp: sys.step(qp, [])))\n    results = []\n    for _ in range(n_steps):\n        qps, _ = step_vmap(qps)\n        results.append(qps.pos[:, 0, :2])\n    return results\n\nHTML(ball_animation(simulate_balls_brax(20, 40)).to_jshtml())\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\njitã‚’ä½¿ã‚ãªã„ã¨braxãŒãªãœã‹numpyã®é–¢æ•°ã‚’å‘¼ã¼ã†ã¨ã—ã¦ã‚¨ãƒ©ãƒ¼ã«ãªã£ãŸã®ã§ã€jitã‚‚ä½µç”¨ã—ã¦ã„ã¾ã™ã€‚\n\n\n5. Haikuã§è¬ã®ãƒ­ãƒœãƒƒãƒˆã‚’å­¦ç¿’ã•ã›ã¦ã¿ã‚‹\nã¨ã„ã†ã‚ã‘ã§ã€braxã®ä½¿ã„æ–¹ã‚’ã–ã£ã¨è¦‹ã¦ã¿ã¾ã—ãŸãŒã€æ¯å›è‡ªåˆ†ã§ãƒ­ãƒœãƒƒãƒˆã‚’è€ƒãˆã‚‹ã®ã¯å¤§å¤‰ã ã—æŸ»èª­è€…ã«ã‚‚æ–‡å¥ã‚’è¨€ã‚ã‚Œã‚‹ãªã®ã§ã€ä»Šå›ã¯è¬ãƒ­ãƒœãƒƒãƒˆã‚’å­¦ç¿’ã•ã›ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ open AI gymé¢¨ã®brax.envs.EnvãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚ä»Šå›ã¯Antã‚’è¨“ç·´ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ gym.makeã«ç›¸å½“ã™ã‚‹ã®ãŒbrax.envs.createã§ã™ã€‚ stepã®APIã¯gymã¨é•ã„å†…éƒ¨çŠ¶æ…‹ãƒ»å ±é…¬ãªã©ãŒå…¥ã£ãŸbrax.envs.Stateã¨ã„ã†ã‚¯ãƒ©ã‚¹ã‚’æ¸¡ã—ã¦æ¬¡ã®Stateã‚’å—ã‘å–ã‚‹ã¨ã„ã†ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ã‚¤ã‚¹ã§ã™ã€‚\n\n\nCode\nimport brax.envs\n\n\ndef render_html(sys: brax.System, qps: t.List[brax.QP]) -&gt; HTML:\n    import uuid\n    import brax.io.html\n\n    html = brax.io.html.render(sys, qps)\n    # A weired trick to show multiple brax viewers...\n    html = html.replace(\"brax-viewer\", f\"brax-viewer-{uuid.uuid4()}\")\n    return HTML(html)\n\n\ndef random_ant() -&gt; HTML:\n    env = brax.envs.create(env_name=\"ant\")\n    prng_key = jax.random.PRNGKey(0)\n    state = env.reset(prng_key)\n    qps = [state.qp]\n    step_jit = jax.jit(env.step)\n    for i in range(10):\n        prng_key, action_key = jax.random.split(prng_key)\n        action = jax.random.normal(action_key, shape=(env.action_size,))\n        state = step_jit(state, action)\n        qps.append(state.qp)\n    return render_html(env.sys, qps)\n\n\nrandom_ant()\n\n\n\n\n  \n    brax visualizer\n    \n  \n  \n    \n    \n    \n  \n\n\n\nãªã‚“ã‹ã€è·³ã­ã¦ã„ã¾ã™ã­ã€‚å¬‰ã—ãã†ã€‚ ã§ã¯ã€ã•ã£ããå­¦ç¿’ã•ã›ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ ä»Šå›ã¯ã€æ·±å±¤å¼·åŒ–å­¦ç¿’ã®ä»£è¡¨çš„ãªæ‰‹æ³•ã§ã‚ã‚‹PPOã‚’ä½¿ã£ã¦ã¿ã¾ã™ã€‚ æœ¬å½“ã¯SACã‚‚ç”¨æ„ã—ãŸã‹ã£ãŸã®ã§ã™ãŒã€æ™‚é–“ãŒãªã‹ã£ãŸã®ã§è«¦ã‚ã¾ã—ãŸã€‚ ã¨ã‚Šã‚ãˆãšã€ä¸‰å±¤MLPã‚’ç”¨æ„ã—ã¾ã—ã‚‡ã†ã€‚ ä¾‹ãˆã°ã€ã“ã‚“ãªæ„Ÿã˜ã®ã‚‚ã®ãŒã‚ã‚Œã°ã„ã„ã§ã™ã€‚\n\n\nCode\ndef mlp_v1(\n    observation: JaxArray,\n    w1: JaxArray,\n    b1: JaxArray,\n    w2: JaxArray,\n    b2: JaxArray,\n    w3: JaxArray,\n    b3: JaxArray,\n) -&gt; JaxArray:\n    x = jnp.dot(observation, w1) + b1\n    x = jnp.tanh(x)\n    x = jnp.dot(x, w2) + b2\n    x = jnp.tanh(x)\n    return jnp.dot(x, w3) + b3\n\n\nã“ã‚Œã‚’jitã—ã¦gradã‚’ã¨ã£ã¦Adamã‹ä½•ã‹ã§ãƒ‘ãƒ©ãƒ¡ã‚¿ã‚’æ›´æ–°ã—ã¦â€¦ã¨ã‚„ã‚Œã°MLPãŒå‹•ãã‚ã‘ã§ã™ãŒã€ãƒ‘ãƒ©ãƒ¡ã‚¿ãŒå¤šã™ãã¦ã¡ã‚‡ã£ã¨é¢å€’ã§ã™ã­ã€‚ ãã“ã§ã“ã“ã§ã¯ã€jaxã§ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’è¨“ç·´ã™ã‚‹éš›ã€ãƒ‘ãƒ©ãƒ¡ã‚¿ã®ç®¡ç†ãªã©ã‚’ã‚„ã£ã¦ãã‚Œã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã‚ã‚‹Haikuã‚’ä½¿ã£ã¦ã¿ã¾ã™ã€‚ ãªãŠã€braxå…¬å¼ã®examplesã§ã¯Flaxã‚’ä½¿ã£ã¦ã„ã¾ã™ã€‚ æ­£ç›´Haikuã‚‚Flaxã‚‚ãã“ã¾ã§å¤‰ã‚ã‚‰ãªã„ã®ã§ã™ãŒã€Flaxã®æ–¹ãŒã‚„ã‚„APIã®æŠ¼ã—ãŒå¼·ã„ï¼ˆPyTorchã§ã„ã†nn.Moduleç›¸å½“ã®ã‚‚ã®ãŒdataclassã§ãªã„ã¨ã„ã‘ãªã‹ã£ãŸã‚Šã¨ã‹ï¼‰å°è±¡ãŒã‚ã‚Šã¾ã™ã€‚ ã¾ãŸã€Haikuã¯DeepmindãŒã€Flaxã¯GoogleãŒé–‹ç™ºã—ã¦ã„ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨ãªã‚Šã¾ã™ã€‚ ã¨ã‚Šã‚ãˆãšã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\ntry:\n    import haiku as hk\n    import optax\n    import chex\n    import distrax\nexcept ImportError as e:\n    ! pip install git+https://github.com/deepmind/dm-haiku \\\n        git+https://github.com/deepmind/optax \\\n        git+https://github.com/deepmind/chex \\\n        git+https://github.com/deepmind/distrax\n    \n    import haiku as hk\n    import optax\n    import chex\n    import distrax\n    \n    clear_output()\n\n\nãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å®šç¾©ã™ã‚‹ãŸã‚ã®Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯theanoã€tensorflowã¨è‰²ã€…ã‚ã‚Šã¾ã—ãŸãŒã€æœ€è¿‘ã¯torch.nn.Moduleã‚„chainer.Linkã®ã‚ˆã†ã«ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®é‡ã¿ãƒ»forwardã®å‡ºåŠ›ãƒ»éš£æ¥ã—ã¦ã„ã‚‹ãƒãƒ¼ãƒ‰ã‚’è¨˜éŒ²ã—ãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½¿ã£ã¦ã€å‹•çš„ã«è¨ˆç®—ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰ã™ã‚‹ã‚‚ã®ãŒå¤šã„ã‹ã¨æ€ã„ã¾ã™ã€‚ ã—ã‹ã—ã€Haikuã«ã‚ˆã‚‹ãã‚Œã¯å°‘ã—ç•°ãªã‚Šã¾ã™ã€‚ãƒã‚¤ãƒ³ãƒˆã¯ã€å‹¾é…ã‚’è¨ˆç®—ã™ã‚‹éƒ¨åˆ†ã¯JaxãŒæ‹…å½“ã™ã‚‹ã®ã§ã€Haikuã¯ãŸã ã€Œãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ‘ãƒ©ãƒ¡ã‚¿ã‚’ç®¡ç†ã™ã‚‹ã ã‘ã€ã§ã„ã„ã¨ã„ã†ã“ã¨ã§ã™ã€‚ ãã®ãŸã‚ã«ã€Haikuã¯transformã¨ã„ã†APIã‚’ç”¨æ„ã—ã¦ã„ã¾ã™ã€‚ ã“ã‚Œã¯è¦‹ãŸã»ã†ãŒæ—©ã„ã§ã—ã‚‡ã†ã€‚\n\n\nCode\ndef mlp_v2(observation: JaxArray) -&gt; JaxArray:\n    w1 = hk.get_parameter(\"w1\", shape=[observation.shape[1], 3], init=jnp.ones)\n    b1 = hk.get_parameter(\"b1\", shape=[3], init=jnp.zeros)\n    w2 = hk.get_parameter(\"w2\", shape=[3, 3], init=jnp.ones)\n    b2 = hk.get_parameter(\"b2\", shape=[3], init=jnp.zeros)\n    w3 = hk.get_parameter(\"w3\", shape=[3, 2], init=jnp.ones)\n    b3 = hk.get_parameter(\"b3\", shape=[2], init=jnp.zeros)\n    x = jnp.dot(observation, w1) + b1\n    x = jnp.tanh(x)\n    x = jnp.dot(x, w2) + b2\n    x = jnp.tanh(x)\n    return jnp.dot(x, w3) + b3\n\nprng_seq = hk.PRNGSequence(0)  # ã“ã‚Œã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã«ã™ã‚‹ã®ã¯è‰¯ããªã„ã§ã™ã€‚çœŸä¼¼ã—ãªã„ã§\ninit, apply = hk.transform(mlp_v2)  # transformã™ã‚‹\n# initã¯ä¹±æ•°ã‚·ãƒ¼ãƒ‰ãƒ»ã‚¤ãƒ³ãƒ—ãƒƒãƒˆã‚’å—ã‘å–ã£ã¦ã€åˆæœŸåŒ–ã—ãŸãƒ‘ãƒ©ãƒ¡ã‚¿ã‚’è¿”ã™é–¢æ•°\nparams = init(next(prng_seq), jnp.zeros((10, 2)))\nprint(params)\n# applyã¯ãƒ‘ãƒ©ãƒ¡ã‚¿ãƒ»ä¹±æ•°ã‚·ãƒ¼ãƒ‰ãƒ»ã‚¤ãƒ³ãƒ—ãƒƒãƒˆã‚’å—ã‘å–ã£ã¦ã€å‡ºåŠ›ã‚’è¿”ã™é–¢æ•°\noutput = apply(params, next(prng_seq), jnp.zeros((10, 2)))\n\n\nFlatMap({\n  '~': FlatMap({\n         'w1': DeviceArray([[1., 1., 1.],\n                            [1., 1., 1.]], dtype=float32),\n         'b1': DeviceArray([0., 0., 0.], dtype=float32),\n         'w2': DeviceArray([[1., 1., 1.],\n                            [1., 1., 1.],\n                            [1., 1., 1.]], dtype=float32),\n         'b2': DeviceArray([0., 0., 0.], dtype=float32),\n         'w3': DeviceArray([[1., 1.],\n                            [1., 1.],\n                            [1., 1.]], dtype=float32),\n         'b3': DeviceArray([0., 0.], dtype=float32),\n       }),\n})\n\n\nã“ã‚“ãªæ„Ÿã˜ã«ãªã‚Šã¾ã™ã€‚ ã¾ã¨ã‚ã‚‹ã¨ã€ - transform(f)ã¯äºŒã¤ã®é–¢æ•°initã€applyã‚’ã‹ãˆã™ - transformã¯fã‚’ã€fã®ä¸­ã§haiku.get_parameterã‚’ä½¿ã£ã¦å‘¼ã³å‡ºã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ã‚¿ã‚’å…¥åŠ›ã¨ã™ã‚‹é–¢æ•°ã«å¤‰æ›ã™ã‚‹ - initã¯ãƒ‘ãƒ©ãƒ¡ã‚¿ã‚’åˆæœŸåŒ–ã—ã¦è¿”ã™ã€‚ãƒ‘ãƒ©ãƒ¡ã‚¿ã¯FlatMapã¨ã„ã†ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã ãŒã“ã‚Œã¯ã»ã¨ã‚“ã©dictã¨åŒã˜ - applyã¯ä¸ãˆã‚‰ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ã‚¿ã‚’ä½¿ã£ã¦æ‰€æœ›ã®è¨ˆç®—ã‚’è¡Œã† ã¨ã„ã†æ„Ÿã˜ã§ã™ã­ã€‚\nã¤ã„ã§ã«ã€ä¸Šã®ä¾‹ã§ã¯hk.PRNGSequenceã¨ã„ã†PRNGKeyã®æ›´æ–°ã‚’å‹æ‰‹ã«ã‚„ã£ã¦ãã‚Œã‚‹ã‚‚ã®ã‚’ä½¿ã£ã¦ã„ã¾ã™ã€‚\nã—ã‹ã—ã€ã“ã‚Œã§ã‚‚ã¾ã é¢å€’ã§ã™ã­ã€‚ å®Ÿéš›ã®ã¨ã“ã‚ã€ã‚ˆãä½¿ã†ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã—ã¦ã¾ã¨ã¾ã£ã¦ã„ã‚‹ã®ã§ã€ã“ã‚Œã‚’ä½¿ãˆã°ã„ã„ã§ã™ã€‚\n\n\nCode\ndef mlp_v3(output_size: int, observation: JaxArray) -&gt; JaxArray:\n    x = hk.Linear(64)(observation)\n    x = jnp.tanh(x)\n    x = hk.Linear(64)(observation)\n    x = jnp.tanh(x)\n    return hk.Linear(output_size)(observation)\n\n\nã“ã‚Œã‚’ä½¿ã£ã¦ã€PPOã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ›¸ã„ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ æ–¹ç­–ã¯æ¨™æº–åå·®ãŒçŠ¶æ…‹ã«ä¾å­˜ã—ãªã„æ­£è¦åˆ†å¸ƒã«ã—ã¾ã™ã€‚\n\n\nCode\nclass NetworkOutput(t.NamedTuple):\n    mean: JaxArray\n    stddev: JaxArray\n    value: JaxArray\n\n\ndef policy_and_value(action_size: int, observation: JaxArray) -&gt; NetworkOutput:\n    mean = mlp_v3(output_size=action_size, observation=observation)\n    value = mlp_v3(output_size=1, observation=observation)\n    logstd = hk.get_parameter(\"logstd\", (1, action_size), init=jnp.zeros)\n    stddev = jnp.ones_like(mean) * jnp.exp(logstd + 1e-8)\n    return NetworkOutput(mean, stddev, value)\n\n\nã“ã‚Œã ã‘ã§ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®é‡ã¿ã¯TruncatedNormalã§åˆæœŸåŒ–ã•ã‚Œã¾ã™ã€‚ä»Šå›ã¯å…¨éƒ¨ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã¾ã¾ã«ã—ã¾ã—ãŸã€‚\næ¬¡ã«ã€ã“ã‚Œã‚’ä½¿ã£ã¦ã€ç’°å¢ƒã¨ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒˆã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ã¿ã¾ã™ã€‚ ã„ã¾ã€braxã®åˆ©ç‚¹ã‚’æ´»ã‹ã™ãŸã‚ã«ã€ 1. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‹ã‚‰æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã‚µãƒ³ãƒ—ãƒ«ã—ã¦ 2. ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã§æ¬¡ã®çŠ¶æ…‹ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ ã¨ã„ã†éç¨‹ã‚’ã™ã¹ã¦jax.jitã®ä¸­ã§ã‚„ã‚‹ã®ãŒç†æƒ³ã§ã™ã‚ˆã­ã€‚\nã§ã™ã‹ã‚‰ã€ãŸã¨ãˆã°ã“ã‚“ãªæ„Ÿã˜ã«ã‚„ã‚Œã°ã„ã„ã§ã™ã€‚\n\n\nCode\nAction = JaxArray\n\n\ndef make_step_function(\n    env: brax.envs.Env,\n) -&gt; t.Tuple[t.Callable[..., t.Any], t.Callable[..., t.Any]]:\n    def step(state: brax.envs.State) -&gt; t.Tuple[brax.envs.State, NetworkOutput, Action]:\n        out = policy_and_value(env.action_size, state.obs)\n        policy = distrax.MultivariateNormalDiag(out.mean, out.stddev)\n        action = policy.sample(seed=hk.next_rng_key())  # transformã™ã‚‹ã¨ã“ã‚ŒãŒä½¿ãˆã¾ã™\n        state = env.step(state, jnp.tanh(action))\n        return state, out, action\n\n    init, apply = hk.transform(step)\n    return jax.jit(init), jax.jit(apply)\n\n\nã“ã“ã§ã€è¡Œå‹•ã®ã‚µãƒ³ãƒ—ãƒ«ã«ã¯distraxã¨ã„ã†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã„ã¾ã—ãŸã€‚ å¹³å‡å€¤ã«ãƒã‚¤ã‚ºã‚’ã„ã‚Œã‚‹ã ã‘ãªã®ã§ã€ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã£ã¦ã‚‚ã‚ã¾ã‚Šå¤‰ã‚ã‚‰ãªã„ã®ã§ã™ãŒâ€¦ã€‚ ã„ã¾ã€å„ã‚¸ãƒ§ã‚¤ãƒ³ãƒˆã«å¯¾ã—ã¦åŠ ãˆã‚‹åŠ›ãŒã€ãã‚Œãã‚Œç‹¬ç«‹ãªæ­£è¦åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚Œã‚‹ã¨ä»®å®šã—ã¦ã„ã‚‹ã®ã§ã€MutliVariateNormDiag(å…±åˆ†æ•£è¡Œåˆ—ãŒå¯¾è§’è¡Œåˆ—ã«ãªã‚‹å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒï¼‰ã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã—ã¦ã„ã¾ã™ã€‚ distrax.Independentã¨distrax.Normalã‚’ä½¿ã£ã¦ã‚‚åŒã˜ã“ã¨ãŒã§ãã¾ã™ã€‚ è¡Œå‹•ã¯ä¸€å¿œtanhã§\\([-1, 1]\\)ã®ç¯„å›²ã«ãªã‚‰ã—ã¦ã„ã¾ã™ã€‚\nã¡ã‚‡ã£ã¨è©¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\nant = brax.envs.create(env_name=\"ant\", batch_size=1)\ninit, step = make_step_function(ant)\ninitial_state = jax.jit(ant.reset)(next(prng_seq))\nparams = init(next(prng_seq), initial_state)\n_next_state, out, action = step(params, next(prng_seq), initial_state)\n# chexã¯ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™\nchex.assert_shape((out.mean, out.stddev, action), (1, ant.action_size))\n\n\nã¨ã„ã†ã‚ã‘ã§ç„¡äº‹ã«stepã‚’JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¦é«˜é€ŸåŒ–ã§ãã¾ã—ãŸã€‚ resetã¯ã»ã¨ã‚“ã©å‘¼ã°ãªã„ã®ã§åˆ¥ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ãªãã¦ã‚‚ã„ã„ã®ã§ã™ãŒã€jitã—ãªã„ã¨braxãŒjnp.DeviceArrayã®ã‹ã‚ã‚Šã«numpyã‚’ä½¿ã„ãŸãŒã£ã¦å°‘ã—é¢å€’ãªã®ã§jitã—ã¦ã„ã¾ã™ã€‚\nã‚ã¨ã¯PPOã‚’å®Ÿè£…ã—ã¦ã„ãã¾ã™ãŒã€æ™‚é–“ã®éƒ½åˆã§æ‰‹çŸ­ã‹ã«ã„ãã¾ã™ã€‚ ã¾ãšã¯GAEã§ã™ã­ã€‚ æ™®é€šã«æ›¸ãã¨jax.jitãŒãƒ«ãƒ¼ãƒ—ã‚¢ãƒ³ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚’è¡Œã£ã¦ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚é–“ãŒæ¿€é…ã«ãªã‚‹ã®ã§ã€jax.lax.fori_loopã¨ã„ã†é»’é­”è¡“ã‚’ä½¿ã„ã¾ã™ã€‚ ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚å®šæ•°ã¯static_argnumsã§æŒ‡å®šã—ã¾ã™ã€‚ vmapã§å„ãƒ¯ãƒ¼ã‚«ãƒ¼ç”¨ã«ä¸¦åˆ—åŒ–ã—ã¾ã™ã€‚\n\n\nCode\nimport functools\n\n@functools.partial(jax.jit, static_argnums=2)\ndef gae(\n    r_t: JaxArray,\n    discount_t: JaxArray,\n    lambda_: float,\n    values: JaxArray,\n) -&gt; chex.Array:\n    chex.assert_rank([r_t, values, discount_t], 1)\n    chex.assert_type([r_t, values, discount_t], float)\n    lambda_ = jnp.ones_like(discount_t) * lambda_\n    delta_t = r_t + discount_t * values[1:] - values[:-1]\n    n = delta_t.shape[0]\n\n    def update(i: int, advantage_t: JaxArray) -&gt; JaxArray:\n        t_ = n - i - 1\n        adv_t = delta_t[t_] + lambda_[t_] * discount_t[t_] * advantage_t[t_ + 1]\n        return jax.ops.index_update(advantage_t, t_, adv_t)\n\n    advantage_t = jax.lax.fori_loop(0, n, update, jnp.zeros(n + 1))\n    return advantage_t[:-1]\n\n\nbatched_gae = jax.vmap(gae, in_axes=(1, 1, None, 1), out_axes=1)\n\n\nãªã‚“ã‹ãƒ–ãƒ­ã‚°ã§æ›¸ãã«ã¯é»’é­”è¡“ã™ãã‚‹æ°—ã‚‚ã—ã¾ã™ãŒâ€¦ã€‚\næ¬¡ã«å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒãƒã‚’æ§‹æˆã™ã‚‹éƒ¨åˆ†ã§ã™ã­ã€‚ ã“ã‚Œã¯ã€æ™®é€šã«PyTorchã¨ã‹ã¨å¤‰ã‚ã‚‰ãªã„ã§ã™ã€‚\n\n\nCode\nimport dataclasses\n\n\n@chex.dataclass\nclass RolloutResult:\n    \"\"\"\n    Required experiences for PPO.\n    \"\"\"\n\n    observations: t.List[JaxArray]\n    actions: t.List[JaxArray] = dataclasses.field(default_factory=list)\n    rewards: t.List[JaxArray] = dataclasses.field(default_factory=list)\n    terminals: t.List[JaxArray] = dataclasses.field(default_factory=list)\n    outputs: t.List[NetworkOutput] = dataclasses.field(default_factory=list)\n\n    def append(\n        self,\n        *,\n        observation: JaxArray,\n        action: JaxArray,\n        reward: JaxArray,\n        output: NetworkOutput,\n        terminal: JaxArray,\n    ) -&gt; None:\n        self.observations.append(observation)\n        self.actions.append(action)\n        self.rewards.append(reward)\n        self.outputs.append(output)\n        self.terminals.append(terminal)\n\n    def clear(self) -&gt; None:\n        self.observations = [self.observations[-1]]\n        self.actions.clear()\n        self.rewards.clear()\n        self.outputs.clear()\n        self.terminals.clear()\n\n\nclass Batch(t.NamedTuple):\n    \"\"\"Batch for PPO, also used as minibatch by indexing.\"\"\"\n\n    observation: JaxArray\n    action: JaxArray\n    reward: JaxArray\n    advantage: JaxArray\n    value_target: JaxArray\n    log_prob: JaxArray\n\n    def __getitem__(self, idx: Array) -&gt; \"Batch\":\n        return self.__class__(\n            observation=self.observation[idx],\n            action=self.action[idx],\n            reward=self.reward[idx],\n            advantage=self.advantage[idx],\n            value_target=self.value_target[idx],\n            log_prob=self.log_prob[idx],\n        )\n\n\n@jax.jit\ndef make_batch(rollout: RolloutResult, next_value: JaxArray) -&gt; Batch:\n    action = jnp.concatenate(rollout.actions)\n    mean, stddev, value = jax.tree_map(lambda *x: jnp.concatenate(x), *rollout.outputs)\n    log_prob = distrax.MultivariateNormalDiag(mean, stddev).log_prob(action)\n    reward = jnp.stack(rollout.rewards)\n    mask = 1.0 - jnp.stack(rollout.terminals)\n    value = jnp.concatenate(\n        (value.reshape(reward.shape), next_value.reshape(1, -1)),\n        axis=0,\n    )\n    advantage = batched_gae(reward, mask * 0.99, 0.95, value)\n    value_target = advantage + value[:-1]\n    return Batch(\n        observation=jnp.concatenate(rollout.observations[:-1]),\n        action=action,\n        reward=jnp.ravel(reward),\n        advantage=jnp.ravel(advantage),\n        value_target=jnp.ravel(value_target),\n        log_prob=log_prob,\n    )\n\n\næ™®é€šã®dataclassesã¯jitã§ããªã„ã®ã§ã€chex.dataclassã‚’ä½¿ã„ã¾ã™ã€‚ ã•ã£ãå°‘ã—ã ã‘è§¦ã‚Œã¾ã—ãŸãŒã€chex.dataclassã¯ä½œæˆã—ãŸdataclassã‚’PyTreeã¨ã—ã¦jaxã«ç™»éŒ²ã—ã¦ãã‚Œã¾ã™ã€‚ å®Ÿã¯flax.struct.dataclassã¨ã„ã†ã ã„ãŸã„åŒã˜ã‚‚ã®ã‚‚ã‚ã£ã¦ã€braxã®å†…éƒ¨ã§ã¯ã“ã‚Œã‚’ä½¿ã£ã¦ã„ã‚‹ã‚ˆã†ã§ã™ã€‚ ã¾ãŸ\\(\\gamma = 0.99, \\lambda = 0.95\\)ã¨ã—ã¾ã—ãŸã€‚\nã„ã‚ˆã„ã‚ˆå­¦ç¿’ã®éƒ¨åˆ†ã§ã™ã­ã€‚ ã¾ãšã€æå¤±é–¢æ•°ã‚’jax.gradã§ãã‚‹ã‚ˆã†ã«æ›¸ãã¾ã™ã€‚\n\n\nCode\ndef ppo_loss(action_size: int, batch: Batch) -&gt; JaxArray:\n    mean, stddev, value = policy_and_value(action_size, batch.observation)\n    # Policy loss\n    policy = distrax.MultivariateNormalDiag(mean, stddev)\n    log_prob = policy.log_prob(batch.action)\n    prob_ratio = jnp.exp(log_prob - batch.log_prob)\n    clipped_ratio = jnp.clip(prob_ratio, 0.8, 1.2)\n    clipped_obj = jnp.fmin(prob_ratio * batch.advantage, clipped_ratio * batch.advantage)\n    policy_loss = -jnp.mean(clipped_obj)\n    # Value loss\n    value_loss = jnp.mean(0.5 * (value - batch.value_target) ** 2)\n    # Entropy regularization\n    entropy_mean = jnp.mean(policy.entropy(), axis=-1)\n    return policy_loss + value_loss - 0.001 * entropy_mean\n\n\n\\(\\epsilon = 0.2\\)ã§å›ºå®šã—ã¦ã„ã‚‹ã®ã§ã€\\([1 - 0.2, 1 + 0.2]\\)ã®ç¯„å›²ã§ã‚¯ãƒªãƒƒãƒ—ã—ã¾ã™ã€‚\nã§ã¯ã“ã‚Œã‚’ä½¿ã£ã¦ã€ä»Šåº¦ã¯ãƒ‘ãƒ©ãƒ¡ã‚¿ã®æ›´æ–°ã‚’å…¨éƒ¨jitã«ã¤ã£ã“ã‚“ã§ã¿ã¾ã—ã‚‡ã†ã€‚ ãƒ‘ãƒ©ãƒ¡ã‚¿ã®æ›´æ–°ã«ã¯optaxã¨ã„ã†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã„ã¾ã™ã€‚è‰²ã€…ãªSGDã®ãƒãƒªã‚¢ãƒ³ãƒˆã‚’å®Ÿè£…ã—ã¦ã„ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ãŒã€åƒ•ã¯ã»ã¨ã‚“ã©Adamã—ã‹ä½¿ã„ã¾ã›ã‚“â€¦ã€‚\n\n\nCode\nimport optax\n\n\ndef make_update_function(\n    action_size: int,\n    opt_update: optax.TransformUpdateFn,\n) -&gt; t.Callable[..., t.Any]:\n    # hk.Paramsã‚’ä½¿ã„å›ã™ã®ã§initã¯æ¨ã¦ã¦ã„ã„\n    # è¡Œå‹•ã‚’ã‚µãƒ³ãƒ—ãƒ«ã—ãªã„ã®ã§ã€without_apply_rngãŒä½¿ãˆã‚‹\n    _, loss_fn = hk.without_apply_rng(hk.transform(lambda batch: ppo_loss(action_size, batch)))\n    grad_fn = jax.grad(loss_fn)\n\n    # ã“ã“ã§jitã—ã¦ã„ã„\n    @jax.jit\n    def update(\n        params: hk.Params,\n        opt_state: optax.OptState,\n        batch: Batch,\n    ) -&gt; t.Tuple[hk.Params, Batch]:\n        grad = grad_fn(params, batch)\n        updates, new_opt_state = opt_update(grad, opt_state)\n        return optax.apply_updates(params, updates), new_opt_state\n\n    return update\n\n\nã•ã¦ã€ã“ã“ã¾ã§æ¥ãŸã‚‰ã‚ã¨ä¸€æ­©ã§ã™ã­ã€‚ æ¬¡ã«é¢å€’ã§ã™ãŒæ¬¡ã®çŠ¶æ…‹ã®valueã‚’ã¨ã£ã¦ãã¦ãƒãƒƒãƒã‚’ä½œã‚‹éƒ¨åˆ†ã‚’æ›¸ãã¾ã™ã€‚\n\n\nCode\ndef make_next_value_function(action_size: int) -&gt; Batch:\n    def next_value_fn(obs: JaxArray) -&gt; JaxArray:\n        output = policy_and_value(action_size, obs)\n        return output.value\n\n    _, next_value_fn = hk.without_apply_rng(hk.transform(next_value_fn))\n    return jax.jit(next_value_fn)\n\n\né€Ÿåº¦ã‚’æ±‚ã‚ã‚‹ãªã‚‰ã€ã“ã‚Œã¯make_batchã¨ä¸€ç·’ã«jitã—ã¦ã—ã¾ã£ã¦ã‚‚ã„ã„ã§ã™ãŒã€ã¾ã‚é¢å€’ãªã®ã§ã“ã‚Œã§ã‚‚ã„ã„ã§ã—ã‚‡ã†ã€‚\nã§ã¯ææ–™ãŒãã‚ã£ãŸã®ã§ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—ã‚’æ›¸ã„ã¦ã„ãã¾ã—ã‚‡ã†ã€‚ é¢å€’ã§ã™ãŒã€è©•ä¾¡ç”¨ã®environmentã‚‚åˆ¥ã«ä½œã‚Šã¾ã™ã€‚\n\n\nCode\ntry:\n    import tqdm\nexcept ImportError as _e:\n    ! pip install tqdm\n    import tqdm\n    clear_output()\n\n\n\n\nCode\nimport datetime\n\nfrom tqdm.notebook import trange\n\n\ndef sample_minibatch_indices(\n    n_instances: int,\n    n_minibatches: int,\n    prng_key: chex.PRNGKey,\n) -&gt; t.Iterable[JaxArray]:\n    indices = jax.random.permutation(prng_key, n_instances)\n    minibatch_size = n_instances // n_minibatches\n    for start in range(0, n_instances, minibatch_size):\n        yield indices[start : start + minibatch_size]\n\n\ndef train_ppo(\n    env_name: str = \"ant\",\n    n_workers: int = 32,\n    n_steps: int = 2048,\n    n_training_steps: int = 10000000,\n    n_optim_epochs: int = 10,\n    n_minibatches: int = 64,\n    eval_freq: int = 20,\n    eval_workers: int = 16,\n    seed: int = 0,\n) -&gt; HTML:\n    # ç’°å¢ƒã¨ã€ç’°å¢ƒã‚’å«ã‚“ã stepé–¢æ•°ã‚’ä½œã‚‹\n    env = brax.envs.create(env_name=env_name, episode_length=1000, batch_size=n_workers)\n    eval_env = brax.envs.create(\n        env_name=env_name,\n        episode_length=1000,\n        batch_size=eval_workers,\n    )\n    network_init, step = make_step_function(env)\n    _, eval_step = make_step_function(eval_env)\n    eval_reset = jax.jit(eval_env.reset)\n    # ä¹±æ•°\n    prng_seq = hk.PRNGSequence(seed)\n    # åˆæœŸçŠ¶æ…‹\n    state = jax.jit(env.reset)(rng=next(prng_seq))\n    rollout = RolloutResult(observations=[state.obs])\n    # Optimizerã¨ãƒ‘ãƒ©ãƒ¡ã‚¿ã‚’åˆæœŸåŒ–ã™ã‚‹\n    optim = optax.chain(optax.clip_by_global_norm(1.0), optax.adam(3e-4, eps=1e-4))\n    update = make_update_function(env.action_size, optim.update)\n    params = network_init(next(prng_seq), state)\n    opt_state = optim.init(params)\n    # next_value\n    next_value_fn = make_next_value_function(env.action_size)\n    n_instances = n_workers * n_steps\n\n    def evaluate(step: int) -&gt; None:\n        eval_state = eval_reset(rng=next(prng_seq))\n        return_ = jnp.zeros(eval_workers)\n        done = jnp.zeros(eval_workers, dtype=bool)\n        for _ in range(1000):\n            eval_state, _, _ = eval_step(params, next(prng_seq), eval_state)\n            return_ = return_ + eval_state.reward * (1.0 - done)\n            done = jnp.bitwise_or(done, eval_state.done.astype(bool))\n        print(f\"Step: {step} Avg. ret: {jnp.mean(return_).item()}\")\n\n    for i in trange(n_training_steps // n_instances):\n        for _ in range(n_steps):\n            state, output, action = step(params, next(prng_seq), state)\n            rollout.append(\n                observation=state.obs,\n                action=action,\n                reward=state.reward,\n                output=output,\n                terminal=state.done,\n            )\n        next_value = next_value_fn(params, state.obs)\n        batch = make_batch(rollout, next_value)\n        rollout.clear()\n        # Batchã‚’ä½œã£ãŸã®ã§ã€ãƒŸãƒ‹ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦å­¦ç¿’\n        for _ in range(n_optim_epochs):\n            for idx in sample_minibatch_indices(\n                n_instances,\n                n_minibatches,\n                next(prng_seq),\n            ):\n                minibatch = batch[idx]\n                params, opt_state = update(params, opt_state, minibatch)\n\n        # æ™‚ã€…è©•ä¾¡ã™ã‚‹\n        if (i + 1) % eval_freq == 0:\n            evaluate(i + 1)\n\n    evaluate(i + 1)\n    # Visualize\n    eval_state = eval_reset(rng=next(prng_seq))\n    qps = []\n    while eval_state.done[0] == 0.0:\n        eval_state, _, _ = eval_step(params, next(prng_seq), eval_state)\n        qps.append(jax.tree_map(lambda qp: qp[0], eval_state.qp))\n    return render_html(eval_env.sys, qps)\n\n\nstart_time = datetime.datetime.now()\nhtml = train_ppo()\nelapsed = datetime.datetime.now() - start_time\nprint(f\"Train completed after {elapsed.total_seconds() / 60:.2f} min.\")\nhtml\n\n\n\n\n\nStep: 20 Avg. ret: -286.34039306640625\nStep: 40 Avg. ret: -273.5491943359375\nStep: 60 Avg. ret: -193.0821990966797\nStep: 80 Avg. ret: -84.20954132080078\nStep: 100 Avg. ret: -45.654090881347656\nStep: 120 Avg. ret: -20.323640823364258\nStep: 140 Avg. ret: -42.74524688720703\nStep: 152 Avg. ret: -5.514527320861816\nTrain completed after 41.34 min.\n\n\n\n\n  \n    brax visualizer\n    \n  \n  \n    \n    \n    \n  \n\n\n\n100ä¸‡ã‚¹ãƒ†ãƒƒãƒ—ã®è¨“ç·´ãŒ41åˆ†ã§çµ‚ã‚ã‚Šã¾ã—ãŸã€‚é€Ÿã„ã§ã™ã­ã‚„ã£ã±ã‚Šã€‚ ãªã‚“ã‹å‰ã«è·³ã­ã™ãã¦ã„ã‚‹å¾®å¦™ãªã®ãŒãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚ºã•ã‚Œã¦ã„ã¾ã™ãŒâ€¦ã€‚\n\n\n6. ã¾ã¨ã‚\nã¨ã„ã†ã‚ã‘ã§ã€ã“ã®ãƒ–ãƒ­ã‚°ã§ã¯Jaxã€Braxã€Haikuã‚’ä½¿ã£ã¦ã€GPUã ã‘ã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ä¸Šã®ãƒ­ãƒœãƒƒãƒˆã‚’è¨“ç·´ã™ã‚‹ä¾‹ã‚’ç¤ºã—ã¾ã—ãŸã€‚ ã‹ãªã‚Šé§†ã‘è¶³ã®è§£èª¬ã«ãªã‚Šã¾ã—ãŸãŒã€ãªã‚“ã¨ãªããƒ—ãƒ­ã‚°ãƒ©ãƒ ã®çµ„ã¿æ–¹ã‚’ç†è§£ã—ã¦ã„ãŸã ã‘ãŸã®ã§ã¯ãªã„ã‹ã¨æ€ã„ã¾ã™ã€‚\nç·æ‹¬ã™ã‚‹ã¨ã€Jaxã¯ã‹ãªã‚Šåºƒã„ç¯„å›²ã®NumPyæ¼”ç®—ã‚’GPU/TPUä¸Šã§é«˜é€Ÿã«å‹•ä½œã™ã‚‹ã‚³ãƒ¼ãƒ‰ã«å¤‰æ›ã§ãã‚‹ã€éå¸¸ã«å¼·åŠ›ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚ ä»Šå›ç´¹ä»‹ã—ãŸvmapã¯ã€ä¾‹ãˆã°ä¸€ã¤ã®GPUä¸Šã§æ¼”ç®—ã‚’ä¸¦åˆ—åŒ–ã™ã‚‹æ©Ÿèƒ½ã§ã™ãŒã€ä»–ã«ã‚‚pmapã«ã‚ˆã‚‹ãƒ‡ãƒã‚¤ã‚¹ã‚’ã¾ãŸã„ã ä¸¦åˆ—åŒ–ã‚‚ã§ãã¾ã™ã€‚ ã§ã™ã‹ã‚‰ç‰¹ã«ã€ - âœ” CPUã¨GPUã®é€šä¿¡ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒæ°—ã«ãªã‚‹ã¨ã - âœ” å¤§è¦æ¨¡ã«ä¸¦åˆ—ãªã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã„ãŸã„ã¨ã\nã¯ã€JaxãŒåŠ¹æœã‚’ç™ºæ®ã™ã‚‹ã¨æ€ã„ã¾ã™ã€‚ã¾ãŸã€jax.lax.fori_loopã‚’ä½¿ã£ã¦ - âœ” Cythonã‚„C++/Rustãªã©ä»–ã®è¨€èªã‚’ä½¿ã‚ãšã«Pythonã®ãƒ«ãƒ¼ãƒ—ã‚’é«˜é€ŸåŒ–ã—ãŸã„ã¨ã\nã«ã‚‚ä½¿ãˆã¾ã™ã€‚ ä¸€æ–¹ã§ã€å˜ã«æ·±å±¤å­¦ç¿’ã‚’é«˜é€ŸåŒ–ã—ãŸã„å ´åˆã€ä¾‹ãˆã° - ğŸ”º PyTorchãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã«ãªã£ã¦ã„ã‚‹å ´åˆ\nãªã©ã¯ã€Jaxã‚„Haiku/Flaxã‚’ä½¿ã†ã“ã¨ã«ã‚ˆã‚‹é«˜é€ŸåŒ–ã®æ©æµã¯ã‚ã¾ã‚Šãªã„ã¨æ€ã„ã¾ã™ã€‚ ã†ã¾ãJitã‚’ä½¿ãˆã°Jaxã®æ–¹ãŒé€Ÿã„ã¨æ€ã„ã¾ã™ãŒã€PyTorchã®CUDAã‚³ãƒ¼ãƒ‰ã¯ã‹ãªã‚Šé€Ÿã„ã§ã™ã‹ã‚‰ã­ã€‚ã¾ãŸã€PyTorchã¨æ¯”è¼ƒã—ãŸéš›ã€ - ğŸ”º å­¦ç¿’ã‚³ã‚¹ãƒˆã«ã¤ã„ã¦ã‚‚Jaxã®æ–¹ãŒå¤§ãã„\nã®ã§ã¯ãªã„ã‹ã¨æ€ã„ã¾ã™ã€‚ ãªã®ã§ã€å€‹äººçš„ã«ã¯å­¦ç¿’ä»¥å¤–ã®éƒ¨åˆ†ã§ãƒ™ã‚¯ãƒˆãƒ«ä¸¦åˆ—åŒ–ãƒ»Jitã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã«ã‚ˆã‚‹ä¸¦åˆ—åŒ–ã®ä½™åœ°ãŒã‚ã‚‹å ´åˆã«ã€Jaxã¯ä¾¿åˆ©ã«ä½¿ãˆã‚‹ã®ã‹ãªã‚ã¨æ€ã„ã¾ã™ã€‚ ãŸã Deepmindã¯AlphaFold2ã‚’å§‹ã‚ã€å¤šãã®Jax+Haikuè£½æ·±å±¤å­¦ç¿’ã‚³ãƒ¼ãƒ‰ã‚’ãƒªãƒªãƒ¼ã‚¹ã—ã¦ã„ã¾ã™ã—ã€ä¸€å¿œèª­ã‚ã‚‹ç¨‹åº¦ã«è¦ªã—ã‚“ã§ãŠãã ã‘ã§ã‚‚ã‚ã‚‹ç¨‹åº¦ã®ãƒ¡ãƒªãƒƒãƒˆã¯ã‚ã‚‹ã¨æ€ã„ã¾ã™ã€‚\nã•ã¦ã€å†’é ­ã®å¤§è¦æ¨¡ã«ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ã¦Sim2Realã‚’é ‘å¼µã‚‹ã®ã‹ã€å®Ÿæ©Ÿã®ãƒ‡ãƒ¼ã‚¿ã§é ‘å¼µã‚‹ã®ã‹ã¨ã„ã†è©±ã«æˆ»ã‚Šã¾ã™ãŒã€ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ã‚¹ã‚±ãƒ¼ãƒ«ã•ã›ãŸã„ã®ã§ã‚ã‚Œã°å¤§è¦æ¨¡ã«ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ãŸã„ãªã‚‰Braxã®ã‚ˆã†ã«ã€Œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’Jaxã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã§ãã‚‹ã‚ˆã†ã«ä½œã‚‹ã€ã¨ã„ã†ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯é¢ç™½ã„ã¨æ€ã„ã¾ã™ã€‚åˆ†å­å‹•åŠ›å­¦è¨ˆç®—ãªã©ã€ç‰©ç†æ¼”ç®—ä»¥å¤–ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¸ã®æ´»ç”¨ã‚‚æœŸå¾…ã•ã‚Œã¾ã™ï¼ˆã¨èã„ãŸã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚åƒ•ã¯åˆ†å­å‹•åŠ›å­¦è¨ˆç®—ãŒä½•ãªã®ã‹ã‚ˆãã‚ã‹ã‚Šã¾ã›ã‚“â€¦ï¼‰ã€‚ ä¸€æ–¹ã§ã€ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãŒå¾®åˆ†å¯èƒ½ã§ã‚ã‚‹ã¨ã„ã†åˆ©ç‚¹ã‚’ã©ã†æ´»ã‹ã™ã®ã‹ã‚‚èˆˆå‘³æ·±ã„ãƒ†ãƒ¼ãƒã§ã™ã€‚åƒ•ã‚‚ä»¥å‰PFNã•ã‚“ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ã§ã€å ±é…¬ãŒå¾®åˆ†å¯èƒ½ãªã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦ã€\\(\\sum_{t = 0}^T\\frac{\\partial r_t}{\\partial \\theta}\\)ã«ã¤ã„ã¦ã®å±±ç™»ã‚Šæ³•ã§æ–¹ç­–ã‚’æ›´æ–°ã™ã‚‹ã®ã‚’è©¦ã—ãŸã“ã¨ãŒã‚ã‚‹ã®ã§ã™ãŒã€å ±é…¬ãŒé ã„ã¨ãªã‹ãªã‹é›£ã—ã„ãªã‚ã¨ã„ã†å°è±¡ã§ã—ãŸã€‚ã†ã¾ã„æ–¹æ³•ãŒã‚ã‚Œã°ã„ã„ã®ã§ã™ãŒâ€¦ã€‚æ„å¤–ã¨å‹¾é…é™ä¸‹ã ã‘ã§ãªãé€²åŒ–è¨ˆç®—ãªã©ã®ãƒ¡ã‚¿ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ã‚¯ã‚¹ã¨çµ„ã¿åˆã‚ã›ã‚‹ã¨é¢ç™½ã„ã‹ã‚‚ã—ã‚Œãªã„ã§ã™ã€‚\nã•ã¦ã€åƒ•ã¯ã‚‚ã†ä¸€ã¤ã‚¢ãƒ‰ãƒ™ãƒ³ãƒˆã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã®è¨˜äº‹ã‚’æ›¸ãäºˆå®šãŒã‚ã£ãŸã®ã§ã™ãŒã€æ™‚é–“ãŒãªã„ã®ã§ä»–ã®äººã«ä»£ã‚ã£ã¦ã‚‚ã‚‰ã†ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“â€¦ã€‚å‡ºãŸã‚‰ãã¡ã‚‰ã‚‚ã‚ˆã‚ã—ããŠé¡˜ã„ã—ã¾ã™ã€‚"
  },
  {
    "objectID": "posts/try_equinox_for_rl_jp.html",
    "href": "posts/try_equinox_for_rl_jp.html",
    "title": "equinoxã§å¼·åŒ–å­¦ç¿’ã—ã¦ã¿ã‚‹",
    "section": "",
    "text": "æœ€è¿‘equinoxã¨ã„ã†jaxãƒ™ãƒ¼ã‚¹ã®æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©ã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã£ã¦ã¿ãŸã®ã§ã™ãŒã€ã“ã‚ŒãŒä¸­ã€…ã„ã„ã¨æ€ã£ãŸã®ã§ç´¹ä»‹ã¤ã„ã§ã«å¼·åŒ–å­¦ç¿’ã—ã¦ã¿ã¾ã™ã€‚ä»–ã®jaxãƒ™ãƒ¼ã‚¹ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«ã¯Deepmindã®haikuã‚„Google Researchã®flaxãŒã‚ã‚Šã¾ã™ã€‚ã“ã®2ã¤ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯å®Ÿéš›ã®ã¨ã“ã‚ã‚ã¾ã‚Šå¤‰ã‚ã‚Šã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã¨ã„ã†ã®ã‚‚ã€jaxã«ã¯è©¦é¨“çš„ã«æ›¸ã‹ã‚ŒãŸstaxã¨ã„ã†æ·±å±¤å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹å®Ÿè£…ãŒã‚ã‚Šã€haikuã‚‚flaxã‚‚staxã‚’ãƒ™ãƒ¼ã‚¹ã«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå¿—å‘çš„ãªModuleã‚’æ¡ç”¨ã—ãŸã‚‚ã®ã ã‹ã‚‰ã§ã™ã€‚ã‚ã‚‹ã„ã¯ã€haikuã‚„flaxã¯ã€Œstaxã‚’PyTorchã£ã½ãã—ãŸã‚‚ã®ã€ã¨è¨€ã£ã¦ã‚‚ã„ã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚equinoxã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã‚ã‚‹Compatibility with init-apply librariesã¨ã„ã†ãƒšãƒ¼ã‚¸ã§ã¯ã€ã“ã‚Œã‚‰ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚„ã‚Šæ–¹ã‚’ã€Œinit-applyã‚¢ãƒ—ãƒ­ãƒ¼ãƒã€ã¨å‘¼ã‚“ã§è»½ãèª¬æ˜ã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã«ã¤ã„ã¦ã–ã£ã¨è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
  },
  {
    "objectID": "posts/try_equinox_for_rl_jp.html#ç’°å¢ƒ",
    "href": "posts/try_equinox_for_rl_jp.html#ç’°å¢ƒ",
    "title": "equinoxã§å¼·åŒ–å­¦ç¿’ã—ã¦ã¿ã‚‹",
    "section": "ç’°å¢ƒ",
    "text": "ç’°å¢ƒ\nä¸€é€šã‚Šequinoxã®ç‰¹å¾´ã‚’ç´¹ä»‹ã—ãŸã¨ã“ã‚ã§ã€ã“ã‚Œã‚’ä½¿ã£ã¦å¼·åŒ–å­¦ç¿’ã—ã¦ã¿ã¾ã™ã€‚ã›ã£ã‹ãjaxã‚’ä½¿ã£ã¦ã„ã‚‹ã®ã¨gymã®APIãŒå¤‰ã‚ã‚Šã¾ãã£ãŸä¸Šã«gymnasiumã«å¤‰ã‚ã£ã¦å…¨ç„¶ã¤ã„ã¦ã„ã‘ãªã„ã®ã§ã€jaxè£½ã®ç’°å¢ƒã‚’ä½¿ã£ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã“ã“ã§ã¯jumanjiã¨ã„ã†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®Mazeã‚’ä½¿ã£ã¦ã¿ã¾ã™ã€‚\n\n\nCode\nimport jumanji\nfrom jumanji.wrappers import AutoResetWrapper\nfrom IPython.display import HTML\n\nenv = jumanji.make(\"Maze-v0\")\nenv = AutoResetWrapper(env)\nn_actions = env.action_spec().num_values\nkey, *keys = jax.random.split(jax.random.PRNGKey(20230720), 11)\nstate, timestep = env.reset(key)\nstates = [state]\nfor key in keys:\n    action = jax.random.randint(key=key, minval=0, maxval=n_actions, shape=())\n    state, timestep = env.step(state, action)\n    states.append(state)\nanim = env.animate(states)\nHTML(anim.to_html5_video().replace('=\"1000\"', '=\"640\"'))  # Change video size\n\n\n\n\n\n\n\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nä½¿ã„ã‚„ã™ãã†ã§ã™ã­ã€‚ãŸã ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã ã¨å‹•ç”»ãŒãƒãƒãƒ£ãƒ¡ãƒãƒ£ãªã‚µã‚¤ã‚ºã ã£ãŸã®ã§é©å½“ã«HTMLã®ã‚¿ã‚°ã‚’æ›¸ãæ›ãˆã¦å°ã•ãã—ã¦ã¾ã™ã€‚ å®Ÿéš›ã«å­¦ç¿’ã™ã‚‹ã¨ãã«ã¯vmapã‚„jitã¨çµ„ã¿åˆã‚ã›ã¦ä½¿ãˆã‚‹ã‚ˆã†ã§ã™ã€‚"
  },
  {
    "objectID": "posts/try_equinox_for_rl_jp.html#ppoã‚’å®Ÿè£…ã—ã¦ã¿ã‚‹",
    "href": "posts/try_equinox_for_rl_jp.html#ppoã‚’å®Ÿè£…ã—ã¦ã¿ã‚‹",
    "title": "equinoxã§å¼·åŒ–å­¦ç¿’ã—ã¦ã¿ã‚‹",
    "section": "PPOã‚’å®Ÿè£…ã—ã¦ã¿ã‚‹",
    "text": "PPOã‚’å®Ÿè£…ã—ã¦ã¿ã‚‹\nã¨ã„ã†ã“ã¨ã§ã€ã“ã®ç’°å¢ƒã‚’å­¦ç¿’ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã“ã“ã§ã¯å®šç•ªã‹ã¤å­¦ç¿’ãŒé«˜é€ŸãªPPOã‚’å®Ÿè£…ã—ã¦ã¿ã¾ã™ã€‚\n\nå…¥åŠ›\nãã‚Œãã‚Œå£ã®ä½ç½®ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ä½ç½®ã€ã‚´ãƒ¼ãƒ«ã®ä½ç½®ã‚’ãã‚Œãã‚Œ10x10ã®ãƒã‚¤ãƒŠãƒªç”»åƒã§è¡¨ç¾ã—ã€3x10x10ã®é…åˆ—ã¨ã—ã¦å…¥åŠ›ã—ã¾ã™ã€‚\n\n\nCode\nfrom jumanji.environments.routing.maze.types import Observation, State\n\ndef obs_to_image(obs: Observation) -&gt; jax.Array:\n    walls = obs.walls.astype(jnp.float32)\n    agent = jnp.zeros_like(walls).at[obs.agent_position].set(1.0)\n    target = jnp.zeros_like(walls).at[obs.target_position].set(1.0)\n    return jnp.stack([walls, agent, target])\n\n\n\n\nãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯\nãŸãŸã¿ã“ã‚“ã§ã‹ã‚‰ReLU + ç·šå½¢ãƒ¬ã‚¤ãƒ¤ã‚’2å›ã¨ã„ã†ã‚·ãƒ³ãƒ—ãƒ«ãªæ§‹æˆã«ã—ã¾ã™ã€‚é€”ä¸­ã¾ã§ã¯ä¾¡å€¤é–¢æ•°ã¨æ–¹ç­–ã¯å…±é€šã§ã„ã„ã§ã—ã‚‡ã†ã€‚æ–¹ç­–ã¯ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«åˆ†å¸ƒã¨ã—ã¾ã™ã€‚é¢å€’ãªã®ã§ã€å…¥åŠ›ã‚µã‚¤ã‚º\\(3 \\times 10 \\times 10\\)ã€è¡Œå‹•æ•°\\(4\\)ã¨ã—ã¦å…¥å‡ºåŠ›ã‚µã‚¤ã‚ºã‚’ãƒ™ã‚¿æ›¸ãã—ã¦ã—ã¾ã„ã¾ã™ã€‚\n\n\nCode\nfrom typing import NamedTuple\n\nfrom jax.nn.initializers import orthogonal\n\n\nclass PPONetOutput(NamedTuple):\n    policy_logits: jax.Array\n    value: jax.Array\n\n\nclass SoftmaxPPONet(eqx.Module):\n    torso: list\n    value_head: eqx.nn.Linear\n    policy_head: eqx.nn.Linear\n\n    def __init__(self, key: jax.Array) -&gt; None:\n        key1, key2, key3, key4, key5 = jax.random.split(key, 5)\n        # Common layers\n        self.torso = [\n            eqx.nn.Conv2d(3, 1, kernel_size=3, key=key1),\n            jax.nn.relu,\n            jnp.ravel,\n            eqx.nn.Linear(64, 64, key=key2),\n            jax.nn.relu,\n        ]\n        self.value_head = eqx.nn.Linear(64, 1, key=key3)\n        policy_head = eqx.nn.Linear(64, 4, key=key4)\n        # Use small value for policy initialization\n        self.policy_head = eqx.tree_at(\n            lambda linear: linear.weight,\n            policy_head,\n            orthogonal(scale=0.01)(key5, policy_head.weight.shape),\n        )\n\n    def __call__(self, x: jax.Array) -&gt; PPONetOutput:\n        for layer in self.torso:\n            x = layer(x)\n        value = self.value_head(x)\n        policy_logits = self.policy_head(x)\n        return PPONetOutput(policy_logits=policy_logits, value=value)\n\n    def value(self, x: jax.Array) -&gt; jax.Array:\n        for layer in self.torso:\n            x = layer(x)\n        return self.value_head(x)\n\n\n\n\nãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ\nPPOã®å®Ÿè£…ã§ã¯1000~8000ã‚¹ãƒ†ãƒƒãƒ—ç¨‹åº¦ç’°å¢ƒã§è¡Œå‹•ã—ãŸå±¥æ­´ã‚’é›†ã‚ã¦ãã‚Œã‚’ä½¿ã£ã¦ä½•åº¦ã‹ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ›´æ–°ã™ã‚‹ã®ãŒæ™®é€šã§ã™ã€‚ã“ã“ã§ã¯jax.lax.scanã‚’ä½¿ã£ã¦ã€Pythonã®ãƒ«ãƒ¼ãƒ—ã‚’ä½¿ã†ã‚ˆã‚Šé«˜é€Ÿãªãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã‚’å®Ÿè£…ã—ã¾ã™ã€‚scanã®é€Ÿåº¦é¢ã§ã®æ©æµã¯å¤§ãã„ã§ã™ãŒã€ä½¿ã„æ–¹ã«ã¯å°‘ã—æ³¨æ„ãŒå¿…è¦ã§ã™ã€‚ç‰¹ã«ã€1ã‚¹ãƒ†ãƒƒãƒ—é€²ã‚ã‚‹é–¢æ•°ã®äºŒç•ªç›®ã®å‡ºåŠ›ã‚’results: list[Result] ã¨ã™ã‚‹ã¨ã€æœ€çµ‚çš„ã«è¿”ã£ã¦ãã‚‹ã®ãŒResult(member1=stack([m1 for m1 in results.member1]), ...)ã«ãªã‚‹ã“ã¨ã¯æŠŠæ¡ã—ã¦ãŠãã¾ã—ã‚‡ã†ã€‚ ã¾ãŸã€exec_rolloutã®å¼•æ•°ãŒeqx.Moduleã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§ã‚ã‚‹SoftmaxPPONetã‚’å«ã‚“ã§ã„ã‚‹ã®ã§ã€eqx.filter_jitã§jitã—ã¦ã‚ã’ã‚‹ã¨ã†ã¾ã„ã“ã¨jitã§ããªã„å€¤ã‚’é™¤å¤–ã—ã¦jitã—ã¦ãã‚Œã¾ã™ã€‚ è¡Œå‹•ã¯æ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å‡ºåŠ›ã‚’softmaxã—ã¦ã‚µãƒ³ãƒ—ãƒ«ã—ã¾ã™ãŒã€Observationã«è¿·è·¯ã§ç§»å‹•å¯èƒ½ãªæ–¹å‘ã‚’æ•™ãˆã¦ãã‚Œã‚‹action_maskãŒå…¥ã£ã¦ã„ã‚‹ã®ã§ã€ã“ã‚Œã‚’ä½¿ã£ã¦å–ã‚Œãªã„è¡Œå‹•ã«ã¯-infã‚’ã‹ã‘ã¦ãƒã‚¹ã‚¯ã—ã¦ãŠãã¾ã—ã‚‡ã†ã€‚ç°¡å˜ãªç’°å¢ƒãªã‚‰ã“ã‚Œã‚’ã‚„ã‚‰ãªãã¦ã‚‚å¤§ä¸ˆå¤«ã ã¨æ€ã†ã®ã§ã™ãŒã€å£ã‚’å¤šã„ã¨ãã¯ã“ã‚ŒãŒãªã„ã¨ã‘ã£ã“ã†é›£ã—ã„ã§ã™ã€‚\n\n\nCode\nimport chex\n\n\n@chex.dataclass\nclass Rollout:\n    \"\"\"Rollout buffer that stores the entire history of one rollout\"\"\"\n\n    observations: jax.Array\n    actions: jax.Array\n    action_masks: jax.Array\n    rewards: jax.Array\n    terminations: jax.Array\n    values: jax.Array\n    policy_logits: jax.Array\n\n\ndef mask_logits(policy_logits: jax.Array, action_mask: jax.Array) -&gt; jax.Array:\n    return jax.lax.select(\n        action_mask,\n        policy_logits,\n        jnp.ones_like(policy_logits) * -jnp.inf,\n    )\n\n\nvmapped_obs2i = jax.vmap(obs_to_image)\n\n\n@eqx.filter_jit\ndef exec_rollout(\n    initial_state: State,\n    initial_obs: Observation,\n    env: jumanji.Environment,\n    network: SoftmaxPPONet,\n    prng_key: jax.Array,\n    n_rollout_steps: int,\n) -&gt; tuple[State, Rollout, Observation, jax.Array]:\n    def step_rollout(\n        carried: tuple[State, Observation],\n        key: jax.Array,\n    ) -&gt; tuple[tuple[State, jax.Array], Rollout]:\n        state_t, obs_t = carried\n        obs_image = vmapped_obs2i(obs_t)\n        net_out = jax.vmap(network)(obs_image)\n        masked_logits = mask_logits(net_out.policy_logits, obs_t.action_mask)\n        actions = jax.random.categorical(key, masked_logits, axis=-1)\n        state_t1, timestep = jax.vmap(env.step)(state_t, actions)\n        rollout = Rollout(\n            observations=obs_image,\n            actions=actions,\n            action_masks=obs_t.action_mask,\n            rewards=timestep.reward,\n            terminations=1.0 - timestep.discount,\n            values=net_out.value,\n            policy_logits=masked_logits,\n        )\n        return (state_t1, timestep.observation), rollout\n\n    (state, obs), rollout = jax.lax.scan(\n        step_rollout,\n        (initial_state, initial_obs),\n        jax.random.split(prng_key, n_rollout_steps),\n    )\n    next_value = jax.vmap(network.value)(vmapped_obs2i(obs))\n    return state, rollout, obs, next_value\n\n\nãƒ†ã‚¹ãƒˆã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚jax.vmapã§ç°¡å˜ã«ç’°å¢ƒã‚’ãƒ™ã‚¯ãƒˆãƒ«ä¸¦åˆ—åŒ–ã§ãã‚‹ã®ãŒjaxè£½ç’°å¢ƒã®åˆ©ç‚¹ãªã®ã§ã€ä»Šå›ã¯16ä¸¦åˆ—ã§å‹•ã‹ã—ã¦ã¿ã¾ã™ã€‚resetã‚’vmapã—ã¦PRNGKeyã‚’16å€‹çªã£è¾¼ã‚€ã¨å‹æ‰‹ã«16ä¸¦åˆ—ã®StateãŒã§ã¦ãã¾ã™ã€‚\n\n\nCode\nkey, net_key, reset_key, rollout_key = jax.random.split(key, 4)\npponet = SoftmaxPPONet(net_key)\ninitial_state, initial_timestep = jax.vmap(env.reset)(jax.random.split(reset_key, 16))\nnext_state, rollout, next_obs, next_value = exec_rollout(\n    initial_state,\n    initial_timestep.observation,\n    env,\n    pponet,\n    rollout_key,\n    512,\n)\nrollout.rewards.shape\n\n\n(512, 16)\n\n\nå…¥åŠ›ãŒ16ä¸¦åˆ—ã«ãªã£ã¦ã„ã‚‹ã“ã¨ã€Rolloutã®å„ãƒ¡ãƒ³ãƒãŒã‚¹ãƒ†ãƒƒãƒ—æ•°xç’°å¢ƒæ•°xâ€¦ã®å¤§ãã•ã«ãªã£ã¦ã„ã‚‹ã“ã¨ãŒç¢ºèªã§ãã¾ã—ãŸã€‚\n\n\nå­¦ç¿’\nãƒ‡ãƒ¼ã‚¿ãŒé›†ã‚ã‚‰ã‚ŒãŸã®ã§å¾Œã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ›´æ–°ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ãã¾ã—ã‚‡ã†ã€‚ã¾ãšGAEã‚’è¨ˆç®—ã—ã¾ã™ã€‚æ„å¤–ã¨ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã«ãªã‚‹ã®ã§fori_loopã§é«˜é€ŸåŒ–ã—ã¦ãŠãã¾ã—ã‚‡ã†ã€‚\n\n\nCode\n@chex.dataclass(frozen=True, mappable_dataclass=False)\nclass Batch:\n    \"\"\"Batch for PPO, indexable to get a minibatch.\"\"\"\n\n    observations: jax.Array\n    action_masks: jax.Array\n    onehot_actions: jax.Array\n    rewards: jax.Array\n    advantages: jax.Array\n    value_targets: jax.Array\n    log_action_probs: jax.Array\n\n    def __getitem__(self, idx: jax.Array):\n        return self.__class__(  # type: ignore\n            observations=self.observations[idx],\n            action_masks=self.action_masks[idx],\n            onehot_actions=self.onehot_actions[idx],\n            rewards=self.rewards[idx],\n            advantages=self.advantages[idx],\n            value_targets=self.value_targets[idx],\n            log_action_probs=self.log_action_probs[idx],\n        )\n\n\ndef compute_gae(\n    r_t: jax.Array,\n    discount_t: jax.Array,\n    values: jax.Array,\n    lambda_: float = 0.95,\n) -&gt; jax.Array:\n    \"\"\"Efficiently compute generalized advantage estimator (GAE)\"\"\"\n\n    gamma_lambda_t = discount_t * lambda_\n    delta_t = r_t + discount_t * values[1:] - values[:-1]\n    n = delta_t.shape[0]\n\n    def update(i: int, advantage_t: jax.Array) -&gt; jax.Array:\n        t = n - i - 1\n        adv_t = delta_t[t] + gamma_lambda_t[t] * advantage_t[t + 1]\n        return advantage_t.at[t].set(adv_t)\n\n    advantage_t = jax.lax.fori_loop(0, n, update, jnp.zeros_like(values))\n    return advantage_t[:-1]\n\n\n@eqx.filter_jit\ndef make_batch(\n    rollout: Rollout,\n    next_value: jax.Array,\n    gamma: float,\n    gae_lambda: float,\n) -&gt; Batch:\n    all_values = jnp.concatenate(\n        [jnp.squeeze(rollout.values), next_value.reshape(1, -1)]\n    )\n    advantages = compute_gae(\n        rollout.rewards,\n        # Set Î³ = 0 when the episode terminates\n        (1.0 - rollout.terminations) * gamma,\n        all_values,\n        gae_lambda,\n    )\n    value_targets = advantages + all_values[:-1]\n    onehot_actions = jax.nn.one_hot(rollout.actions, 4)\n    _, _, *obs_shape = rollout.observations.shape\n    log_action_probs = jnp.sum(\n        jax.nn.log_softmax(rollout.policy_logits) * onehot_actions,\n        axis=-1,\n    )\n    return Batch(\n        observations=rollout.observations.reshape(-1, *obs_shape),\n        action_masks=rollout.action_masks.reshape(-1, 4),\n        onehot_actions=onehot_actions.reshape(-1, 4),\n        rewards=rollout.rewards.ravel(),\n        advantages=advantages.ravel(),\n        value_targets=value_targets.ravel(),\n        log_action_probs=log_action_probs.ravel(),\n    )\n\n\n\n\nCode\nbatch = make_batch(rollout, next_value, 0.99, 0.95)\nbatch.advantages.shape, batch.onehot_actions.shape, batch.log_action_probs.shape\n\n\n((8192,), (8192, 4), (8192,))\n\n\n\\(512 \\times 16 = 8192\\)ãªã®ã§å¤§ä¸ˆå¤«ãã†ã§ã™ã­ã€‚ã‚ã¨ã¯ã“ã‚Œã§ä½œã£ãŸBatchã‹ã‚‰ãƒŸãƒ‹ãƒãƒƒãƒã‚’ã‚µãƒ³ãƒ—ãƒ«ã—ã¦ã€æå¤±é–¢æ•°ã‚’æœ€å°åŒ–ã™ã‚‹ã‚ˆã†ã«å‹¾é…é™ä¸‹ã§æ›´æ–°ã—ã¾ã™ã€‚jaxç•Œéšˆã§ã¯å®šç•ªã®optaxã‚’ä½¿ã„ã¾ã—ã‚‡ã†ã€‚ã“ã®æ™‚ã€ä»¥ä¸‹ã®3ç‚¹ã«æ³¨æ„ã—ã¾ã™ã€‚\n\nå‰ç¯€ã§èª¬æ˜ã—ãŸã‚ˆã†ã«ã€jax.gradã®ã‹ã‚ã‚Šã«eqx.filter_gradã‚’ä½¿ã†\nSoftmaxPPONetã¯jax.nn.reluãªã©jax.jitã®å¼•æ•°ã¨ã—ã¦ä½¿ãˆãªã„å‹ã‚’æŒã£ã¦ã„ã‚‹ã®ã§ã€jax.lax.scanã®å¼•æ•°ã«ã™ã‚‹å‰ã«eqx.partitionã§åˆ†è§£ã™ã‚‹\nåŒæ§˜ã«ã€SoftmaxPPONetã¯ãã®ã¾ã¾optaxã®åˆæœŸåŒ–ãƒ»ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆé–¢æ•°ã®å¼•æ•°ã«ã§ããªã„ã®ã§ã€eqx.partitionã§åˆ†è§£ã™ã‚‹ã‹eqx.filterã§jax.Arrayä»¥å¤–ã®ãƒ¡ãƒ³ãƒã‚’é™¤å¤–ã—ã¦ãŠã\n\nã¾ãŸã€ãƒŸãƒ‹ãƒãƒƒãƒæ›´æ–°ã®ãƒ«ãƒ¼ãƒ—ã§jax.lax.scanã‚’ä½¿ã„ãŸã„å ´åˆã€ã„ãã¤ã‹æ–¹æ³•ãŒã‚ã‚‹ã¨æ€ã†ã®ã§ã™ãŒã€ã“ã“ã§ã¯ãƒŸãƒ‹ãƒãƒƒãƒã‚µã‚¤ã‚º\\(N\\)ã€æ›´æ–°å›æ•°\\(M\\)ã€æ›´æ–°ã‚¨ãƒãƒƒã‚¯æ•°\\(K\\)å›ã€å…¨ä½“ã®ãƒãƒƒãƒã‚µã‚¤ã‚º\\(N \\times M\\)ã¨ã—ã¦ã€\n\n\\(0, 1, 2, ..., NM - 1\\)ã®é †åˆ—ã‚’\\(K\\)å€‹ä½œã‚‹\nãƒãƒƒãƒã®å„è¦ç´ ã‚’1ã§ä½œã£ãŸé †åˆ—ã®ã‚‚ã¨ä¸¦ã³æ›¿ãˆãŸã‚‚ã®ã‚’\\(K\\)å€‹ä½œã‚‹\nå„ãƒ¡ãƒ³ãƒã‚’jnp.concatenateã§ãã£ã¤ã‘ã¦å¤§ãã•\\(MK \\times N \\times ...\\)ã®é…åˆ—ã«reshapeã™ã‚‹\n\nã¨ã„ã†æ–¹æ³•ã‚’ä½¿ã„ã¾ã—ãŸã€‚ã¡ã‚‡ã£ã¨é¢å€’ã§ã™ã—ã€æ­£ç›´ã“ã“ã¾ã§é«˜é€ŸåŒ–ã—ãªãã¦ã‚‚ã„ã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã­ã€‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒä¸å®‰ãªå ´åˆã¯\\(K\\)ã®ãƒ«ãƒ¼ãƒ—ã‚’Pythonã§æ›¸ãã®ã‚‚ã‚¢ãƒªã‹ãªã¨æ€ã„ã¾ã™ãŒã€ä»Šå›ã¯å…¥åŠ›ãŒ\\(3\\times 10\\times 10\\)ãªã®ã§å¤§ä¸ˆå¤«ãã†ã§ã™ã­ã€‚\n\n\nCode\nimport optax\n\n\ndef loss_function(\n    network: SoftmaxPPONet,\n    batch: Batch,\n    ppo_clip_eps: float,\n) -&gt; jax.Array:\n    net_out = jax.vmap(network)(batch.observations)\n    # Policy loss\n    log_pi = jax.nn.log_softmax(\n        jax.lax.select(\n            batch.action_masks,\n            net_out.policy_logits,\n            jnp.ones_like(net_out.policy_logits * -jnp.inf),\n        )\n    )\n    log_action_probs = jnp.sum(log_pi * batch.onehot_actions, axis=-1)\n    policy_ratio = jnp.exp(log_action_probs - batch.log_action_probs)\n    clipped_ratio = jnp.clip(policy_ratio, 1.0 - ppo_clip_eps, 1.0 + ppo_clip_eps)\n    clipped_objective = jnp.fmin(\n        policy_ratio * batch.advantages,\n        clipped_ratio * batch.advantages,\n    )\n    policy_loss = -jnp.mean(clipped_objective)\n    # Value loss\n    value_loss = jnp.mean(0.5 * (net_out.value - batch.value_targets) ** 2)\n    # Entropy regularization\n    entropy = jnp.mean(-jnp.exp(log_pi) * log_pi)\n    return policy_loss + value_loss - 0.01 * entropy\n\n\nvmapped_permutation = jax.vmap(jax.random.permutation, in_axes=(0, None), out_axes=0)\n\n\n@eqx.filter_jit\ndef update_network(\n    batch: Batch,\n    network: SoftmaxPPONet,\n    optax_update: optax.TransformUpdateFn,\n    opt_state: optax.OptState,\n    prng_key: jax.Array,\n    minibatch_size: int,\n    n_epochs: int,\n    ppo_clip_eps: float,\n) -&gt; tuple[optax.OptState, SoftmaxPPONet]:\n    # Prepare update function\n    dynamic_net, static_net = eqx.partition(network, eqx.is_array)\n\n    def update_once(\n        carried: tuple[optax.OptState, SoftmaxPPONet],\n        batch: Batch,\n    ) -&gt; tuple[tuple[optax.OptState, SoftmaxPPONet], None]:\n        opt_state, dynamic_net = carried\n        network = eqx.combine(dynamic_net, static_net)\n        grad = eqx.filter_grad(loss_function)(network, batch, ppo_clip_eps)\n        updates, new_opt_state = optax_update(grad, opt_state)\n        dynamic_net = optax.apply_updates(dynamic_net, updates)\n        return (new_opt_state, dynamic_net), None\n\n    # Prepare minibatches\n    batch_size = batch.observations.shape[0]\n    permutations = vmapped_permutation(jax.random.split(prng_key, n_epochs), batch_size)\n    minibatches = jax.tree_map(\n        # Here, x's shape is [batch_size, ...]\n        lambda x: x[permutations].reshape(-1, minibatch_size, *x.shape[1:]),\n        batch,\n    )\n    # Update network n_epochs x n_minibatches times\n    (opt_state, updated_dynet), _ = jax.lax.scan(\n        update_once,\n        (opt_state, dynamic_net),\n        minibatches,\n    )\n    return opt_state, eqx.combine(updated_dynet, static_net)\n\n\nã¨ã„ã†ã‚ã‘ã§ã€éƒ¨å“ãŒå…¨éƒ¨ã§ããŸã®ã§å­¦ç¿’ã‚’å›ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã¾ãšä½•ã‚‚å£ãŒãªã„ç°¡å˜ãªè¿·è·¯ã§è©¦ã—ã¦ã¿ã¾ã™ã€‚ç°¡å˜ãªè¿·è·¯ã‚’ä½œã‚Šã¾ã—ã‚‡ã†ã€‚ junmanjiã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®è¿·è·¯ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’ã‚³ãƒ”ãƒšã—ã¦é©å½“ã«æ›¸ãæ›ãˆã¾ã—ãŸã€‚ã¾ãŸã€ã“ã®ç’°å¢ƒã§ã¯ã‚´ãƒ¼ãƒ«ã§ã®ã¿å ±é…¬ãŒä¸ãˆã‚‰ã‚Œã‚‹ã®ã§ã€å˜ç´”ã«å ±é…¬å’Œ/çµ‚äº†ã—ãŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®æ•° ã§ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚ãŸã‚Šã®å¹³å‡ãƒªã‚¿ãƒ¼ãƒ³ãŒæ±‚ã‚ã‚‰ã‚Œã¾ã™ã€‚ã“ã‚Œã‚’è¨“ç·´çŠ¶æ³ã®æŒ‡æ¨™ã¨ã—ã¦å‡ºåŠ›ã—ã¦ãŠãã¾ã—ã‚‡ã†ã€‚ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¨å‹˜ã§æ±ºã‚ã¾ã—ãŸã€‚\n\n\nCode\nfrom jumanji.environments.routing.maze.generator import Generator\nfrom jumanji.environments.routing.maze.types import Position, State\n\n\nclass TestGenerator(Generator):\n    def __init__(self) -&gt; None:\n        super().__init__(num_rows=10, num_cols=10)\n\n    def __call__(self, key: chex.PRNGKey) -&gt; State:\n        walls = jnp.zeros((10, 10), dtype=bool)\n        agent_position = Position(row=0, col=0)\n        target_position = Position(row=9, col=9)\n\n        # Build the state.\n        return State(\n            agent_position=agent_position,\n            target_position=target_position,\n            walls=walls,\n            action_mask=None,\n            key=key,\n            step_count=jnp.array(0, jnp.int32),\n        )\n\n\n\n\nCode\ndef run_training(\n    key: jax.Array,\n    adam_lr: float = 3e-4,\n    adam_eps: float = 1e-7,\n    gamma: float = 0.99,\n    gae_lambda: float = 0.95,\n    n_optim_epochs: int = 10,\n    minibatch_size: int = 1024,\n    n_agents: int = 16,\n    n_rollout_steps: int = 512,\n    n_total_steps: int = 16 * 512 * 100,\n    ppo_clip_eps: float = 0.2,\n    **env_kwargs,\n) -&gt; SoftmaxPPONet:\n    key, net_key, reset_key = jax.random.split(key, 3)\n    pponet = SoftmaxPPONet(net_key)\n    env = AutoResetWrapper(jumanji.make(\"Maze-v0\", **env_kwargs))\n    adam_init, adam_update = optax.adam(adam_lr, eps=adam_eps)\n    opt_state = adam_init(eqx.filter(pponet, eqx.is_array))\n    env_state, timestep = jax.vmap(env.reset)(jax.random.split(reset_key, 16))\n    obs = timestep.observation\n\n    n_loop = n_total_steps // (n_agents * n_rollout_steps)\n    return_reporting_interval = 1 if n_loop &lt; 10 else n_loop // 10\n    n_episodes, reward_sum = 0.0, 0.0\n    for i in range(n_loop):\n        key, rollout_key, update_key = jax.random.split(key, 3)\n        env_state, rollout, obs, next_value = exec_rollout(\n            env_state,\n            obs,\n            env,\n            pponet,\n            rollout_key,\n            n_rollout_steps,\n        )\n        batch = make_batch(rollout, next_value, gamma, gae_lambda)\n        opt_state, pponet = update_network(\n            batch,\n            pponet,\n            adam_update,\n            opt_state,\n            update_key,\n            minibatch_size,\n            n_optim_epochs,\n            ppo_clip_eps,\n        )\n        n_episodes += jnp.sum(rollout.terminations).item()\n        reward_sum += jnp.sum(rollout.rewards).item()\n        if i &gt; 0 and (i % return_reporting_interval == 0):\n            print(f\"Mean episodic return: {reward_sum / n_episodes}\")\n            n_episodes = 0.0\n            reward_sum = 0.0\n    return pponet\n\n\n\n\nCode\nimport datetime\n\nstarted = datetime.datetime.now()\nkey, training_key = jax.random.split(key)\ntrained_net = run_training(training_key, n_total_steps=16 * 512 * 10, generator=TestGenerator())\nelapsed = datetime.datetime.now() - started\nprint(f\"Elapsed time: {elapsed.total_seconds():.2}s\")\n\n\nMean episodic return: 0.40782122905027934\nMean episodic return: 0.967741935483871\nMean episodic return: 1.0\nMean episodic return: 1.0\nMean episodic return: 1.0\nMean episodic return: 1.0\nMean episodic return: 1.0\nMean episodic return: 1.0\nMean episodic return: 1.0\nElapsed time: 3.2s\n\n\nç´„8ä¸‡ã‚¹ãƒ†ãƒƒãƒ—ãŒ3ç§’ã¡ã‚‡ã£ã¨ã§å­¦ç¿’ã§ãã¾ã—ãŸã€‚é€Ÿã„ã§ã™ã­ã€‚å­¦ç¿’ã—ãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã©ã‚“ãªã‚‚ã‚“ã‹è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\n@eqx.filter_jit\ndef visualization_rollout(\n    key: jax.random.PRNGKey,\n    pponet: SoftmaxPPONet,\n    env: jumanji.Environment,\n    n_steps: int,\n) -&gt; list[State]:\n    def step_rollout(\n        carried: tuple[State, Observation],\n        key: jax.Array,\n    ) -&gt; tuple[tuple[State, jax.Array], State]:\n        state_t, obs_t = carried\n        obs_image = obs_to_image(obs_t)\n        net_out = pponet(obs_image)\n        action, _ = sample_action(key, net_out.policy_logits, obs_t.action_mask)\n        state_t1, timestep = env.step(state_t, action)\n        return (state_t1, timestep.observation), state_t1\n\n    initial_state, timestep = env.reset(key)\n    _, states = jax.lax.scan(\n        step_rollout,\n        (initial_state, timestep.observation),\n        jax.random.split(key, n_steps),\n    )\n    leaves, treedef = jax.tree_util.tree_flatten(states)\n    return [initial_state] + [treedef.unflatten(leaf) for leaf in zip(*leaves)]\n\n\n\n\nCode\nenv = AutoResetWrapper(jumanji.make(\"Maze-v0\", generator=TestGenerator()))\nkey, eval_key = jax.random.split(key)\nstates = visualization_rollout(eval_key, trained_net, env, 40)\nanim = env.animate(states)\nHTML(anim.to_html5_video().replace('=\"1000\"', '=\"640\"'))\n\n\n\n\n\n\n\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nã•ã™ãŒã«ã“ã®ãã‚‰ã„ãªã‚‰ç°¡å˜ãã†ã§ã™ã­ã€‚æ¬¡ã¯ã‚‚ã†å°‘ã—é›£ã—ãã—ã¦ã¿ãŸã„ã¨ã“ã‚ã§ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å®Œå…¨ãƒ©ãƒ³ãƒ€ãƒ ç”Ÿæˆã•ã‚Œã‚‹è¿·è·¯ã¯ã‚„ãŸã‚‰é…ã‹ã£ãŸã®ã§ã€é©å½“ã«è‡ªä½œã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã‚¹ã‚¿ãƒ¼ãƒˆã€ã‚´ãƒ¼ãƒ«ã‚’ãã‚Œãã‚Œ3ç®‡æ‰€ã‹ã‚‰åŒç¢ºç‡ã§ã‚µãƒ³ãƒ—ãƒ«ã—ã€åˆè¨ˆ9é€šã‚Šã®çµ„ã¿åˆã‚ã›ã‚’è§£ãã¾ã™ã€‚ã¾ã‚ã“ã®ãã‚‰ã„ãªã‚‰ãŸã¶ã‚“ã„ã‘ã‚‹ã§ã—ã‚‡ã†ã€‚10å€ã®80ä¸‡ã‚¹ãƒ†ãƒƒãƒ—å­¦ç¿’ã•ã›ã¦ã¿ã¾ã™ã€‚\n\n\nCode\nclass MedDifficultyGenerator(Generator):\n    WALLS = [\n        [0, 0, 1, 0, 0, 1, 0, 1, 1, 0],\n        [0, 0, 1, 0, 0, 1, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 1, 1, 0, 1, 1],\n        [1, 1, 1, 1, 0, 0, 0, 0, 1, 0],\n        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],\n        [0, 0, 1, 0, 1, 1, 0, 0, 0, 0],\n        [1, 0, 1, 0, 1, 1, 0, 1, 1, 1],\n        [0, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n        [1, 1, 1, 0, 0, 1, 0, 1, 1, 0],\n        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n    ]\n    def __init__(self) -&gt; None:\n        super().__init__(num_rows=10, num_cols=10)\n\n    def __call__(self, key: chex.PRNGKey) -&gt; State:\n        key, config_key = jax.random.split(key)\n        walls = jnp.array(self.WALLS).astype(bool)\n        agent_cfg, target_cfg = jax.random.randint(config_key, (2,), 0, 2)\n        agent_position = jax.lax.switch(\n            agent_cfg,\n            [\n                lambda: Position(row=0, col=0),\n                lambda: Position(row=9, col=0),\n                lambda: Position(row=0, col=9),\n            ]\n        )\n        target_position = jax.lax.switch(\n            target_cfg,\n            [\n                lambda: Position(row=3, col=9),\n                lambda: Position(row=7, col=8),\n                lambda: Position(row=7, col=0),\n            ]\n        )\n        # Build the state.\n        return State(\n            agent_position=agent_position,\n            target_position=target_position,\n            walls=walls,\n            action_mask=None,\n            key=key,\n            step_count=jnp.array(0, jnp.int32),\n        )\n\n\n\n\nCode\nimport datetime\n\nstarted = datetime.datetime.now()\nkey, training_key = jax.random.split(key)\ntrained_net = run_training(\n    training_key,\n    n_total_steps=16 * 512 * 100,\n    generator=MedDifficultyGenerator(),\n)\nelapsed = datetime.datetime.now() - started\nprint(f\"Elapsed time: {elapsed.total_seconds():.2}s\")\n\n\nMean episodic return: 0.2812202097235462\nMean episodic return: 0.4077407740774077\nMean episodic return: 0.5992010652463382\nMean episodic return: 0.7285136501516684\nMean episodic return: 0.75\nMean episodic return: 0.8620564808110065\nMean episodic return: 0.9868290258449304\nMean episodic return: 0.9977788746298124\nMean episodic return: 0.9970540974825924\nElapsed time: 1.2e+01s\n\n\n\n\nCode\nenv = AutoResetWrapper(jumanji.make(\"Maze-v0\", generator=MedDifficultyGenerator()))\nkey, eval_key = jax.random.split(key)\nstates = visualization_rollout(eval_key, trained_net, env, 100)\nanim = env.animate(states)\nHTML(anim.to_html5_video().replace('=\"1000\"', '=\"640\"'))\n\n\n\n\n\n\n\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nã‚„ã‚„é€¡å·¡ãŒè¦‹ã‚‰ã‚Œã¾ã™ãŒã‚´ãƒ¼ãƒ«ã«ã¯è¡Œã‘ã¦ã‚‹ã£ã½ã„ã§ã™ã­ã€‚"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "RLog2: RL blog 2",
    "section": "",
    "text": "Blog posts on reinforcement learning and other technical stuff with some code"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RLog2",
    "section": "",
    "text": "Quick Python basic course for OIST new students\n\n\n\n\n\n\nen\n\n\neducation\n\n\n\n\n\n\n\n\n\nSep 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nImplement fast 2D physics simulation with Jax\n\n\n\n\n\n\nen\n\n\nRL\n\n\nphysics\n\n\n\n\n\n\n\n\n\nNov 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nJaxã§é«˜é€Ÿãª2Dç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè£…ã—ã¦ã¿ã‚‹\n\n\n\n\n\n\nja\n\n\nRL\n\n\nphysics\n\n\n\n\n\n\n\n\n\nSep 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTry reinforcement learning with equinox\n\n\n\n\n\n\nja\n\n\nRL\n\n\ndeep\n\n\n\n\n\n\n\n\n\nJul 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nequinoxã§å¼·åŒ–å­¦ç¿’ã—ã¦ã¿ã‚‹\n\n\n\n\n\n\nja\n\n\nRL\n\n\ndeep\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding what self-attention is doing\n\n\n\n\n\n\nen\n\n\nNLP\n\n\ndeep\n\n\n\n\n\n\n\n\n\nMay 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nAttentionãŒä½•ã‚’ã‚„ã£ã¦ã„ã‚‹ã®ã‹ç†è§£ã—ã‚ˆã†\n\n\n\n\n\n\nja\n\n\nNLP\n\n\ndeep\n\n\n\n\n\n\n\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 5.12 Racetrack from the Reinforcement Learning textbook\n\n\n\n\n\n\nen\n\n\nRL\n\n\nbasic\n\n\n\n\n\n\n\n\n\nJul 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nJaxãƒ»Braxãƒ»Haikuã§GPUå¼•ãã“ã‚‚ã‚Šå­¦ç¿’\n\n\n\n\n\n\nja\n\n\nRL\n\n\ndeep\n\n\n\n\n\n\n\n\n\nDec 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nã‚ˆã‚Šè‰¯ã„å•é¡Œè¨­è¨ˆã¸å‘ã‘ã¦ï¼š ä½•ãŒå¼·åŒ–å­¦ç¿’ã‚’é›£ã—ãã™ã‚‹ã®ã‹ã‚’ç†è§£ã—ã‚ˆã†\n\n\n\n\n\n\nja\n\n\nRL\n\n\nbasic\n\n\n\n\n\n\n\n\n\nDec 22, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/fast_2d_physics_in_jax.html",
    "href": "posts/fast_2d_physics_in_jax.html",
    "title": "Jaxã§é«˜é€Ÿãª2Dç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè£…ã—ã¦ã¿ã‚‹",
    "section": "",
    "text": "GPUä¸Šã§ã®é«˜é€Ÿãªç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã€(RLHFã‚„Offline RLã«æŠ¼ã•ã‚Œæ°—å‘³ã¨ã¯ã„ãˆ)å¼·åŒ–å­¦ç¿’ç•Œéšˆã§ã¯è©±é¡Œã®ãƒˆãƒ”ãƒƒã‚¯ã§ã™ã‚ˆã­ã€‚ã¾ãŸå˜ç´”ã«ã€GPUä¸Šã§çˆ†é€Ÿã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒçµ‚ã‚ã‚‹ã®ã¯ãªã‹ãªã‹æ¥½ã—ã„ã‚‚ã®ã§ã™ã€‚ NVIDIA IsaacSymã‚‚ã‚ã‚Šã¾ã™ãŒã€jaxã§å¼·åŒ–å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã‚’é«˜é€ŸåŒ–ã—ãŸã„ãªã‚‰braxãŒä¾¿åˆ©ã§ã™ã€‚ä»¥å‰ç´¹ä»‹ã™ã‚‹ãƒ–ãƒ­ã‚°ã‚‚æ›¸ãã¾ã—ãŸãŒã€ç¾åœ¨ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã¯ã‚ˆã‚Šç²¾åº¦ã®ã„ã„æ‰‹æ³•ãŒé¸ã¹ã‚‹ã‚ˆã†ã«ãªã£ã¦ã„ã¦ã€æ™®é€šã«MuJoCoã®ä»£ã‚ã‚Šã«ä½¿ãˆãã†ãªæ„Ÿã˜ã§ã™ã€‚ã—ã‹ã—ã€æœ€è¿‘å˜ç´”ãª2æ¬¡å…ƒç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§braxãŒä½¿ãˆãªã„ã‹ãªï¼Ÿã¨æ€ã£ã¦æ¤œè¨ã—ã¦ã¿ãŸã¨ã“ã‚ã€ç„¡ç†ã§ã¯ãªã„ã®ã ã‘ã‚Œã©ã©ã†ã«ã‚‚ä½¿ã„ã¥ã‚‰ã„ãªâ€¦ã¨ã„ã†å°è±¡ã§ã—ãŸã€‚ã¾ãŸã€äºŒæ¬¡å…ƒç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ã™ã‚‹ã®ã«ã€ä¸‰æ¬¡å…ƒã®ãƒœãƒ¼ãƒ«ã¨ã‹ã§å½“ãŸã‚Šåˆ¤å®šã‚’è¡Œã†ã®ã¯ã¡ã‚‡ã£ã¨è¨ˆç®—è³‡æºãŒã‚‚ã£ãŸã„ãªã„æ°—ã‚‚ã—ã¾ã™ã€‚ãªã‚‰è‡ªåˆ†ã§ä½œã£ã¦ã—ã¾ãˆã°ã„ã„ã‚“ã˜ã‚ƒãªã„ã‹ï¼Ÿã¨ã„ã†ã“ã¨ã§ã‚„ã£ã¦ã¿ã¾ã—ãŸã€‚"
  },
  {
    "objectID": "posts/fast_2d_physics_in_jax.html#è¡çªåˆ¤å®šã‚’è¡Œã†",
    "href": "posts/fast_2d_physics_in_jax.html#è¡çªåˆ¤å®šã‚’è¡Œã†",
    "title": "Jaxã§é«˜é€Ÿãª2Dç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè£…ã—ã¦ã¿ã‚‹",
    "section": "è¡çªåˆ¤å®šã‚’è¡Œã†",
    "text": "è¡çªåˆ¤å®šã‚’è¡Œã†\næ¬¡ã«è¡çªåˆ¤å®šã‚’å®Ÿè£…ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚å††ã¨å††ã—ã‹ãªã„ã®ã§åˆ¤å®šè‡ªä½“ã¯ç°¡å˜ã§ã™ãŒã€å¾Œã§è¡çªå¾Œã®ç‰©ç†çŠ¶æ…‹ã‚’æ±‚ã‚ã‚‹ãŸã‚ã«è¨ˆç®—ã™ã‚‹ã¨ãã®ãŸã‚ã€è¡çªã—ãŸå ´æ‰€ã‚„ãã‚Œã«ã‚ˆã£ã¦ç™ºç”Ÿã—ãŸã‚¤ãƒ³ãƒ‘ãƒ«ã‚¹ã¨ã„ã£ãŸæƒ…å ±ã‚’ä¿å­˜ã—ã¦ãŠãå¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ ã¾ãŸã€è¡çªåˆ¤å®šã‚’è¡Œã†éš›ã€å…¨ã¦ã®ãƒšã‚¢ã«ã¤ã„ã¦è¡çªã‚’æ¤œå‡ºã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’ãƒŠã‚¤ãƒ¼ãƒ–ã«æ›¸ãã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚\nfor i in range(N):\n    for j in range(i + 1, N):\n        check_contact(i, j)\nã§ã™ãŒã€è¡çªæ¤œå‡ºã¯å„ãƒšã‚¢ã«ã¤ã„ã¦ç‹¬ç«‹ã«è¡Œãˆã‚‹ã®ã§ã€jax.vmapã‚’ä½¿ã£ã¦ä¸¦åˆ—åŒ–ã—ãŸã„ã¨ã“ã‚ã§ã™ã€‚ãã“ã§ã€ã“ã®ãƒ«ãƒ¼ãƒ—ã‚’æ‰‹å‹•ã§ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ«ã—ã€ã‚ã‚‰ã‹ã˜ã‚ãƒšã‚¢ã‚’ç”Ÿæˆã—ã¦ã‹ã‚‰vmapã§ä¸¦åˆ—åŒ–ã—ãŸè¡çªæ¤œå‡ºé–¢æ•°ã‚’å‘¼ã¶ã‚ˆã†ã«ã—ã¾ã—ãŸã€‚ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ä¸­ã®generate_self_pairsãŒå…¨ãƒšã‚¢ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°ã«ãªã‚Šã¾ã™ã€‚\n\n\nCode\nfrom typing import Any, Callable\n\nAxis = Sequence[int] | int\n\n\ndef safe_norm(x: jax.Array, axis: Axis | None = None) -&gt; jax.Array:\n    is_zero = jnp.allclose(x, 0.0)\n    x = jnp.where(is_zero, jnp.ones_like(x), x)\n    n = jnp.linalg.norm(x, axis=axis)\n    return jnp.where(is_zero, 0.0, n)  # pyright: ignore\n\n\ndef normalize(x: jax.Array, axis: Axis | None = None) -&gt; tuple[jax.Array, jax.Array]:\n    norm = safe_norm(x, axis=axis)\n    n = x / (norm + 1e-6 * (norm == 0.0))\n    return n, norm\n\n\ndef tree_map2(\n    f: Callable[..., Any],\n    tree: Any,\n    *rest: Any,\n    is_leaf: Callable[[Any], bool] | None = None\n) -&gt; tuple[Any, Any]:\n    \"\"\"Same as tree_map, but returns a tuple\"\"\"\n    leaves, treedef = jax.tree_util.tree_flatten(tree, is_leaf)\n    all_leaves = [leaves] + [treedef.flatten_up_to(r) for r in rest]\n    result = [f(*xs) for xs in zip(*all_leaves)]\n    a = treedef.unflatten([elem[0] for elem in result])\n    b = treedef.unflatten([elem[1] for elem in result])\n    return a, b\n\n\ndef generate_self_pairs(x: jax.Array) -&gt; tuple[jax.Array, jax.Array]:\n    \"\"\"Returns two arrays that iterate over all combination of elements in x and y.\"\"\"\n    # x.shape[0] &gt; 1\n    chex.assert_axis_dimension_gt(x, 0, 1)\n    n = x.shape[0]\n    # (a, a, a, b, b, c)\n    outer_loop = jnp.repeat(\n        x,\n        jnp.arange(n - 1, -1, -1),\n        axis=0,\n        total_repeat_length=n * (n - 1) // 2,\n    )\n    # (b, c, d, c, d, d)\n    inner_loop = jnp.concatenate([x[i:] for i in range(1, len(x))])\n    return outer_loop, inner_loop\n\n\n@chex.dataclass\nclass Contact(PyTreeOps):\n    pos: jax.Array\n    normal: jax.Array\n    penetration: jax.Array\n    elasticity: jax.Array\n    friction: jax.Array\n\n    def contact_dim(self) -&gt; int:\n        return self.pos.shape[1]\n\n@jax.vmap\ndef _circle_to_circle_impl(\n    a: Circle,\n    b: Circle,\n    a_pos: Position,\n    b_pos: Position,\n    isactive: jax.Array,\n) -&gt; Contact:\n    a2b_normal, dist = normalize(b_pos.xy - a_pos.xy)\n    penetration = a.radius + b.radius - dist\n    a_contact = a_pos.xy + a2b_normal * a.radius\n    b_contact = b_pos.xy - a2b_normal * b.radius\n    pos = (a_contact + b_contact) * 0.5\n    # Filter penetration\n    penetration = jnp.where(isactive, penetration, jnp.ones_like(penetration) * -1)\n    return Contact(\n        pos=pos,\n        normal=a2b_normal,\n        penetration=penetration,\n        elasticity=(a.elasticity + b.elasticity) * 0.5,\n        friction=(a.friction + b.friction) * 0.5,\n    )\n\ndef check_circle_to_circle(\n    space: Space,\n    position: Position,\n    is_active: jax.Array,\n) -&gt; tuple[Contact, Circle, Circle]:\n    circle1, circle2 = tree_map2(generate_self_pairs, space.circle)\n    pos1, pos2 = tree_map2(generate_self_pairs, position)\n    is_active = jnp.logical_and(*generate_self_pairs(is_active))\n    contacts = _circle_to_circle_impl(circle1, circle2, pos1, pos2, is_active)\n    return contacts, circle1, circle2\n\n\n\n\nCode\nimport seaborn as sns\nfrom matplotlib.patches import Arrow\n\nN = 5\npalette = sns.color_palette(\"husl\", N)\n\n\ncircles = Circle(\n    mass=jnp.ones(N),\n    radius=jnp.ones(N),\n    moment=jnp.ones(N) * 0.5,\n    elasticity=jnp.ones(N) * 0.5,\n    friction=jnp.ones(N) * 0.2,\n    rgba=jnp.array([p + (1.0,) for p in palette]),\n)\nspace = Space(gravity=jnp.array([0.0, -9.8]), circle=circles)\np = Position(\n    angle=jnp.zeros(N),\n    xy=jnp.array([[-3, 4.0], [0.0, 2.0], [5.0, 3], [-3, 1], [2, 0]]),\n)\nv_xy = jnp.concatenate((jnp.zeros((N - 2, 2)), jnp.array([[0, 10.0], [-2.0, 8.0]])))\nv = Velocity(angle=jnp.zeros(N), xy=v_xy)\nf = Force(angle=jnp.zeros(N), xy=jnp.zeros((N, 2)))\nstate = State(p=p, v=v, f=f, is_active=jnp.ones(N, dtype=bool))\npositions = [state.p]\ncontact_list = []\nfor i in range(10):\n    state = update_velocity(space, circles, state)\n    state = update_position(space, state)\n    positions.append(state.p)\n    contacts, _, _ = check_circle_to_circle(space, state.p, state.is_active)\n    total_index = 0\n    for j in range(N):\n        for k in range(j + 1, N):\n            if contacts.penetration[total_index] &gt; 0:\n                contact_list.append(contacts.get_slice(total_index))\n            total_index += 1\nfig, ax = plt.subplots()\nax.set_aspect(\"equal\", adjustable=\"box\")\nax.set_xlim((-5, 5))\nax.set_ylim((-5, 5))\nvisualize_balls(ax, space.circle, positions)\nfor contact in contact_list:\n    arrow = Arrow(*contact.pos, *contact.normal, width=0.2, color=\"r\")\n    ax.add_patch(arrow)\nfig\n\n\n\n\n\n\n\n\n\nè¡çªå¾Œã®å‡¦ç†ã‚’å®Ÿè£…ã—ã¦ã„ãªã„ã®ã§ç‰©ä½“ãŒã™ã‚Šã¬ã‘ã¦ã„ã¾ã™ãŒã€è¡çªè‡ªä½“ã¯ãã¡ã‚“ã¨æ¤œå‡ºã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ã§ã™ã€‚"
  },
  {
    "objectID": "posts/fast_2d_physics_in_jax.html#è¡çªã—ãŸå¾Œã©ã†ã™ã‚Œã°ã„ã„ã®",
    "href": "posts/fast_2d_physics_in_jax.html#è¡çªã—ãŸå¾Œã©ã†ã™ã‚Œã°ã„ã„ã®",
    "title": "Jaxã§é«˜é€Ÿãª2Dç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè£…ã—ã¦ã¿ã‚‹",
    "section": "è¡çªã—ãŸå¾Œã©ã†ã™ã‚Œã°ã„ã„ã®",
    "text": "è¡çªã—ãŸå¾Œã©ã†ã™ã‚Œã°ã„ã„ã®\nè¡çªã‚’æ¤œå‡ºã—ãŸå¾Œã¯ã€ç‰©ä½“ãŒé‡ãªã‚‰ãªã„ã‚ˆã†ã«è¡çªæ™‚ã«ç™ºç”Ÿã—ãŸã‚¤ãƒ³ãƒ‘ãƒ«ã‚¹ã«é–¢ã™ã‚‹åˆ¶ç´„ã¤ãæ–¹ç¨‹å¼ã‚’è§£ãå¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãã®ãŸã‚ã«è‰²ã€…ãªæ–¹æ³•ãŒã‚ã‚‹ã®ã§ã™ãŒã€ä»Šå›ã¯Chipmunkãªã©2æ¬¡å…ƒç‰©ç†ã‚¨ãƒ³ã‚¸ãƒ³ã§ã‚ˆãä½¿ã‚ã‚Œã¦ã„ã‚‹Sequential Impulseã¨å‘¼ã°ã‚Œã‚‹æ–¹æ³•ã‚’ä½¿ã£ã¦ã¿ã¾ã™ã€‚ãªãŠã€ã“ã®è³‡æ–™ã¯Box2Dã®Webãƒšãƒ¼ã‚¸ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã‚‚ã®ã§ã™ã€‚\nã§ã¯ã€Sequential Impulseã§ã¯ã©ã®ã‚ˆã†ã«ã—ã¦è¡çªã‚’è§£æ±ºã™ã‚‹ã®ã§ã—ã‚‡ã†ã‹ã€‚ã¾ãšã€ç‰©ä½“ã®è¡çªæ™‚ã«ã‚¤ãƒ³ãƒ‘ãƒ«ã‚¹ãŒç™ºç”Ÿã™ã‚‹ã¨ã„ã†è¡çªãƒ¢ãƒ‡ãƒ«ã‚’ä»®å®šã—ã€ç™ºç”Ÿã—ãŸã‚¤ãƒ³ãƒ‘ãƒ«ã‚¹ã‚’\\(\\mathbf{p}\\)ã¨ãŠãã¾ã™ã€‚é¢å€’ãªã®ã§è§’é€Ÿåº¦ã¯ä¸€åˆ‡è€ƒãˆãªã„ã“ã¨ã«ã—ã¾ã™ã€‚ã“ã®ã¨ãã€ç‰©ä½“1ã®é€Ÿåº¦ã‚’\\(\\mathbf{v}_1\\)ã€è³ªé‡ã‚’\\(m_1\\)ã€ç‰©ä½“2ã®é€Ÿåº¦ã‚’\\(\\mathbf{v}_2\\)ã€è³ªé‡ã‚’\\(m_2\\)ã¨ãŠãã¨ã€ã‚¤ãƒ³ãƒ‘ãƒ«ã‚¹ãŒç™ºç”Ÿã—ãŸå¾Œã®é€Ÿåº¦ã¯ \\[\n\\begin{align*}\n\\mathbf{v}_1 = \\mathbf{v}_1^{\\mathrm{old}} - \\mathbf{p} / m_1 \\\\\n\\mathbf{v}_2 = \\mathbf{v}_2^{\\mathrm{old}} + \\mathbf{p} / m_2\n\\end{align*}\n\\] ã¨ãªã‚Šã¾ã™ã€‚ã“ã®ã¨ãã€\\(\\mathbf{p}\\)ã®æ–¹å‘ã¯è¡çªã®æ³•ç·šãƒ™ã‚¯ãƒˆãƒ«\\(\\mathbf{n}\\)ãªã®ã§\\(\\mathbf{p} = p\\mathbf{n}\\)ã¨è¡¨ã›ã¾ã™ã€‚ã‚ˆã£ã¦ã€çµå±€\\(p\\)ã‚’æ±‚ã‚ã‚Œã°ã„ã„ã§ã™ã€‚è¡çªã—ãŸç‚¹ã«ãŠã‘ã‚‹ç›¸å¯¾é€Ÿåº¦ã‚’\\(\\Delta \\mathbf{v} = \\mathbf{v}_2 - \\mathbf{v}_1\\)ã¨ãŠãã¾ã™ã€‚ã“ã®ã¨ãã€\\(\\Delta \\mathbf{v} \\cdot n = 0\\)ãªã®ã§ã€ä¸Šã®2å¼ã¨åˆã‚ã›ã¦ã€\\(p = \\frac{-\\Delta \\mathbf{v}^{\\mathrm{old}}\\cdot \\mathbf{n}}{\\frac{1}{m_1} + \\frac{1}{m_2}}\\)ã¨ãªã‚Šã¾ã™ã€‚è§’é€Ÿåº¦ã‚„æ‘©æ“¦ã‚’è€ƒæ…®ã™ã‚‹ã¨ã‚‚ã†å°‘ã—ã‚„ã‚„ã“ã—ããªã‚Šã¾ã™ãŒã€åŸºæœ¬ã¯ã“ã‚“ãªæ„Ÿã˜ã§ã™ã€‚\nã“ã†ã—ã¦è¨ˆç®—ã—ãŸã‚¤ãƒ³ãƒ‘ãƒ«ã‚¹ã‚’å…¨ã¦ã®è¡çªã«å¯¾ã—ã¦é©ç”¨ã—ã€ã‚¤ãƒ³ãƒ‘ãƒ«ã‚¹ãŒå°ã•ããªã‚‹ã¾ã§ç¹°ã‚Šè¿”ã—ã¾ã™ã€‚ã—ã‹ã—ã€ã“ã®æ‰‹æ³•ã¯ç‰©ä½“ã®ã‚ã‚Šã“ã¿ã‚’è€ƒæ…®ã—ã¦ã„ãªã„ã®ã§ã€ã“ã‚Œã ã‘ã ã¨ç‰©ä½“ãŒã‚ã‚Šã“ã‚“ã ã¾ã¾ã«ãªã£ã¦ã—ã¾ã†ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚ ã‚ã‚Šè¾¼ã¿ã‚’æ¸›ã‚‰ã™ãŸã‚ã®æ‰‹æ³•ã¯ã„ãã¤ã‹ã‚ã‚Šã¾ã™ãŒã€ä¸»ã«\n\nã©ã®ãã‚‰ã„ã‚ã‚Šè¾¼ã‚“ã§ã„ã‚‹ã‹ã«å¿œã˜ã¦ãƒã‚¤ã‚¢ã‚¹é€Ÿåº¦\\(v_\\mathrm{bias} = \\frac{\\beta}{\\Delta t}\\max(0, \\delta - \\delta_\\mathrm{slop})\\)(\\(\\delta\\)ã¯ã‚ã‚Šã“ã¿ã®é•·ã•ã€\\(\\delta_\\mathrm{slop}\\)ã¯è¨±å®¹ã•ã‚Œã‚‹ã‚ã‚Šã“ã¿ã®é•·ã•)ã‚’åŠ ãˆ\\(p = \\frac{-\\Delta \\mathbf{v}^{\\mathrm{old}}\\cdot \\mathbf{n} + v_\\mathrm{bias}}{\\frac{1}{m_1} + \\frac{1}{m_2}}\\)ã¨ã™ã‚‹ (Baumegarte)\né€Ÿåº¦ã‚’æ›´æ–°ã—ãŸå¾Œã«ã‚‚ã†ä¸€å›Positionã«é–¢ã™ã‚‹åˆ¶ç´„ã‚’è§£ã„ã¦æ“¬ä¼¼çš„ãªé€Ÿåº¦ã‚’åŠ ãˆã‚‹ (Nonlinear Gauss Seidel, NGS)\n\nã¨ã„ã†2ç¨®é¡ã®æ‰‹æ³•ãŒã‚ã‚Šã¾ã™ã€‚å…ˆç¨‹ç´¹ä»‹ã—ãŸBox2Dã®è³‡æ–™ã‚„Chipmunk2Dã§ã¯1ãŒã€ç¾åœ¨ã®Box2Dã§ã¯2ãŒä½¿ã‚ã‚Œã¦ã„ã¾ã™ã€‚è©³ã—ãã¯Box2D 3.0ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚ ä»Šå›ã¯è‹¥å¹²é«˜é€Ÿãª1ã®æ‰‹æ³•ã‚’å®Ÿè£…ã—ã‚ˆã†ã‹ã¨æ€ã£ãŸã®ã§ã™ãŒã€ã“ã®æ–¹æ³•ã ã¨2ã¤ã®ç‰©ä½“ãŒåŒã˜æ–¹å‘ã«é€²ã‚“ã§ã„ã‚‹æ™‚ã¯ã‚ã‚Šã“ã¿ã‚’è§£æ¶ˆã§ããªã„ã®ã§ã€çµå±€2ã®æ‰‹æ³•ã‚’å®Ÿè£…ã—ã¾ã—ãŸã€‚å…·ä½“çš„ã«ã‚½ãƒ«ãƒã®å®Ÿè£…ã¨ã—ã¦ã¯ã€\n\nè¡çªã«ã‚ˆã‚Šç™ºç”Ÿã™ã‚‹ã‚¤ãƒ³ãƒ‘ãƒ«ã‚¹ã«é–¢ã™ã‚‹åˆ¶ç´„ã‚’è§£ã\nå¼¾æ€§ã«ã‚ˆã‚Šç™ºç”Ÿã™ã‚‹ã‚¤ãƒ³ãƒ‘ãƒ«ã‚¹ã‚’åŠ ãˆã‚‹\nä½ç½®ã«é–¢ã™ã‚‹åˆ¶ç´„ã‚’è§£ã\n\nã¨ã„ã†3ã¤ã®ã‚¹ãƒ†ãƒƒãƒ—ã«åˆ†ã‘ã¦å®Ÿè£…ã™ã‚Œã°ã„ã„ã§ã™ã€‚\nã¾ãŸã€ã•ã£ãä¸¦åˆ—åŒ–ã®ãŸã‚æ‰‹å‹•ã§å…¨ãƒšã‚¢ã«å¯¾ã™ã‚‹ãƒ«ãƒ¼ãƒ—ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ«ã—ã¾ã—ãŸãŒã€Sequential Impulseã®å®Ÿè£…ã§ã‚‚ã“ã‚ŒãŒä½¿ãˆã¾ã™ã€‚ãŸã ã—ã€ã‚¤ãƒ³ãƒ‘ãƒ«ã‚¹ã‚’åŠ ãˆãŸå¾Œã®é€Ÿåº¦ã®æ›´æ–°ã¯ã€v_update[i][j]ã«iç•ªç›®ã®ç‰©ä½“ã¨jç•ªç›®ã®ç‰©ä½“ã®è¡çªã«ã‚ˆã‚Šç”Ÿã˜ã‚‹iç•ªç›®ã®ç‰©ä½“ã®é€Ÿåº¦å¤‰åŒ–ãŒå…¥ã£ã¦ã„ã‚‹ã¨ã—ã¦ã€\nfor i in range(N):\n    for j in range(i + 1, N):\n        obj[i].velocity += v_update[i][j]\n        obj[j].velocity += v_update[j][i]\nã®ã‚ˆã†ã«å„è¡çªã«ã‚ˆã‚Šç”Ÿã˜ãŸé€Ÿåº¦å¤‰åŒ–ã‚’ç‰©ä½“ã«ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã‚‚ã„ã¡ã„ã¡ãƒ«ãƒ¼ãƒ—ã§æ›¸ãã¨é…ããªã£ã¦ã—ã¾ã†ã®ã§ã™ãŒã€ã•ã£ãã®generate_self_pairsã§\\(0, 1, 2, ..., N - 1\\)ã®ãƒšã‚¢ã‚’ç”Ÿæˆã—ã¦ãŠã„ã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ã™ã‚‹ã¨ãƒ«ãƒ¼ãƒ—ãªã—ã§æ›¸ã‘ã¾ã™ã€‚ç´°ã‹ãè¨€ã†ã¨ã€generate_self_pairsã§ã¯ãƒ«ãƒ¼ãƒ—ãŒä½¿ã‚ã‚Œã¦ã„ã‚‹ã®ã§ã™ãŒã€jax.jitã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ãŸæ™‚ã«è¨ˆç®—çµæœãŒã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã‚‹ã¯ãšãªã®ã§æ°—ã«ã—ãªãã¦ã‚‚ã„ã„ã§ã™ã€‚\nã¨ã„ã†ã‚ã‘ã§å®Ÿè£…ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚å®Ÿè£…ã¯åŸºæœ¬çš„ã«Box2d-Liteã¨é–‹ç™ºä¸­ã®æœ€æ–°ç‰ˆã§ã‚ã‚‹Box2D 3.0ã‚’å‚è€ƒã«ã—ã¾ã—ãŸã€‚ã¾ãŸã€Box2Dä½œè€…ã®Erin Cattoæ°ã«ã‚ˆã‚‹è¬›æ¼”ã®å†…å®¹ã‚’Typescriptã§å®Ÿè£…ã—ãŸãƒªãƒã‚¸ãƒˆãƒªãŒã‚ã£ãŸã®ã§ã“ã‚Œã‚‚å‚è€ƒã«ã—ã¾ã—ãŸã€‚\n\n\nCode\nimport functools\n\n\n@chex.dataclass\nclass ContactHelper:\n    tangent: jax.Array\n    mass_normal: jax.Array\n    mass_tangent: jax.Array\n    v_bias: jax.Array\n    bounce: jax.Array\n    r1: jax.Array\n    r2: jax.Array\n    inv_mass1: jax.Array\n    inv_mass2: jax.Array\n    inv_moment1: jax.Array\n    inv_moment2: jax.Array\n    local_anchor1: jax.Array\n    local_anchor2: jax.Array\n    allow_bounce: jax.Array\n\n\n@chex.dataclass\nclass VelocitySolver:\n    v1: Velocity\n    v2: Velocity\n    pn: jax.Array\n    pt: jax.Array\n    contact: jax.Array\n\n    def update(self, new_contact: jax.Array) -&gt; Self:\n        continuing_contact = jnp.logical_and(self.contact, new_contact)\n        pn = jnp.where(continuing_contact, self.pn, jnp.zeros_like(self.pn))\n        pt = jnp.where(continuing_contact, self.pt, jnp.zeros_like(self.pt))\n        return self.replace(pn=pn, pt=pt, contact=new_contact)\n\n\ndef init_solver(n: int) -&gt; VelocitySolver:\n    return VelocitySolver(\n        v1=Velocity.zeros(n),\n        v2=Velocity.zeros(n),\n        pn=jnp.zeros(n),\n        pt=jnp.zeros(n),\n        contact=jnp.zeros(n, dtype=bool),\n    )\n\n\ndef _pv_gather(\n    p1: _PositionLike,\n    p2: _PositionLike,\n    orig: _PositionLike,\n) -&gt; _PositionLike:\n    indices = jnp.arange(len(orig.angle))\n    outer, inner = generate_self_pairs(indices)\n    p1_xy = jnp.zeros_like(orig.xy).at[outer].add(p1.xy)\n    p1_angle = jnp.zeros_like(orig.angle).at[outer].add(p1.angle)\n    p2_xy = jnp.zeros_like(orig.xy).at[inner].add(p2.xy)\n    p2_angle = jnp.zeros_like(orig.angle).at[inner].add(p2.angle)\n    return p1.__class__(xy=p1_xy + p2_xy, angle=p1_angle + p2_angle)\n\n\ndef _vmap_dot(xy1: jax.Array, xy2: jax.Array) -&gt; jax.Array:\n    \"\"\"Dot product between nested vectors\"\"\"\n    chex.assert_equal_shape((xy1, xy2))\n    orig_shape = xy1.shape\n    a = xy1.reshape(-1, orig_shape[-1])\n    b = xy2.reshape(-1, orig_shape[-1])\n    return jax.vmap(jnp.dot, in_axes=(0, 0))(a, b).reshape(*orig_shape[:-1])\n\n\ndef _sv_cross(s: jax.Array, v: jax.Array) -&gt; jax.Array:\n    \"\"\"Cross product with scalar and vector\"\"\"\n    x, y = _get_xy(v)\n    return jnp.stack((y * -s, x * s), axis=-1)\n\n\ndef _dv2from1(v1: Velocity, r1: jax.Array, v2: Velocity, r2: jax.Array) -&gt; jax.Array:\n    \"\"\"Compute relative veclotiy from v2/r2 to v1/r1\"\"\"\n    rel_v1 = v1.xy + _sv_cross(v1.angle, r1)\n    rel_v2 = v2.xy + _sv_cross(v2.angle, r2)\n    return rel_v2 - rel_v1\n\n\ndef _effective_mass(\n    inv_mass: jax.Array,\n    inv_moment: jax.Array,\n    r: jax.Array,\n    n: jax.Array,\n) -&gt; jax.Array:\n    rn2 = jnp.cross(r, n) ** 2\n    return inv_mass + inv_moment * rn2\n\n\ndef init_contact_helper(\n    space: Space,\n    contact: Contact,\n    a: Shape,\n    b: Shape,\n    p1: Position,\n    p2: Position,\n    v1: Velocity,\n    v2: Velocity,\n) -&gt; ContactHelper:\n    r1 = contact.pos - p1.xy\n    r2 = contact.pos - p2.xy\n\n    inv_mass1, inv_mass2 = a.inv_mass(), b.inv_mass()\n    inv_moment1, inv_moment2 = a.inv_moment(), b.inv_moment()\n    kn1 = _effective_mass(inv_mass1, inv_moment1, r1, contact.normal)\n    kn2 = _effective_mass(inv_mass2, inv_moment2, r2, contact.normal)\n    nx, ny = _get_xy(contact.normal)\n    tangent = jnp.stack((-ny, nx), axis=-1)\n    kt1 = _effective_mass(inv_mass1, inv_moment1, r1, tangent)\n    kt2 = _effective_mass(inv_mass2, inv_moment2, r2, tangent)\n    clipped_p = jnp.clip(space.allowed_penetration - contact.penetration, a_max=0.0)\n    v_bias = -space.bias_factor / space.dt * clipped_p\n    # k_normal, k_tangent, and v_bias should have (N(N-1)/2, N_contacts) shape\n    chex.assert_equal_shape((contact.friction, kn1, kn2, kt1, kt2, v_bias))\n    # Compute elasiticity * relative_vel\n    dv = _dv2from1(v1, r1, v2, r2)\n    vn = _vmap_dot(dv, contact.normal)\n    return ContactHelper(\n        tangent=tangent,\n        mass_normal=1 / (kn1 + kn2),\n        mass_tangent=1 / (kt1 + kt2),\n        v_bias=v_bias,\n        bounce=vn * contact.elasticity,\n        r1=r1,\n        r2=r2,\n        inv_mass1=inv_mass1,\n        inv_mass2=inv_mass2,\n        inv_moment1=inv_moment1,\n        inv_moment2=inv_moment2,\n        local_anchor1=p1.inv_rotate(r1),\n        local_anchor2=p2.inv_rotate(r2),\n        allow_bounce=vn &lt;= -space.bounce_threshold,\n    )\n\n\n@jax.vmap\ndef apply_initial_impulse(\n    contact: Contact,\n    helper: ContactHelper,\n    solver: VelocitySolver,\n) -&gt; VelocitySolver:\n    \"\"\"Warm starting by applying initial impulse\"\"\"\n    p = helper.tangent * solver.pt + contact.normal * solver.pn\n    v1 = solver.v1 - Velocity(\n        angle=helper.inv_moment1 * jnp.cross(helper.r1, p),\n        xy=p * helper.inv_mass1,\n    )\n    v2 = solver.v2 + Velocity(\n        angle=helper.inv_moment2 * jnp.cross(helper.r2, p),\n        xy=p * helper.inv_mass2,\n    )\n    return solver.replace(v1=v1, v2=v2)\n\n\n@jax.vmap\ndef apply_velocity_normal(\n    contact: Contact,\n    helper: ContactHelper,\n    solver: VelocitySolver,\n) -&gt; VelocitySolver:\n    \"\"\"\n    Apply velocity constraints to the solver.\n    Suppose that each shape has (N_contact, 1) or (N_contact, 2).\n    \"\"\"\n    # Relative veclocity (from shape2 to shape1)\n    dv = _dv2from1(solver.v1, helper.r1, solver.v2, helper.r2)\n    vt = jnp.dot(dv, helper.tangent)\n    dpt = -helper.mass_tangent * vt\n    # Clamp friction impulse\n    max_pt = contact.friction * solver.pn\n    pt = jnp.clip(solver.pt + dpt, a_min=-max_pt, a_max=max_pt)\n    dpt_clamped = helper.tangent * (pt - solver.pt)\n    # Velocity update by contact tangent\n    dvt1 = Velocity(\n        angle=-helper.inv_moment1 * jnp.cross(helper.r1, dpt_clamped),\n        xy=-dpt_clamped * helper.inv_mass1,\n    )\n    dvt2 = Velocity(\n        angle=helper.inv_moment2 * jnp.cross(helper.r2, dpt_clamped),\n        xy=dpt_clamped * helper.inv_mass2,\n    )\n    # Compute Relative velocity again\n    dv = _dv2from1(solver.v1 + dvt1, helper.r1, solver.v2 + dvt2, helper.r2)\n    vn = _vmap_dot(dv, contact.normal)\n    dpn = helper.mass_normal * (-vn + helper.v_bias)\n    # Accumulate and clamp impulse\n    pn = jnp.clip(solver.pn + dpn, a_min=0.0)\n    dpn_clamped = contact.normal * (pn - solver.pn)\n    # Velocity update by contact normal\n    dvn1 = Velocity(\n        angle=-helper.inv_moment1 * jnp.cross(helper.r1, dpn_clamped),\n        xy=-dpn_clamped * helper.inv_mass1,\n    )\n    dvn2 = Velocity(\n        angle=helper.inv_moment2 * jnp.cross(helper.r2, dpn_clamped),\n        xy=dpn_clamped * helper.inv_mass2,\n    )\n    # Filter dv\n    dv1, dv2 = jax.tree_map(\n        lambda x: jnp.where(solver.contact, x, jnp.zeros_like(x)),\n        (dvn1 + dvt1, dvn2 + dvt2),\n    )\n    # Summing up dv per each contact pair\n    return VelocitySolver(\n        v1=dv1,\n        v2=dv2,\n        pn=pn,\n        pt=pt,\n        contact=solver.contact,\n    )\n\n\n@jax.vmap\ndef apply_bounce(\n    contact: Contact,\n    helper: ContactHelper,\n    solver: VelocitySolver,\n) -&gt; tuple[Velocity, Velocity]:\n    \"\"\"\n    Apply bounce (resititution).\n    Suppose that each shape has (N_contact, 1) or (N_contact, 2).\n    \"\"\"\n    # Relative veclocity (from shape2 to shape1)\n    dv = _dv2from1(solver.v1, helper.r1, solver.v2, helper.r2)\n    vn = jnp.dot(dv, contact.normal)\n    pn = -helper.mass_normal * (vn + helper.bounce)\n    dpn = contact.normal * pn\n    # Velocity update by contact normal\n    dv1 = Velocity(\n        angle=-helper.inv_moment1 * jnp.cross(helper.r1, dpn),\n        xy=-dpn * helper.inv_mass1,\n    )\n    dv2 = Velocity(\n        angle=helper.inv_moment2 * jnp.cross(helper.r2, dpn),\n        xy=dpn * helper.inv_mass2,\n    )\n    # Filter dv\n    allow_bounce = jnp.logical_and(solver.contact, helper.allow_bounce)\n    return jax.tree_map(\n        lambda x: jnp.where(allow_bounce, x, jnp.zeros_like(x)),\n        (dv1, dv2),\n    )\n\n\n@chex.dataclass\nclass PositionSolver:\n    p1: Position\n    p2: Position\n    contact: jax.Array\n    min_separation: jax.Array\n\n\n@functools.partial(jax.vmap, in_axes=(None, None, None, 0, 0, 0))\ndef correct_position(\n    bias_factor: float | jax.Array,\n    linear_slop: float | jax.Array,\n    max_linear_correction: float | jax.Array,\n    contact: Contact,\n    helper: ContactHelper,\n    solver: PositionSolver,\n) -&gt; PositionSolver:\n    \"\"\"\n    Correct positions to remove penetration.\n    Suppose that each shape in contact and helper has (N_contact, 1) or (N_contact, 2).\n    p1 and p2 should have xy: (1, 2) angle (1, 1) shape\n    \"\"\"\n    # (N_contact, 2)\n    r1 = solver.p1.rotate(helper.local_anchor1)\n    r2 = solver.p2.rotate(helper.local_anchor2)\n    ga2_ga1 = r2 - r1 + solver.p2.xy - solver.p1.xy\n    separation = jnp.dot(ga2_ga1, contact.normal) - contact.penetration\n    c = jnp.clip(\n        bias_factor * (separation + linear_slop),\n        a_min=-max_linear_correction,\n        a_max=0.0,\n    )\n    kn1 = _effective_mass(helper.inv_mass1, helper.inv_moment1, r1, contact.normal)\n    kn2 = _effective_mass(helper.inv_mass2, helper.inv_moment2, r2, contact.normal)\n    k_normal = kn1 + kn2\n    impulse = jnp.where(k_normal &gt; 0.0, -c / k_normal, jnp.zeros_like(c))\n    pn = impulse * contact.normal\n    p1 = Position(\n        angle=-helper.inv_moment1 * jnp.cross(r1, pn),\n        xy=-pn * helper.inv_mass1,\n    )\n    p2 = Position(\n        angle=helper.inv_moment2 * jnp.cross(r2, pn),\n        xy=pn * helper.inv_mass2,\n    )\n    min_sep = jnp.fmin(solver.min_separation, separation)\n    # Filter separation\n    p1, p2 = jax.tree_map(\n        lambda x: jnp.where(solver.contact, x, jnp.zeros_like(x)),\n        (p1, p2),\n    )\n    return solver.replace(p1=p1, p2=p2, min_separation=min_sep)\n\n\ndef fake_fori_loop(start, end, step, initial):\n    \"\"\"For debugging. Just replace jax.lax.fori_loop with this.\"\"\"\n    state = initial\n    for i in range(start, end):\n        state = step(i, state)\n    return state\n\n\ndef apply_seq_impulses(\n    space: Space,\n    solver: VelocitySolver,\n    p: Position,\n    v: Velocity,\n    contact: Contact,\n    a: Shape,\n    b: Shape,\n) -&gt; tuple[Velocity, Position, VelocitySolver]:\n    \"\"\"Resolve collisions by Sequential Impulse method\"\"\"\n    p1, p2 = tree_map2(generate_self_pairs, p)\n    v1, v2 = tree_map2(generate_self_pairs, v)\n    helper = init_contact_helper(space, contact, a, b, p1, p2, v1, v2)\n    solver = apply_initial_impulse(\n        contact,\n        helper,\n        solver.replace(v1=v1, v2=v2),\n    )\n\n    def vstep(\n        _n_iter: int,\n        vs: tuple[Velocity, VelocitySolver],\n    ) -&gt; tuple[Velocity, VelocitySolver]:\n        v_i, solver_i = vs\n        solver_i1 = apply_velocity_normal(contact, helper, solver_i)\n        v_i1 = _pv_gather(solver_i1.v1, solver_i1.v2, v_i) + v_i\n        v1, v2 = tree_map2(generate_self_pairs, v_i1)\n        return v_i1, solver_i1.replace(v1=v1, v2=v2)\n\n    v, solver = jax.lax.fori_loop(0, space.n_velocity_iter, vstep, (v, solver))\n    rest_v1, rest_v2 = apply_bounce(contact, helper, solver)\n    v = _pv_gather(rest_v1, rest_v2, v) + v\n\n    def pstep(\n        _n_iter: int,\n        ps: tuple[Position, PositionSolver],\n    ) -&gt; tuple[Position, PositionSolver]:\n        p_i, solver_i = ps\n        solver_i1 = correct_position(\n            space.bias_factor,\n            space.linear_slop,\n            space.max_linear_correction,\n            contact,\n            helper,\n            solver_i,\n        )\n        p_i1 = _pv_gather(solver_i1.p1, solver_i1.p2, p_i) + p_i\n        p1, p2 = tree_map2(generate_self_pairs, p_i1)\n        return p_i1, solver_i1.replace(p1=p1, p2=p2)\n\n    pos_solver = PositionSolver(\n        p1=p1,\n        p2=p2,\n        contact=solver.contact,\n        min_separation=jnp.zeros_like(p1.angle),\n    )\n    p, pos_solver = jax.lax.fori_loop(0, space.n_position_iter, pstep, (p, pos_solver))\n    return v, p, solver\n\n\nãªã‹ãªã‹è¤‡é›‘ã«ãªã‚Šã¾ã—ãŸãŒã€å®Ÿè£…ã§ãã¾ã—ãŸã€‚è¡çªã•ã›ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\nfrom celluloid import Camera\nfrom IPython.display import HTML\n\n\ndef animate_balls(\n    fig,\n    ax: Axes,\n    circles: Circle,\n    positions: Iterable[Position],\n) -&gt; HTML:\n    pos = list(positions)\n    camera = Camera(fig)\n    circle_list = circles.tolist()\n    for pi in pos:\n        for pij, circle in zip(pi.tolist(), circle_list):\n            circle_patch = CirclePatch(\n                xy=pij.xy,\n                radius=circle.radius,\n                fill=False,\n                color=circle.rgba.tolist(),\n            )\n            ax.add_patch(circle_patch)\n        camera.snap()\n    return HTML(camera.animate().to_jshtml())\n\n\n\n\nCode\nspace = Space(gravity=jnp.array([0.0, -9.8]), dt=0.04, bias_factor=0.2, circle=circles)\nstate = State(p=p, v=v, f=f, is_active=jnp.ones(N, dtype=bool))\npositions = [state.p]\nsolver = init_solver(N * (N - 1) // 2)\n\n\n@jax.jit\ndef step(state: State, solver: VelocitySolver) -&gt; tuple[State, VelocitySolver]:\n    state = update_velocity(space, space.circle, state)\n    contacts, c1, c2 = check_circle_to_circle(space, state.p, state.is_active)\n    v, p, solver = apply_seq_impulses(\n        space,\n        solver.update(contacts.penetration &gt;= 0),\n        state.p,\n        state.v,\n        contacts,\n        c1,\n        c2,\n    )\n    return update_position(space, state.replace(v=v, p=p)), solver\n\n\nfor i in range(30):\n    state, solver = step(state, solver)\n    positions.append(state.p)\nfig, ax = plt.subplots()\nax.set_aspect(\"equal\", adjustable=\"box\")\nax.set_xlim((-10, 10))\nax.set_ylim((-10, 10))\nanimate_balls(fig, ax, space.circle, positions)\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\næœ€åˆã®è¡çªã§è‹¥å¹²ã®ã‚ã‚Šã“ã¿ãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ãŒã€ä¸€å¿œå¤§ä¸ˆå¤«ãã†ã§ã™ã­ã€‚"
  },
  {
    "objectID": "posts/fast_2d_physics_in_jax.html#ç·šåˆ†",
    "href": "posts/fast_2d_physics_in_jax.html#ç·šåˆ†",
    "title": "Jaxã§é«˜é€Ÿãª2Dç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè£…ã—ã¦ã¿ã‚‹",
    "section": "ç·šåˆ†",
    "text": "ç·šåˆ†\nå††åŒå£«ã®è¡çªãŒå®Ÿè£…ã§ããŸã¨ã“ã‚ã§ã€æ¬¡ã¯å‡¸å¤šè§’å½¢ã®å®Ÿè£…â€¦ã¨è¨€ã„ãŸã„ã¨ã“ã‚ã§ã™ãŒã€ãã‚Œã¯å¾Œå›ã—ã«ã—ã¦ã€ã‚²ãƒ¼ãƒ ã«ã¯æ¬ ã‹ã›ãªã„ã€Œç·šåˆ†ã€ã‚’å®Ÿè£…ã—ã¦ã¿ã¾ã™ã€‚ç¾å®Ÿä¸–ç•Œã«ã¯ãã‚“ãªã‚‚ã®å­˜åœ¨ã—ãªã„ã®ã§ã™ãŒã€ã‚²ãƒ¼ãƒ ã‚„ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ä¸–ç•Œã§ã¯ã€ã©ã†ã—ã¦ã‚‚åœ°é¢ã‚„æŸµã¨ã„ã£ãŸå¢ƒç•Œã‚’è¡¨ç¾ã™ã‚‹å¿…è¦ãŒç”Ÿã˜ã¦ãã¾ã™ã€‚ã“ã†ã„ã£ãŸã‚‚ã®ã‚’ä¾‹ãˆã°ã€Œã‚ã¡ã‚ƒãã¡ã‚ƒé‡ã„é•·æ–¹å½¢ã€ã¨ã—ã¦è¡¨ç¾ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ãŒã€ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’çµ„ã‚€ãƒ¦ãƒ¼ã‚¶ãƒ¼å´ã‹ã‚‰ã™ã‚‹ã¨ã„ã¡ã„ã¡é•·æ–¹å½¢ã®å¤§ãã•ã ã£ãŸã‚Šã‚’å®šç¾©ã™ã‚‹ã®ã¯é¢å€’ãªã®ã§ã€ã€Œç„¡é™ã®è³ªé‡ã‚’æŒã¤ç·šã€ã¨ã—ã¦æ‰±ãˆãŸã»ã†ãŒæ¥½ã§ã™ã‚ˆã­ã€‚ã¨ã„ã†ã‚ã‘ã§ç·šåˆ†ã‚’å®Ÿè£…ã—ã¦ã¿ã¾ã™ã€‚ç·šåˆ†ã®è¡çªåˆ¤å®šã¯è–¬ã®ã‚«ãƒ—ã‚»ãƒ«ğŸ’Šã®ã‚ˆã†ã«ä¸¡ç«¯ãŒä¸¸ããªã£ã¦ã„ã‚‹ã‚„ã¤(Box2Dã ã¨ã‚«ãƒ—ã‚»ãƒ«ã¨å‘¼ã°ã‚Œã¦ã„ã‚‹ã®ã§ã‚«ãƒ—ã‚»ãƒ«ã¨å‘¼ã³ã¾ã™)ã¨å®Ÿè£…ãŒã»ã¼åŒã˜ãªã®ã§ã€ã‚«ãƒ—ã‚»ãƒ«ã‚‚ä¸€ç·’ã«å®Ÿè£…ã—ã¦ã—ã¾ã„ã¾ã™ã€‚ãŸã ã‚«ãƒ—ã‚»ãƒ«åŒå£«ã®è¡çªã¯é¢å€’ãªã®ã§ã€ã¨ã‚Šã‚ãˆãšå††ã¨ã‚«ãƒ—ã‚»ãƒ«ã ã‘å®Ÿè£…ã—ã¾ã—ã‚‡ã†ã€‚æ–°ã—ã„å›³å½¢ã‚’åŠ ãˆãŸã®ã§ã€Spaceã‚‚ä½œã‚Šç›´ã™å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã¨ã‚Šã‚ãˆãšdataclassã«å…¨ã‚·ã‚§ã‚¤ãƒ—ã‚’ã¤ã£ã“ã‚“ã§ãŠã„ã¦ã€å„ã‚·ã‚§ã‚¤ãƒ—ã®çµ„ã¿åˆã‚ã›ã”ã¨ã«è¡çªåˆ¤å®šã‚’è¡Œã„ã€è¡çªè§£æ±ºã®ã¨ãã¯jnp.concatenateã§å…¨éƒ¨ãã£ã¤ã‘ã¦ã‹ã‚‰vmapã§ä¸€åº¦ã«ã‚„ã‚‹ã¨ã„ã†å®Ÿè£…æ–¹é‡ã«ã—ã¾ã—ãŸã€‚ã•ã£ãã®ãƒšã‚¢ã«å¯¾ã™ã‚‹ãƒ«ãƒ¼ãƒ—ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ«ã™ã‚‹ã¨ã“ã‚ã¯ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒšã‚¢ã‚’æŒã£ã¦ãŠã„ã¦é©å½“ãªã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’è¶³ã—ã¦ãŠã‘ã°ãã®ã¾ã¾ä½¿ãˆã¾ã™ã€‚\n\n\nCode\nfrom matplotlib.patches import Rectangle\n\n@chex.dataclass\nclass Capsule(Shape):\n    length: jax.Array\n    radius: jax.Array\n\n\n@chex.dataclass\nclass Segment(Shape):\n    length: jax.Array\n\n    def to_capsule(self) -&gt; Capsule:\n        return Capsule(\n            mass=self.mass,\n            moment=self.moment,\n            elasticity=self.elasticity,\n            friction=self.friction,\n            rgba=self.rgba,\n            length=self.length,\n            radius=jnp.zeros_like(self.length),\n        )\n\n\ndef _length_to_points(length: jax.Array) -&gt; tuple[jax.Array, jax.Array]:\n    a = jnp.stack((length * -0.5, length * 0.0), axis=-1)\n    b = jnp.stack((length * 0.5, length * 0.0), axis=-1)\n    return a, b\n\n\n@jax.vmap\ndef _capsule_to_circle_impl(\n    a: Capsule,\n    b: Circle,\n    a_pos: Position,\n    b_pos: Position,\n    isactive: jax.Array,\n) -&gt; Contact:\n    # Move b_pos to capsule's coordinates\n    pb = a_pos.inv_transform(b_pos.xy)\n    p1, p2 = _length_to_points(a.length)\n    edge = p2 - p1\n    s1 = jnp.dot(pb - p1, edge)\n    s2 = jnp.dot(p2 - pb, edge)\n    in_segment = jnp.logical_and(s1 &gt;= 0.0, s2 &gt;= 0.0)\n    ee = jnp.sum(jnp.square(edge), axis=-1, keepdims=True)\n    # Closest point\n    # s1 &lt; 0: pb is left to the capsule\n    # s2 &lt; 0: pb is right to the capsule\n    # else: pb is in between capsule\n    pa = jax.lax.select(\n        in_segment,\n        p1 + edge * s1 / ee,\n        jax.lax.select(s1 &lt; 0.0, p1, p2),\n    )\n    a2b_normal, dist = normalize(pb - pa)\n    penetration = a.radius + b.radius - dist\n    a_contact = pa + a2b_normal * a.radius\n    b_contact = pb - a2b_normal * b.radius\n    pos = a_pos.transform((a_contact + b_contact) * 0.5)\n    xy_zeros = jnp.zeros_like(b_pos.xy)\n    a2b_normal_rotated = a_pos.replace(xy=xy_zeros).transform(a2b_normal)\n    # Filter penetration\n    penetration = jnp.where(isactive, penetration, jnp.ones_like(penetration) * -1)\n    return Contact(\n        pos=pos,\n        normal=a2b_normal_rotated,\n        penetration=penetration,\n        elasticity=(a.elasticity + b.elasticity) * 0.5,\n        friction=(a.friction + b.friction) * 0.5,\n    )\n\n\n@chex.dataclass\nclass ShapeDict:\n    circle: Circle | None = None\n    segment: Segment | None = None\n    capsule: Capsule | None = None\n    \n    def concat(self) -&gt; Shape:\n        shapes = [s.to_shape() for s in self.values() if s is not None]\n        return jax.tree_map(lambda *args: jnp.concatenate(args, axis=0), *shapes)\n\n\n@chex.dataclass\nclass StateDict:\n    circle: State | None = None\n    segment: State | None = None\n    capsule: State | None = None\n\n    def concat(self) -&gt; None:\n        states = [s for s in self.values() if s is not None]\n        return jax.tree_map(lambda *args: jnp.concatenate(args, axis=0), *states)\n\n    def offset(self, key: str) -&gt; int:\n        total = 0\n        for k, state in self.items():\n            if k == key:\n                return total\n            if state is not None:\n                total += state.p.batch_size()\n        raise RuntimeError(\"Unreachable\")\n        \n    def _get(self, name: str, state: State) -&gt; State | None:\n        if self[name] is None:\n            return None\n        else:\n            start = self.offset(name)\n            end = start + self[name].p.batch_size()\n            return state.get_slice(jnp.arange(start, end))\n        \n    def update(self, statec: State) -&gt; Self:\n        circle = self._get(\"circle\", statec)\n        segment = self._get(\"segment\", statec)\n        capsule = self._get(\"capsule\", statec)\n        return self.__class__(circle=circle, segment=segment, capsule=capsule)\n\n\nContactFn = Callable[[StateDict], tuple[Contact, Shape, Shape]]\n\n\ndef _pair_outer(x: jax.Array, reps: int) -&gt; jax.Array:\n    return jnp.repeat(x, reps, axis=0, total_repeat_length=x.shape[0] * reps)\n\n\ndef _pair_inner(x: jax.Array, reps: int) -&gt; jax.Array:\n    return jnp.tile(x, (reps,) + (1,) * (x.ndim - 1))\n\n\ndef generate_pairs(x: jax.Array, y: jax.Array) -&gt; tuple[jax.Array, jax.Array]:\n    \"\"\"Returns two arrays that iterate over all combination of elements in x and y\"\"\"\n    xlen, ylen = x.shape[0], y.shape[0]\n    return _pair_outer(x, ylen), _pair_inner(y, xlen)\n\n\ndef _circle_to_circle(\n    shaped: ShapeDict,\n    stated: StateDict,\n) -&gt; tuple[Contact, Circle, Circle]:\n    circle1, circle2 = tree_map2(generate_self_pairs, shaped.circle)\n    pos1, pos2 = tree_map2(generate_self_pairs, stated.circle.p)\n    is_active = jnp.logical_and(*generate_self_pairs(stated.circle.is_active))\n    contacts = _circle_to_circle_impl(\n        circle1,\n        circle2,\n        pos1,\n        pos2,\n        is_active,\n    )\n    return contacts, circle1, circle2\n\n\ndef _capsule_to_circle(\n    shaped: ShapeDict,\n    stated: StateDict,\n) -&gt; tuple[Contact, Capsule, Circle]:\n    capsule = jax.tree_map(\n        functools.partial(_pair_outer, reps=shaped.circle.mass.shape[0]),\n        shaped.capsule,\n    )\n    circle = jax.tree_map(\n        functools.partial(_pair_inner, reps=shaped.capsule.mass.shape[0]),\n        shaped.circle,\n    )\n    pos1, pos2 = tree_map2(generate_pairs, stated.capsule.p, stated.circle.p)\n    is_active = jnp.logical_and(\n        *generate_pairs(stated.capsule.is_active, stated.circle.is_active)\n    )\n    contacts = _capsule_to_circle_impl(\n        capsule,\n        circle,\n        pos1,\n        pos2,\n        is_active,\n    )\n    return contacts, capsule, circle\n\n\ndef _segment_to_circle(\n    shaped: ShapeDict,\n    stated: StateDict,\n) -&gt; tuple[Contact, Segment, Circle]:\n    segment = jax.tree_map(\n        functools.partial(_pair_outer, reps=shaped.circle.mass.shape[0]),\n        shaped.segment,\n    )\n    circle = jax.tree_map(\n        functools.partial(_pair_inner, reps=shaped.segment.mass.shape[0]),\n        shaped.circle,\n    )\n    pos1, pos2 = tree_map2(generate_pairs, stated.segment.p, stated.circle.p)\n    is_active = jnp.logical_and(\n        *generate_pairs(stated.segment.is_active, stated.circle.is_active)\n    )\n    contacts = _capsule_to_circle_impl(\n        segment.to_capsule(),\n        circle,\n        pos1,\n        pos2,\n        is_active,\n    )\n    return contacts, segment, circle\n\n\n_CONTACT_FUNCTIONS = {\n    (\"circle\", \"circle\"): _circle_to_circle,\n    (\"capsule\", \"circle\"): _capsule_to_circle,\n    (\"segment\", \"circle\"): _segment_to_circle,\n}\n\n\n@chex.dataclass\nclass ContactWithMetadata:\n    contact: Contact\n    shape1: Shape\n    shape2: Shape\n    outer_index: jax.Array\n    inner_index: jax.Array\n\n    def gather_p_or_v(\n        self,\n        outer: _PositionLike,\n        inner: _PositionLike,\n        orig: _PositionLike,\n    ) -&gt; _PositionLike:\n        xy_outer = jnp.zeros_like(orig.xy).at[self.outer_index].add(outer.xy)\n        angle_outer = jnp.zeros_like(orig.angle).at[self.outer_index].add(outer.angle)\n        xy_inner = jnp.zeros_like(orig.xy).at[self.inner_index].add(inner.xy)\n        angle_inner = jnp.zeros_like(orig.angle).at[self.inner_index].add(inner.angle)\n        return orig.__class__(angle=angle_outer + angle_inner, xy=xy_outer + xy_inner)\n\n\n@chex.dataclass\nclass ExtendedSpace:\n    gravity: jax.Array\n    shaped: ShapeDict\n    dt: jax.Array | float = 0.1\n    linear_damping: jax.Array | float = 0.95\n    angular_damping: jax.Array | float = 0.95\n    bias_factor: jax.Array | float = 0.2\n    n_velocity_iter: int = 8\n    n_position_iter: int = 2\n    linear_slop: jax.Array | float = 0.005\n    max_linear_correction: jax.Array | float = 0.2\n    allowed_penetration: jax.Array | float = 0.005\n    bounce_threshold: float = 1.0\n\n    def check_contacts(self, stated: StateDict) -&gt; ContactWithMetadata:\n        contacts = []\n        for (n1, n2), fn in _CONTACT_FUNCTIONS.items():\n            if stated[n1] is not None and stated[n2] is not None:\n                contact, shape1, shape2 = fn(self.shaped, stated)\n                len1, len2 = stated[n1].p.batch_size(), stated[n2].p.batch_size()\n                offset1, offset2 = stated.offset(n1), stated.offset(n2)\n                if n1 == n2:\n                    outer_index, inner_index = generate_self_pairs(jnp.arange(len1))\n                else:\n                    outer_index, inner_index = generate_pairs(\n                        jnp.arange(len1),\n                        jnp.arange(len2),\n                    )\n                contact_with_meta = ContactWithMetadata(\n                    contact=contact,\n                    shape1=shape1.to_shape(),\n                    shape2=shape2.to_shape(),\n                    outer_index=outer_index + offset1,\n                    inner_index=inner_index + offset2,\n                )\n                contacts.append(contact_with_meta)\n        return jax.tree_map(lambda *args: jnp.concatenate(args, axis=0), *contacts)\n    \n    def n_possible_contacts(self) -&gt; int:\n        n = 0\n        for n1, n2 in _CONTACT_FUNCTIONS.keys():\n            if self.shaped[n1] is not None and self.shaped[n2] is not None:\n                len1, len2 = len(self.shaped[n1].mass), len(self.shaped[n2].mass)\n                if n1 == n2:\n                    n += len1 * (len1 - 1) // 2\n                else:\n                    n += len1 * len2\n        return n\n\n\ndef animate_balls_and_segments(\n    fig,\n    ax: Axes,\n    circles: Circle,\n    segments: Segment,\n    c_pos: Iterable[Position],\n    s_pos: Position,\n) -&gt; HTML:\n    camera = Camera(fig)\n    circle_list = circles.tolist()\n    # Lower left\n    segment_ll = s_pos.transform(\n        jnp.stack((-segments.length * 0.5, jnp.zeros_like(segments.length)), axis=1)\n    )\n    for pi in c_pos:\n        for pij, circle in zip(pi.tolist(), circle_list):\n            circle_patch = CirclePatch(\n                xy=pij.xy,\n                radius=circle.radius,\n                fill=False,\n                color=circle.rgba.tolist(),\n            )\n            ax.add_patch(circle_patch)\n        for ll, pj, segment in zip(segment_ll, s_pos.tolist(), segments.tolist()):\n            rect_patch = Rectangle(\n                xy=ll,\n                width=segment.length,\n                angle=(pj.angle / jnp.pi).item() * 180,\n                height=0.1,\n            )\n            ax.add_patch(rect_patch)\n        camera.snap()\n    return HTML(camera.animate().to_jshtml())\n\ndef solve_constraints(\n    space: Space,\n    solver: VelocitySolver,\n    p: Position,\n    v: Velocity,\n    contact_with_meta: ContactWithMetadata,\n) -&gt; tuple[Velocity, Position, VelocitySolver]:\n    \"\"\"Resolve collisions by Sequential Impulse method\"\"\"\n    outer, inner = contact_with_meta.outer_index, contact_with_meta.inner_index\n\n    def get_pairs(p_or_v: _PositionLike) -&gt; tuple[_PositionLike, _PositionLike]:\n        return p_or_v.get_slice(outer), p_or_v.get_slice(inner)\n\n    p1, p2 = get_pairs(p)\n    v1, v2 = get_pairs(v)\n    helper = init_contact_helper(\n        space,\n        contact_with_meta.contact,\n        contact_with_meta.shape1,\n        contact_with_meta.shape2,\n        p1,\n        p2,\n        v1,\n        v2,\n    )\n    # Warm up the velocity solver\n    solver = apply_initial_impulse(\n        contact_with_meta.contact,\n        helper,\n        solver.replace(v1=v1, v2=v2),\n    )\n\n    def vstep(\n        _n_iter: int,\n        vs: tuple[Velocity, VelocitySolver],\n    ) -&gt; tuple[Velocity, VelocitySolver]:\n        v_i, solver_i = vs\n        solver_i1 = apply_velocity_normal(contact_with_meta.contact, helper, solver_i)\n        v_i1 = contact_with_meta.gather_p_or_v(solver_i1.v1, solver_i1.v2, v_i) + v_i\n        v1, v2 = get_pairs(v_i1)\n        return v_i1, solver_i1.replace(v1=v1, v2=v2)\n\n    v, solver = jax.lax.fori_loop(0, space.n_velocity_iter, vstep, (v, solver))\n    bv1, bv2 = apply_bounce(contact_with_meta.contact, helper, solver)\n    v = contact_with_meta.gather_p_or_v(bv1, bv2, v) + v\n\n    def pstep(\n        _n_iter: int,\n        ps: tuple[Position, PositionSolver],\n    ) -&gt; tuple[Position, PositionSolver]:\n        p_i, solver_i = ps\n        solver_i1 = correct_position(\n            space.bias_factor,\n            space.linear_slop,\n            space.max_linear_correction,\n            contact_with_meta.contact,\n            helper,\n            solver_i,\n        )\n        p_i1 = contact_with_meta.gather_p_or_v(solver_i1.p1, solver_i1.p2, p_i) + p_i\n        p1, p2 = get_pairs(p_i1)\n        return p_i1, solver_i1.replace(p1=p1, p2=p2)\n\n    pos_solver = PositionSolver(\n        p1=p1,\n        p2=p2,\n        contact=solver.contact,\n        min_separation=jnp.zeros_like(p1.angle),\n    )\n    p, pos_solver = jax.lax.fori_loop(0, space.n_position_iter, pstep, (p, pos_solver))\n    return v, p, solver\n\n\ndef dont_solve_constraints(\n    _space: Space,\n    solver: VelocitySolver,\n    p: Position,\n    v: Velocity,\n    _contact_with_meta: ContactWithMetadata,\n) -&gt; tuple[Velocity, Position, VelocitySolver]:\n    return v, p, solver\n\n\nN_SEG = 3\nsegments = Segment(\n    mass=jnp.ones(N_SEG) * jnp.inf,\n    moment=jnp.ones(N_SEG) * jnp.inf,\n    elasticity=jnp.ones(N_SEG) * 0.5,\n    friction=jnp.ones(N_SEG) * 1.0,\n    rgba=jnp.ones((N_SEG, 4)),\n    length=jnp.array([4 * jnp.sqrt(2), 4, 4 * jnp.sqrt(2)]),\n)\ncpos = jnp.array([[2, 2], [4, 3], [3, 6], [6, 5], [5, 7]], dtype=jnp.float32)\nstated = StateDict(\n    circle=State(\n        p=Position(xy=cpos, angle=jnp.zeros(N)),\n        v=Velocity.zeros(N),\n        f=Force.zeros(N),\n        is_active=jnp.array([True, True, True, True, True]),\n    ),\n    segment=State(\n        p=Position(\n            xy=jnp.array([[-2.0, 2.0], [2, 0], [6, 2]], dtype=jnp.float32),\n            angle=jnp.array([jnp.pi * 1.75, 0, jnp.pi * 0.25]),\n        ),\n        v=Velocity.zeros(N_SEG),\n        f=Force.zeros(N_SEG),\n        is_active=jnp.ones(N_SEG, dtype=bool),\n    ),\n)\nspace = ExtendedSpace(\n    gravity=jnp.array([0.0, -9.8]),\n    linear_damping=1.0,\n    angular_damping=1.0,\n    dt=0.04,\n    bias_factor=0.2,\n    n_velocity_iter=6,\n    shaped=ShapeDict(circle=circles, segment=segments),\n)\n\n\n@jax.jit\ndef step(stated: StateDict, solver: VelocitySolver) -&gt; StateDict:\n    state = update_velocity(space, space.shaped.concat(), stated.concat())\n    contact_with_meta = space.check_contacts(stated.update(state))\n    # Check there's any penetration\n    contacts = contact_with_meta.contact.penetration &gt;= 0\n    v, p, solver = jax.lax.cond(\n        jnp.any(contacts),\n        solve_constraints,\n        dont_solve_constraints,\n        space,\n        solver.update(contacts),\n        state.p,\n        state.v,\n        contact_with_meta,\n    )\n    statec = update_position(space, state.replace(v=v, p=p))\n    return stated.update(statec)\n\n\nã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã—ã¦ã¿ã¾ã™ã€‚\n\n\nCode\npositions = [stated[\"circle\"].p]\nsolver = init_solver(space.n_possible_contacts())\n\nfor i in range(50):\n    stated = step(stated, solver)\n    positions.append(stated[\"circle\"].p)\nfig, ax = plt.subplots()\nax.set_aspect(\"equal\", adjustable=\"box\")\nax.set_xlim((-10, 10))\nax.set_ylim((0, 10))\nanimate_balls_and_segments(\n    fig,\n    ax,\n    space.shaped[\"circle\"],\n    space.shaped[\"segment\"],\n    positions,\n    stated[\"segment\"].p,\n)\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nã¡ã‚‡ã£ã¨ã‚ã‚Šã“ã‚“ã§ã„ã¾ã™ãŒã¾ã‚ä¸€å¿œè¨ˆç®—ã§ãã¦ã¯ã„ãã†ã§ã™ã€‚ä¸€å¿œç°¡å˜ã«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã—ã¦ã¿ã¾ã™ã€‚\n\n\nCode\n%timeit step(stated, solver)\n\n\nãƒœãƒ¼ãƒ«ã®æ•°ãŒã¾ã å°‘ãªã„ã§ã™ãŒã€1ã‚¹ãƒ†ãƒƒãƒ—ã‚ãŸã‚Šç´„700ãƒã‚¤ã‚¯ãƒ­ç§’ã¨ã„ã†ã“ã¨ã§ã€ã‹ãªã‚Šé«˜é€Ÿã«ã§ããŸã®ã§ã¯ãªã„ã§ã—ã‚‡ã†ã‹ã€‚"
  },
  {
    "objectID": "posts/python_basic_en.html",
    "href": "posts/python_basic_en.html",
    "title": "Quick Python basic course for OIST new students",
    "section": "",
    "text": "In this course, I aim to cover introductory topics in software engeneering and Python programming in short.\nThis teaching material is prepared for programming/CS course for OIST new students that was cancelled due to low interests.\nYou can open this notebook in Google colab or download .ipynb file from the right pane. Please understand that Colab notebook is a variant of jupyter notebook customized by Google, so many features are different from standard jupyter notebook or jupyter lab."
  },
  {
    "objectID": "posts/python_basic_en.html#exercise-0",
    "href": "posts/python_basic_en.html#exercise-0",
    "title": "Quick Python basic course for OIST new students",
    "section": "Exercise 0",
    "text": "Exercise 0\nExecute the cell below.\n\n\nCode\n3 + 1\n\n\n4"
  },
  {
    "objectID": "posts/python_basic_en.html#what-is-a-programming-language",
    "href": "posts/python_basic_en.html#what-is-a-programming-language",
    "title": "Quick Python basic course for OIST new students",
    "section": "What is a programming language?",
    "text": "What is a programming language?\nLetâ€™s recall that mondern compueters have this kind of architecture:\n\n\nCode\nfrom io import BytesIO\nfrom urllib import request\n\nfrom PIL import Image\n\nfd = BytesIO(\n    request.urlopen(\n        \"https://upload.wikimedia.org/wikipedia/commons/0/08/Computer_architecture_block_diagram.png\"\n    ).read()\n)\nImage.open(fd)\n\n\n\n\n\n\n\n\n\nThis is called von Neumann architecture and the basis of all modern computers. However, it is unclear how you can give instructions to the computer. Software, or program, is a way to do that. So, basically, software is the set of instructions to what to do. We can send software to the computer, and the software is stored in its memory and executed.\nNote that software can be either very lower level or higher level. Computer itself can only understand very lower level language. For example, in X86-64 assembly which is common in todayâ€™s intel and AMD desktop CPU, \\(3 + 1\\) in Python is written as below.\nmov al, 3   ; Move 3 into the AL register\nadd al, 1   ; Add 1 to AL\nThis is quite low-level operation, and we, lazy programmers, donâ€™t want to write this kind of very low-level operation by hand at every time!\nThus, we need a high-level language that can bridge our thoughts and low level machine language. There are two types of laguages with different execution scheme:\n\nCompiled language (C, C++, Rust)\nInterpreter language (Javascript, Python, Ruby)\n\nIn compiled language, the program is converted to the machine language and then executed. For example, 3 + 1 is converted to mov al, 3; add al, 1 as the example above. Because itâ€™s once covnerted to machine languages, the code in compiled langauges are as fast as machine language.\nAnother one is interpreter language. In this case, software is executed within another software called interpreter without compiling it to machine language. They are often slower than compiled languages, but they are easier and good for begginners. Python fits this category, and Python interpreter is running behind this notebook on a part of really big computer clusters in Google.\nAlso, there are some kinds of programming languages with different philosophy:\n\nProcedural (C, Rust)\nObject-oriented (C++, Python)\nFunctional (Lisp, Haskell)\n\nPython is an object-oriented language, but it inherits lots of features from procedural langauges. We generally focus on the Python as procedural languages in the first half of this course, because it may be the easiest way to get into programming."
  },
  {
    "objectID": "posts/python_basic_en.html#arithmetic-operations",
    "href": "posts/python_basic_en.html#arithmetic-operations",
    "title": "Quick Python basic course for OIST new students",
    "section": "Arithmetic operations",
    "text": "Arithmetic operations\nPrease also refer to the official tutorial. We can use +-*/ for basic arithmetic operations.\n\n\nCode\n3 + 5 * 4\n\n\n23\n\n\nPower is **, and the modulo is %.\n\n\nCode\n(2 ** 4) % 10\n\n\n6\n\n\nAddition is evaluated later than multiplication and division. We can use () to change the order.\n\n\nCode\n(3 + 5) * 4\n\n\n32"
  },
  {
    "objectID": "posts/python_basic_en.html#exercise-1.1",
    "href": "posts/python_basic_en.html#exercise-1.1",
    "title": "Quick Python basic course for OIST new students",
    "section": "Exercise 1.1",
    "text": "Exercise 1.1\nCompute how many seconds are in a day."
  },
  {
    "objectID": "posts/python_basic_en.html#variable",
    "href": "posts/python_basic_en.html#variable",
    "title": "Quick Python basic course for OIST new students",
    "section": "Variable",
    "text": "Variable\nVariable is the most important concept. You may think of it is a container that has some value, and we can name it.\n\n\nCode\nvar1 = 30\nprint(var1)\nvar1 = 40\nprint(var1)\n\n\n30\n40\n\n\n= is the syntax that assigns some value into the variable. There are some operators that changes the value in the variable in place.\nprint is what Iâ€™m not explained yet, but please understand that it just shows the content of the given variable (or, more precisely, the evaluated result of the given experssion).\n\n\nCode\nvar1 += 40\nprint(var1)\n\n\n80"
  },
  {
    "objectID": "posts/python_basic_en.html#expression-and-statements",
    "href": "posts/python_basic_en.html#expression-and-statements",
    "title": "Quick Python basic course for OIST new students",
    "section": "Expression and statements",
    "text": "Expression and statements\nIn Python, what we can assign into the variable is called expression. Expression includes numerical values, texts, and values themselves.\n\n\nCode\nvar1 = 20\nvar2 = -100\nvar3 = var2\nvar3\n\n\n-100\n\n\nIn Colab/Jupyter notebook, the last expression in the cell is printed out. In the cell above, the content of var3 is printed out.\nOn the other hand, the assignement like var1 = 20 is called statement. If the cell ends with statement, the notebook doesnâ€™t show anything."
  },
  {
    "objectID": "posts/python_basic_en.html#for-loop",
    "href": "posts/python_basic_en.html#for-loop",
    "title": "Quick Python basic course for OIST new students",
    "section": "For loop",
    "text": "For loop\nThe power of the computer is in its speed. Todays computers can executre millionds of operations in a second. Thus, the most basic usage to utilize the computation power of computers is to make it repeat something. for loop the most basic way to do this. There is a range object that is often used with for loop, which represents certain numerical range.\n\n\nCode\nrange(10)\n\n\nrange(0, 10)\n\n\nFor loop repeats something with this range.\n\n\nCode\nfor i in range(10):\n    print(i)\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nHere, the program in the for loop is executed with variable i with all values in the range [0, 10) assigned.\nWait, did you notice this redundant four spaces?\nfor i in range(10):\nâ–¡â–¡â–¡â–¡print(i)\nThese spaces are called indentation and used to indicate that these statements are inside the for loop block. This indentation is used everywhere in Python, as weâ€™ll see later. Note that two spaces and three spaces also work, but you should to have the same indentation width throughout your code.\nLetâ€™s go back to the for loop. Itâ€™s useful to accumulate values by for loop.\n\n\nCode\nsum_0_100 = 0\nfor i in range(101):\n    sum_0_100 += i\nsum_0_100\n\n\n5050\n\n\n\nExercise 2.1\nCompute \\(\\sum_{i=0}^{100} 2^k\\)."
  },
  {
    "objectID": "posts/python_basic_en.html#if-statement",
    "href": "posts/python_basic_en.html#if-statement",
    "title": "Quick Python basic course for OIST new students",
    "section": "If statement",
    "text": "If statement\nThe for loop is powerful, but not very frexible. It does the same thing everytime. However, we sometimes want to make the computer to do a differnt thing sometimes. Here, if statement is useful. if statement works with boolean values.\n\n\nCode\nTrue\n\n\nTrue\n\n\n\n\nCode\nFalse\n\n\nFalse\n\n\nIn Python, True and False are boolean types. Some expression returns boolean type as the result. For example, comparing numbers by &gt; and &lt; returns True or False.\n\n\nCode\n10 &lt; 40\n\n\nTrue\n\n\n\n\nCode\n40 &gt; 10\n\n\nTrue\n\n\nWe can use a == b for checking two numbers a and b are equal. Also, &gt;= and &lt;= represent \\(\\geq\\) and \\(\\leq\\).\n\n\nCode\n40 == 40\n\n\nTrue\n\n\nThe if statement executes the code inside the block if the expression is evaluated as True.\n\n\nCode\nvar_a = 100\n\nif var_a  &gt; 90:\n    print(\"Hey\")\n\n\nHey\n\n\nHere, print(\"Hey\") is executed because var_a &gt; 90 is True.\n\n\nCode\nif var_a  &gt; 100:\n    print(\"No hey\")\n\n\nIf the given expression is evaluated as False, nothing happens.\nWe can use if statement combined with for loop. For example, we can accumulate the sum of even numbers between \\(0\\) and \\(100\\) by:\n\n\nCode\neven_sum_0_100 = 0\n\nfor i in range(0, 101):\n    if i % 2 == 0:\n        even_sum_0_100 += i\n\neven_sum_0_100\n\n\n2550"
  },
  {
    "objectID": "posts/python_basic_en.html#more-complex-logic-with-if-statement",
    "href": "posts/python_basic_en.html#more-complex-logic-with-if-statement",
    "title": "Quick Python basic course for OIST new students",
    "section": "More complex logic with if statement",
    "text": "More complex logic with if statement\nif statement can have else branch if needed. If the given expression is False, statements in the else block is executed.\n\n\nCode\nif var_a  &gt; 100:\n    print(\"Hey\")\nelse:\n    print(\"Not hey\")\n\n\nNot hey\n\n\nIf you need more branches, if statement can be really complex with elif (shorthand of else if) blocks.\n\n\nCode\nif var_a &gt; 100:\n    print(\"Cond 1\")\nelif var_a == 100:\n    print(\"Cond 2\")\nelif var_a == 99:\n    print(\"Cond 3\")\nelse:\n    print(\"Cond 4\")\n\n\nCond 2\n\n\nThe condition also can be a bit more complex. Python has and and or operators for boolean values, which does logical operations.\n\n\nCode\nTrue and True, True and False, False and False\n\n\n(True, False, False)\n\n\n\n\nCode\nTrue or True, True or False, False or False\n\n\n(True, True, False)\n\n\nThese operators are often combined with if statement to express complex conditions.\n\n\nCode\nvar_b = 200\n\nif var_a &gt;= 100 and var_b &gt;= 200:\n    print(\"Hey!\")\n\n\nHey!\n\n\n\nExercise 2.2\nFix the program below (From https://utokyo-ipp.github.io/2/2-3.html).\n\n\nCode\nx = -1\nif x &lt; 3:\n    print(\"x is larger than or equal to 2, and less than 3\")\nelif x &lt; 2:\n    print(\"x is larger than or equal to 1, and less than 2\")\nelif x &lt; 1:\n    print(\"x is less than 1\")\nelse:\n    print(\"x is larger or equal to 3\")\n\n\nx is larger than or equal to 2, and less than 3"
  },
  {
    "objectID": "posts/python_basic_en.html#numerical-types",
    "href": "posts/python_basic_en.html#numerical-types",
    "title": "Quick Python basic course for OIST new students",
    "section": "Numerical types",
    "text": "Numerical types\n\nint(interger) type\nPython has some builtin data types. So far, we used int type.\n\n\nCode\ntype(30)\n\n\nint\n\n\nOn machine, int types have a simple representation by binary. Letâ€™s see it:\n\n\nCode\nbin(4)\n\n\n'0b100'\n\n\nWe call the minimum unit of this binary representation (i.e., 0 or 1) bit. Although there is some tricks to where we store the bit for minus (-), for intergers, thatâ€™s it.\n\n\nfloat type\nBut, for general real numbers like 3.14, the situation is a bit complex.\n\n\nCode\ntype(3.14)\n\n\nfloat\n\n\nReal values are called float type in Python and some other programming languages. What does it mean? This example of representing \\(12.345\\) from Wikipedia article may be easy to understand.\n\\[\n12.345=\\underbrace{12345}_{\\text{significand}}\\times\\underbrace{10}_{\\text{base}}\\underbrace{{}^{-3}}^{\\text{exponent}}\n\\]\nSo, in computer, real values are stored by two intergers (significand and exponent) with the fixed base. Base \\(2\\) is commonly used. This approximation of real numbers are called floating-point numbers, from which the type name float is derived.\nOn most computers available nowadays, Pythonâ€™s float type has 1 bit for sign (\\(+/-\\)), 11 bits for exponent, and 52 bits for significand. Thus, it has some limitations in precision.\n\n\nCode\n10 / 3\n\n\n3.3333333333333335\n\n\nThe answer should be \\(3.33333....\\), but because it has only 52 bits for significand, it canâ€™t express \\(3.333333333333333\\).\nThis limit of floating point representation sometimes causes large errors in some scienctific applications, and there are bunch of researches how to deal with the error.\nBTW, please it is worth noting that the division operator / always returns float type. To get the interger as the result of division, use // instead.\n\n\nCode\n8 / 4, type(8 / 4)\n\n\n(2.0, float)\n\n\n\n\nCode\n8 //4, type(8 // 4)\n\n\n(2, int)"
  },
  {
    "objectID": "posts/python_basic_en.html#execercise-3.1",
    "href": "posts/python_basic_en.html#execercise-3.1",
    "title": "Quick Python basic course for OIST new students",
    "section": "Execercise 3.1",
    "text": "Execercise 3.1\nDisplay the largest number in Python float.\n\nOptional: the hidden power of Python integer\nSo, as we learned the limitation of Python float type, it seems natural to assume that Python int type has the same limitation by the restricted number of bits. Letâ€™s try.\n\n\nCode\n2 ** 120, 2 ** 240\n\n\n(1329227995784915872903807060280344576,\n 1766847064778384329583297500742918515827483896875618958121606201292619776)\n\n\nWait, what is that? We can somehow compute very large number using Python int.\nThis is because Python interger has 2 internal representations:\n\nStandard 64bit interger ranges from \\(-9223372036854775808\\) to \\(9223372036854775808\\)\nList of 64bit intergers to represent large numbers\n\nPython automatically switches into the later representation, so it can compute really big numbers.\nNote that itâ€™s unusual. In many programming languages, int is often 64bit and sometimes 32bit, thus the precision is limited."
  },
  {
    "objectID": "posts/python_basic_en.html#text-and-list",
    "href": "posts/python_basic_en.html#text-and-list",
    "title": "Quick Python basic course for OIST new students",
    "section": "Text and List",
    "text": "Text and List\n\nText\nWe can use â€œâ€ and â€™â€™ to represent texts. Both have the same effect.\n\n\nCode\n\"Hey, programming is fun!\"\n\n\n'Hey, programming is fun!'\n\n\n\n\nCode\n'Hey, programming is fun!'\n\n\n'Hey, programming is fun!'\n\n\nThis is called str type (prefix of string).\n\n\nList\nList is a convenient data type to store multiple values in one variable. We can construct a list by [].\n\n\nCode\n[\"Hey\", 2, 4, [\"Yay\", \"Me\"]]\n\n\n['Hey', 2, 4, ['Yay', 'Me']]\n\n\n\n\nIndexing and List operatons\n[] has another meaning: it can be a special operator for getting a part of list and str.\n\n\nCode\na = [1, 2, 3, 4, 5]\na[0]\n\n\n1\n\n\nWe can update the value with this indexing syntax.\n\n\nCode\na[0] -= 1\na\n\n\n[0, 2, 3, 4, 5]\n\n\nThe indexing starts from zero. If the index is negative (say, -i, it indicates length of the list - i - 1.\n\n\nCode\na[-1], a[-2]\n\n\n(5, 4)\n\n\nThere is a special syntax called slice combined with []. a[i:j] returns a part of the list from index i to j - 1.\n\n\nCode\na[2: 4]\n\n\n[3, 4]\n\n\nWe can skip either start and end of the slice. Then, the default values (0 and the length of the list) are used. We can even skip both and write a[:], but itâ€™s just the same as a.\n\n\nCode\na[2:], a[:4], a[:]\n\n\n([3, 4, 5], [1, 2, 3, 4], [1, 2, 3, 4, 5])\n\n\nWe can concatanate lists by +. If you just want to add a value, you can use the syntax list.append.\n\n\nCode\na.append(-3)\na\n\n\n[1, 2, 3, 4, 5, -1]\n\n\n\n\nCode\na + [-2, -1]\n\n\n[1, 2, 3, 4, 5, -1, -2, -1]\n\n\nSame operations can be done for str.\n\n\nCode\n\"Hey, programming is fun!\"[: -4] + \"not fun!\"\n\n\n'Hey, programming is not fun!'\n\n\nBecause text is so common in human society, there are plenty of methods to manipulate str. One example is the templating string.\n\n\nCode\n\"{} is fun\".format(\"Programming\")\n\n\n'Programming is fun'\n\n\nWith this str.format, we can embed arbitary string and some values implicitly convertible to str to the point where {} indicates in the string. Because this is so common, Python has a special syntax for formatting called f-string.\n\n\nCode\nf\"The answer of 1 + 2 = {1 + 2}\"\n\n\n'The answer of 1 + 2 = 3'\n\n\nf-string is so special in that inside {} we can write any Python expression. The result is converted to string and embed in the result."
  },
  {
    "objectID": "posts/python_basic_en.html#internal-representation-of-text",
    "href": "posts/python_basic_en.html#internal-representation-of-text",
    "title": "Quick Python basic course for OIST new students",
    "section": "Internal representation of text",
    "text": "Internal representation of text\nstr in the machine is just numbers. We can convert it like:\n\n\nCode\nb = \"Hey\".encode(\"utf-8\")\nb\n\n\nb'Hey'\n\n\nb has a special type called bytes, which is a specialized list for small intergers. Actually itâ€™s just a sequence of numbers\n\n\nCode\nb[0], b[1], b[2]\n\n\n(72, 101, 121)\n\n\nSo, H is 72, e is 101, and y is 121 on the machine. For complex characters like â˜€, more complex rules are used to convert it to numbers. See UTF-8 if you are curious.\n\nExcercise 3.1\nConstruct a list that contains all English alphabet in small case.\nNote that you can convert number to str by:\n\n\nCode\na = 97\na.to_bytes().decode(\"utf-8\")\n\n\n'a'"
  },
  {
    "objectID": "posts/python_basic_en.html#other-convenient-types",
    "href": "posts/python_basic_en.html#other-convenient-types",
    "title": "Quick Python basic course for OIST new students",
    "section": "Other Convenient Types",
    "text": "Other Convenient Types\n\nTuple\nVery similar to list, but has a bit different syntax and immutable. Use () to construct tuple.\n\n\nCode\n(1, 2, \"Sun\", 4)\n\n\n(1, 2, 'Sun', 4)\n\n\nWe donâ€™t need to use () in some situations. For example, as the last value of Notebook cell.\n\n\nCode\n1, 2, \"Sun\", 4\n\n\n(1, 2, 'Sun', 4)\n\n\nBut Iâ€™d recommend you to always use () for tuple, especially for begginers.\nBecause tuple is immutable, we canâ€™t update values in tuple.\n\n\nCode\nt = (1, 2, \"Sun\", 4)\nt[0] += 4\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[97], line 2\n      1 t = (1, 2, \"Sun\", 4)\n----&gt; 2 t[0] += 4\n\nTypeError: 'tuple' object does not support item assignment\n\n\n\n+ is allowed because it creates a new tuple.\n\n\nCode\nt + (5, 6)\n\n\n(1, 2, 'Sun', 4, 5, 6)\n\n\n\n\nDict\nDict is a set of key and value pairs with fast access by key. We can make list by {} syntax.\n\n\nCode\nuser = {\n    \"Name\": \"Me\",\n    \"ID\": 345,\n    0: 10,\n}\n\n\nWe can use the index syntax to dict, though slicing [:] is not supported.\n\n\nCode\nuser[\"Name\"]\n\n\n'Me'\n\n\n\n\nNoneType\nIt sounds a bit weird, but None is the special type that represents nothing.\n\n\nCode\nNone\n\n\nThe type of None is NoneType, and None is the only value of this type.\n\n\nCode\ntype(None)\n\n\nNoneType\n\n\nNone is surprisingly useful in programming when some values can be missing, but maybe difficult for begginers to understand its value."
  },
  {
    "objectID": "posts/python_basic_en.html#recursion",
    "href": "posts/python_basic_en.html#recursion",
    "title": "Quick Python basic course for OIST new students",
    "section": "Recursion",
    "text": "Recursion\nAs long as reusability, there is another clever way to use function, called recursion.\nFor example, letâ€™s assume that we want to compute \\(\\sum_{i = 0}^{100} i\\). As we learned, we can compute this easily by for loop. However, we can do the same thing using a function recursion.\n\n\nCode\ndef sum_to_0(i):\n    if i == 0:\n        return 0\n    else:\n        return sum_to_0(i - 1) + i\n\nsum_to_0(100)\n\n\n5050\n\n\nBecause \\(\\sum_{i = 0}^{k} i = k + \\sum_{i = 0}^{k - 1} i\\) for all \\(k &gt; 0\\), we can call the function sum_to_0 in the function sum_to_0. This technique is called recursion. Iâ€™d say that for loop is often better because itâ€™s simple, but sometimes recursion works like a charm.\n\nExecercise 4.1\nImplement Euclidian Algorithm using recursion."
  },
  {
    "objectID": "posts/python_basic_en.html#scope",
    "href": "posts/python_basic_en.html#scope",
    "title": "Quick Python basic course for OIST new students",
    "section": "Scope",
    "text": "Scope\nVariable scope around the function is so important. You can remember two rules:\n\nIn functions, we can refer to the variable outside of the function.\nVariables inside the function is not visible from outside.\n\n\n\nCode\nvar_out = 100\n\ndef scope_demo():\n    print(var_out)\n    var_in = 200\n\nscope_demo()\nprint(var_in)\n\n\n100\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[143], line 8\n      5     var_in = 200\n      7 scope_demo()\n----&gt; 8 print(var_in)\n\nNameError: name 'var_in' is not defined\n\n\n\nSo, while we can read var_out inside scope_demo function, var_in is only available inside the function.\nImportantly, we canâ€™t change the value of outside variable by assignment inside a function.\n\n\nCode\nvar_out = 100\n\ndef scope_demo2():\n    var_out = 200\n\nscope_demo2()\nvar_out\n\n\n100\n\n\nItâ€™s a bit confusing, but when a new variable is assigned in a function, it is treated as the variable inside the function scope. If you really want to avoid this, there are two ways:\n\nUse global\nUse container types like list or dict\n\n\n\nCode\nvar_out = 100\n\ndef scope_demo3():\n    global var_out\n    var_out = 200\n\nscope_demo3()\nvar_out\n\n\n200\n\n\nBy placing global var_out, it magically worked. Also, we can change the value of dict inside a function.\n\n\nCode\nvar_out = {0: 100}\n\ndef scope_demo3():\n    var_out[0] *= 2\n\nscope_demo3()\nvar_out\n\n\n{0: 200}\n\n\nThis is maybe a bit weird behavior for you (and even for me). But my recommendation is avoid referencing variables outside the function as much as possible. It can confuse your brain. Also, not depending on outside variables improves copy-pastablity of your code. I mean, if your code depends on some variables outside the function, you need to copy everything to another file if you want to use. But, if the function is completely independent, it just works by only copy-pasting the function.\nNote that your Python can work without writing any function, but I recommend to put everything inside function in your Python code. That will make your life easier."
  },
  {
    "objectID": "posts/python_basic_en.html#type-annotation",
    "href": "posts/python_basic_en.html#type-annotation",
    "title": "Quick Python basic course for OIST new students",
    "section": "Type annotation",
    "text": "Type annotation\nWe can annotate function arguments with type.\n\n\nCode\ndef typed_function(a: int, b: str) -&gt; str:\n    return f\"a: {a} b: {b}\"\n\n\nThe value after -&gt; indicates the type of return value. It is good to annotate arguments for readability."
  },
  {
    "objectID": "posts/python_basic_en.html#basic-class",
    "href": "posts/python_basic_en.html#basic-class",
    "title": "Quick Python basic course for OIST new students",
    "section": "Basic class",
    "text": "Basic class\nSo far, we learned that Python has many builtin types like list or dict. You can define you own one using class syntax.\n\n\nCode\nclass YourClass:\n    pass\n\n\n\n\nCode\ny = YourClass()\ntype(y)\n\n\n__main__.YourClass\n\n\nThatâ€™s it. Your class can have some special functions called method, that is called by .method() syntax. For example, \"{}\".format is a method of str class.\n\n\nCode\nclass ClassWithMethod:\n    def method(self) -&gt; int:\n        return 5\n\n\n\n\nCode\nClassWithMethod().method()\n\n\n5\n\n\nYour class can have any values called members. To initialize your class with specific members, you can use a special method called __init__.\n\n\nCode\nclass ClassWithMembers:\n    def __init__(self):\n        self.name = \"Me\"\n        self.number = 10\n\n\n\n\nCode\nc = ClassWithMembers()\nc.name, c.number\n\n\n('Me', 10)\n\n\nThe special function used to create an instance of class is called constructor, and it also can take arguments.\nclass ClassWithFlexibleMembers: def init(self, name: str, number: int) -&gt; None: self.name = name self.number = number\nc = ClassWithFlexibleMembers(â€œMewâ€, 33) c.name, c.number\nNote that the return type of __init__ is always None. __init__ is called in the constructor inside Python, and itâ€™s not equal to the constructor."
  },
  {
    "objectID": "posts/understanding-attention-en.html",
    "href": "posts/understanding-attention-en.html",
    "title": "Understanding what self-attention is doing",
    "section": "",
    "text": "ChatGPT is getting a lot of buzz these days. I donâ€™t use it much because I donâ€™t like to worry about prompts, but my friend uses it to write papers, and my mom uses it to just talk, which makes me feel a little sorry for being an unfriendly sonâ€¦ A neural network called Transformer is the success of language generative models like ChatGPT. A layer called Multihead Attention is repeatedly applied to a sequence of input tokens to form a complex model. In this blog we will focus on a simplified version of Multihead Attention, Singlehead Self-Attention, study what it does, and try to write some code to run it."
  },
  {
    "objectID": "posts/understanding-attention-en.html#sequence-of-tokens",
    "href": "posts/understanding-attention-en.html#sequence-of-tokens",
    "title": "Understanding what self-attention is doing",
    "section": "Sequence of tokens",
    "text": "Sequence of tokens\nA token sequence is literally a sequence consisting of tokens. A token is an element of a finite set. For practical use, this includes substrings obtained by byte pair encoding, but you donâ€™t need to worry about that for now. Let \\(V\\) be a set of tokens, and number them \\([Nv] := {1, ... , Nv}\\). Write \\(x = x[1: l]\\) for the token sequence. Also, let \\(L\\) be the maximum length of the token sequence."
  },
  {
    "objectID": "posts/understanding-attention-en.html#from-a-token-to-a-vector",
    "href": "posts/understanding-attention-en.html#from-a-token-to-a-vector",
    "title": "Understanding what self-attention is doing",
    "section": "From a token to a vector",
    "text": "From a token to a vector\nUsing a \\(d_e \\times Nv\\)-dimensional matrix \\(W_e\\), the token embedding is obtained from the \\(v\\)th token by \\(e = W_e[:, v]\\). This will be a \\(d_e\\)-dimensional vector. Note that we write \\(W[i, :]\\) for the \\(i\\)-th row vector and \\(W[:, j]\\) for the \\(j\\)-th column vector in the numpy style. This matrix \\(W_e\\) seems to be learned by gradient descent."
  },
  {
    "objectID": "posts/understanding-attention-en.html#position-is-also-embedded",
    "href": "posts/understanding-attention-en.html#position-is-also-embedded",
    "title": "Understanding what self-attention is doing",
    "section": "Position is also embedded",
    "text": "Position is also embedded\nUsing a \\(d_p \\times L\\)-dimensional matrix \\(W_p\\), a positional embedding is obtained by \\(p = W_p[:, l]\\) from the information that there is a token at \\(l\\)th place in the token sequence. This is also a vector with length \\(d_e\\). To be honest, I am not sure what it means, but we can add this to the token embedding described earlier to obtain the embedding for the \\(t\\)th token \\(x[t]\\) in the token sequence \\(x\\) by \\(e = W_e[:, x[t]] + W_p[:, t]\\). Is it safe to add this? I donâ€™t know. The position embedding may be learned, but in the paper Attention Is All You Need, where Transformer was first proposed, it is constructed as follows.\n\\[\n\\begin{align*}\nW_p[2i - 1, t] &= \\sin (\\frac{t}{L^{2i / d_e}}) \\\\\nW_p[2i, t] &= \\cos (\\frac{t}{L^{2i / d_e}}) \\\\\n&~~~~~(0 &lt; 2i \\leq d_e)\n\\end{align*}\n\\] Letâ€™s visualize it with\\(L=50, d_e = 5\\).\n\n\nCode\nimport matplotlib\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nmatplotlib.font_manager.fontManager.addfont(\"NotoEmoji-Medium.ttf\")\n\nL = 50\nd_e = 5\nx = np.arange(L)\nfor i in range(1, 1 + d_e):\n    if i % 2 == 0:\n        w_p = np.sin(x / L ** (i / d_e))\n    else:\n        w_p = np.cos(x / L ** ((i - 1) / d_e))\n    _ = plt.plot(x, w_p, label=f\"i={i}\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nSo this embedding seems to embed words at different frequencies for each component. I suspect that this allows us to consider the position in a short context at the same time."
  },
  {
    "objectID": "posts/understanding-attention-en.html#training-a-markov-model",
    "href": "posts/understanding-attention-en.html#training-a-markov-model",
    "title": "Understanding what self-attention is doing",
    "section": "Training a Markov model",
    "text": "Training a Markov model\nLetâ€™s start with a simple model to generate the weather. Letâ€™s assume that the next dayâ€™s weather is stochastically determined based on the previous dayâ€™s weather. Note that ğŸŒ§ï¸, â˜ï¸, and â˜€ï¸ are multibyte characters, and implement the following.\n\n\nCode\nimport dataclasses\n\n_GEN = np.random.Generator(np.random.PCG64(20230508))\n_MARKOV = {\n    \"\": [0.3, 0.4, 0.3],\n    \"ğŸŒ§ï¸\": [0.6, 0.3, 0.1],\n    \"â˜ï¸\": [0.3, 0.4, 0.3],\n    \"â˜€ï¸\": [0.2, 0.3, 0.5],\n}\n\ndef markov(prev: str) -&gt; str:\n    prob = _MARKOV[prev[-2:]]\n    return prev + _GEN.choice(WEATHERS, p=prob)\n\n\ndef generate(f, n: int, init: str = \"\"):\n    value = init\n    for _ in range(n):\n        value = f(value)\n    return value\n\n\n@dataclasses.dataclass\nclass Dataset:\n    weathers: list[str]\n    embeddings: jax.Array\n    next_weather_indices: jax.Array\n    \n    def __len__(self) -&gt; int:\n        return len(self.weathers)\n\n\ndef make_dataset(f, seq_len, size) -&gt; Dataset:\n    w_list, e_list, nw_list = [], [], []\n    for _ in range(size):\n        weathers = generate(f, seq_len + 1)\n        e = jnp.array(get_embedding(weathers[:-2]))\n        w_list.append(weathers)\n        e_list.append(e)\n        nw_list.append(WEATHERS.index(weathers[-2:]))\n    return Dataset(w_list, jnp.stack(e_list), jnp.array(nw_list))\n\n\ngenerated = generate(markov, 10)\ngenerated, get_embedding(generated)\n\n\n('ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸â˜ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸â˜€ï¸',\n array([[1.  , 1.  , 1.  , 0.  , 1.  , 0.  , 1.  , 1.  , 0.  , 0.  ],\n        [0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  ],\n        [0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 1.  , 1.  ],\n        [0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 ]]))\n\n\nThe generated weathers look like this. Since we now only want to predict the weather for the next day, the output of the model should be a probability distribution over a set {ğŸŒ§ï¸, â˜ï¸, â˜€ï¸}. Since Self-Attention will return a \\(d_\\textrm{out} \\times T\\) matrix for an embedded column of length \\(T\\), we set \\(d_\\textrm{out} = 3\\) and apply the softmax function to Attentionâ€™s output \\(\\tilde{V}\\) to obtain \\(P_t = \\textrm{softmax}(\\tilde{V}[:, t])\\). We model each element of \\(P_t\\) as representing the probability that it will be on the next day ğŸŒ§ï¸, â˜ï¸, or â˜€ï¸. Let this be trained to maximize the sum of log-likelihood \\(\\sum_t \\log P_t(\\textrm{next weather})\\).\n\n\nCode\nfrom typing import Callable\n\nimport optax\n\n\ndef attn_neglogp(model: eqx.Module, seq: jax.Array, next_w: jax.Array) -&gt; jax.Array:\n    batch_size = seq.shape[0]\n    tilde_v = jax.vmap(model)(seq)  # B x OUT x SEQ_LEN\n    logp = jax.nn.log_softmax(tilde_v, axis=1)  # B x OUT x SEQ_LEN\n    logp_masked = logp * jax.nn.one_hot(next_w, num_classes=3).reshape(-1, 3, 1)\n    return -jnp.mean(jnp.sum(logp_masked.reshape(batch_size, -1), axis=-1))\n\ndef train(\n    n_total_epochs: int,\n    minibatch_size: int,\n    model: eqx.Module,\n    ds: Dataset,\n    test_ds: Dataset,\n    key: jax.Array,\n    learning_rate: float = 1e-2,\n    loss_fn: Callable[[eqx.Module, jax.Array, jax.Array], jax.Array] = attn_neglogp,\n) -&gt; tuple[eqx.Module, jax.Array, list[float], list[float]]:\n    n_data = len(ds)\n    optim = optax.adam(learning_rate)\n\n    @eqx.filter_jit\n    def train_1step(\n        model: eqx.Module,\n        seq: jax.Array,\n        next_w: jax.Array,\n        opt_state: optax.OptState,\n    ) -&gt; tuple[jax.Array, eqx.Module, optax.OptState]:\n        loss, grads = eqx.filter_value_and_grad(loss_fn)(model, seq, next_w)\n        updates, opt_state = optim.update(grads, opt_state)\n        model = eqx.apply_updates(model, updates)\n        return loss, model, opt_state\n\n    opt_state = optim.init(model)\n    n_optim_epochs = n_data // minibatch_size\n    loss_list, eval_list = [], []\n    for epoch in range(n_total_epochs // n_optim_epochs):\n        key, perm_key = jax.random.split(key)\n        indices = jax.random.permutation(perm_key, n_data, independent=True)\n        for _ in range(n_optim_epochs):\n            e = ds.embeddings[indices]\n            next_w = ds.next_weather_indices[indices]\n            loss, model, opt_state = train_1step(model, e, next_w, opt_state)\n            loss_list.append(loss.item())\n            test_loss = jax.jit(loss_fn)(\n                model,\n                test_ds.embeddings,\n                test_ds.next_weather_indices,\n            )\n            eval_list.append(test_loss.item())\n    return model, key, loss_list, eval_list\n\n\nLetâ€™s run it. I use \\(6\\) for \\(d_textrm{attn}\\) and \\(10\\) for the sequence length \\(T\\).\n\n\nCode\nD_ATTN = 6\nSEQ_LEN = 10\nkey = jax.random.PRNGKey(1234)\nmodel = MaskedAttention(4, D_ATTN, 3, key)\nds = make_dataset(markov, SEQ_LEN, 1000)\ntest_ds = make_dataset(markov, SEQ_LEN, 1000)\nmodel, key, loss_list, eval_list = train(500, 100, model, ds, test_ds, key, 1e-2)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on Markov model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\n@jax.jit\ndef accuracy(model: eqx.Module, seq: jax.Array, next_w: jax.Array) -&gt; float:\n    tilde_v = jax.vmap(model)(seq)  # B x OUT x SEQ_LEN\n    inferred = jnp.argmax(tilde_v[:, :, 0], axis=1)\n    n_correct = jnp.sum(inferred == next_w)\n    return n_correct / seq.shape[0]\n\nf\"Accuracy: {accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.49800002574920654'\n\n\n\n\n\n\n\n\n\nThe loss is no longer dropping around 100 epochs, so it seemes to have converged. Letâ€™s see what has actually been learned. For now, letâ€™s try generating weather. This is not very meaningful in this case, but I thought it would be good to learn the generative process. It seems that the beam search is often used, but since itâ€™s complex, I use a simpler method this time. Starting from â˜ï¸, we sample the next weather from the categorical distribution, and keep adding to it.\n\n\nCode\ndef generate_from_model(\n    model: eqx.Module,\n    key: jax.Array,\n    seq_len: int,\n    init: str = \"â˜ï¸\",\n) -&gt; tuple[str, jax.Array]:\n    @jax.jit\n    def step(\n        model: eqx.Module,\n        seq: jax.Array,\n        key: jax.Array,\n    ) -&gt; tuple[jax.Array, jax.Array]:\n        sample_key, key = jax.random.split(key)\n        tilde_v = model(seq)  # 3 x len(seq)\n        sampled = jax.random.categorical(key=sample_key, logits=tilde_v[:, 0])\n        return sampled, key\n\n    generated = init\n    for _ in range(seq_len):\n        next_w, key = step(model, get_embedding(generated), key)\n        generated += WEATHERS[next_w.item()]\n    return generated, key\n\n\ngenerated, key = generate_from_model(model, key, 20)\ngenerated\n\n\n'â˜ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸â˜ï¸ğŸŒ§ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜ï¸â˜€ï¸â˜ï¸'\n\n\nThis is what the predicted weathers look like. Of course, this doesnâ€™t tell us anything. Next, letâ€™s visualize the contents of Self-Attention for some data in the test data.\n\n\nCode\n@jax.jit\ndef get_attn(model: eqx.Module, seq: jax.Array) -&gt; jax.Array:\n    q = model.w_q @ seq + model.b_q\n    k = model.w_k @ seq + model.b_k\n    score = causal_mask(q.T @ k) / model.sqrt_d_attn\n    return jax.nn.softmax(score, axis=-1)\n\n\ndef visualize_attn(ax, model: eqx.Module, ds: Dataset, index: int = 0) -&gt; None:\n    attn = np.array(get_attn(model, ds.embeddings[index]))\n    im = ax.imshow(attn)\n    ax.set_xticks(\n        np.arange(10),\n        labels=[ds.weathers[index][i * 2] for i in range(10)],\n        fontname=\"Noto Emoji\",\n    )\n    ax.set_yticks(\n        np.arange(10),\n        labels=[ds.weathers[index][i * 2] for i in range(10)],\n        fontname=\"Noto Emoji\",\n    )\n    for i in [np.argmin(attn), np.argmax(attn)]:\n        # Show min and max values\n        im.axes.text(i % 10, i // 10, f\"{attn.flatten()[i]:.1f}\", color=\"gray\")\n\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\nvisualize_attn(ax1, model, test_ds, 1)\nvisualize_attn(ax2, model, test_ds, 2)\n\n\n\n\n\n\n\n\n\nNote that I could not use color emojis in matplotlib. We observe that: 1. â€˜last day -&gt; last dayâ€™ has the largest attention 2. â€˜other days -&gt; last dayâ€™ also has larger Attention 3. the other factors are almost irrelevant\nThe first is natural since we trained a weather sequence generated from a Markov model. The attentions from other days to last days are actually unnecessary, but also were taken."
  },
  {
    "objectID": "posts/understanding-attention-en.html#when-future-events-depend-on-multiple-independently-occurring-past-events",
    "href": "posts/understanding-attention-en.html#when-future-events-depend-on-multiple-independently-occurring-past-events",
    "title": "Understanding what self-attention is doing",
    "section": "When future events depend on multiple independently occurring past events",
    "text": "When future events depend on multiple independently occurring past events\nNext, letâ€™s train some more complex data. This time, we will generate 11 days of weather in the following way: 1. Generate weather for days 1, 4, and 8 independently 2. Generate the weather for days 2 and 3 using a Markov chain with the weather for day 1 as the initial condition; generate the weather for days 5, 6, 7, 9, and 10 in the same way, based on the weather for days 4 and 8. 3. Generate the weather for day 11 stochastically based on the weather for days 1, 4, and 8.\nLetâ€™s see if the self-attention layer can learn this.\n\n\nCode\ndef _make_table() -&gt; dict[str, list[float]]:\n    candidates = []\n    for i in range(1, 9):\n        for j in range(1, 9):\n            for k in range(1, 9):\n                if i + j + k == 10:\n                    candidates.append((i, j, k))\n    table = {}\n    for i in WEATHERS:\n        for j in WEATHERS:\n            for k in WEATHERS:\n                table[i + j + k] = [p / 10 for p in _GEN.choice(candidates)]\n    return table\n\n_ONE_FOUR_8_TABLE = _make_table()\n\ndef one_four_8(prev: str) -&gt; str:\n    length = len(prev) // 2\n    if length == 10:\n        p = _ONE_FOUR_8_TABLE[prev[0: 2] + prev[6: 8] + prev[14: 16]]\n        return prev + _GEN.choice(WEATHERS, p=p)\n    elif length == 4 or length == 8:\n        return prev + _GEN.choice(WEATHERS, p=_MARKOV[\"\"])\n    else:\n        return markov(prev)\n    \ngenerate(one_four_8, 11)\n\n\n'â˜€ï¸â˜ï¸â˜ï¸â˜€ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜ï¸â˜€ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸'\n\n\nOK, letâ€™s do it.\n\n\nCode\nmodel = MaskedAttention(4, D_ATTN, 3, key)\nds = make_dataset(one_four_8, SEQ_LEN, 5000)\ntest_ds = make_dataset(one_four_8, SEQ_LEN, 1000)\nmodel, key, loss_list, eval_list = train(500, 100, model, ds, test_ds, key, 1e-2)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on 1-4-8 model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\nvisualize_attn(ax1, model, test_ds, 1)\nvisualize_attn(ax2, model, test_ds, 2)\n\nf\"Accuracy: {accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.4020000100135803'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt converged, but the accuracy is poor and the attention is also not very effective. Attentions are given to days 1, 4, and 8, but as in the previous experiment, the last dayâ€™s attention is larger."
  },
  {
    "objectID": "posts/understanding-attention-en.html#do-we-need-attention",
    "href": "posts/understanding-attention-en.html#do-we-need-attention",
    "title": "Understanding what self-attention is doing",
    "section": "Do we need attention?",
    "text": "Do we need attention?\nAs smart readers may have noticed, we donâ€™t need self-attention to represent the two weather sequences we have learned so far. This is because the internal correlation of the input weather sequence has no bearing on the task at all, since the first one determines the weather of the previous day (day 10) and the next one determines the weather of days 1, 4, 8 to 11. So, letâ€™s train with a linear model + softmax (the so-called multinomial logistic regression).\n\n\nCode\nclass LinearModel(eqx.Module):\n    w: jax.Array\n    b: jax.Array\n\n    def __init__(self, d_in: int, d_out: int, key: jax.Array) -&gt; None:\n        w_key, b_key = jax.random.split(key)\n        self.w = jax.random.normal(w_key, (d_out, d_in))\n        self.b = jax.random.normal(b_key, (d_out,))\n\n    def __call__(self, seq: jax.Array) -&gt; jax.Array:\n        return self.w @ seq.flatten() + self.b\n\n\ndef linear_neglogp(model: eqx.Module, seq: jax.Array, next_w: jax.Array) -&gt; jax.Array:\n    logp = jax.nn.log_softmax(jax.vmap(model)(seq), axis=1)  # B x OUT\n    logp_masked = logp * jax.nn.one_hot(next_w, num_classes=3)\n    return -jnp.mean(jnp.sum(logp_masked, axis=1))\n\n\nmodel = LinearModel(4 * SEQ_LEN, 3, key)\nmodel, key, loss_list, eval_list = train(\n    500, 100, model, ds, test_ds, key, 1e-2, linear_neglogp\n)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on 1-4-8 model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\n\n@jax.jit\ndef linear_accuracy(model: eqx.Module, seq: jax.Array, next_w: jax.Array) -&gt; float:\n    tilde_v = jax.vmap(model)(seq)  # B x OUT\n    inferred = jnp.argmax(tilde_v, axis=1)\n    n_correct = jnp.sum(inferred == next_w)\n    return n_correct / seq.shape[0]\n\n\nf\"Accuracy: {linear_accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.44200003147125244'\n\n\n\n\n\n\n\n\n\nThis looks better. So, when is the self-attention useful?\n\n(Compared to MLP, etc.) when you donâ€™t want to make the number of parameters depend on the length \\(L\\) of the token sequence\n\nNote that in self-attention the number of parameters is \\((d_\\textrm{in} + 1)(2d_\\textrm{attn} + d_\\textrm{out})\\), while in the linear model it is \\((d_\\textrm{in}L + 1)d_\\textrm{out}\\). In the linear model, the number of parameters increases linearly with the length of the token sequence. Note, however, that self-attention requires \\(O(L^2)\\) memory usage for \\(q^\\top k\\), although Self-attention Does Not Need \\(O(n^2)\\) Memory shows an efficient \\(O(\\sqrt{L})\\) implementation. Still, it may be consume more memory thatn simple RNN or CNN.\n\n(Compared to RNN, CNN, etc.) when there is a long-term dependency in the token series\n\nCompared to CNN and RNN, the advantage of self-attention is that \\(q^\\top k\\) can represent arbitrary dependencies between tokens in one layer. However, since \\(q^\\top k[i, j]\\) is obtained only by linear operations on the two embeddings \\(e[i], e[j]\\), if the two embeddings are dependent via some nonlinear function, the relationship cannot be represented by a single self-attention layer.\n\n(Compared to RNN) when you want to do fast and parallel batch training\n\nThe operation of computing self-attention, namely the computation of \\(\\textrm{softmax}(q^\\top k)\\) can be parallelized per query. This is useful when you want to get a parallelized implementation that works fast on single or many GPUs.\nSo, although it has the advantage of not depending on \\(L\\) for the number of parameters compared to one linear layer, I am not sure if the self-attention can actually be more expressive or efficient. Let another blog post do some more theoretical stuff, I will try some more."
  },
  {
    "objectID": "posts/understanding-attention-en.html#when-there-are-hidden-variables",
    "href": "posts/understanding-attention-en.html#when-there-are-hidden-variables",
    "title": "Understanding what self-attention is doing",
    "section": "When there are hidden variables",
    "text": "When there are hidden variables\nGenerate a weather sequence in the following way. Look at the weather for the past \\(n\\) days, and if ğŸŒ§ï¸ has appeared \\(k\\) times, let \\(\\frac{n - k}{2n}\\) be the probability that the weather for the next day will be ğŸŒ§ï¸. Assign probabilities for â˜ï¸ and â˜€ï¸ in the same way. Generate a long weather sequence in this way and create a dataset by gathering randomly sampled subsequences.\n\n\nCode\nfrom functools import partial\n\ndef ndays_model(prev: str, n: int = 10) -&gt; str:\n    counts = np.zeros(3)\n    prev_n = prev[-2 * n: ]\n    for i in range(n):\n        prev_w_i = prev_n[i * 2: i * 2 + 2]\n        counts[WEATHERS.index(prev_w_i)] += 1\n    prob = (n - counts) / (n * 2)\n    return prev + _GEN.choice(WEATHERS, p=prob)\n\ngenerate(ndays_model, 100, generate(markov, 10))                \n\n\n'â˜ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜ï¸ğŸŒ§ï¸â˜ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸â˜€ï¸â˜ï¸â˜€ï¸â˜ï¸â˜€ï¸â˜ï¸â˜€ï¸â˜ï¸â˜€ï¸ğŸŒ§ï¸â˜€ï¸â˜ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸â˜€ï¸â˜€ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸â˜ï¸â˜ï¸ğŸŒ§ï¸â˜ï¸â˜ï¸â˜ï¸â˜€ï¸ğŸŒ§ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜ï¸â˜€ï¸â˜ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸â˜ï¸ğŸŒ§ï¸â˜ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜ï¸â˜€ï¸ğŸŒ§ï¸â˜ï¸ğŸŒ§ï¸â˜ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜ï¸â˜ï¸â˜ï¸â˜ï¸ğŸŒ§ï¸â˜ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸â˜€ï¸â˜ï¸ğŸŒ§ï¸â˜€ï¸â˜€ï¸ğŸŒ§ï¸â˜€ï¸â˜ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜ï¸â˜€ï¸'\n\n\nThe generated weather sequence looks like this. First, letâ€™s traing the linear model on a 10-day model. In this case there is no hidden variable.\n\n\nCode\ndef make_ndays_dataset(seq_len, size, n: int = 10) -&gt; Dataset:\n    weathers = generate(partial(ndays_model, n=n), seq_len * size * 2, generate(markov, n * 2))\n    w_list, e_list, nw_list = [], [], []\n    for _ in range(size):\n        start = _GEN.integers(0, seq_len * size * 2 - 11)\n        w = weathers[start * 2 : start * 2 + (seq_len + 1) * 2]\n        e = jnp.array(get_embedding(w[:-2]))\n        w_list.append(w)\n        e_list.append(e)\n        nw_list.append(WEATHERS.index(w[-2:]))\n    return Dataset(w_list, jnp.stack(e_list), jnp.array(nw_list))\n\n\nds = make_ndays_dataset(SEQ_LEN, 5000, n=10)\ntest_ds = make_ndays_dataset(SEQ_LEN, 1000, n=10)\n\nmodel = LinearModel(4 * SEQ_LEN, 3, key)\nmodel, key, loss_list, eval_list = train(\n    500, 100, model, ds, test_ds, key, 1e-2, linear_neglogp\n)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on 10days model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nf\"Accuracy: {linear_accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.41600000858306885'\n\n\n\n\n\n\n\n\n\nTrain the self-attention layer next.\n\n\nCode\nmodel = MaskedAttention(4, D_ATTN, 3, key)\nmodel, key, loss_list, eval_list = train(500, 100, model, ds, test_ds, key, 1e-2)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on 10days model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\nvisualize_attn(ax1, model, test_ds, 1)\nvisualize_attn(ax2, model, test_ds, 2)\n\nf\"Accuracy: {accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.3630000054836273'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe self-attention is now worse than the linear model. Next, letâ€™s turn the 10-day model into a 15-day model with hidden variables. First, we train the linear layer.\n\n\nCode\nds = make_ndays_dataset(SEQ_LEN, 5000, n=15)\ntest_ds = make_ndays_dataset(SEQ_LEN, 1000, n=15)\n\nmodel = LinearModel(4 * SEQ_LEN, 3, key)\nmodel, key, loss_list, eval_list = train(\n    500, 100, model, ds, test_ds, key, 1e-2, linear_neglogp\n)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on 15days model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nf\"Accuracy: {linear_accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.34200000762939453'\n\n\n\n\n\n\n\n\n\nThen train the self-attention layer.\n\n\nCode\nmodel = MaskedAttention(4, D_ATTN, 3, key)\nmodel, key, loss_list, eval_list = train(500, 100, model, ds, test_ds, key, 1e-2)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on 15days model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\nvisualize_attn(ax1, model, test_ds, 1)\nvisualize_attn(ax2, model, test_ds, 2)\n\nf\"Accuracy: {accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.35600000619888306'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttention is a bit better, but there is no meaningful difference in accuracy."
  },
  {
    "objectID": "posts/understanding-attention-en.html#what-about-non-linear",
    "href": "posts/understanding-attention-en.html#what-about-non-linear",
    "title": "Understanding what self-attention is doing",
    "section": "What about non-linear?",
    "text": "What about non-linear?\nIf the linear model performs better even with hidden variables, it probably means that the task is still linearly solvable. So letâ€™s consider more difficult nonlinear data. Let \\(y\\) be a vector created by assigning 0, 1, and 2 to ğŸŒ§ï¸, â˜ï¸, and â˜€ï¸ in the 10-day weather sequence, respectively. Also, let \\(\\beta = (0, 1, 2, 3, 2, 1, 0, 1, 2, 3)^\\top\\). Let \\((y(2 - y)\\cdot \\beta)\\mod 3\\) be the weather for the next day. To make the data a bit stohcastic, letâ€™s assign other weathers 2% probability.\n\n\nCode\n_BETA = np.tile([0, 1, 2, 3, 2, 1], (10,))\n\ndef dotmod_model(prev: str, n: int =10) -&gt; str:\n    y = np.zeros(n, dtype=int)\n    prev_n = prev[-2 * n:]\n    for i in range(n):\n        prev_w_i = prev_n[i * 2: i * 2 + 2]\n        y[i] = WEATHERS.index(prev_w_i) + 1\n    prob = [0.02, 0.02, 0.02]\n    prob[np.dot(y * (2 - y), _BETA[: n]) % 3] = 0.96\n    return prev + _GEN.choice(WEATHERS, p=prob)\n\ndef make_dotmod_dataset(seq_len, size, n: int = 10) -&gt; Dataset:\n    weathers = generate(partial(dotmod_model, n=n), seq_len * size * 2, generate(markov, n * 2))\n    w_list, e_list, nw_list = [], [], []\n    for _ in range(size):\n        start = _GEN.integers(0, seq_len * size * 2 - 11)\n        w = weathers[start * 2 : start * 2 + (seq_len + 1) * 2]\n        e = jnp.array(get_embedding(w[:-2]))\n        w_list.append(w)\n        e_list.append(e)\n        nw_list.append(WEATHERS.index(w[-2:]))\n    return Dataset(w_list, jnp.stack(e_list), jnp.array(nw_list))\n\ngenerate(dotmod_model, 100, generate(markov, 10))\n\n\n'â˜ï¸â˜ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜ï¸â˜€ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸â˜ï¸ğŸŒ§ï¸â˜ï¸â˜ï¸â˜ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜ï¸â˜€ï¸â˜ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸â˜ï¸ğŸŒ§ï¸â˜ï¸â˜ï¸ğŸŒ§ï¸â˜€ï¸â˜ï¸â˜€ï¸â˜ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜ï¸ğŸŒ§ï¸â˜€ï¸â˜ï¸â˜ï¸â˜ï¸â˜ï¸â˜€ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸â˜ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜ï¸â˜ï¸â˜€ï¸â˜ï¸â˜ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸â˜ï¸ğŸŒ§ï¸â˜ï¸â˜ï¸â˜ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜ï¸â˜€ï¸â˜ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸â˜ï¸ğŸŒ§ï¸â˜ï¸â˜ï¸â˜ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜ï¸â˜€ï¸â˜ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸â˜ï¸â˜ï¸â˜ï¸ğŸŒ§ï¸â˜€ï¸â˜ï¸â˜ï¸â˜ï¸â˜ï¸â˜€ï¸ğŸŒ§ï¸â˜€ï¸'\n\n\nWe were able to generate a weather sequence that at a quick glance does not seem to be legal. Letâ€™s try to train it. Letâ€™s start with a linear model.\n\n\nCode\nds = make_dotmod_dataset(SEQ_LEN, 5000)\ntest_ds = make_dotmod_dataset(SEQ_LEN, 1000)\n\nmodel = LinearModel(4 * SEQ_LEN, 3, key)\nmodel, key, loss_list, eval_list = train(\n    500, 100, model, ds, test_ds, key, 1e-2, linear_neglogp\n)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on Dotmod model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nf\"Accuracy: {linear_accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.7440000176429749'\n\n\n\n\n\n\n\n\n\nSuprsingly, the accuracy is quite high. Then letâ€™s train the self-attention layer.\n\n\nCode\nmodel = MaskedAttention(4, D_ATTN, 3, key)\nmodel, key, loss_list, eval_list = train(500, 100, model, ds, test_ds, key, 1e-2)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on Dotmod model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\nvisualize_attn(ax1, model, test_ds, 1)\nvisualize_attn(ax2, model, test_ds, 2)\n\nf\"Accuracy: {accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.4350000321865082'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgain, the self-attention was not better. So maybe we can at least say that the self-attention itself is not very good at approximating nonlinear function like modulo."
  },
  {
    "objectID": "posts/try_equinox_for_rl_en.html",
    "href": "posts/try_equinox_for_rl_en.html",
    "title": "Try reinforcement learning with equinox",
    "section": "",
    "text": "I recently tried equinox, a jax-based library for defining and managing neural nets, and I liked it. So in this blog post I will introduce equinox and demonstrate its use with a reinforcement learning example. Other jax-based NN libraries include haiku from Deepmind and flax from Google Research. Both are actually quite similar because their designs are based on stax, which is a reference implementation by the Jax developers. In other words, I would say that both haiku and flax are libraries that have added a PyTorch-like Module to stax. An equinox document briefly summarizes the approach of haiku and flax, calling it the â€˜init-apply approachâ€™. Letâ€™s start this blog post by paraphrasing that document."
  },
  {
    "objectID": "posts/try_equinox_for_rl_en.html#nn-parameter-management-by-init-apply-approach",
    "href": "posts/try_equinox_for_rl_en.html#nn-parameter-management-by-init-apply-approach",
    "title": "Try reinforcement learning with equinox",
    "section": "NN parameter management by init-apply approach",
    "text": "NN parameter management by init-apply approach\nIn stax, haiku, and flax, this problem of parameter management is solved by the so-called â€œinit-apply approachâ€, which can be summarized as:\n\nthe model has no parameters\ninstead, the model is just a set of two functions: init and apply\n\n\ninit takes an example input, initializes the parameters, and returns the initial parameter\napply takes input and parameters, and then compute the output of the model\n\nSo the APIs for flax and haiku are as follows. While there are a number of differences (e.g., flax uses __call__ (while haiku uses forward like PyTorch (just like how Chainer and PyTorch differ), flax employs dataclasses.dataclass for Module class), their design is quite similar.\nclass Linear(Module):\n    def __call__(self, x):\n        batch_size = x.shape[0]\n        w = self.parameter(output, shape=(batch_size, self.output_size) init=self.w_init)\n        b = self.parameter(output, shape=(1, self.output_size) init=self.w_init)\n        return w @ x.T + b\n\nmodel = Linear(output_size=10)\nparams = model.init(jnp.zeros(1, 100))\nresult = model.apply(params, jnp.zeros(10, 100))\nIn the above example, I used Module to represent the abstract class for defining NN. Note that he method self.parameter in the above example is the counterpart of the parameter registration functions in flax and haiku. In the case of haiku, the params is the following dict.\n{\n    \"Linear/~/w\": [...],\n    \"Linear/~/b\": [...],\n}\nSo, it means that users need to know this naming scheme to access a specific value of parameter. stax doesnâ€™t have Module feature. Instead, it provides function combinator to combine multiple init and apply. Then letâ€™s summarize the pros and cons of this approach.\nPros\n\nNo need to specify the shape of the input Array when initializing the Module\n\nParameters are initialized when init is called\n\nFunctions and data are separated\n\nModule has no variables that can be changed, and is treated completely separately from its parameters\n\n\nDisadvantages.\n\n(Only haiku/flax) Module is redundant\n\nModule only has â€œmodel settingsâ€ such as output dimensions\n\nDirect access to parameter elements is cumbersome\n\nFor example, in haiku, each parameter element can be accessed with params[\"Linear/~/w\"], so we need to understand this naming scheme for keys to check parameters\n\nNot very object oriented\n\nBecause classes donâ€™t have data\n\n(Only haiku/flax) Parameter calls in Module need to be converted to functions that can be used by jax.grad.\n\nFor example, haiku.transform converts functions that include parameter calls with haiku.get_parameter into functions that take parameters as arguments"
  },
  {
    "objectID": "posts/try_equinox_for_rl_en.html#try-reinforcement-learning",
    "href": "posts/try_equinox_for_rl_en.html#try-reinforcement-learning",
    "title": "Try reinforcement learning with equinox",
    "section": "Try reinforcement learning",
    "text": "Try reinforcement learning"
  },
  {
    "objectID": "posts/try_equinox_for_rl_en.html#environment",
    "href": "posts/try_equinox_for_rl_en.html#environment",
    "title": "Try reinforcement learning with equinox",
    "section": "Environment",
    "text": "Environment\nNow that we have introduced the features of equinox, letâ€™s try reinforcement learning with it. Since we are using jax and since the API of Gym has changed a lot and changed to gymnasium we canâ€™t keep up with it at all, letâ€™s use an environment made by jax. Here I use Maze from the jumanji.\n\n\nCode\nimport jumanji\nfrom jumanji.wrappers import AutoResetWrapper\nfrom IPython.display import HTML\n\nenv = jumanji.make(\"Maze-v0\")\nenv = AutoResetWrapper(env)\nn_actions = env.action_spec().num_values\nkey, *keys = jax.random.split(jax.random.PRNGKey(20230720), 11)\nstate, timestep = env.reset(key)\nstates = [state]\nfor key in keys:\n    action = jax.random.randint(key=key, minval=0, maxval=n_actions, shape=())\n    state, timestep = env.step(state, action)\n    states.append(state)\nanim = env.animate(states)\nHTML(anim.to_html5_video().replace('=\"1000\"', '=\"640\"'))  # Change video size\n\n\n\n\n\n\n\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nIt looks easy to use. However, the default size of the video was too big, so I replaced the HTML tags to make it smaller. It seems to be usable in combination with vmap and jit for actual learning."
  },
  {
    "objectID": "posts/try_equinox_for_rl_en.html#lets-implement-ppo.",
    "href": "posts/try_equinox_for_rl_en.html#lets-implement-ppo.",
    "title": "Try reinforcement learning with equinox",
    "section": "Letâ€™s implement PPO.",
    "text": "Letâ€™s implement PPO.\nSo, letâ€™s try to learn this environment. Here we will implement PPO, because itâ€™s fast.\n\nInput\nInput as an array with 3x10x10 shape, each representing a 10x10 binary image of the position of the wall, the position of the agent, and the position of the goal, respectively.\n\n\nCode\nfrom jumanji.environments.routing.maze.types import Observation, State\n\ndef obs_to_image(obs: Observation) -&gt; jax.Array:\n    walls = obs.walls.astype(jnp.float32)\n    agent = jnp.zeros_like(walls).at[obs.agent_position].set(1.0)\n    target = jnp.zeros_like(walls).at[obs.target_position].set(1.0)\n    return jnp.stack([walls, agent, target])\n\n\n\n\nNetwork\nI use a simple network with a 2D convolution layer, followed by ReLU activations and two linear layers. Policy and value networks share the first two layers, and the policy is modeled by a categorical distribution. To make things simpler, I will write the input and output sizes as \\(3 \\times 10 \\times 10\\) and \\(4\\) for the number of actions.\n\n\nCode\nfrom typing import NamedTuple\n\nfrom jax.nn.initializers import orthogonal\n\n\nclass PPONetOutput(NamedTuple):\n    policy_logits: jax.Array\n    value: jax.Array\n\n\nclass SoftmaxPPONet(eqx.Module):\n    torso: list\n    value_head: eqx.nn.Linear\n    policy_head: eqx.nn.Linear\n\n    def __init__(self, key: jax.Array) -&gt; None:\n        key1, key2, key3, key4, key5 = jax.random.split(key, 5)\n        # Common layers\n        self.torso = [\n            eqx.nn.Conv2d(3, 1, kernel_size=3, key=key1),\n            jax.nn.relu,\n            jnp.ravel,\n            eqx.nn.Linear(64, 64, key=key2),\n            jax.nn.relu,\n        ]\n        self.value_head = eqx.nn.Linear(64, 1, key=key3)\n        policy_head = eqx.nn.Linear(64, 4, key=key4)\n        # Use small value for policy initialization\n        self.policy_head = eqx.tree_at(\n            lambda linear: linear.weight,\n            policy_head,\n            orthogonal(scale=0.01)(key5, policy_head.weight.shape),\n        )\n\n    def __call__(self, x: jax.Array) -&gt; PPONetOutput:\n        for layer in self.torso:\n            x = layer(x)\n        value = self.value_head(x)\n        policy_logits = self.policy_head(x)\n        return PPONetOutput(policy_logits=policy_logits, value=value)\n\n    def value(self, x: jax.Array) -&gt; jax.Array:\n        for layer in self.torso:\n            x = layer(x)\n        return self.value_head(x)\n\n\n\n\nRollout\nIn PPO implementations, it is common to collect a history of environmental interaction about 500~8000 steps and use it to update the network several times. Here we use jax.lax.scan to implement a rollout that is faster than native Python for loop. While scan is super fast, its usage requires a bit of care. In particular, if you set the second output of the function that moves forward one step to results: list[Result], keep in mind that the final return will be Result(member1=stack([m1 for m1 in results.member1]), ...) Also, since the argument of exec_rollout contains a SoftmaxPPONet which is an instance of eqx.Module, you canâ€™t use jax.jit directly. Instead, you need to use eqx.filter_jit that decomposes the module to parameters and non-parameters automatically. Actions are sampled by a categorical distribution obtained by applying softmax to the output of the policy network, but since Observation contains an action_mask that tells you the direction in which you can move in the maze, you can mask the actions you cannot take with this by applying -inf to policy logits. In a simple environment, you donâ€™t need to do this, but mazes with many walls are difficult to solve without this masking.\n\n\nCode\nimport chex\n\n\n@chex.dataclass\nclass Rollout:\n    \"\"\"Rollout buffer that stores the entire history of one rollout\"\"\"\n\n    observations: jax.Array\n    actions: jax.Array\n    action_masks: jax.Array\n    rewards: jax.Array\n    terminations: jax.Array\n    values: jax.Array\n    policy_logits: jax.Array\n\n\ndef mask_logits(policy_logits: jax.Array, action_mask: jax.Array) -&gt; jax.Array:\n    return jax.lax.select(\n        action_mask,\n        policy_logits,\n        jnp.ones_like(policy_logits) * -jnp.inf,\n    )\n\n\nvmapped_obs2i = jax.vmap(obs_to_image)\n\n\n@eqx.filter_jit\ndef exec_rollout(\n    initial_state: State,\n    initial_obs: Observation,\n    env: jumanji.Environment,\n    network: SoftmaxPPONet,\n    prng_key: jax.Array,\n    n_rollout_steps: int,\n) -&gt; tuple[State, Rollout, Observation, jax.Array]:\n    def step_rollout(\n        carried: tuple[State, Observation],\n        key: jax.Array,\n    ) -&gt; tuple[tuple[State, jax.Array], Rollout]:\n        state_t, obs_t = carried\n        obs_image = vmapped_obs2i(obs_t)\n        net_out = jax.vmap(network)(obs_image)\n        masked_logits = mask_logits(net_out.policy_logits, obs_t.action_mask)\n        actions = jax.random.categorical(key, masked_logits, axis=-1)\n        state_t1, timestep = jax.vmap(env.step)(state_t, actions)\n        rollout = Rollout(\n            observations=obs_image,\n            actions=actions,\n            action_masks=obs_t.action_mask,\n            rewards=timestep.reward,\n            terminations=1.0 - timestep.discount,\n            values=net_out.value,\n            policy_logits=masked_logits,\n        )\n        return (state_t1, timestep.observation), rollout\n\n    (state, obs), rollout = jax.lax.scan(\n        step_rollout,\n        (initial_state, initial_obs),\n        jax.random.split(prng_key, n_rollout_steps),\n    )\n    next_value = jax.vmap(network.value)(vmapped_obs2i(obs))\n    return state, rollout, obs, next_value\n\n\nLetâ€™s test it. The advantage of the jax environment is that you can easily vector-parallelize the environment with jax.vmap, so this time we will run it in 16 parallel. If you apply vmap to reset and give it 16 PRNGKeys, you will get 16 parallel State.\n\n\nCode\nkey, net_key, reset_key, rollout_key = jax.random.split(key, 4)\npponet = SoftmaxPPONet(net_key)\ninitial_state, initial_timestep = jax.vmap(env.reset)(jax.random.split(reset_key, 16))\nnext_state, rollout, next_obs, next_value = exec_rollout(\n    initial_state,\n    initial_timestep.observation,\n    env,\n    pponet,\n    rollout_key,\n    512,\n)\nrollout.rewards.shape\n\n\n(512, 16)\n\n\nWe can confirm that the inputs are 16 in parallel, and each member of Rollout has n. steps x n. env x .... shape.\n\n\nLearning\nNow that the data has been collected, it is time to write the code to update the network. First, compute GAE. Since it is unexpectedly a bottleneck, letâ€™s speed it up with fori_loop.\n\n\nCode\n@chex.dataclass(frozen=True, mappable_dataclass=False)\nclass Batch:\n    \"\"\"Batch for PPO, indexable to get a minibatch.\"\"\"\n\n    observations: jax.Array\n    action_masks: jax.Array\n    onehot_actions: jax.Array\n    rewards: jax.Array\n    advantages: jax.Array\n    value_targets: jax.Array\n    log_action_probs: jax.Array\n\n    def __getitem__(self, idx: jax.Array):\n        return self.__class__(  # type: ignore\n            observations=self.observations[idx],\n            action_masks=self.action_masks[idx],\n            onehot_actions=self.onehot_actions[idx],\n            rewards=self.rewards[idx],\n            advantages=self.advantages[idx],\n            value_targets=self.value_targets[idx],\n            log_action_probs=self.log_action_probs[idx],\n        )\n\n\ndef compute_gae(\n    r_t: jax.Array,\n    discount_t: jax.Array,\n    values: jax.Array,\n    lambda_: float = 0.95,\n) -&gt; jax.Array:\n    \"\"\"Efficiently compute generalized advantage estimator (GAE)\"\"\"\n\n    gamma_lambda_t = discount_t * lambda_\n    delta_t = r_t + discount_t * values[1:] - values[:-1]\n    n = delta_t.shape[0]\n\n    def update(i: int, advantage_t: jax.Array) -&gt; jax.Array:\n        t = n - i - 1\n        adv_t = delta_t[t] + gamma_lambda_t[t] * advantage_t[t + 1]\n        return advantage_t.at[t].set(adv_t)\n\n    advantage_t = jax.lax.fori_loop(0, n, update, jnp.zeros_like(values))\n    return advantage_t[:-1]\n\n\n@eqx.filter_jit\ndef make_batch(\n    rollout: Rollout,\n    next_value: jax.Array,\n    gamma: float,\n    gae_lambda: float,\n) -&gt; Batch:\n    all_values = jnp.concatenate(\n        [jnp.squeeze(rollout.values), next_value.reshape(1, -1)]\n    )\n    advantages = compute_gae(\n        rollout.rewards,\n        # Set Î³ = 0 when the episode terminates\n        (1.0 - rollout.terminations) * gamma,\n        all_values,\n        gae_lambda,\n    )\n    value_targets = advantages + all_values[:-1]\n    onehot_actions = jax.nn.one_hot(rollout.actions, 4)\n    _, _, *obs_shape = rollout.observations.shape\n    log_action_probs = jnp.sum(\n        jax.nn.log_softmax(rollout.policy_logits) * onehot_actions,\n        axis=-1,\n    )\n    return Batch(\n        observations=rollout.observations.reshape(-1, *obs_shape),\n        action_masks=rollout.action_masks.reshape(-1, 4),\n        onehot_actions=onehot_actions.reshape(-1, 4),\n        rewards=rollout.rewards.ravel(),\n        advantages=advantages.ravel(),\n        value_targets=value_targets.ravel(),\n        log_action_probs=log_action_probs.ravel(),\n    )\n\n\n\n\nCode\nbatch = make_batch(rollout, next_value, 0.99, 0.95)\nbatch.advantages.shape, batch.onehot_actions.shape, batch.log_action_probs.shape\n\n\n((8192,), (8192, 4), (8192,))\n\n\nLooks OK. Now we can sample a mini-batch from the Batch we created with this and update it with gradient descent to minimize the loss function. Here I use optax, which is a standard in the jax community. At this point, note the following three points.\n\nUse eqx.filter_grad instead of jax.grad as explained in the previous sections\nSoftmaxPPONet has types that cannot be used as jax.jit arguments, such as jax.nn.relu, so use eqx.partition to break them up before using them as jax.lax.scan arguments\nSimilarly, SoftmaxPPONet cannot be used as an argument to optax initialization/update functions as is, so it should be broken down with eqx.partition or eqx.filter to exclude members other than jax.Array.\n\nAlso, if you want to use jax.lax.scan in a mini-batch update loop, there are several ways to do it. What I do here is that, assuming mini-batch size \\(N\\), number of updates \\(M\\), number of update epochs \\(K\\) times, overall batch size \\(N \\times M\\),\n\nMake permutations of \\(0, 1, 2, ... , NM - 1\\) \\(K\\) times.\nMake \\(K\\) copies of Batch shuffled by the permutations we made\nConcatenate each member with jnp.concatenate and reshape to an array with size \\(MK \\times N \\times ...\\).\n\nItâ€™s a bit difficult, and you may not need to speed it up this fast. If you are worried about memory usage, you could write a \\(K\\) loop in Python, but in this case, the input is \\(3\\times 10\\times 10\\), so it seems to be ok.\n\n\nCode\nimport optax\n\n\ndef loss_function(\n    network: SoftmaxPPONet,\n    batch: Batch,\n    ppo_clip_eps: float,\n) -&gt; jax.Array:\n    net_out = jax.vmap(network)(batch.observations)\n    # Policy loss\n    log_pi = jax.nn.log_softmax(\n        jax.lax.select(\n            batch.action_masks,\n            net_out.policy_logits,\n            jnp.ones_like(net_out.policy_logits * -jnp.inf),\n        )\n    )\n    log_action_probs = jnp.sum(log_pi * batch.onehot_actions, axis=-1)\n    policy_ratio = jnp.exp(log_action_probs - batch.log_action_probs)\n    clipped_ratio = jnp.clip(policy_ratio, 1.0 - ppo_clip_eps, 1.0 + ppo_clip_eps)\n    clipped_objective = jnp.fmin(\n        policy_ratio * batch.advantages,\n        clipped_ratio * batch.advantages,\n    )\n    policy_loss = -jnp.mean(clipped_objective)\n    # Value loss\n    value_loss = jnp.mean(0.5 * (net_out.value - batch.value_targets) ** 2)\n    # Entropy regularization\n    entropy = jnp.mean(-jnp.exp(log_pi) * log_pi)\n    return policy_loss + value_loss - 0.01 * entropy\n\n\nvmapped_permutation = jax.vmap(jax.random.permutation, in_axes=(0, None), out_axes=0)\n\n\n@eqx.filter_jit\ndef update_network(\n    batch: Batch,\n    network: SoftmaxPPONet,\n    optax_update: optax.TransformUpdateFn,\n    opt_state: optax.OptState,\n    prng_key: jax.Array,\n    minibatch_size: int,\n    n_epochs: int,\n    ppo_clip_eps: float,\n) -&gt; tuple[optax.OptState, SoftmaxPPONet]:\n    # Prepare update function\n    dynamic_net, static_net = eqx.partition(network, eqx.is_array)\n\n    def update_once(\n        carried: tuple[optax.OptState, SoftmaxPPONet],\n        batch: Batch,\n    ) -&gt; tuple[tuple[optax.OptState, SoftmaxPPONet], None]:\n        opt_state, dynamic_net = carried\n        network = eqx.combine(dynamic_net, static_net)\n        grad = eqx.filter_grad(loss_function)(network, batch, ppo_clip_eps)\n        updates, new_opt_state = optax_update(grad, opt_state)\n        dynamic_net = optax.apply_updates(dynamic_net, updates)\n        return (new_opt_state, dynamic_net), None\n\n    # Prepare minibatches\n    batch_size = batch.observations.shape[0]\n    permutations = vmapped_permutation(jax.random.split(prng_key, n_epochs), batch_size)\n    minibatches = jax.tree_map(\n        # Here, x's shape is [batch_size, ...]\n        lambda x: x[permutations].reshape(-1, minibatch_size, *x.shape[1:]),\n        batch,\n    )\n    # Update network n_epochs x n_minibatches times\n    (opt_state, updated_dynet), _ = jax.lax.scan(\n        update_once,\n        (opt_state, dynamic_net),\n        minibatches,\n    )\n    return opt_state, eqx.combine(updated_dynet, static_net)\n\n\nSo now that we have all the components, letâ€™s start learning. First, letâ€™s try a simple maze with no walls. I copied and pasted junmanjiâ€™s default maze generator and rewrote it. In this environment, the reward is given only at the goal, so the average return per episode is simply calculated by \\(\\frac{\\sum R}{\\mathrm{Num.~episodes}}\\), which is used as a progress indicator. I selected each hyperparameter from my experience.\n\n\nCode\nfrom jumanji.environments.routing.maze.generator import Generator\nfrom jumanji.environments.routing.maze.types import Position, State\n\n\nclass TestGenerator(Generator):\n    def __init__(self) -&gt; None:\n        super().__init__(num_rows=10, num_cols=10)\n\n    def __call__(self, key: chex.PRNGKey) -&gt; State:\n        walls = jnp.zeros((10, 10), dtype=bool)\n        agent_position = Position(row=0, col=0)\n        target_position = Position(row=9, col=9)\n\n        # Build the state.\n        return State(\n            agent_position=agent_position,\n            target_position=target_position,\n            walls=walls,\n            action_mask=None,\n            key=key,\n            step_count=jnp.array(0, jnp.int32),\n        )\n\n\n\n\nCode\ndef run_training(\n    key: jax.Array,\n    adam_lr: float = 3e-4,\n    adam_eps: float = 1e-7,\n    gamma: float = 0.99,\n    gae_lambda: float = 0.95,\n    n_optim_epochs: int = 10,\n    minibatch_size: int = 1024,\n    n_agents: int = 16,\n    n_rollout_steps: int = 512,\n    n_total_steps: int = 16 * 512 * 100,\n    ppo_clip_eps: float = 0.2,\n    **env_kwargs,\n) -&gt; SoftmaxPPONet:\n    key, net_key, reset_key = jax.random.split(key, 3)\n    pponet = SoftmaxPPONet(net_key)\n    env = AutoResetWrapper(jumanji.make(\"Maze-v0\", **env_kwargs))\n    adam_init, adam_update = optax.adam(adam_lr, eps=adam_eps)\n    opt_state = adam_init(eqx.filter(pponet, eqx.is_array))\n    env_state, timestep = jax.vmap(env.reset)(jax.random.split(reset_key, 16))\n    obs = timestep.observation\n\n    n_loop = n_total_steps // (n_agents * n_rollout_steps)\n    return_reporting_interval = 1 if n_loop &lt; 10 else n_loop // 10\n    n_episodes, reward_sum = 0.0, 0.0\n    for i in range(n_loop):\n        key, rollout_key, update_key = jax.random.split(key, 3)\n        env_state, rollout, obs, next_value = exec_rollout(\n            env_state,\n            obs,\n            env,\n            pponet,\n            rollout_key,\n            n_rollout_steps,\n        )\n        batch = make_batch(rollout, next_value, gamma, gae_lambda)\n        opt_state, pponet = update_network(\n            batch,\n            pponet,\n            adam_update,\n            opt_state,\n            update_key,\n            minibatch_size,\n            n_optim_epochs,\n            ppo_clip_eps,\n        )\n        n_episodes += jnp.sum(rollout.terminations).item()\n        reward_sum += jnp.sum(rollout.rewards).item()\n        if i &gt; 0 and (i % return_reporting_interval == 0):\n            print(f\"Mean episodic return: {reward_sum / n_episodes}\")\n            n_episodes = 0.0\n            reward_sum = 0.0\n    return pponet\n\n\n\n\nCode\nimport datetime\n\nstarted = datetime.datetime.now()\nkey, training_key = jax.random.split(key)\ntrained_net = run_training(training_key, n_total_steps=16 * 512 * 10, generator=TestGenerator())\nelapsed = datetime.datetime.now() - started\nprint(f\"Elapsed time: {elapsed.total_seconds():.2}s\")\n\n\nMean episodic return: 0.40782122905027934\nMean episodic return: 0.967741935483871\nMean episodic return: 1.0\nMean episodic return: 1.0\nMean episodic return: 1.0\nMean episodic return: 1.0\nMean episodic return: 1.0\nMean episodic return: 1.0\nMean episodic return: 1.0\nElapsed time: 3.2s\n\n\nAbout 80,000 steps were learned in a little over 3 seconds. Thatâ€™s fast. Letâ€™s see what the trained agent looks like.\n\n\nCode\n@eqx.filter_jit\ndef visualization_rollout(\n    key: jax.random.PRNGKey,\n    pponet: SoftmaxPPONet,\n    env: jumanji.Environment,\n    n_steps: int,\n) -&gt; list[State]:\n    def step_rollout(\n        carried: tuple[State, Observation],\n        key: jax.Array,\n    ) -&gt; tuple[tuple[State, jax.Array], State]:\n        state_t, obs_t = carried\n        obs_image = obs_to_image(obs_t)\n        net_out = pponet(obs_image)\n        action, _ = sample_action(key, net_out.policy_logits, obs_t.action_mask)\n        state_t1, timestep = env.step(state_t, action)\n        return (state_t1, timestep.observation), state_t1\n\n    initial_state, timestep = env.reset(key)\n    _, states = jax.lax.scan(\n        step_rollout,\n        (initial_state, timestep.observation),\n        jax.random.split(key, n_steps),\n    )\n    leaves, treedef = jax.tree_util.tree_flatten(states)\n    return [initial_state] + [treedef.unflatten(leaf) for leaf in zip(*leaves)]\n\n\n\n\nCode\nenv = AutoResetWrapper(jumanji.make(\"Maze-v0\", generator=TestGenerator()))\nkey, eval_key = jax.random.split(key)\nstates = visualization_rollout(eval_key, trained_net, env, 40)\nanim = env.animate(states)\nHTML(anim.to_html5_video().replace('=\"1000\"', '=\"640\"'))\n\n\n\n\n\n\n\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nAs expected, this one looks easy enough. Next time, we would like to make it a little more difficult. The default maze, which is generated completely at random, is very slow, so letâ€™s make our own maze. The maze samples the start and the goal from 3 different locations with the same probability, and solves a total of 9 combinations. Letâ€™s try to train the maze with 800,000 steps, which is 10 times as many as the default maze.\n\n\nCode\nclass MedDifficultyGenerator(Generator):\n    WALLS = [\n        [0, 0, 1, 0, 0, 1, 0, 1, 1, 0],\n        [0, 0, 1, 0, 0, 1, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 1, 1, 0, 1, 1],\n        [1, 1, 1, 1, 0, 0, 0, 0, 1, 0],\n        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],\n        [0, 0, 1, 0, 1, 1, 0, 0, 0, 0],\n        [1, 0, 1, 0, 1, 1, 0, 1, 1, 1],\n        [0, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n        [1, 1, 1, 0, 0, 1, 0, 1, 1, 0],\n        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n    ]\n    def __init__(self) -&gt; None:\n        super().__init__(num_rows=10, num_cols=10)\n\n    def __call__(self, key: chex.PRNGKey) -&gt; State:\n        key, config_key = jax.random.split(key)\n        walls = jnp.array(self.WALLS).astype(bool)\n        agent_cfg, target_cfg = jax.random.randint(config_key, (2,), 0, 2)\n        agent_position = jax.lax.switch(\n            agent_cfg,\n            [\n                lambda: Position(row=0, col=0),\n                lambda: Position(row=9, col=0),\n                lambda: Position(row=0, col=9),\n            ]\n        )\n        target_position = jax.lax.switch(\n            target_cfg,\n            [\n                lambda: Position(row=3, col=9),\n                lambda: Position(row=7, col=8),\n                lambda: Position(row=7, col=0),\n            ]\n        )\n        # Build the state.\n        return State(\n            agent_position=agent_position,\n            target_position=target_position,\n            walls=walls,\n            action_mask=None,\n            key=key,\n            step_count=jnp.array(0, jnp.int32),\n        )\n\n\n\n\nCode\nimport datetime\n\nstarted = datetime.datetime.now()\nkey, training_key = jax.random.split(key)\ntrained_net = run_training(\n    training_key,\n    n_total_steps=16 * 512 * 100,\n    generator=MedDifficultyGenerator(),\n)\nelapsed = datetime.datetime.now() - started\nprint(f\"Elapsed time: {elapsed.total_seconds():.2}s\")\n\n\nMean episodic return: 0.2812202097235462\nMean episodic return: 0.4077407740774077\nMean episodic return: 0.5992010652463382\nMean episodic return: 0.7285136501516684\nMean episodic return: 0.75\nMean episodic return: 0.8620564808110065\nMean episodic return: 0.9868290258449304\nMean episodic return: 0.9977788746298124\nMean episodic return: 0.9970540974825924\nElapsed time: 1.2e+01s\n\n\n\n\nCode\nenv = AutoResetWrapper(jumanji.make(\"Maze-v0\", generator=MedDifficultyGenerator()))\nkey, eval_key = jax.random.split(key)\nstates = visualization_rollout(eval_key, trained_net, env, 100)\nanim = env.animate(states)\nHTML(anim.to_html5_video().replace('=\"1000\"', '=\"640\"'))\n\n\n\n\n\n\n\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nThere is some hesitation, but it looks like we are getting to the goal."
  },
  {
    "objectID": "posts/understanding-attention.html",
    "href": "posts/understanding-attention.html",
    "title": "AttentionãŒä½•ã‚’ã‚„ã£ã¦ã„ã‚‹ã®ã‹ç†è§£ã—ã‚ˆã†",
    "section": "",
    "text": "ChatGPTãŒå¤§ãƒã‚ºãƒªã—ã¦ã„ã‚‹æ˜¨ä»Šã§ã™ã€‚åƒ•ã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è€ƒãˆã‚‹ã®ãŒé¢å€’ãªã®ã§ï¼ˆãˆãˆâ€¦)ã‚ã¾ã‚Šä½¿ã‚ãªã„ã®ã§ã™ãŒã€å‹äººãŒè«–æ–‡ã‚’æ›¸ãã®ã«ä½¿ã£ã¦ã„ãŸã‚Šã€åƒ•ã®æ¯è¦ªãŒè©±ã—ç›¸æ‰‹ã«ä½¿ã£ã¦ã„ãŸã‚Šã™ã‚‹ã‚ˆã†ã§ã™ã€‚è¦ªä¸å­ãªæ¯å­ã§ã”ã‚ã‚“ãªã•ã„ã¨ã„ã†æ„Ÿã˜ã‚‚ã—ã¾ã™ã€‚ ã¨ã“ã‚ã§ã€ChatGPTã®ã‚ˆã†ãªè¨€èªç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã§ã—ã°ã—ã°åˆ©ç”¨ã•ã‚Œã¦ã„ã‚‹ã®ãŒã€Transformerã¨å‘¼ã°ã‚Œã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ§‹æˆã§ã™ã€‚Transformerã§ã¯ã€Multihead Attentionã¨å‘¼ã°ã‚Œã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’å…¥åŠ›ã®ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã«å¯¾ã—ç¹°ã‚Šè¿”ã—é©ç”¨ã—è¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«ã‚’æ§‹æˆã—ã¾ã™ã€‚ã“ã®ãƒ–ãƒ­ã‚°ã§ã¯ã€ãã®ç°¡ç•¥ç‰ˆã§ã‚ã‚‹1ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®Attention(Multiheadã§ã¯ãªã„)ã«ç€ç›®ã—ã€ã“ã‚ŒãŒä½•ã‚’ã—ã¦ã„ã‚‹ã®ã‹ã‚’å‹‰å¼·ã—ã€ã¤ã„ã§ã«ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦å‹•ã‹ã—ã¦ã¿ã¾ã™ã€‚"
  },
  {
    "objectID": "posts/understanding-attention.html#æœ€åˆã«åˆ—ãŒã‚ã£ãŸ",
    "href": "posts/understanding-attention.html#æœ€åˆã«åˆ—ãŒã‚ã£ãŸ",
    "title": "AttentionãŒä½•ã‚’ã‚„ã£ã¦ã„ã‚‹ã®ã‹ç†è§£ã—ã‚ˆã†",
    "section": "æœ€åˆã«åˆ—ãŒã‚ã£ãŸ",
    "text": "æœ€åˆã«åˆ—ãŒã‚ã£ãŸ\nãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã¨ã„ã†ã®ã¯æ–‡å­—é€šã‚Šãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰ãªã‚‹åˆ—ã®ã“ã¨ã§ã™ã€‚ãƒˆãƒ¼ã‚¯ãƒ³ã¯æœ‰é™é›†åˆã®è¦ç´ ã§ã™ã€‚å®Ÿç”¨ä¸Šã¯byte pair encodingã«ã‚ˆã‚Šå¾—ã‚‰ã‚ŒãŸéƒ¨åˆ†æ–‡å­—åˆ—ãªã©ãŒã“ã‚Œã«è©²å½“ã—ã¾ã™ãŒã€ã¨ã‚Šã‚ãˆãšæ°—ã«ã—ãªãã¦ã„ã„ã§ã™ã€‚ãƒˆãƒ¼ã‚¯ãƒ³ã®é›†åˆã‚’\\(V\\)ã¨ã—ã€\\([Nv] := {1, ..., Nv}\\)ã¨ç•ªå·ä»˜ã‘ã—ã¦ãŠãã¾ã™ã€‚ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‚’\\(x = x[1: l]\\)ã¨æ›¸ãã¾ã™ã€‚ã¾ãŸã€ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã®æœ€å¤§ã®é•·ã•ã‚’\\(L\\)ã¨ã—ã¾ã™ã€‚ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦é€£ç¶šå€¤ã‚„ç„¡é™é›†åˆã¯æ‰±ãˆãªã„ã¨æ€ã„ã¾ã™ãŒã€ç´ äººãªã®ã§ä½•ã‹æŠœã‘é“ãŒã‚ã‚‹ã‹ã©ã†ã‹ã¯çŸ¥ã‚Šã¾ã›ã‚“ã€‚"
  },
  {
    "objectID": "posts/understanding-attention.html#ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«ã«",
    "href": "posts/understanding-attention.html#ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«ã«",
    "title": "AttentionãŒä½•ã‚’ã‚„ã£ã¦ã„ã‚‹ã®ã‹ç†è§£ã—ã‚ˆã†",
    "section": "ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«ã«",
    "text": "ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«ã«\né©å½“ãª\\(d_e \\times Nv\\)æ¬¡å…ƒã®è¡Œåˆ—\\(W_e\\)ã‚’ä½¿ã£ã¦ã€\\(v\\)ç•ªç›®ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰åŸ‹ã‚è¾¼ã¿ï¼ˆToken embeddingï¼‰ã‚’ \\(e = W_e[:, v]\\)ã«ã‚ˆã‚Šå¾—ã¾ã™ã€‚ã“ã‚Œã¯\\(d_e\\)æ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ã«ãªã‚Šã¾ã™ã€‚ãªãŠã€numpyé¢¨ã«\\(i\\)ç•ªç›®ã®è¡Œãƒ™ã‚¯ãƒˆãƒ«ã‚’\\(W[i, :]\\)ã€\\(j\\)ç•ªç›®ã®åˆ—ãƒ™ã‚¯ãƒˆãƒ«ã‚’\\(W[:, j]\\)ã¨æ›¸ã„ã¦ã„ã¾ã™ã€‚ã“ã®è¡Œåˆ—\\(W_e\\)ã¯å‹¾é…é™ä¸‹ã«ã‚ˆã‚Šå­¦ç¿’ã•ã‚Œã‚‹ã‚ˆã†ã§ã™ã€‚"
  },
  {
    "objectID": "posts/understanding-attention.html#ã¤ã„ã§ã«ä½ç½®ã‚‚ãƒ™ã‚¯ãƒˆãƒ«ã«",
    "href": "posts/understanding-attention.html#ã¤ã„ã§ã«ä½ç½®ã‚‚ãƒ™ã‚¯ãƒˆãƒ«ã«",
    "title": "AttentionãŒä½•ã‚’ã‚„ã£ã¦ã„ã‚‹ã®ã‹ç†è§£ã—ã‚ˆã†",
    "section": "ã¤ã„ã§ã«ä½ç½®ã‚‚ãƒ™ã‚¯ãƒˆãƒ«ã«",
    "text": "ã¤ã„ã§ã«ä½ç½®ã‚‚ãƒ™ã‚¯ãƒˆãƒ«ã«\né©å½“ãª\\(d_p \\times L\\)æ¬¡å…ƒã®è¡Œåˆ—\\(W_p\\)ã‚’ä½¿ã£ã¦ã€ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ä¸­ã®\\(l\\)ç•ªç›®ã«ãƒˆãƒ¼ã‚¯ãƒ³ãŒã‚ã‚‹ã¨ã„ã†æƒ…å ±ã‹ã‚‰ã€ä½ç½®åŸ‹ã‚è¾¼ã¿ï¼ˆPositional embeddingï¼‰ã‚’ \\(p = W_p[:, l]\\)ã«ã‚ˆã‚Šå¾—ã¾ã™ã€‚ã“ã‚Œã‚‚\\(d_e\\)æ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ã«ãªã‚Šã¾ã™ã€‚æ­£ç›´ãªã‚“ã®æ„å‘³ãŒã‚ã‚‹ã®ã‹ã‚ˆãã‚ã‹ã‚‰ãªã„ã®ã§ã™ãŒã€ã“ã‚Œã‚’å…ˆç¨‹ã®ãƒˆãƒ¼ã‚¯ãƒ³åŸ‹ã‚è¾¼ã¿ã«è¶³ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³åˆ—\\(x\\)ã®\\(t\\)ç•ªç›®ã®ãƒˆãƒ¼ã‚¯ãƒ³\\(x[t]\\)ã«å¯¾ã™ã‚‹åŸ‹ã‚è¾¼ã¿ã‚’\\(e = W_e[:, x[t]] + W_p[:, t]\\)ã«ã‚ˆã£ã¦å¾—ã¾ã™ã€‚ã“ã‚Œè¶³ã—ã¦å¤§ä¸ˆå¤«ãªã®ã‹ãªï¼Ÿã£ã¦æ€ã†ã‚“ã§ã™ãŒã€‚ ä½ç½®åŸ‹ã‚è¾¼ã¿ã¯ã€å­¦ç¿’ã•ã‚Œã‚‹ã“ã¨ã‚‚ã‚ã‚‹ã‚ˆã†ã§ã™ãŒã€TransformerãŒæœ€åˆã«ææ¡ˆã•ã‚ŒãŸAttention Is All You Needã®è«–æ–‡ã§ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚ \\[\n\\begin{align*}\nW_p[2i - 1, t] &= \\sin (\\frac{t}{L^{2i / d_e}}) \\\\\nW_p[2i, t] &= \\cos (\\frac{t}{L^{2i / d_e}}) \\\\\n&~~~~~(0 &lt; 2i \\leq d_e)\n\\end{align*}\n\\] ã“ã‚Œã‚’\\(L=50, d_e = 5\\)ã¨ã—ã¦å¯è¦–åŒ–ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\nimport matplotlib\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nmatplotlib.font_manager.fontManager.addfont(\"NotoEmoji-Medium.ttf\")\n\nL = 50\nd_e = 5\nx = np.arange(L)\nfor i in range(1, 1 + d_e):\n    if i % 2 == 0:\n        w_p = np.sin(x / L ** (i / d_e))\n    else:\n        w_p = np.cos(x / L ** ((i - 1) / d_e))\n    _ = plt.plot(x, w_p, label=f\"i={i}\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nã¨ã„ã†ã‚ã‘ã§ã€ã“ã®åŸ‹ã‚ã“ã¿ã¯å„æˆåˆ†ã”ã¨ã«ç•°ãªã‚‹å‘¨æ³¢æ•°ã§ã®å˜èªã‚’åŸ‹ã‚è¾¼ã‚€ã‚ˆã†ã§ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€çŸ­ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ä¸­ã§ã®ä½ç½®ã‚‚åŒæ™‚ã«è€ƒæ…®ã§ãã‚‹ã®ã‹ãªã€‚"
  },
  {
    "objectID": "posts/understanding-attention.html#ãƒãƒ«ã‚³ãƒ•ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’",
    "href": "posts/understanding-attention.html#ãƒãƒ«ã‚³ãƒ•ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’",
    "title": "AttentionãŒä½•ã‚’ã‚„ã£ã¦ã„ã‚‹ã®ã‹ç†è§£ã—ã‚ˆã†",
    "section": "ãƒãƒ«ã‚³ãƒ•ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’",
    "text": "ãƒãƒ«ã‚³ãƒ•ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’\nã¾ãšç°¡å˜ãªãƒ¢ãƒ‡ãƒ«ã§å¤©æ°—ã‚’ç”Ÿæˆã—ã¦ã¿ã¾ã™ã€‚æ¬¡ã®æ—¥ã®å¤©æ°—ã¯ã€å‰ã®æ—¥ã®å¤©æ°—ã«ã‚‚ã¨ã¥ã„ã¦ç¢ºç‡çš„ã«æ±ºã¾ã‚‹ã“ã¨ã«ã—ã¾ã—ã‚‡ã†ã€‚ğŸŒ§ï¸ãƒ»â˜ï¸ãƒ»â˜€ï¸ãŒãƒãƒ«ãƒãƒã‚¤ãƒˆæ–‡å­—ã§ã‚ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ã€ä»¥ä¸‹ã®ã‚ˆã†ã«å®Ÿè£…ã—ã¾ã™ã€‚\n\n\nCode\nimport dataclasses\n\n_GEN = np.random.Generator(np.random.PCG64(20230508))\n_MARKOV = {\n    \"\": [0.3, 0.4, 0.3],\n    \"ğŸŒ§ï¸\": [0.6, 0.3, 0.1],\n    \"â˜ï¸\": [0.3, 0.4, 0.3],\n    \"â˜€ï¸\": [0.2, 0.3, 0.5],\n}\n\ndef markov(prev: str) -&gt; str:\n    prob = _MARKOV[prev[-2:]]\n    return prev + _GEN.choice(WEATHERS, p=prob)\n\n\ndef generate(f, n: int, init: str = \"\"):\n    value = init\n    for _ in range(n):\n        value = f(value)\n    return value\n\n\n@dataclasses.dataclass\nclass Dataset:\n    weathers: list[str]\n    embeddings: jax.Array\n    next_weather_indices: jax.Array\n    \n    def __len__(self) -&gt; int:\n        return len(self.weathers)\n\n\ndef make_dataset(f, seq_len, size) -&gt; Dataset:\n    w_list, e_list, nw_list = [], [], []\n    for _ in range(size):\n        weathers = generate(f, seq_len + 1)\n        e = jnp.array(get_embedding(weathers[:-2]))\n        w_list.append(weathers)\n        e_list.append(e)\n        nw_list.append(WEATHERS.index(weathers[-2:]))\n    return Dataset(w_list, jnp.stack(e_list), jnp.array(nw_list))\n\n\ngenerated = generate(markov, 10)\ngenerated, get_embedding(generated)\n\n\n('ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸â˜ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸â˜€ï¸',\n array([[1.  , 1.  , 1.  , 0.  , 1.  , 0.  , 1.  , 1.  , 0.  , 0.  ],\n        [0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  ],\n        [0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 1.  , 1.  ],\n        [0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 ]]))\n\n\nã“ã‚“ãªæ„Ÿã˜ã§ã™ã€‚ã„ã¾ã€æ¬¡ã®æ—¥ã®å¤©æ°—ã ã‘äºˆæ¸¬ã—ãŸã„ã®ã§ã€ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã¯é›†åˆ{ğŸŒ§ï¸ãƒ»â˜ï¸ãƒ»â˜€ï¸}ä¸Šã§ã®ç¢ºç‡åˆ†å¸ƒãŒé©åˆ‡ã§ã—ã‚‡ã†ã€‚Attentionã¯é•·ã•\\(T\\)ã®åŸ‹ã‚è¾¼ã¿åˆ—ã«å¯¾ã—ã¦é•·ã•\\(d_\\textrm{out} \\times T\\)ã®è¡Œåˆ—ã‚’ã‹ãˆã—ã¾ã™ã€‚ãªã®ã§ã€\\(d_\\textrm{out} = 3\\)ã¨ã—ã€Attentionã®å‡ºåŠ›\\(\\tilde{V}\\)ã«å¯¾ã—ã¦ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é–¢æ•°ã‚’é©ç”¨ã—ã€\\(P_t = \\textrm{softmax}(\\tilde{V}[:, t])\\)ã¨ã—ã¾ã™ã€‚ã“ã®ã¨ãã€\\(P_t\\)ã®å„è¦ç´ ãŒæ¬¡ã®æ—¥ğŸŒ§ï¸ãƒ»â˜ï¸ãƒ»â˜€ï¸ã«ãªã‚‹ç¢ºç‡ã‚’è¡¨ã™ã¨ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã¾ã™ã€‚ã“ã‚Œã‚’ã€å¯¾æ•°å°¤åº¦ã®å’Œ\\(\\sum_t \\log P_t(\\textrm{next weather})\\)ã‚’æœ€å¤§åŒ–ã™ã‚‹ã‚ˆã†ã«å­¦ç¿’ã—ã¾ã—ã‚‡ã†ã€‚å­¦ç¿’ã®ã‚³ãƒ¼ãƒ‰ã‚’å®šç¾©ã—ã¾ã™ã€‚\n\n\nCode\nfrom typing import Callable\n\nimport optax\n\n\ndef attn_neglogp(model: eqx.Module, seq: jax.Array, next_w: jax.Array) -&gt; jax.Array:\n    batch_size = seq.shape[0]\n    tilde_v = jax.vmap(model)(seq)  # B x OUT x SEQ_LEN\n    logp = jax.nn.log_softmax(tilde_v, axis=1)  # B x OUT x SEQ_LEN\n    logp_masked = logp * jax.nn.one_hot(next_w, num_classes=3).reshape(-1, 3, 1)\n    return -jnp.mean(jnp.sum(logp_masked.reshape(batch_size, -1), axis=-1))\n\ndef train(\n    n_total_epochs: int,\n    minibatch_size: int,\n    model: eqx.Module,\n    ds: Dataset,\n    test_ds: Dataset,\n    key: jax.Array,\n    learning_rate: float = 1e-2,\n    loss_fn: Callable[[eqx.Module, jax.Array, jax.Array], jax.Array] = attn_neglogp,\n) -&gt; tuple[eqx.Module, jax.Array, list[float], list[float]]:\n    n_data = len(ds)\n    optim = optax.adam(learning_rate)\n\n    @eqx.filter_jit\n    def train_1step(\n        model: eqx.Module,\n        seq: jax.Array,\n        next_w: jax.Array,\n        opt_state: optax.OptState,\n    ) -&gt; tuple[jax.Array, eqx.Module, optax.OptState]:\n        loss, grads = eqx.filter_value_and_grad(loss_fn)(model, seq, next_w)\n        updates, opt_state = optim.update(grads, opt_state)\n        model = eqx.apply_updates(model, updates)\n        return loss, model, opt_state\n\n    opt_state = optim.init(model)\n    n_optim_epochs = n_data // minibatch_size\n    loss_list, eval_list = [], []\n    for epoch in range(n_total_epochs // n_optim_epochs):\n        key, perm_key = jax.random.split(key)\n        indices = jax.random.permutation(perm_key, n_data, independent=True)\n        for _ in range(n_optim_epochs):\n            e = ds.embeddings[indices]\n            next_w = ds.next_weather_indices[indices]\n            loss, model, opt_state = train_1step(model, e, next_w, opt_state)\n            loss_list.append(loss.item())\n            test_loss = jax.jit(loss_fn)(\n                model,\n                test_ds.embeddings,\n                test_ds.next_weather_indices,\n            )\n            eval_list.append(test_loss.item())\n    return model, key, loss_list, eval_list\n\n\nã“ã‚Œã‚’å®Ÿéš›ã«èµ°ã‚‰ã›ã¦ã¿ã¾ã™ã€‚é©å½“ã«ã€Attentionã®æ¬¡å…ƒã‚’6ã€å¤©æ°—åˆ—ã®é•·ã•ã‚’10ã«ã—ã¾ã™ã€‚\n\n\nCode\nD_ATTN = 6\nSEQ_LEN = 10\nkey = jax.random.PRNGKey(1234)\nmodel = MaskedAttention(4, D_ATTN, 3, key)\nds = make_dataset(markov, SEQ_LEN, 1000)\ntest_ds = make_dataset(markov, SEQ_LEN, 1000)\nmodel, key, loss_list, eval_list = train(500, 100, model, ds, test_ds, key, 1e-2)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on Markov model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\n@jax.jit\ndef accuracy(model: eqx.Module, seq: jax.Array, next_w: jax.Array) -&gt; float:\n    tilde_v = jax.vmap(model)(seq)  # B x OUT x SEQ_LEN\n    inferred = jnp.argmax(tilde_v[:, :, 0], axis=1)\n    n_correct = jnp.sum(inferred == next_w)\n    return n_correct / seq.shape[0]\n\nf\"Accuracy: {accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.49800002574920654'\n\n\n\n\n\n\n\n\n\n100ã‚¨ãƒãƒƒã‚¯ã‚ãŸã‚Šã§ãƒ­ã‚¹ãŒè½ã¡ãªããªã£ã¦ã„ã‚‹ã®ã§åæŸã¯ã—ã¦ã„ãã†ã§ã™ã€‚å®Ÿéš›ã«ä½•ã‚’å­¦ç¿’ã—ãŸã®ã‹ç¢ºèªã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã¨ã‚Šã‚ãˆãšå¤©æ°—ã‚’ç”Ÿæˆã—ã¦ã¿ã¾ã™ã€‚ã“ã‚“ãªã‚‚ã®è¦‹ã¦ã‚‚ä½•ã‚‚ã‚ã‹ã‚‰ãªã„ã®ã§ã™ãŒã€ç”Ÿæˆã®æµã‚Œã‚’ç¢ºèªã—ã¦ãŠãã«ã¯ã„ã„ã‹ãªã¨ã€‚ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒãŒä½¿ã‚ã‚Œã‚‹ã“ã¨ãŒå¤šã„ã‚ˆã†ã§ã™ãŒã€é¢å€’ãªã®ã§ä»Šå›ã¯ã‚‚ã£ã¨ç°¡å˜ãªæ–¹æ³•ã‚’ä½¿ã„ã¾ã™ã€‚â˜ï¸ã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆã—ã¦ã€ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«åˆ†å¸ƒã‹ã‚‰æ¬¡ã®å¤©æ°—ã‚’ã‚µãƒ³ãƒ—ãƒ«ã—ã€ã©ã‚“ã©ã‚“è¶³ã—ã¦ã„ãã“ã¨ã«ã—ã¾ã™ã€‚\n\n\nCode\ndef generate_from_model(\n    model: eqx.Module,\n    key: jax.Array,\n    seq_len: int,\n    init: str = \"â˜ï¸\",\n) -&gt; tuple[str, jax.Array]:\n    @jax.jit\n    def step(\n        model: eqx.Module,\n        seq: jax.Array,\n        key: jax.Array,\n    ) -&gt; tuple[jax.Array, jax.Array]:\n        sample_key, key = jax.random.split(key)\n        tilde_v = model(seq)  # 3 x len(seq)\n        sampled = jax.random.categorical(key=sample_key, logits=tilde_v[:, 0])\n        return sampled, key\n\n    generated = init\n    for _ in range(seq_len):\n        next_w, key = step(model, get_embedding(generated), key)\n        generated += WEATHERS[next_w.item()]\n    return generated, key\n\n\ngenerated, key = generate_from_model(model, key, 20)\ngenerated\n\n\n'â˜ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸â˜ï¸ğŸŒ§ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜ï¸â˜€ï¸â˜ï¸'\n\n\nã“ã‚“ãªæ„Ÿã˜ã«ãªã‚Šã¾ã—ãŸã€‚å½“ãŸã‚Šå‰ã§ã™ãŒã“ã‚Œã‚’è¦‹ãŸã¨ã“ã‚ã§ä½•ã‚‚ã‚ã‹ã‚‰ãªã„ã§ã™ã­ã€‚æ¬¡ã«ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä¸­ã®é©å½“ãªãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—Attentionã®ä¸­èº«ã‚’å¯è¦–åŒ–ã—ã¦ã¿ã¾ã™ã€‚\n\n\nCode\n@jax.jit\ndef get_attn(model: eqx.Module, seq: jax.Array) -&gt; jax.Array:\n    q = model.w_q @ seq + model.b_q\n    k = model.w_k @ seq + model.b_k\n    score = causal_mask(q.T @ k) / model.sqrt_d_attn\n    return jax.nn.softmax(score, axis=-1)\n\n\ndef visualize_attn(ax, model: eqx.Module, ds: Dataset, index: int = 0) -&gt; None:\n    attn = np.array(get_attn(model, ds.embeddings[index]))\n    im = ax.imshow(attn)\n    ax.set_xticks(\n        np.arange(10),\n        labels=[ds.weathers[index][i * 2] for i in range(10)],\n        fontname=\"Noto Emoji\",\n    )\n    ax.set_yticks(\n        np.arange(10),\n        labels=[ds.weathers[index][i * 2] for i in range(10)],\n        fontname=\"Noto Emoji\",\n    )\n    for i in [np.argmin(attn), np.argmax(attn)]:\n        # Show min and max values\n        im.axes.text(i % 10, i // 10, f\"{attn.flatten()[i]:.1f}\", color=\"gray\")\n\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\nvisualize_attn(ax1, model, test_ds, 1)\nvisualize_attn(ax2, model, test_ds, 2)\n\n\n\n\n\n\n\n\n\nmatplotlibã§ã‚«ãƒ©ãƒ¼çµµæ–‡å­—ãŒä½¿ãˆãªã‹ã£ãŸã®ã§ãƒ¢ãƒã‚¯ãƒ­ã®çµµæ–‡å­—ã«ã—ã¾ã—ãŸã€‚ã¨ã„ã†ã‚ã‘ã§ã€ 1. ç›´å‰ã®å¤©æ°—â†’ç›´å‰ã®å¤©æ°— ã®AttentionãŒæœ€ã‚‚å¤§ãã„ 2. ä»–ã®æ—¥ã®å¤©æ°—â†’ç›´å‰ã®å¤©æ°— ã®Attentionã‚‚å¤§ãã„ 3. ä»–ã¯ã»ã¨ã‚“ã©é–¢ä¿‚ãªã„\nã¨ã„ã£ãŸã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ãƒãƒ«ã‚³ãƒ•ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ç”Ÿæˆã—ãŸå¤©æ°—åˆ—ã‚’å­¦ç¿’ã•ã›ãŸã®ã§ã€1ã¯å½“ãŸã‚Šå‰ã§ã™ã‚ˆã­ã€‚2ã®ä»–ã®æ—¥ã®å¤©æ°—â†’ç›´å‰ã®å¤©æ°—ã®é–¢ä¿‚ã‚‚å®Ÿéš›ã¯ã„ã‚‰ãªã„ã®ã§ã™ãŒã€æ³¨æ„ã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ã§ã™ã€‚"
  },
  {
    "objectID": "posts/understanding-attention.html#ç‹¬ç«‹ã«ç™ºç”Ÿã—ãŸéå»ã®è¤‡æ•°ã®äº‹è±¡ã«ä¾å­˜ã—ã¦å°†æ¥ã®å‡ºæ¥äº‹ãŒæ±ºã¾ã‚‹å ´åˆ",
    "href": "posts/understanding-attention.html#ç‹¬ç«‹ã«ç™ºç”Ÿã—ãŸéå»ã®è¤‡æ•°ã®äº‹è±¡ã«ä¾å­˜ã—ã¦å°†æ¥ã®å‡ºæ¥äº‹ãŒæ±ºã¾ã‚‹å ´åˆ",
    "title": "AttentionãŒä½•ã‚’ã‚„ã£ã¦ã„ã‚‹ã®ã‹ç†è§£ã—ã‚ˆã†",
    "section": "ç‹¬ç«‹ã«ç™ºç”Ÿã—ãŸéå»ã®è¤‡æ•°ã®äº‹è±¡ã«ä¾å­˜ã—ã¦å°†æ¥ã®å‡ºæ¥äº‹ãŒæ±ºã¾ã‚‹å ´åˆ",
    "text": "ç‹¬ç«‹ã«ç™ºç”Ÿã—ãŸéå»ã®è¤‡æ•°ã®äº‹è±¡ã«ä¾å­˜ã—ã¦å°†æ¥ã®å‡ºæ¥äº‹ãŒæ±ºã¾ã‚‹å ´åˆ\næ¬¡ã«ã€ã‚‚ã†å°‘ã—è¤‡é›‘ãªãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã•ã›ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ä»Šåº¦ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ãªæ–¹æ³•ã§11æ—¥ã¶ã‚“ã®å¤©æ°—ã‚’ç”Ÿæˆã—ã¾ã™ã€‚ 1. 1æ—¥ç›®ã€4æ—¥ç›®ã€8æ—¥ç›®ã®å¤©æ°—ã‚’ç‹¬ç«‹ã«ç”Ÿæˆã™ã‚‹ 2. 2,3æ—¥ç›®ã®å¤©æ°—ã‚’1æ—¥ç›®ã®å¤©æ°—ã‚’åˆæœŸçŠ¶æ…‹ã¨ã™ã‚‹ãƒãƒ«ã‚³ãƒ•é€£é–ã«ã‚ˆã‚Šç”Ÿæˆã™ã‚‹ã€‚5,6,7,9,10æ—¥ç›®ã®å¤©æ°—ã«ã¤ã„ã¦ã‚‚ã€4æ—¥ç›®ãƒ»8æ—¥ç›®ã®å¤©æ°—ã«ã‚‚ã¨ã¥ã„ã¦åŒæ§˜ã«ç”Ÿæˆã™ã‚‹ã€‚ 3. 11æ—¥ç›®ã®å¤©æ°—ã‚’1æ—¥ç›®ã€4æ—¥ç›®ã€8æ—¥ç›®ã®å¤©æ°—ã‹ã‚‰ç¢ºç‡çš„ã«ç”Ÿæˆã™ã‚‹ã€‚\nã“ã‚Œã‚’å­¦ç¿’ã§ãã‚‹ã‹è©¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\ndef _make_table() -&gt; dict[str, list[float]]:\n    candidates = []\n    for i in range(1, 9):\n        for j in range(1, 9):\n            for k in range(1, 9):\n                if i + j + k == 10:\n                    candidates.append((i, j, k))\n    table = {}\n    for i in WEATHERS:\n        for j in WEATHERS:\n            for k in WEATHERS:\n                table[i + j + k] = [p / 10 for p in _GEN.choice(candidates)]\n    return table\n\n_ONE_FOUR_8_TABLE = _make_table()\n\ndef one_four_8(prev: str) -&gt; str:\n    length = len(prev) // 2\n    if length == 10:\n        p = _ONE_FOUR_8_TABLE[prev[0: 2] + prev[6: 8] + prev[14: 16]]\n        return prev + _GEN.choice(WEATHERS, p=p)\n    elif length == 4 or length == 8:\n        return prev + _GEN.choice(WEATHERS, p=_MARKOV[\"\"])\n    else:\n        return markov(prev)\n    \ngenerate(one_four_8, 11)\n\n\n'â˜€ï¸â˜€ï¸â˜€ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜ï¸â˜ï¸ğŸŒ§ï¸â˜ï¸'\n\n\nã“ã‚“ãªæ„Ÿã˜ã§ã™ã­ã€‚ã§ã¯å­¦ç¿’ã•ã›ã¾ã—ã‚‡ã†ã€‚ã•ã£ãã‚ˆã‚Šã‚‚å°‘ã—ãƒ‡ãƒ¼ã‚¿ãŒè¤‡é›‘ãªã®ã§ã€ã‚µãƒ³ãƒ—ãƒ«ã®æ•°ã‚’å¢—ã‚„ã—ã¦ã¿ã¾ã™ã€‚\n\n\nCode\nmodel = MaskedAttention(4, D_ATTN, 3, key)\nds = make_dataset(one_four_8, SEQ_LEN, 5000)\ntest_ds = make_dataset(one_four_8, SEQ_LEN, 1000)\nmodel, key, loss_list, eval_list = train(500, 100, model, ds, test_ds, key, 1e-2)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on 1-4-8 model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\nvisualize_attn(ax1, model, test_ds, 1)\nvisualize_attn(ax2, model, test_ds, 2)\n\nf\"Accuracy: {accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.3890000283718109'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\næå¤±ã¯å°ã•ããªã£ã¦ã„ã¾ã™ãŒAccuracyãŒæ‚ªãã¾ãŸAttentionã®å‡ºæ–¹ã‚‚å¾®å¦™ã§ã™ã­ã€‚ä¸€å¿œ1ãƒ»48æ—¥ç›®ã«ã‚‚æ³¨æ„ãŒã„ã£ã¦ã„ã¾ã™ãŒã€å…ˆã»ã©ã®å®Ÿé¨“ã¨åŒã˜ãæœ€å¾Œã®æ—¥ã®æ³¨æ„ãŒå¤§ãã‚ã«å‡ºã¦ã„ã¾ã™ã­ã€‚"
  },
  {
    "objectID": "posts/understanding-attention.html#attentionã„ã‚‰ãªã„ã‚“ã˜ã‚ƒ",
    "href": "posts/understanding-attention.html#attentionã„ã‚‰ãªã„ã‚“ã˜ã‚ƒ",
    "title": "AttentionãŒä½•ã‚’ã‚„ã£ã¦ã„ã‚‹ã®ã‹ç†è§£ã—ã‚ˆã†",
    "section": "Attentionã„ã‚‰ãªã„ã‚“ã˜ã‚ƒâ€¦",
    "text": "Attentionã„ã‚‰ãªã„ã‚“ã˜ã‚ƒâ€¦\nå‹˜ã®ã„ã„èª­è€…ã®æ–¹ã¯ãŠæ°—ã¥ãã‹ã¨æ€ã„ã¾ã™ãŒã€ã“ã“ã¾ã§å­¦ç¿’ã•ã›ãŸ2ã¤ã®å¤©æ°—åˆ—ã‚’è¡¨ç¾ã™ã‚‹ã®ã«ã€self-attentionã®ã‚ˆã†ãªå°é›£ã—ã„ã‚‚ã®ã¯ã„ã‚‰ãªã„ã§ã™ã‚ˆã­ã€‚æœ€åˆã®ã‚‚ã®ã¯å‰æ—¥ï¼ˆ10æ—¥ç›®)ã®å¤©æ°—ã€æ¬¡ã®ã‚„ã¤ã¯1ãƒ»4ãƒ»8æ—¥ç›®ã‹ã‚‰11æ—¥ç›®ã®å¤©æ°—ãŒæ±ºå®šã•ã‚Œã‚‹ãŸã‚ã€å…¥åŠ›ã•ã‚ŒãŸå¤©æ°—åˆ—ã®å†…éƒ¨ç›¸é–¢ãŒã‚¿ã‚¹ã‚¯ã«ä¸€åˆ‡é–¢ä¿‚ãªã„ã‹ã‚‰ã§ã™ã€‚ã¨ã„ã†ã‚ã‘ã§ã€ç·šå½¢ãƒ¢ãƒ‡ãƒ«+ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹(ã„ã‚ã‚†ã‚‹multinomial logistic regressionã¨ã„ã†ã‚„ã¤)ã§å­¦ç¿’ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n\n\nCode\nclass LinearModel(eqx.Module):\n    w: jax.Array\n    b: jax.Array\n\n    def __init__(self, d_in: int, d_out: int, key: jax.Array) -&gt; None:\n        w_key, b_key = jax.random.split(key)\n        self.w = jax.random.normal(w_key, (d_out, d_in))\n        self.b = jax.random.normal(b_key, (d_out,))\n\n    def __call__(self, seq: jax.Array) -&gt; jax.Array:\n        return self.w @ seq.flatten() + self.b\n\n\ndef linear_neglogp(model: eqx.Module, seq: jax.Array, next_w: jax.Array) -&gt; jax.Array:\n    logp = jax.nn.log_softmax(jax.vmap(model)(seq), axis=1)  # B x OUT\n    logp_masked = logp * jax.nn.one_hot(next_w, num_classes=3)\n    return -jnp.mean(jnp.sum(logp_masked, axis=1))\n\n\nmodel = LinearModel(4 * SEQ_LEN, 3, key)\nmodel, key, loss_list, eval_list = train(\n    500, 100, model, ds, test_ds, key, 1e-2, linear_neglogp\n)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on 1-4-8 model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\n\n@jax.jit\ndef linear_accuracy(model: eqx.Module, seq: jax.Array, next_w: jax.Array) -&gt; float:\n    tilde_v = jax.vmap(model)(seq)  # B x OUT\n    inferred = jnp.argmax(tilde_v, axis=1)\n    n_correct = jnp.sum(inferred == next_w)\n    return n_correct / seq.shape[0]\n\n\nf\"Accuracy: {linear_accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.4230000078678131'\n\n\n\n\n\n\n\n\n\næ™®é€šã«ã“ã£ã¡ã®ã»ã†ãŒè‰¯ã•ãã†ã§ã™ã­â€¦ã€‚ã§ã¯ã€Attentionã¯ã©ã†ã„ã†æ™‚ã«å½¹ã«ç«‹ã¤ã®ã§ã—ã‚‡ã†ã‹ã€‚\n\n(MLPç­‰ã¨æ¯”è¼ƒ) ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã®é•·ã•\\(L\\)ã«ä¾å­˜ã•ã›ãŸããªã„ã¨ã\n\nAttentionã§ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ãŒ \\((d_\\textrm{in} + 1)(2d_\\textrm{attn} + d_\\textrm{out})\\)ã«ãªã‚‹ã®ã«å¯¾ã—ã€ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã§ã¯\\((d_\\textrm{in}L + 1)d_\\textrm{out}\\)ã«ãªã‚‹ã“ã¨ã«æ³¨æ„ã—ã¾ã—ã‚‡ã†ã€‚ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã§ã¯ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã®é•·ã•ã«æ¯”ä¾‹ã—ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ãŒå¢—ãˆã¦ã—ã¾ã„ã¾ã™ã€‚ãŸã ã—ã€Attentionã§ã¯\\(q^\\top k\\)ã‚’ä¿æŒã™ã‚‹ã®ã«\\(O(L^2)\\)ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¿…è¦ãªç‚¹ã«æ³¨æ„ãŒå¿…è¦ã§ã™ã€‚ã‚‚ã£ã¨ã‚‚ã€Self-attention Does Not Need \\(O(n^2)\\) Memoryã§ã¯åŠ¹ç‡çš„ãª\\(O(\\sqrt{L})\\)ã®å®Ÿè£…ãŒç¤ºã•ã‚Œã¦ãŠã‚Šã€ã¾ã‚ä½•ã¨ã‹ãªã‚‹ã¨ã„ãˆã°ãªã‚‹ã‚ˆã†ã§ã™ãŒã€ãã‚Œã§ã‚‚å˜ç´”ãªRNNã‚„CNNã‚ˆã‚Šé…ããªã‚Šã¾ã™ã€‚\n\n(RNNãƒ»CNNç­‰ã¨æ¯”è¼ƒ) ãƒˆãƒ¼ã‚¯ãƒ³ç³»åˆ—ã«é•·æœŸé–“ã®ä¾å­˜é–¢ä¿‚ãŒå­˜åœ¨ã™ã‚‹å ´åˆ\n\nCNNã‚„RNNã¨æ¯”ã¹ãŸã¨ãã€\\(q^\\top k\\)ã«ã‚ˆã‚Šä¸€å±¤ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§ä»»æ„ã®ãƒˆãƒ¼ã‚¯ãƒ³é–“ã®ä¾å­˜é–¢ä¿‚ãŒè¡¨ç¾ã§ãã‚‹ã®ã¯Attentionã®åˆ©ç‚¹ã¨è¨€ãˆã‚‹ã§ã—ã‚‡ã†ã€‚ãŸã ã—\\(q^\\top k[i, j]\\)ã¯2ã¤ã®åŸ‹ã‚è¾¼ã¿\\(e[i], e[j]\\)ã«å¯¾ã—ã¦ç·šå½¢ãªæ¼”ç®—ã®ã¿ã§å¾—ã‚‰ã‚Œã‚‹ãŸã‚ã€ã“ã®2ã¤ã®åŸ‹ã‚è¾¼ã¿ãŒä½•ã‹éç·šå½¢ãªé–¢æ•°ã‚’ä»‹ã—ã¦ä¾å­˜ã—ã¦ã„ã‚‹å ´åˆã€ãã®é–¢ä¿‚ã¯ä¸€å±¤ã®Attentionã§ã¯è¡¨ç¾ã§ãã¾ã›ã‚“ã€‚\nã¨ã„ã†ã‚ã‘ã§ã€ä¸€å±¤ã®ç·šå½¢ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¨æ¯”è¼ƒã™ã‚‹ã¨ã€ãƒ‘ãƒ©ãƒ¡ã‚¿æ•°ãŒ\\(L\\)ã«ä¾å­˜ã—ãªã„ã¨ã„ã†ãƒ¡ãƒªãƒƒãƒˆã¯ã‚ã‚‹ã‚‚ã®ã®ã€å®Ÿéš›Attentionã‚’ä½¿ã†ã¨ã‚‚ã£ã¨è‰²ã€…ãªé–¢æ•°ãŒå­¦ç¿’ã§ãã‚‹ã®ã‹ã¨ã„ã†ã¨ã‚ˆãã‚ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚‚ã†å°‘ã—è©¦ã—ã¦ã¿ã¾ã™ã€‚"
  },
  {
    "objectID": "posts/understanding-attention.html#éš ã‚Œå¤‰æ•°ãŒã‚ã‚‹å ´åˆ",
    "href": "posts/understanding-attention.html#éš ã‚Œå¤‰æ•°ãŒã‚ã‚‹å ´åˆ",
    "title": "AttentionãŒä½•ã‚’ã‚„ã£ã¦ã„ã‚‹ã®ã‹ç†è§£ã—ã‚ˆã†",
    "section": "éš ã‚Œå¤‰æ•°ãŒã‚ã‚‹å ´åˆ",
    "text": "éš ã‚Œå¤‰æ•°ãŒã‚ã‚‹å ´åˆ\nä»¥ä¸‹ã®æ–¹æ³•ã§å¤©æ°—åˆ—ã‚’ç”Ÿæˆã—ã¾ã™ã€‚éå»\\(n\\)æ—¥é–“ã®å¤©æ°—ã‚’è¦‹ã¦ã€ğŸŒ§ï¸ã®ç™»å ´å›æ•°ãŒ\\(k\\)å›ãªã‚‰ã€æ¬¡ã®æ—¥ã®å¤©æ°—ãŒğŸŒ§ï¸ã«ãªã‚‹ç¢ºç‡ã‚’\\(\\frac{n - k}{2n}\\)ã¨ã—ã¾ã™ã€‚â˜ï¸ã€â˜€ï¸ã«ã¤ã„ã¦ã‚‚åŒæ§˜ã«ç¢ºç‡ã‚’å‰²ã‚Šå½“ã¦ã¾ã™ã€‚ã“ã®æ–¹æ³•ã§å¤§é‡ã«å¤©æ°—åˆ—ã‚’ç”Ÿæˆã—ã¦é©å½“ãªéƒ¨åˆ†åˆ—ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œã‚Šã¾ã™ã€‚\n\n\nCode\nfrom functools import partial\n\ndef ndays_model(prev: str, n: int = 10) -&gt; str:\n    counts = np.zeros(3)\n    prev_n = prev[-2 * n: ]\n    for i in range(n):\n        prev_w_i = prev_n[i * 2: i * 2 + 2]\n        counts[WEATHERS.index(prev_w_i)] += 1\n    prob = (n - counts) / (n * 2)\n    return prev + _GEN.choice(WEATHERS, p=prob)\n\ngenerate(ndays_model, 100, generate(markov, 10))                \n\n\n'â˜€ï¸â˜€ï¸â˜€ï¸ğŸŒ§ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜€ï¸â˜ï¸â˜€ï¸â˜€ï¸ğŸŒ§ï¸â˜ï¸ğŸŒ§ï¸â˜ï¸â˜ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜ï¸â˜ï¸â˜€ï¸ğŸŒ§ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜ï¸â˜€ï¸â˜ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸â˜ï¸ğŸŒ§ï¸â˜ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜ï¸â˜€ï¸ğŸŒ§ï¸â˜ï¸ğŸŒ§ï¸â˜ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜ï¸â˜ï¸â˜ï¸â˜ï¸ğŸŒ§ï¸â˜ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸â˜€ï¸â˜ï¸ğŸŒ§ï¸â˜€ï¸â˜€ï¸ğŸŒ§ï¸â˜€ï¸â˜ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜ï¸â˜€ï¸â˜€ï¸ğŸŒ§ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸â˜ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸â˜€ï¸ğŸŒ§ï¸â˜ï¸â˜€ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸â˜€ï¸â˜ï¸ğŸŒ§ï¸â˜€ï¸â˜ï¸â˜ï¸'\n\n\nç”Ÿæˆã•ã‚ŒãŸå¤©æ°—åˆ—ã¯ã“ã‚“ãªæ„Ÿã˜ã§ã™ã€‚ã¾ãšç·šå½¢ãƒ¢ãƒ‡ãƒ«ã‚’10æ—¥ãƒ¢ãƒ‡ãƒ«ã§å­¦ç¿’ã•ã›ã¾ã™ã€‚ã“ã®å ´åˆéš ã‚Œå¤‰æ•°ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n\n\nCode\ndef make_ndays_dataset(seq_len, size, n: int = 10) -&gt; Dataset:\n    weathers = generate(partial(ndays_model, n=n), seq_len * size * 2, generate(markov, n * 2))\n    w_list, e_list, nw_list = [], [], []\n    for _ in range(size):\n        start = _GEN.integers(0, seq_len * size * 2 - 11)\n        w = weathers[start * 2 : start * 2 + (seq_len + 1) * 2]\n        e = jnp.array(get_embedding(w[:-2]))\n        w_list.append(w)\n        e_list.append(e)\n        nw_list.append(WEATHERS.index(w[-2:]))\n    return Dataset(w_list, jnp.stack(e_list), jnp.array(nw_list))\n\n\nds = make_ndays_dataset(SEQ_LEN, 5000, n=10)\ntest_ds = make_ndays_dataset(SEQ_LEN, 1000, n=10)\n\nmodel = LinearModel(4 * SEQ_LEN, 3, key)\nmodel, key, loss_list, eval_list = train(\n    500, 100, model, ds, test_ds, key, 1e-2, linear_neglogp\n)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on 10days model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nf\"Accuracy: {linear_accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.3760000169277191'\n\n\n\n\n\n\n\n\n\næ¬¡ã«Self-Attentionã‚’å­¦ç¿’ã•ã›ã¾ã™ã€‚\n\n\nCode\nmodel = MaskedAttention(4, D_ATTN, 3, key)\nmodel, key, loss_list, eval_list = train(500, 100, model, ds, test_ds, key, 1e-2)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on 10days model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\nvisualize_attn(ax1, model, test_ds, 1)\nvisualize_attn(ax2, model, test_ds, 2)\n\nf\"Accuracy: {accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.3320000171661377'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nã»ã¨ã‚“ã©å¤‰ã‚ã‚Šã¾ã›ã‚“ã­ã€‚æ¬¡ã«éš ã‚Œå¤‰æ•°ãŒã‚ã‚‹ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã€å…ˆç¨‹ã®10æ—¥ãƒ¢ãƒ‡ãƒ«ã‚’15æ—¥ãƒ¢ãƒ‡ãƒ«ã«ã—ã¦ã¿ã¾ã™ã€‚ã¾ãšã€ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã•ã›ã¾ã™ã€‚\n\n\nCode\nds = make_ndays_dataset(SEQ_LEN, 5000, n=15)\ntest_ds = make_ndays_dataset(SEQ_LEN, 1000, n=15)\n\nmodel = LinearModel(4 * SEQ_LEN, 3, key)\nmodel, key, loss_list, eval_list = train(\n    500, 100, model, ds, test_ds, key, 1e-2, linear_neglogp\n)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on 15days model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nf\"Accuracy: {linear_accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.36900001764297485'\n\n\n\n\n\n\n\n\n\næ¬¡ã«Self-Attentionã‚’å­¦ç¿’ã•ã›ã¾ã™ã€‚\n\n\nCode\nmodel = MaskedAttention(4, D_ATTN, 3, key)\nmodel, key, loss_list, eval_list = train(500, 100, model, ds, test_ds, key, 1e-2)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on 15days model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\nvisualize_attn(ax1, model, test_ds, 1)\nvisualize_attn(ax2, model, test_ds, 2)\n\nf\"Accuracy: {accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.3140000104904175'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nã“ã®å ´åˆã‚‚çµå±€Self-Attentionã®ã»ã†ãŒæ‚ªããªã£ã¦ã—ã¾ã„ã¾ã—ãŸã€‚æ‚²ã—ã„ã€‚"
  },
  {
    "objectID": "posts/understanding-attention.html#éç·šå½¢ãªã‚‰ã©ã†ã‹",
    "href": "posts/understanding-attention.html#éç·šå½¢ãªã‚‰ã©ã†ã‹",
    "title": "AttentionãŒä½•ã‚’ã‚„ã£ã¦ã„ã‚‹ã®ã‹ç†è§£ã—ã‚ˆã†",
    "section": "éç·šå½¢ãªã‚‰ã©ã†ã‹",
    "text": "éç·šå½¢ãªã‚‰ã©ã†ã‹\néš ã‚Œå¤‰æ•°ãŒã‚ã£ã¦ã‚‚ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã®æ–¹ãŒæ€§èƒ½ãŒã„ã„ã¨ã„ã†ã“ã¨ã¯ã€ãŸã¶ã‚“ç·šå½¢ã§è§£ã‘ã‚‹ã‚¿ã‚¹ã‚¯ã§ã¯ã©ã†ã‚ãŒã„ã¦ã‚‚ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã«å‹ã¦ãªã„ã¨ã„ã†ã“ã¨ãªã®ã§ã—ã‚‡ã†ã€‚ãªã®ã§ã‚‚ã£ã¨é›£ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’è€ƒãˆã¾ã™ã€‚ 10æ—¥ã¶ã‚“ã®å¤©æ°—åˆ—ã®ğŸŒ§ï¸ã€â˜ï¸ã€â˜€ï¸ã«ãã‚Œãã‚Œ0, 1, 2ã‚’å‰²ã‚Šå½“ã¦ã¦ä½œã£ãŸãƒ™ã‚¯ãƒˆãƒ«ã‚’\\(y\\)ã¨ã—ã¾ã™ã€‚ã¾ãŸã€\\(\\beta = (0, 1, 2, 3, 2, 1, 0, 1, 2, 3)^\\top\\)ã¨ã—ã¾ã™ã€‚ã“ã®ã¨ãã€æ¬¡ã®æ—¥ã®å¤©æ°—ã‚’\\((y(2 - y)\\cdot \\beta) \\mod 3\\)ã¨ã—ã¾ã™ã€‚ã“ã‚Œã ã¨èŠ¸ãŒãªã„ã®ã§ä¸€å¿œä»–ã®å¤©æ°—ã‚‚2%ãã‚‰ã„ã®ç¢ºç‡ã§å‡ºã‚‹ã‚ˆã†ã«ã—ã¦ãŠãã¾ã™ã€‚\n\n\nCode\n_BETA = np.tile([0, 1, 2, 3, 2, 1], (10,))\n\ndef dotmod_model(prev: str, n: int =10) -&gt; str:\n    y = np.zeros(n, dtype=int)\n    prev_n = prev[-2 * n:]\n    for i in range(n):\n        prev_w_i = prev_n[i * 2: i * 2 + 2]\n        y[i] = WEATHERS.index(prev_w_i) + 1\n    prob = [0.02, 0.02, 0.02]\n    prob[np.dot(y * (2 - y), _BETA[: n]) % 3] = 0.96\n    return prev + _GEN.choice(WEATHERS, p=prob)\n\ndef make_dotmod_dataset(seq_len, size, n: int = 10) -&gt; Dataset:\n    weathers = generate(partial(dotmod_model, n=n), seq_len * size * 2, generate(markov, n * 2))\n    w_list, e_list, nw_list = [], [], []\n    for _ in range(size):\n        start = _GEN.integers(0, seq_len * size * 2 - 11)\n        w = weathers[start * 2 : start * 2 + (seq_len + 1) * 2]\n        e = jnp.array(get_embedding(w[:-2]))\n        w_list.append(w)\n        e_list.append(e)\n        nw_list.append(WEATHERS.index(w[-2:]))\n    return Dataset(w_list, jnp.stack(e_list), jnp.array(nw_list))\n\ngenerate(dotmod_model, 100, generate(markov, 10))\n\n\n'ğŸŒ§ï¸ğŸŒ§ï¸â˜ï¸â˜ï¸â˜ï¸â˜€ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜ï¸â˜€ï¸â˜€ï¸â˜ï¸ğŸŒ§ï¸â˜€ï¸â˜ï¸â˜ï¸â˜ï¸â˜ï¸â˜€ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸â˜ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜ï¸â˜ï¸â˜€ï¸â˜ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸â˜ï¸ğŸŒ§ï¸â˜ï¸â˜ï¸â˜ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜ï¸â˜€ï¸â˜ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸â˜ï¸ğŸŒ§ï¸â˜ï¸â˜ï¸â˜ï¸â˜ï¸â˜€ï¸â˜ï¸â˜ï¸â˜€ï¸â˜ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸â˜ï¸ğŸŒ§ï¸â˜ï¸â˜ï¸â˜ï¸â˜€ï¸â˜€ï¸â˜ï¸â˜ï¸â˜€ï¸â˜ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸ğŸŒ§ï¸â˜ï¸ğŸŒ§ï¸â˜ï¸â˜ï¸ğŸŒ§ï¸â˜€ï¸â˜ï¸â˜€ï¸â˜ï¸ğŸŒ§ï¸ğŸŒ§ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜ï¸ğŸŒ§ï¸â˜€ï¸â˜ï¸â˜ï¸â˜ï¸â˜ï¸â˜€ï¸ğŸŒ§ï¸â˜€ï¸â˜€ï¸â˜ï¸ğŸŒ§ï¸â˜ï¸â˜ï¸â˜ï¸'\n\n\nã±ã£ã¨è¦‹ã§ã¯ã¾ã‚‹ã§æ³•å‰‡æ€§ãŒã‚ã‹ã‚‰ãªã„å¤©æ°—åˆ—ãŒç”Ÿæˆã§ãã¾ã—ãŸã€‚ã“ã‚Œã‚’å­¦ç¿’ã•ã›ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã¾ãšã¯ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚\n\n\nCode\nds = make_dotmod_dataset(SEQ_LEN, 5000)\ntest_ds = make_dotmod_dataset(SEQ_LEN, 1000)\n\nmodel = LinearModel(4 * SEQ_LEN, 3, key)\nmodel, key, loss_list, eval_list = train(\n    500, 100, model, ds, test_ds, key, 1e-2, linear_neglogp\n)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on Dotmod model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nf\"Accuracy: {linear_accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.6990000605583191'\n\n\n\n\n\n\n\n\n\næ„å¤–ã«7å‰²è¿‘ãæ­£è§£ã—ã¦ã„ã¾ã™ã­ã€‚æ¬¡ã«Self-Attentionã‚’å­¦ç¿’ã—ã¦ã¿ã¾ã™ã€‚\n\n\nCode\nmodel = MaskedAttention(4, D_ATTN, 3, key)\nmodel, key, loss_list, eval_list = train(500, 100, model, ds, test_ds, key, 1e-2)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on Dotmod model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\nvisualize_attn(ax1, model, test_ds, 1)\nvisualize_attn(ax2, model, test_ds, 2)\n\nf\"Accuracy: {accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.4480000138282776'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nã¾ãŸã—ã¦ã‚‚Self-Attentionã®ã»ã†ãŒã ã‚ã¨ã„ã†çµæœã«ãªã‚Šã¾ã—ãŸã€‚"
  }
]