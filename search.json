[
  {
    "objectID": "posts/sandb-exercise-racetrack.html",
    "href": "posts/sandb-exercise-racetrack.html",
    "title": "Exercise 5.12 Racetrack from the Reinforcement Learning textbook",
    "section": "",
    "text": "Here I demonstrate the execise 5.12 of the textbook Reinforcement Learning: An Introduction by Richard Sutton and Andrew G. Barto, using both of planning method and Monte Carlo method. Basic knowledge of Python (&gt;= 3.7) and NumPy are assumed. Some konwledge of matplotlib and Python typing library also helps.\nContact: yuji.kanagawa@oist.jp"
  },
  {
    "objectID": "posts/sandb-exercise-racetrack.html#modeling-the-problem-in-code",
    "href": "posts/sandb-exercise-racetrack.html#modeling-the-problem-in-code",
    "title": "Exercise 5.12 Racetrack from the Reinforcement Learning textbook",
    "section": "Modeling the problem in code",
    "text": "Modeling the problem in code\nLet’s start from writing the problem in code. What are important in this phase? Here, I’d like to emphasize the importantness of looking back at the definition of the environment. I.e., in reinforcement learning (RL), environments are modelled by Markov decision process (MDP), consisting of states, actions, transition function, and reward function. So first let’s check the definition of states and actions in the problem statement. It’s (somehow) not very straightforward, but we can find &gt; In our simplified racetrack, the car is at one of a discrete set of grid positions, the cells in the diagram. The velocity is also discrete, a number of grid cells moved horizontally and vertically per time step.\nSo, a state consists of position and velocity of the car (a.k.a. agent). What about actions?\n\nThe actions are increments to the velocity components. Each may be changed by +1, −1, or 0 in each step, for a total of nine (\\(3 \\times 3\\)) actions.\n\nSo there are 9 actions for each direction (↓↙←↖↑↗→↘ or no acceleration). Here, we can also notice that the total number of states is given by \\(\\textrm{Num. positions} \\times \\textrm{Num. choices of velocity}\\). And the texbook also says &gt; Both velocity components are restricted to be nonnegative and less than 5, and they cannot both be zero except at the starting line.\nSo there are 24 possible velocity at the non-starting positions:\n\n\nCode\nimport itertools\nlist(itertools.product(range(5), range(5)))[1:]\n\n\n[(0, 1),\n (0, 2),\n (0, 3),\n (0, 4),\n (1, 0),\n (1, 1),\n (1, 2),\n (1, 3),\n (1, 4),\n (2, 0),\n (2, 1),\n (2, 2),\n (2, 3),\n (2, 4),\n (3, 0),\n (3, 1),\n (3, 2),\n (3, 3),\n (3, 4),\n (4, 0),\n (4, 1),\n (4, 2),\n (4, 3),\n (4, 4)]\n\n\nAgain, number of states is given by (roughly) \\(24 \\times \\textrm{Num. positions}\\) and number of actions is \\(9\\). Sounds not very easy problem with many positions.\nSo, let’s start the coding from representing the state and actions. There are multiple ways, but I prefer to NumPy array for representing everything.\nLet’s consider a ASCII representation of the map (or track) like this:\n\n\nCode\nSMALL_TRACK = \"\"\"\n###      F\n##       F\n##       F\n#      ###\n#      ###\n#      ###\n#SSSSS####\n\"\"\"\n\n\nHere, S denotes a starting positiona, F denotes a finishing position, # denotes a wall, and  denotes a road. We have this track as a 2D NumPy Array, and encode agent’s position as an index of this array.\n\n\nCode\nimport numpy as np\n\ndef ascii_to_array(ascii_track: str) -&gt; np.ndarray:\n    \"\"\"Convert the ascii (string) map to a NumPy array.\"\"\"\n\n    lines = [line for line in ascii_track.split(\"\\n\") if len(line) &gt; 0]\n    byte_lines = [list(bytes(line, encoding=\"utf-8\")) for line in lines]\n    return np.array(byte_lines, dtype=np.uint8)\n\ntrack = ascii_to_array(SMALL_TRACK)\nprint(track)\nposition = np.array([0, 0])\ntrack[tuple(position)] == int.from_bytes(b'#', \"big\")\n\n\n[[35 35 35 32 32 32 32 32 32 70]\n [35 35 32 32 32 32 32 32 32 70]\n [35 35 32 32 32 32 32 32 32 70]\n [35 32 32 32 32 32 32 35 35 35]\n [35 32 32 32 32 32 32 35 35 35]\n [35 32 32 32 32 32 32 35 35 35]\n [35 83 83 83 83 83 35 35 35 35]]\n\n\nTrue\n\n\nThen, agent’s velocity and acceleration are also naturally represented by an array. And, we represent an action as an index of an array consisting of all possible acceleration vetors:\n\n\nCode\nnp.array(list(itertools.product([-1, 0, 1], [-1, 0, 1])))\n\n\narray([[-1, -1],\n       [-1,  0],\n       [-1,  1],\n       [ 0, -1],\n       [ 0,  0],\n       [ 0,  1],\n       [ 1, -1],\n       [ 1,  0],\n       [ 1,  1]])\n\n\nThe next step is to represent a transition function as a black box simulator. Note that we will visit another representation by transition matrix, but implementing the simulator is easier. Basically, the simulator should take an agent’s action and current state, and then return the next state. Let’s call this function step. However, let’s make it return some other things to make the implementation easier. Reward function sounds fairly easy to implement given the agent’s position. &gt; The rewards are −1 for each step until the car crosses the finish line.\nAlso, we have to handle the termination of the episode. &gt; Each episode begins in one of the randomly selected start states with both velocity components zero and ends when the car crosses the finish line.\nSo the resulting step function should return a tuple (state, reward, termination). The below cell contains my implementation of the simulator with matplotlib visualization. The step function is so complicated to handle the case where the agent goes through a wall, so readers are encouraged to just run their eyes through.\n\n\nCode\n# collapse-hide\n\nfrom typing import List, NamedTuple, Optional, Tuple\n\nfrom IPython.display import display\nfrom matplotlib import pyplot as plt\nfrom matplotlib.axes import Axes\nfrom matplotlib.colors import ListedColormap\n\n\nclass State(NamedTuple):\n    position: np.ndarray\n    velocity: np.ndarray\n\n\nclass RacetrackEnv:\n    \"\"\"Racetrack environment\"\"\"\n\n    EMPTY = int.from_bytes(b\" \", \"big\")\n    WALL = int.from_bytes(b\"#\", \"big\")\n    START = int.from_bytes(b\"S\", \"big\")\n    FINISH = int.from_bytes(b\"F\", \"big\")\n\n    def __init__(\n        self,\n        ascii_track: str,\n        noise_prob: float = 0.1,\n        seed: int = 0,\n    ) -&gt; None:\n        self._track = ascii_to_array(ascii_track)\n        self._max_height, self._max_width = self._track.shape\n        self._noise_prob = noise_prob\n        self._actions = np.array(list(itertools.product([-1, 0, 1], [-1, 0, 1])))\n        self._no_accel = 4\n        self._random_state = np.random.RandomState(seed=seed)\n        self._start_positions = np.argwhere(self._track == self.START)\n        self._state_indices = None\n        self._ax = None\n        self._agent_fig = None\n        self._arrow_fig = None\n        \n    def state_index(self, state: State) -&gt; int:\n        \"\"\"Returns a state index\"\"\"\n        (y, x), (vy, vx) = state\n        return y * self._max_width * 25 + x * 25 + vy * 5 + vx\n        \n\n    def _all_passed_positions(\n        self,\n        start: np.ndarray,\n        velocity: np.ndarray,\n    ) -&gt; Tuple[List[np.ndarray], bool]:\n        \"\"\"\n        List all positions that the agent passes over.\n        Here we assume that the y-directional velocity is already flipped by -1.\n        \"\"\"\n\n        maxv = np.max(np.abs(velocity))\n        if maxv == 0:\n            return [start], False\n        one_step_vector = velocity / maxv\n        pos = start + 0.0\n        traj = []\n        for i in range(maxv):\n            pos = pos + one_step_vector\n            ceiled = np.ceil(pos).astype(int)\n            if self._is_out(ceiled):\n                return traj, True\n            traj.append(ceiled)\n        # To prevent numerical issue\n        traj[-1] = start + velocity\n        return traj, False\n\n    def _is_out(self, position: np.ndarray) -&gt; bool:\n        \"\"\"Returns whether the given position is out of the map.\"\"\"\n        y, x = position\n        return y &lt; 0 or x &gt;= self._max_width\n\n    def step(self, state: State, action: int) -&gt; Tuple[State, float, bool]:\n        \"\"\"\n        Taking the current state and an agents' action, returns the next state,\n        reward and a boolean flag that indicates that the current episode terminates.\n        \"\"\"\n        position, velocity = state\n        if self._random_state.rand() &lt; self._noise_prob:\n            accel = self._actions[self._no_accel]\n        else:\n            accel = self._actions[action]\n        # velocity is clipped so that only ↑→ directions are possible\n        next_velocity = np.clip(velocity + accel, a_min=0, a_max=4)\n        # If both of velocity is 0, cancel the acceleration\n        if np.sum(next_velocity) == 0:\n            next_velocity = velocity\n        # List up trajectory. y_velocity is flipped to adjust the coordinate system.\n        traj, went_out = self._all_passed_positions(\n            position,\n            next_velocity * np.array([-1, 1]),\n        )\n        passed_wall, passed_finish = False, False\n        for track in map(lambda pos: self._track[tuple(pos)], traj):\n            passed_wall |= track == self.WALL\n            passed_finish |= track == self.FINISH\n        if not passed_wall and passed_finish:  # Goal!\n            return State(traj[-1], next_velocity), 0, True\n        elif passed_wall or went_out:  # Crasshed to the wall or run outside\n            return self.reset(), -1.0, False\n        else:\n            return State(traj[-1], next_velocity), -1, False\n\n    def reset(self) -&gt; State:\n        \"\"\"Randomly assigns a start position of the agent.\"\"\"\n        n_starts = len(self._start_positions)\n        initial_pos_idx = self._random_state.choice(n_starts)\n        initial_pos = self._start_positions[initial_pos_idx]\n        initial_velocity = np.array([0, 0])\n        return State(initial_pos, initial_velocity)\n\n    def render(\n        self,\n        state: Optional[State] = None,\n        movie: bool = False,\n        ax: Optional[Axes] = None,\n    ) -&gt; Axes:\n        \"\"\"Render the map and (optinally) the agents' position and velocity.\"\"\"\n        if self._ax is None or ax is not None:\n            if ax is None:\n                _, ax = plt.subplots(1, 1, figsize=(8, 8))\n            ax.set_aspect(\"equal\")\n            ax.set_xticks([])\n            ax.set_yticks([])\n            # Map the track to one of [0, 1, 2, 3] to that simple colormap works\n            map_array = np.zeros_like(track)\n            symbols = [self.EMPTY, self.WALL, self.START, self.FINISH]\n            for i in range(track.shape[0]):\n                for j in range(track.shape[1]):\n                    map_array[i, j] = symbols.index(self._track[i, j])\n            cm = ListedColormap(\n                [\"w\", \".75\", \"xkcd:reddish orange\", \"xkcd:kermit green\"]\n            )\n            map_img = ax.imshow(\n                map_array,\n                cmap=cm,\n                vmin=0,\n                vmax=4,\n                alpha=0.8,\n            )\n            if ax.get_legend() is None:\n                descriptions = [\"Empty\", \"Wall\", \"Start\", \"Finish\"]\n                for i in range(1, 4):\n                    if np.any(map_array == i):\n                        ax.plot([0.0], [0.0], color=cm(i), label=descriptions[i])\n                ax.legend(fontsize=12, loc=\"lower right\")\n            self._ax = ax\n        if state is not None:\n            if not movie and self._agent_fig is not None:\n                self._agent_fig.remove()\n            if not movie and self._arrow_fig is not None:\n                self._arrow_fig.remove()\n            pos, vel = state\n            self._agent_fig = self._ax.plot(pos[1], pos[0], \"k^\", markersize=20)[0]\n            # Show velocity\n            self._arrow_fig = self._ax.annotate(\n                \"\",\n                xy=(pos[1], pos[0] + 0.2),\n                xycoords=\"data\",\n                xytext=(pos[1] - vel[1], pos[0] + vel[0] + 0.2),\n                textcoords=\"data\",\n                arrowprops={\"color\": \"xkcd:blueberry\", \"alpha\": 0.6, \"width\": 2},\n            )\n        return self._ax\n\n\nsmalltrack = RacetrackEnv(SMALL_TRACK)\nstate = smalltrack.reset()\nprint(state)\ndisplay(smalltrack.render(state=state).get_figure())\nnext_state, reward, termination = smalltrack.step(state, 7)\nprint(next_state)\nsmalltrack.render(state=next_state)\n\n\nState(position=array([6, 5]), velocity=array([0, 0]))\nState(position=array([5, 5]), velocity=array([1, 0]))\n\n\n\n\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\nNote that the vertical velocity is negative, so that we can simply represent the coordinate by an array index."
  },
  {
    "objectID": "posts/sandb-exercise-racetrack.html#solve-a-small-problem-by-dynamic-programming",
    "href": "posts/sandb-exercise-racetrack.html#solve-a-small-problem-by-dynamic-programming",
    "title": "Exercise 5.12 Racetrack from the Reinforcement Learning textbook",
    "section": "Solve a small problem by dynamic programming",
    "text": "Solve a small problem by dynamic programming\nOK, now we have a simulator, so let’s solve the problem! However, before stepping into reinforcement learning, it’s better to compute an optimal policy in a small problem for sanity check. To do so, we need a transition matrix p, which is a \\(|\\mathcal{S}| \\times |\\mathcal{A}| \\times |\\mathcal{S}|\\) NumPy array where p[i][j][k] is the probability of transiting from i to k when action j is taken. Also, we need a \\(|\\mathcal{S}| \\times |\\mathcal{A}| \\times |\\mathcal{S}|\\) reward matrix r. Note that this representation is not general as \\(R_t\\) can be stochastic, but since the only stochasticity of rewards is the noise to actions in this problem, this notion suffices. It is often not very straightforward to get p and r from the problem definition, but basically we need to give 0-indexd indices to each state (0, 1, 2, ...) and fill the array. Here, I index each state by \\(\\textrm{Idx(S)} = y \\times 25 \\times W + x \\times 25 + vy \\times 5 + vx\\), where \\((x, y)\\) is a position, \\((vx, vy\\)) is a velocity, and \\(W\\) is the width of the map.\n\n\nCode\n# collapse-hide\nfrom typing import Iterable\n\n\ndef get_p_and_r(env: RacetrackEnv) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Taking RacetrackEnv, returns the transition probability p and reward fucntion r of the env.\"\"\"\n    n_states = env._max_height * env._max_width * 25\n    n_actions = len(env._actions)\n    p = np.zeros((n_states, n_actions, n_states))\n    r = np.ones((n_states, n_actions, n_states)) * -1\n    noise = env._noise_prob\n\n    def state_prob(*indices):\n        \"\"\"Returns a |S| length zero-initialized array where specified elements are filled\"\"\"\n        prob = 1.0 / len(indices)\n        res = np.zeros(n_states)\n        for idx in indices:\n            res[idx] = prob\n        return res\n\n    # List up all states and memonize starting states\n    states = []\n    starting_states = []\n    for y in range(env._max_height):\n        for x in range(env._max_width):\n            track = env._track[y][x]\n            for y_velocity in range(5):\n                for x_velocity in range(5):\n                    state = State(np.array([y, x]), np.array([y_velocity, x_velocity]))\n                    states.append(state)\n                    if track == env.START:\n                        starting_states.append(env.state_index(state))\n\n    for state in states:\n        position, velocity = state\n        i = env.state_index(state)\n        track = env._track[tuple(position)]\n        # At a terminating state or unreachable, the agent cannot move\n        if (\n            track == env.FINISH\n            or track == env.WALL\n            or (np.sum(velocity) == 0 and track != env.START)\n        ):\n            r[i] = 0\n            p[i, :] = state_prob(i)\n        # Start or empty states\n        else:\n            # First, compute next state probs without noise\n            next_state_prob = []\n            for j, action in enumerate(env._actions):\n                next_velocity = np.clip(velocity + action, a_min=0, a_max=4)\n                if np.sum(next_velocity) == 0:\n                    next_velocity = velocity\n                traj, went_out = env._all_passed_positions(\n                    position,\n                    next_velocity * np.array([-1, 1]),\n                )\n                passed_wall, passed_finish = False, False\n                for track in map(lambda pos: env._track[tuple(pos)], traj):\n                    passed_wall |= track == env.WALL\n                    passed_finish |= track == env.FINISH\n                if passed_wall or (went_out and not passed_finish):\n                    #  Run outside or crasheed to the wall\n                    next_state_prob.append(state_prob(*starting_states))\n                else:\n                    next_state_idx = env.state_index(State(traj[-1], next_velocity))\n                    next_state_prob.append(state_prob(next_state_idx))\n                    if passed_finish:\n                        r[i, j, next_state_idx] = 0.0\n            # Then linearly mix the transition probs with noise\n            for j in range(n_actions):\n                p[i][j] = (\n                    noise * next_state_prob[env._no_accel]\n                    + (1.0 - noise) * next_state_prob[j]\n                )\n\n    return p, r\n\n\nThen, let’s compute the optimal Q value by value iteration. So far, we only learned dynamic programming with discount factor \\(\\gamma\\), so let’s use \\(\\gamma =0.95\\) that is sufficiently large for this small problem. \\(\\epsilon = 0.000001\\) is used as a convergence threshold. Let’s show the elapsed time and the required number of iteration.\n\n\nCode\nimport datetime\n\n\nclass ValueIterationResult(NamedTuple):\n    q: np.ndarray\n    v: np.ndarray\n    elapsed: datetime.timedelta\n    n_iterations: int\n\n\ndef value_iteration(\n    p: np.ndarray,\n    r: np.ndarray,\n    discount: float,\n    epsilon: float = 1e-6,\n) -&gt; ValueIterationResult:\n    n_states, n_actions, _ = p.shape\n    q = np.zeros((n_states, n_actions))\n    v = np.zeros(n_states)\n    n_iterations = 0\n    start = datetime.datetime.now()\n    while True:\n        n_iterations += 1\n        v_old = v.copy()\n        for s in range(n_states):\n            # Q(s, a) = ∑ p(s, a, s') * (r(s, a, s') + γ v(s')\n            for a in range(n_actions):\n                q[s, a] = np.dot(p[s, a], r[s, a] + discount * v)\n            # V(s) = max_a Q(s, a)\n            v[s] = np.max(q[s])\n        if np.linalg.norm(v - v_old, ord=np.inf) &lt; epsilon:\n            break\n    return ValueIterationResult(q, v, datetime.datetime.now() - start, n_iterations)\n\n\np, r = get_p_and_r(smalltrack)\nvi_result = value_iteration(p, r, discount=0.95)\nprint(f\"Elapsed: {vi_result.elapsed.total_seconds()} n_iter: {vi_result.n_iterations}\")\n\n\nElapsed: 4.235676 n_iter: 10\n\n\nIt took longer that a second on my laptop, even for this small problem. Some technical notes on value iteration (\\(R_\\textrm{max} = 1.0\\) is assumed for simplicity): - Each iteration takes \\(O(|\\mathcal{S}| ^ 2 |\\mathcal{A}|)\\) time - The required iteration number is bounded by \\(\\frac{\\log \\epsilon}{\\gamma - 1}\\) - In our case, \\(\\frac{\\log \\epsilon}{\\gamma - 1} \\approx 270\\), so our computation converged quicker than theory - Thus the total computation time is bounded by \\(O(|\\mathcal{S}| ^ 2 |\\mathcal{A}|\\frac{\\log \\epsilon}{\\gamma - 1})\\) - For a convergence threshold \\(\\epsilon\\), \\(\\max |V(s) - V^*(s)| &lt; \\frac{\\gamma\\epsilon}{1 - \\gamma}\\) is guranteed - In our case, \\(\\frac{\\gamma\\epsilon}{1 - \\gamma} \\approx 0.00002\\) - This is called relative error\nLet’s visualize the V value and an optimal trajectory. celluloid) is used for making an animation.\n\n\nCode\nfrom typing import Callable\n\nfrom IPython.display import HTML\n\ntry:\n    from celluloid import Camera\nexcept ImportError as _e:\n    ! pip install celluloid --user\n    from celluloid import Camera\n\nPolicy = Callable[[int], int]\n\n\ndef smalltrack_optimal_policy(state_idx: int) -&gt; int:\n    return np.argmax(vi_result.q[state_idx])\n\n\ndef show_rollout(\n    env: RacetrackEnv,\n    policy: Policy,\n    v: np.ndarray = vi_result.v,\n    title: Optional[str] = None,\n) -&gt; HTML:\n    state = env.reset()\n    prev_termination = False\n    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n    camera = Camera(fig)\n    initial = True\n    while True:\n        env.render(state=state, movie=True, ax=ax)\n        state_idx = env.state_index(state)\n        ax.text(3, 0.5, f\"V(s): {v[state_idx]:02}\", c=\"red\")\n        camera.snap()\n        if prev_termination:\n            break\n        state, _, prev_termination = env.step(state, policy(state_idx))\n    if title is not None:\n        ax.text(3, 0.1, title, c=\"k\")\n    return HTML(camera.animate(interval=1000).to_jshtml())\n\n\nshow_rollout(smalltrack, smalltrack_optimal_policy)\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\nThe computed optimal policy and value seems correct."
  },
  {
    "objectID": "posts/sandb-exercise-racetrack.html#monte-carlo-prediction",
    "href": "posts/sandb-exercise-racetrack.html#monte-carlo-prediction",
    "title": "Exercise 5.12 Racetrack from the Reinforcement Learning textbook",
    "section": "Monte-Carlo prediction",
    "text": "Monte-Carlo prediction\nThen let’s try ‘reinforcement learning’. First, I implemeted ‘First visit Monte-Carlo prediction’, which evaluates a (Markovian) policy \\(\\pi\\) by doing a simulation multiple times, and calculates the average of received returns. Here, I evaluate the optimal policy \\(\\pi^*\\) obtained by value iteration.\n\n\nCode\n# collapse-hide\n\nfrom typing import Union\n\n\ndef first_visit_mc_prediction(\n    policy: Policy,\n    env: RacetrackEnv,\n    n_episodes: int,\n    discount: float = 0.95,\n    record_all_values: bool = False,\n) -&gt; Tuple[np.ndarray, List[np.ndarray]]:\n    \"\"\"Predict value function corresponding to the policy by First-visit MC prediction\"\"\"\n    n_states = env._max_width * env._max_height * 25\n    v = np.zeros(n_states)\n    all_values = []\n    # Note that we have to have a list of returns for each state!\n    # So the maximum memory usage would be Num.States x Num.Episodes\n    all_returns = [[] for _ in range(n_states)]\n    for i in range(n_episodes):\n        if record_all_values:\n            all_values.append(v.copy())\n        state = env.reset()\n        visited_states = [env.state_index(state)]\n        received_rewards = []\n        # Rollout the policy until the episode ends\n        while True:\n            # Sample an action from the policy\n            action = policy(env.state_index(state))\n            # Step the simulator\n            state, reward, termination = env.step(state, action)\n            visited_states.append(env.state_index(state))\n            received_rewards.append(reward)\n            if termination:\n                break\n        # Compute return\n        traj_len = len(received_rewards)\n        returns = np.zeros(traj_len)\n        # Gt = Rt when t = T\n        returns[-1] = received_rewards[-1]\n        # Iterating from T - 2, T - 1, ..., to 0\n        for t in reversed(range(traj_len - 1)):\n            # Gt = Rt + γGt+1\n            returns[t] = received_rewards[t] + discount * returns[t + 1]\n        updated = set()\n        # Update the value\n        for i, state in enumerate(visited_states[: -1]):\n            # If the state is already visited, skip it\n            if state in updated:\n                continue\n            updated.add(state)\n            all_returns[state].append(returns[i].item())\n            # V(St) ← average(Returns(St))\n            v[state] = np.mean(all_returns[state])\n    return v, all_values\n\n\nv, all_values = first_visit_mc_prediction(\n    smalltrack_optimal_policy,\n    smalltrack,\n    1000,\n    record_all_values=True,\n)\nvalue_diff = []\nfor i, mc_value in enumerate(all_values + [v]):\n    value_diff.append(np.mean(np.abs(mc_value - vi_result.v)))\nplt.plot(value_diff)\nplt.xlabel(\"Num. Episodes\")\nplt.ylabel(\"Avg. |V* - V|\")\n\n\nText(0, 0.5, 'Avg. |V* - V|')\n\n\n\n\n\nIt looks like that the difference between \\(V^*\\) and the value function estimated by MC prediction converges after 600 steps, but it’s still larger than \\(0\\), because \\(\\pi^*\\) doesn’t visit all states. Let’s plot the difference between value functions only on starting states.\n\n\nCode\nstart_states = []\nfor x in range(1, 6):\n    idx = smalltrack.state_index(State(position=np.array([6, x]), velocity=np.array([0, 0])))\n    start_states.append(idx)\nstart_values = []\nfor i, mc_value in enumerate(all_values + [v]):\n    start_values.append(np.mean(np.abs(mc_value - vi_result.v)[start_states]))\nplt.plot(start_values)\nplt.xlabel(\"Num. Episodes\")\nplt.ylabel(\"|V*(start) - V(start)| only on possible states\")\nplt.ylim((0.0, 0.5))\n\n\n(0.0, 0.5)\n\n\n\n\n\nHere, we can confirm that the estimated value certainly converged close to 0.0, while fractuating a bit. Note that the magnitude of fractuation is larger than the relative error we allowed for value iteration (\\(0.00002\\)), implying the difficulty of convergence."
  },
  {
    "objectID": "posts/sandb-exercise-racetrack.html#monte-carlo-control",
    "href": "posts/sandb-exercise-racetrack.html#monte-carlo-control",
    "title": "Exercise 5.12 Racetrack from the Reinforcement Learning textbook",
    "section": "Monte-Carlo Control",
    "text": "Monte-Carlo Control\nNow we successfully estimate \\(V^\\pi\\) using Monte Carlo method, so then let’s try to learn a sub-optimal \\(\\pi\\) directly using Monte Carlo method. In the textbook, three methods are introduced: - Monte Carlo ES (Exploring Starts) - On-policy first visit Monte Carlo Control - Off-policy first visit Monte Carlo Control\nHere, let’s try all three methods and compare the resulting value functions. However, we cannot naively implement the pseudo code in the textbook, due to a ‘loop’ problem. Since the car that crashed into the wall is returned to a starting point, the episode length can be infinitte depending on a policy. As a remedy for this problem, I limit the length of the episode as \\(H\\). Supposing that we ignore the future rewards smaller than \\(\\epsilon\\), how to set \\(H\\)? Just by solving \\(\\gamma^H R &lt; \\epsilon\\), we get \\(H &gt; \\frac{\\log \\epsilon}{\\log \\gamma}\\), which is about \\(270\\) in case \\(\\gamma = 0.95\\) and \\(\\epsilon = 0.000001\\).\nBelow are the implementation of three methods. A few notes about implementation: - Monte Carlo ES requires a set of all possible states, which is implemented in valid_states function. - For On-Policy MC, \\(\\epsilon\\) is decreased from 0.5 to 0.01 - We can use arbitary policy in Off-Policy MC, but I used the same \\(\\epsilon\\)-soft policy as On-Policy MC.\n\n\nCode\n# collapse-hide\n\ndef valid_states(env: RacetrackEnv) -&gt; List[State]:\n    states = []\n    for y in range(env._max_height):\n        for x in range(env._max_width):\n            track = env._track[y][x]\n            if track == env.WALL:\n                continue\n            for y_velocity in range(5):\n                for x_velocity in range(5):\n                    state = State(np.array([y, x]), np.array([y_velocity, x_velocity]))\n                    if track != env.START and (x_velocity &gt; 0 or y_velocity &gt; 0):\n                        states.append(state)\n    return states\n\n\ndef mc_es(\n    env: RacetrackEnv,\n    n_episodes: int,\n    discount: float = 0.95,\n    record_all_values: bool = False,\n    seed: int = 999,\n) -&gt; Tuple[np.ndarray, List[np.ndarray]]:\n    \"\"\"Monte-Carlo Control with Exploring Starts\"\"\"\n    n_states = env._max_width * env._max_height * 25\n    n_actions = len(env._actions)\n    random_state = np.random.RandomState(seed=seed)\n    q = np.zeros((n_states, n_actions))\n    pi = random_state.randint(9, size=n_states)\n    all_values = []\n    all_returns = [[[] for _ in range(n_actions)] for _ in range(n_states)]\n    possible_starts = valid_states(env)\n    max_episode_length = int(np.ceil(np.log(1e-6) / np.log(discount)))\n    for i in range(n_episodes):\n        if record_all_values:\n            all_values.append(q.copy())\n        state = possible_starts[random_state.choice(len(possible_starts))]\n        visited_states = [env.state_index(state)]\n        taken_actions = []\n        received_rewards = []\n        initial = True\n        for _ in range(max_episode_length):\n            if initial:\n                # Randomly sample the first action\n                action = random_state.randint(9)\n                initial = False\n            else:\n                # Take an action following the policy\n                action = pi[env.state_index(state)]\n            taken_actions.append(action)\n            # Step the simulator\n            state, reward, termination = env.step(state, action)\n            visited_states.append(env.state_index(state))\n            received_rewards.append(reward)\n            if termination:\n                break\n        # Compute return\n        traj_len = len(received_rewards)\n        returns = np.zeros(traj_len)\n        # Gt = Rt when t = T\n        returns[-1] = received_rewards[-1]\n        # Iterating from T - 2, T - 1, ..., to 0\n        for t in reversed(range(traj_len - 1)):\n            # Gt = Rt + γGt+1\n            returns[t] = received_rewards[t] + discount * returns[t + 1]\n        updated = set()\n        # Update the value\n        for i, (state, action) in enumerate(zip(visited_states[:-1], taken_actions)):\n            # If the state is already visited, skip it\n            if (state, action) in updated:\n                continue\n            updated.add((state, action))\n            all_returns[state][action].append(returns[i].item())\n            # Q(St, At) ← average(Returns(St, At))\n            q[state, action] = np.mean(all_returns[state][action])\n            pi[state] = np.argmax(q[state])\n    return q, all_values\n\n\ndef on_policy_fist_visit_mc(\n    env: RacetrackEnv,\n    n_episodes: int,\n    discount: float = 0.95,\n    epsilon: float = 0.1,\n    epsilon_final: float = 0.1,\n    record_all_values: bool = False,\n    seed: int = 999,\n) -&gt; Tuple[np.ndarray, List[np.ndarray]]:\n    \"\"\"On-policy first visit Monte-Carlo\"\"\"\n\n    n_states = env._max_width * env._max_height * 25\n    n_actions = len(env._actions)\n    random_state = np.random.RandomState(seed=seed)\n    q = np.zeros((n_states, n_actions))\n    pi = random_state.randint(9, size=n_states)\n    all_values = []\n    all_returns = [[[] for _ in range(n_actions)] for _ in range(n_states)]\n    possible_starts = valid_states(env)\n    max_episode_length = int(np.ceil(np.log(1e-6) / np.log(discount)))\n    epsilon_decay = (epsilon - epsilon_final) / n_episodes\n    for i in range(n_episodes):\n        if record_all_values:\n            all_values.append(q.copy())\n        state = env.reset()\n        visited_states = [env.state_index(state)]\n        taken_actions = []\n        received_rewards = []\n        for _ in range(max_episode_length):\n            # ε-soft policy\n            if random_state.rand() &lt; epsilon:\n                action = random_state.randint(9)\n            else:\n                action = pi[env.state_index(state)]\n            taken_actions.append(action)\n            # Step the simulator\n            state, reward, termination = env.step(state, action)\n            visited_states.append(env.state_index(state))\n            received_rewards.append(reward)\n            if termination:\n                break\n        # Below code is the same as mc_es\n        # Compute return\n        traj_len = len(received_rewards)\n        returns = np.zeros(traj_len)\n        # Gt = Rt when t = T\n        returns[-1] = received_rewards[-1]\n        # Iterating from T - 2, T - 1, ..., to 0\n        for t in reversed(range(traj_len - 1)):\n            # Gt = Rt + γGt+1\n            returns[t] = received_rewards[t] + discount * returns[t + 1]\n        updated = set()\n        # Update the value\n        for i, (state, action) in enumerate(zip(visited_states[:-1], taken_actions)):\n            # If the state is already visited, skip it\n            if (state, action) in updated:\n                continue\n            updated.add((state, action))\n            all_returns[state][action].append(returns[i].item())\n            # Q(St, At) ← average(Returns(St, At))\n            q[state, action] = np.mean(all_returns[state][action])\n            pi[state] = np.argmax(q[state])\n        epsilon -= epsilon_decay\n    return q, all_values\n\n\ndef off_policy_mc(\n    env: RacetrackEnv,\n    n_episodes: int,\n    discount: float = 0.95,\n    record_all_values: bool = False,\n    epsilon: float = 0.1,\n    epsilon_final: float = 0.1,\n    seed: int = 999,\n) -&gt; Tuple[np.ndarray, List[np.ndarray]]:\n    \"\"\"Off-policy MC control\"\"\"\n\n    n_states = env._max_width * env._max_height * 25\n    n_actions = len(env._actions)\n    random_state = np.random.RandomState(seed=seed)\n    q = np.zeros((n_states, n_actions))\n    c = np.zeros_like(q)\n    pi = np.argmax(q, axis=1)\n    all_values = []\n    possible_starts = valid_states(env)\n    max_episode_length = int(np.ceil(np.log(1e-6) / np.log(discount)))\n    epsilon_decay = (epsilon - epsilon_final) / n_episodes\n    for i in range(n_episodes):\n        if record_all_values:\n            all_values.append(q.copy())\n        state = env.reset()\n        visited_states = [env.state_index(state)]\n        taken_actions = []\n        received_rewards = []\n        acted_optimally = []\n        for _ in range(max_episode_length):\n            # ε-soft policy\n            if random_state.rand() &lt; epsilon:\n                action = random_state.randint(9)\n            else:\n                action = pi[env.state_index(state)]\n            acted_optimally.append(action == pi[env.state_index(state)])\n            taken_actions.append(action)\n            # Step the simulator\n            state, reward, termination = env.step(state, action)\n            visited_states.append(env.state_index(state))\n            received_rewards.append(reward)\n            if termination:\n                break\n        g = 0\n        w = 1.0\n        for i, (state, action) in enumerate(zip(visited_states[:-1], taken_actions)):\n            g = discount * g + received_rewards[i]\n            c[state, action] += w\n            q[state, action] += w / c[state, action] * (g - q[state, action])\n            pi[state] = np.argmax(q[state])\n            if action == pi[state]:\n                break\n            else:\n                if acted_optimally[i]:\n                    w *= 1.0 - epsilon + epsilon / n_actions\n                else:\n                    w *= epsilon / n_actions\n        epsilon -= epsilon_decay\n    return q, all_values\n\n\nmces_result = mc_es(smalltrack, 3000, record_all_values=True)\non_mc_result = on_policy_fist_visit_mc(\n    smalltrack,\n    3000,\n    epsilon=0.5,\n    epsilon_final=0.01,\n    record_all_values=True,\n)\noff_mc_result = off_policy_mc(\n    smalltrack,\n    3000,\n    epsilon=0.5,\n    epsilon_final=0.01,\n    record_all_values=True,\n)\n\n\nLet’s plot the results. Here I plotted the difference from the optimal value function and the number of states that the policy choices the optimal action.\n\n\nCode\ndef value_diff(q_values: List[np.ndarray]) -&gt; List[float]:\n    diffs = []\n    for i, q in enumerate(q_values):\n        diff = np.abs(np.max(q, axis=-1) - vi_result.v)[start_states]\n        diffs.append(np.mean(diff))\n    return diffs\n\n\ndef n_optimal_actions(q_values: List[np.ndarray]) -&gt; List[int]:\n    n_optimal = []\n    optimal_actions = np.argmax(vi_result.q, axis=-1)\n    for i, q in enumerate(q_values):\n        greedy = np.argmax(q, axis=-1)\n        n_optimal.append(np.sum(greedy == optimal_actions))\n    return n_optimal\n\n\n_, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\nmc_es_value_diff = value_diff([mces_result[0]] + mces_result[1])\non_mc_value_diff = value_diff([on_mc_result[0]] + on_mc_result[1])\noff_mc_value_diff = value_diff([off_mc_result[0]] + off_mc_result[1])\nax1.plot(mc_es_value_diff, label=\"MC-ES\")\nax1.plot(on_mc_value_diff, label=\"On-Policy\")\nax1.plot(off_mc_value_diff, label=\"Off-Policy\")\nax1.set_xlabel(\"Num. Episodes\")\nax1.set_ylabel(\"Avg. |V* - V|\")\nax1.set_title(\"Diff. from V*\")\nmc_es_nopt = n_optimal_actions([mces_result[0]] + mces_result[1])\non_mc_nopt =  n_optimal_actions([on_mc_result[0]] + on_mc_result[1])\noff_mc_nopt =  n_optimal_actions([off_mc_result[0]] + off_mc_result[1])\nax1.legend(fontsize=12, loc=\"upper right\")\nax2.plot(mc_es_nopt, label=\"MC-ES\")\nax2.plot(on_mc_nopt, label=\"On-Policy\")\nax2.plot(off_mc_nopt, label=\"Off-Policy\")\nax2.set_xlabel(\"Num. Episodes\")\nax2.set_ylabel(\"Num. Optimal Actions\")\nax2.legend(fontsize=12, loc=\"upper right\")\nax2.set_title(\"Num. of optimal actions\")\n\n\nText(0.5, 1.0, 'Num. of optimal actions')\n\n\n\n\n\nSome observations from the results: - On-Policy MC converges to the optimal policy the fastest, though the convergence of its value function is the slowest - MC-ES struggles to distinguish optimal and non-optimal actions at some states, probably because of the lack of exploration during an episode. - Compared to MC-ES and On-Policy MC, the peak of value differences of Off-Policy MC is much milder. - A randomly initialized policy is often caught in a loop and cannot reach to the goal. The value of such a policy is really small (\\(-1 -1 * 0.95 - 1 * 0.95^2 - ... \\approx -20\\)). However, Off-Policy MC uses important sampling to decay the rewards by uncertain actions, resulting the smaller value differences.\nHere I visualized sampled plays from all three methods. On-Policy MC looks the most efficient.\n\n\nCode\nfor q, name in zip([mces_result[0], on_mc_result[0], off_mc_result[0]], [\"MC-ES\", \"On-Policy\", \"Off-Policy\"]):\n    display(show_rollout(smalltrack, lambda i: np.argmax(q[i]), np.argmax(q, axis=-1), name))\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/jax-brax-haiku.html",
    "href": "posts/jax-brax-haiku.html",
    "title": "Jax・Brax・HaikuでGPU引きこもり学習",
    "section": "",
    "text": "強化学習若手の会 Advent Calendar 2021 18日目\n\n\n0. 時勢のあいさつとか\n\nNote: このブログは強化学習若手の会 Advent Calendar 2021 18日目の記事として書かれました\n\nこんにちは。 コロナ禍も終わりが見えつつある（と思ったらオミクロン株が…）2021年もあとわずか。寒さも厳しくなってきましたが、皆さん如何お過ごしでしょうか。 とは言ったものの、僕は沖縄にいるので、それほど寒くはないのですが…。\n若手の会のアドベントカレンダーということで、国内でのコミュニティの活動について、最初に総括してみたいと思います。\n\n若手の会では模倣学習の勉強会をしましたが、結局2回しか続きませんでした。\n苦手の会のもくもく会はフリスビーやマラソンの練習とバッティングしてやらなくなってしまいましたが、最近日程を変えて、火曜日の夜に始めました。暇な方一緒にもくもくしましょう。\n強化学習の講義資料の翻訳をしています。難しいですが、けっこう勉強になります。有限サンプルでのバウンドを、初めて勉強しました。興味がある方は、ぜひ一緒にやりましょう。\n昨年のブログを書いてからはや一年、せっかく専用のブログを作ったので今年もいい感じにmatplotlib芸強化学習の記事を書いていきたいと思っていましたが、結局何も書きませんでした。\n\n最近は人工進化や人工生命の研究も始めたので、もはや「強化学習の人」と名乗っていいのかよくわかりませんが、今後も国内のコミュニティに何か貢献できればと思います。\n今年は強化学習に対する楽観論も悲観論も多く目にした一年でした。 David SilverやSuttonはReward is Enoughという強気な論文を出し、知的なシステムはおよそ全て報酬最大化で作れると主張しました。 さすがに強気すぎると思いますが、その後Reward is enough for convex MDPsやOn the Expressivity of Markov Rewardといったテクニカルな論文が出てきたのは面白いです。 また、オフライン強化学習・教師なし強化学習の論文が増えてきたと思います。 ざっくり、オフライン強化学習 = 強化学習 - 探索、教師なし強化学習 = 強化学習 - 報酬と思ってもらって問題ないでしょう。 何を隠そう僕の修士論文も単なる「階層型強化学習」だったのですが、リジェクト諸般の事情により教師なしに魔改造して再投稿しました。 Sergey LevineにいたってはUnderstanding the World Through Actionというタイトルが強い論文の中で、「大量にデータを集めてオフライン教師なし強化学習をすれば世界を理解できる（≒世界を理解しているのと同等のシステムが作れる？）」と言っています。面白い方向性だと思います。 一方で、みんな大好きルービックキューブ論文を出したOpen AIのロボティクスチームは、「とりあえず今データがある領域に注力する」とのことで解散してしまいました。 このブログを書いている最中にWebGPTの論文を目にしましたが、今後は言語モデル＋強化学習で色々やっていくのでしょうか。品川さんは喜びそうですが、僕なんかはこういう到底自分でできないものは「テレビの中の研究」という感じがして一歩引いてしまいます（最近は、テレビとかたとえに使うと古いのかな…）。 Open AIのロボティクスは、Sim2Realにこだわりすぎたのでは？という意見を某所でお聞きしました。実際そうなのかは知りませんが、大規模にシミュレーションしてSim2Realを頑張るのか、実機のデータで頑張るのかというのは、面白い視点ですよね。\nOpen AIが今までほど強化学習に注力しなくなったことで、Open AI gymをはじめ強化学習研究で使われてきたソフトウェア群にも、色々と情勢の変化がありそうです。 1. OpenAI Gymのメンテナが変わりました。これからはOpen AIではなくメリーランド大学の学生さんがメンテナになるようです。mujoco-pyなど関連するライブラリについては相変わらず放置されています。 2. DeepmindがMuJoCoを買い取って無料にしました。今後ソースコードも公開されるようです。 3. Googleから新しくbraxというシミュレータが公開されました。\nそんなわけで、僕はこれまでmujoco-py + gymで作成したカスタム環境でたくさん実験をやってきましたが、MuJoCoを使うにしてもdm_controlを使うとか、はたまたbraxにしてしまうとか、別の選択肢を検討したくなってきました。 このブログでは、とりあえずbraxを試してみようと思います。\n\n\n1. はじめに: シミュレーション・謎ロボット・GPU\n本題に入りますが、ざっくり、強化学習とは、報酬から行動を学習する枠組みだと言うことができます。 では何の行動を学習させたいのでしょうか。 ゲームのAIだったり、チャットボットだったり、色々な選択肢があると思いますが、どういうわけかシミュレータ上で動く謎ロボットというのがポピュラーな選択肢です。\nこのブログをごらんの方の中には、こういったナナフシのような謎ロボットの画像を目にしたことがある方も多いのではないでしょうか。\n\n\n\nHalfCheetah\n\n\nこれはOpen AI gymのHalfCheetahというロボットです。足が2本なのでハーフなのだと思いますが、なんとも残酷なネーミングです。愛玩されるため病気のまま品種改良されてきた犬猫のような哀愁が漂います。\nMuJoCoシミュレーターに「こことここがジョイントで、可動域はこうです。床は白黒でお願いします」みたいなXMLファイルを渡すと、こういうロボットを作ってくれます。 もしくは、dm_controlなどのPythonライブラリにXMLを作らせることもできます。 このような謎ロボットが実験で広く使われている要因として、 - みんなが使っているから - Atariなどのゲームより高速 - ジョイントの速さ・位置などの完全な内部状態が手に入る - マルコフ性について心配しなくてもいい - 色々カスタマイズできて便利だから - 普通のロボットを訓練するためのテストにちょうどいいから\nなどの理由があると思いますが、なんだかんだみんなが使っているからというのが大きい気がします。\nところで、このMuJoCoシミュレータというのは非常に高速に動作するのですが、CPU上でしか動作しません。 今日使われている深層学習のコードは、その計算量のほとんどを占める行列演算がベクトル並列化ととても相性がいいため、ネットワークやバッチサイズが大きくなればなるほどGPU上で高速に動作します。 となると、GPUで学習を回している場合、どうしてもCPUからGPUにデータを転送するボトルネックが発生し、高速化の妨げになります。 そこで、GPU上でシミュレーションを行えるようにしたのが、今回紹介するbraxというシミュレータです。\n\n\n2. Jaxでnumpy演算を高速化してみる\nでは、braxはCUDAか何かで書かれているのかな？と思ったかもしれませんが、なんと全てPythonで書かれているのです。 その鍵となるのがjaxというライブラリです。 おもむろに、インストールしてみましょう。\n\n\nCode\n! pip install jax\n\n\nドキュメントの冒頭に’JAX is Autograd and XLA’とありますが、Jaxは - Numpy演算をXLAに変換するコンパイラ(Tensorflow) - jax.jit - XLAはTensorflowのバックエンドとして開発された中間言語で、GPU/TPU用にすごく速いコードを生成できる - Numpy演算を追跡して勾配を計算する機能 - jax.grad/jax.vjp など\nの2つのコア機能を核とするライブラリです。 この節では、ひとまず前者の「XLAに変換するコンパイラ」としての機能に焦点を当ててみます。\nコンパイラはJIT方式で実装されており、 1. jax.jitに関数fを渡す (f_compiled = jax.jit(f)） 2. コンパイルされる関数f_compiledを最初に呼び出したとき、jaxはPythonの関数をXLAにコンパイルする 3. 2回目以降関数呼び出しが高速になる という処理の流れになります。\nでは、さっそく何かシミュレーションしてみましょう。 適当に天井からボールを落としてみましょう。\n\n\nCode\nimport typing as t\n\nimport numpy as np\nfrom IPython.display import HTML, clear_output\ntry:\n    import pandas as pd\n    import seaborn as sns\n    from celluloid import Camera\n    from matplotlib import pyplot as plt\n    from matplotlib.animation import ArtistAnimation\nexcept ImportError as _e:\n    ! pip isntall pandas seaborn celluloid\n    clear_output()\n    \n    import pandas as pd\n    import seaborn as sns\n    from celluloid import Camera\n    from matplotlib import pyplot as plt\n    from matplotlib.animation import ArtistAnimation\n\nsns.set_theme(style=\"darkgrid\")\n\nArray = np.ndarray\nGRAVITY = -9.8\n\n\ndef move_balls(\n    ball_positions: Array,\n    ball_velocities: Array,\n    delta_t: float = 0.1,\n) -&gt; Array:\n    accel_x = np.zeros(ball_positions.shape[0])\n    accel_y = np.ones(ball_positions.shape[0]) * GRAVITY * delta_t  # y方向にGΔt加速\n    new_velocities = np.stack((accel_x, accel_y), axis=1) + ball_velocities\n    new_positions = ball_positions + delta_t * new_velocities\n    return new_positions, new_velocities\n\n\ndef simulate_balls(\n    n_balls: int,\n    n_steps: int = 100,\n    forward: t.Callable[[Array], Array] = move_balls,\n) -&gt; t.List[Array]:\n    p = np.random.uniform(size=(n_balls, 2), low=-50, high=50.0)\n    v = np.random.randn(n_balls, 2)\n    results = [p]\n    for _ in range(n_steps):\n        p, v = forward(p, v)\n        results.append(p)\n    return results\n\n\n適当にボールを20個落としてみます。\n\n\nCode\ndef ball_animation(balls: t.Iterable[Array]) -&gt; ArtistAnimation:\n    fig = plt.figure(figsize=(8, 8))\n    ax = fig.add_subplot()\n    ax.set_xlim(-50, 50)\n    ax.set_ylim(-50, 50)\n    camera = Camera(fig)\n    for ball_batch in balls:\n        ax.scatter(ball_batch[:, 0], ball_batch[:, 1], color=\"red\", alpha=0.7)\n        camera.snap()\n    return camera.animate()\n\n\nHTML(ball_animation(simulate_balls(20, 40)).to_jshtml())\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\nでは、このシミュレーションをするのに、どれくらい時間がかかるでしょうか。ボールの数を変えてベンチマークしてみましょう。\n\n\nCode\ndef bench(\n    f: t.Callable[..., t.Any],\n    inputs: t.Iterable[t.Any],\n    number: int = 10,\n) -&gt; t.List[float]:\n    import timeit\n\n    return [timeit.Timer(lambda: f(x)).timeit(number=number) for x in inputs]\n\n\ndef bench_and_plot(f: t.Callable[..., t.Any], title: str) -&gt; pd.DataFrame:\n    inputs = [4000, 8000, 16000, 32000, 64000]\n    result = pd.DataFrame({\"x\": inputs, \"y\": bench(f, inputs)})\n    result[\"Method\"] = [title] * len(inputs)\n    ax = sns.lineplot(data=result, x=\"x\", y=\"y\")\n    ax.set_title(title)\n    ax.set_xlabel(\"Num. of balls\")\n    ax.set_ylabel(\"Time (sec.)\")\n    return result\n\n\nnumpy_result = bench_and_plot(simulate_balls, \"NumPy\")\n\n\n\n\n\nおおむね線形に実行時間が増えていることがわかります。このコードを、jaxを使って高速化してみましょう。 基本的にはnumpyをjax.numpyに置き換えればいいです。\n\n\nCode\nimport jax\nimport jax.numpy as jnp\n\nJaxArray = jnp.DeviceArray\n\ndef move_balls_jax(\n    ball_positions: JaxArray,\n    ball_velocities: JaxArray,\n    delta_t: float = 0.1,\n) -&gt; JaxArray:\n    accel_x = jnp.zeros(ball_positions.shape[0])\n    accel_y = jnp.ones(ball_positions.shape[0]) * GRAVITY * delta_t\n    new_velocities = jnp.stack((accel_x, accel_y), axis=1) + ball_velocities\n    new_positions = ball_positions + delta_t * new_velocities\n    return new_positions, new_velocities\n\n\nでは同じようにベンチマークをとってみましょう。\n\n\nCode\njax_nojit_result = bench_and_plot(\n    lambda n: simulate_balls(n, forward=move_balls_jax),\n    \"JAX (without JIT)\",\n)\n\n\n\n\n\n謎の挙動を見せているし、すごく遅いですね。今度はJITコンパイルしてみましょう。 jax.jit(f, backend=\"cpu\")で関数をCPU上で動くXLAコードにコンパイルできます。\n\n\nCode\njax_cpu_result = bench_and_plot(\n    lambda n: simulate_balls(n, forward=jax.jit(move_balls_jax, backend=\"cpu\")),\n    \"JAX (with JIT on CPU)\",\n)\n\n\n\n\n\nすごく速くなりました。今度はGPUでやってみます。\n\n\nCode\njax_gpu_result = bench_and_plot(\n    lambda n: simulate_balls(n, forward=jax.jit(move_balls_jax, backend=\"gpu\")),\n    \"JAX (with JIT for GPU)\",\n)\n\n\n\n\n\n圧倒的に速いですね。一応線形に実行時間が増えてはいますが…。 なお、今回は学内スパコンのNVIDIA P100 GPUを使用しています。\n\n\nCode\nax = sns.lineplot(\n    data=pd.concat(\n        [numpy_result, jax_nojit_result, jax_cpu_result, jax_gpu_result],\n        ignore_index=True,\n    ),\n    x=\"x\",\n    y=\"y\",\n    style=\"Method\",\n    hue=\"Method\",\n)\nax.set_title(\"Ball benchmark\")\nax.set_xlabel(\"Num. of balls\")\nax.set_ylabel(\"Time (sec.)\")\nNone\n\n\n\n\n\nこのボールの数だとGPUは線形に計算時間が増えているように見えませんね。 まあ何はともあれ、GPU用にJITコンパイルしてあげると速そうだなあ、という感じがします。\n\n\n3. Jaxで勾配を計算してみる\nJaxは単に速いNumPyとしての機能に加え、自動微分によって、関数\\(f(x, y, z, ...)\\)の各\\(x, y, z,...\\)による偏微分\\(\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}, \\frac{\\partial f}{\\partial z}, ...\\)を計算する機能を持っています。ここではjax.gradによる勾配の計算だけを紹介します。\nなんか、適当に関数を最適化してみましょう。まずは、適当に関数を決めてみます。 \\(z = x^2 + y^2 + y\\) にしました。\n\n\nCode\ndef f(x, y):\n    return x ** 2 + y ** 2 + y\n\n\ndef plot_f(traj: t.Optional[Array] = None) -&gt; None:\n    x, y = np.meshgrid(np.linspace(-5, 5, 100), np.linspace(-5, 5, 100))\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection=\"3d\")\n    ax.plot_surface(\n        x,\n        y,\n        f(x, y),\n        cmap=sns.color_palette(\"flare\", as_cmap=True),\n        alpha=0.8,\n        linewidth=0,\n    )\n    if traj is not None:\n        ax.plot(traj[:, 0], traj[:, 1], traj[:, 2], color=\"blue\")\n    ax.set_xlabel(\"x\", fontsize=14)\n    ax.set_ylabel(\"y\", fontsize=14)\n    ax.set_zlabel(\"z\", fontsize=14, horizontalalignment=\"right\")\n    ax.set_title(\"f\", fontsize=15)\n\n\nplot_f()\n\n\n\n\n\n\\((x, y) = (5, 5)\\)でのこの関数の勾配を計算してみます。勾配を計算してほしい引数をjax.grad(argnums=...)で指定します。\n\n\nCode\njax.grad(f, argnums=(0, 1))(jnp.array(5.0), jnp.array(5.0))\n\n\n(DeviceArray(10., dtype=float32, weak_type=True),\n DeviceArray(11., dtype=float32, weak_type=True))\n\n\n\\(\\frac{\\partial z}{\\partial x}, \\frac{\\partial z}{\\partial y}\\)を計算してくれました。 せっかくなので、最急降下法してみましょう。\n\n\nCode\ndef steepest_descent(alpha: float = 0.01) -&gt; JaxArray:\n    f_grad = jax.grad(f, argnums=(0, 1))\n    x, y = jnp.array(5.0), jnp.array(5.0)\n    traj = []\n    while True:\n        traj.append((x, y, f(x, y)))\n        x_grad, y_grad = f_grad(x, y)\n        if jnp.linalg.norm(jnp.array([x_grad, y_grad])) &lt; 0.05:\n            break\n        x -= alpha * x_grad\n        y -= alpha * y_grad\n    return jnp.array(traj)\n\nplot_f(steepest_descent())\n\n\n\n\n\n最急降下方向に進んでくれているように見えます。 ところで、gradはトップダウン型リバースモード自動微分（誤差逆伝播法の難しい言い方です）を採用しているので、リバースモードでVector Jacobian Productを計算するvjpという関数が使われています。 フォーワードモードで計算するjvpという関数もあります。 このあたりの機能は、ただネットワークを学習させたいだけならほとんど使いませんが、一応やってみましょう。\n\n\nCode\nprimals, f_vjp = jax.vjp(f, 5.0, 5.0)\nprint(f\"VJP value: {primals.item()} grad: {[x.item() for x in f_vjp(1.0)]}\")\nvalue, grad = jax.jvp(f, (5.0, 5.0), (1.0, 1.0))\nprint(f\"JVP value: {value.item()} grad: {grad.item()}\")\n\n\nVJP value: 55.0 grad: [10.0, 11.0]\nJVP value: 55.0 grad: 21.0\n\n\nフォーワードモードの場合勾配となんかのベクトルvとの内積がでてきます。 このあたり、色々な教科書に書いてあると思いますが、Forward modeとReverse modeの違いなど、Probabilistic Machine Learning: An Introductionの13章が特にわかりやすいと思います。興味がある方は参考にしてみてください。\n\n\n4. Braxを使ってみる\nじゃあMuJoCoみたいな物理シミュレーターもJaxで書いてしまえば勝手にGPU上で動いて速いんじゃない？というモチベーションで作られたのがbraxです。 簡単に特徴をまとめてみます。\n\nJaxで記述されているため、jitで高速化できる\nProtocol Bufferでシステムを定義 (cf. MuJoCoはXML）\ndataclassQPを使った簡潔な状態記述\n\nQは正準座標、Pは運動量らしい\n\nOpenAI gym風のEnv APIやAnt・Halfcheetahなどの謎ロボット\n\nおもむろにインストールしてみます。\n\n\nCode\ntry:\n    import brax\nexcept ImportError:\n    !pip install git+https://github.com/google/brax.git@main\n    clear_output()\n    import brax\n\n\nさっきと同じ、ボールを動かしてみましょう。さっきはxy座標で動かしましたが、brax\n\n\nCode\ndef make_ball() -&gt; None:\n    config = brax.Config(dt=0.1, substeps=4)\n    # ボールを追加\n    ball = config.bodies.add(name=\"ball\", mass=1)\n    capsule = ball.colliders.add().capsule\n    capsule.radius = 0.5\n    # y座標に重力\n    config.gravity.y = GRAVITY\n    return config\n\n\ndef make_qp(p, v) -&gt; brax.QP:\n    return brax.QP(\n        pos=jnp.array([[p[0], p[1], 0.0]]),  # position\n        vel=jnp.array([[v[0], v[1], 0.0]]),  # velocity\n        rot=jnp.zeros((1,4)),  # rotation\n        ang=jnp.zeros((1, 3)),  # angular velocity\n    )\n\n\ndef simulate_one_ball_brax(n_steps: int = 100) -&gt; t.List[Array]:\n    sys = brax.System(make_ball())\n    qp = make_qp([0.0, 48.0], [1.0, 0.0])\n    results = []\n    for _ in range(n_steps):\n        qp, _ = sys.step(qp, [])\n        results.append(qp.pos[:2])\n    return results\n\n\nHTML(ball_animation(simulate_one_ball_brax(40)).to_jshtml())\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\nここで、4つのAPIを使いました。 - brax.Configでシステムを定義 - brax.System(config)でシステムを作成 - brax.QPで初期位置・速度・アングル等を作成 - brax.System.step(qp, ...)で1ステップシミュレーションした結果を取得\nボールが一つだとなんとなく物足りないですね。増やしてみましょう。そのためには、jax.vmapでsys.stepをベクトル化します。 デフォルトで、vmapは引数のテンソルに対する演算をaxis=0でバッチ化します。 このあたりはin_axes=(1, 0, ...)とかやれば調節できますが、今回はデフォルトでOKです。\n[make_qp(*pv) for pv in zip(p, v)]で、List[brax.QP]を作っていますが、これをjax.tree_mapでもう一回QPに戻しています。\nList[QP(p=(0, 0), v(0, 0)), QP(..), ...] \nが\nQP(\n    p=[(0, 0), (0.1, 0.2),. ...], \n    v=[(0, 0), (1, 2), ...],\n)\nに変換される感じです。 このジャーゴンは便利なので覚えてもいいと思います。 ちなみに、treemapのノードが葉かどうかはオブジェクトがPyTreeか否かによります。 これは「以上のデータ構造をJaxは暗に木構造だとみなします。不足なら自分で登録してください」という話なので、最初は面食らうと思います。 これを陽なAPIでやろうにするとRustやScalaにあるtraitが必要なので、悪い設計ではないと思いますが。 というわけで、コードはこんな感じになります。\n\n\nCode\ndef simulate_balls_brax(n_balls: int, n_steps: int = 100) -&gt; t.List[Array]:\n    sys = brax.System(make_ball())\n    p = np.random.uniform(size=(n_balls, 2), low=-50, high=50.0)\n    v = np.random.randn(n_balls, 2)\n    qps = [make_qp(*pv) for pv in zip(p, v)]\n    qps = jax.tree_map(lambda *args: jnp.stack(args), *qps)\n    # ここで\n    step_vmap = jax.jit(jax.vmap(lambda qp: sys.step(qp, [])))\n    results = []\n    for _ in range(n_steps):\n        qps, _ = step_vmap(qps)\n        results.append(qps.pos[:, 0, :2])\n    return results\n\nHTML(ball_animation(simulate_balls_brax(20, 40)).to_jshtml())\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\njitを使わないとbraxがなぜかnumpyの関数を呼ぼうとしてエラーになったので、jitも併用しています。\n\n\n5. Haikuで謎のロボットを学習させてみる\nというわけで、braxの使い方をざっと見てみましたが、毎回自分でロボットを考えるのは大変だし査読者にも文句を言われるなので、今回は謎ロボットを学習させてみましょう。 open AI gym風のbrax.envs.Envがサポートされています。今回はAntを訓練してみましょう。 gym.makeに相当するのがbrax.envs.createです。 stepのAPIはgymと違い内部状態・報酬などが入ったbrax.envs.Stateというクラスを渡して次のStateを受け取るというインターフェイスです。\n\n\nCode\nimport brax.envs\n\n\ndef render_html(sys: brax.System, qps: t.List[brax.QP]) -&gt; HTML:\n    import uuid\n    import brax.io.html\n\n    html = brax.io.html.render(sys, qps)\n    # A weired trick to show multiple brax viewers...\n    html = html.replace(\"brax-viewer\", f\"brax-viewer-{uuid.uuid4()}\")\n    return HTML(html)\n\n\ndef random_ant() -&gt; HTML:\n    env = brax.envs.create(env_name=\"ant\")\n    prng_key = jax.random.PRNGKey(0)\n    state = env.reset(prng_key)\n    qps = [state.qp]\n    step_jit = jax.jit(env.step)\n    for i in range(10):\n        prng_key, action_key = jax.random.split(prng_key)\n        action = jax.random.normal(action_key, shape=(env.action_size,))\n        state = step_jit(state, action)\n        qps.append(state.qp)\n    return render_html(env.sys, qps)\n\n\nrandom_ant()\n\n\n\n\n  \n    brax visualizer\n    \n  \n  \n    \n    \n    \n  \n\n\n\nなんか、跳ねていますね。嬉しそう。 では、さっそく学習させてみましょう。 今回は、深層強化学習の代表的な手法であるPPOを使ってみます。 本当はSACも用意したかったのですが、時間がなかったので諦めました。 とりあえず、三層MLPを用意しましょう。 例えば、こんな感じのものがあればいいです。\n\n\nCode\ndef mlp_v1(\n    observation: JaxArray,\n    w1: JaxArray,\n    b1: JaxArray,\n    w2: JaxArray,\n    b2: JaxArray,\n    w3: JaxArray,\n    b3: JaxArray,\n) -&gt; JaxArray:\n    x = jnp.dot(observation, w1) + b1\n    x = jnp.tanh(x)\n    x = jnp.dot(x, w2) + b2\n    x = jnp.tanh(x)\n    return jnp.dot(x, w3) + b3\n\n\nこれをjitしてgradをとってAdamか何かでパラメタを更新して…とやればMLPが動くわけですが、パラメタが多すぎてちょっと面倒ですね。 そこでここでは、jaxでニューラルネットワークを訓練する際、パラメタの管理などをやってくれるライブラリであるHaikuを使ってみます。 なお、brax公式のexamplesではFlaxを使っています。 正直HaikuもFlaxもそこまで変わらないのですが、Flaxの方がややAPIの押しが強い（PyTorchでいうnn.Module相当のものがdataclassでないといけなかったりとか）印象があります。 また、HaikuはDeepmindが、FlaxはGoogleが開発しているライブラリとなります。 とりあえずインストールしてみましょう。\n\n\nCode\ntry:\n    import haiku as hk\n    import optax\n    import chex\n    import distrax\nexcept ImportError as e:\n    ! pip install git+https://github.com/deepmind/dm-haiku \\\n        git+https://github.com/deepmind/optax \\\n        git+https://github.com/deepmind/chex \\\n        git+https://github.com/deepmind/distrax\n    \n    import haiku as hk\n    import optax\n    import chex\n    import distrax\n    \n    clear_output()\n\n\nニューラルネットワークを定義するためのPythonライブラリはtheano、tensorflowと色々ありましたが、最近はtorch.nn.Moduleやchainer.Linkのように、ネットワークの重み・forwardの出力・隣接しているノードを記録したオブジェクトを使って、動的に計算グラフを構築するものが多いかと思います。 しかし、Haikuによるそれは少し異なります。ポイントは、勾配を計算する部分はJaxが担当するので、Haikuはただ「ネットワークのパラメタを管理するだけ」でいいということです。 そのために、HaikuはtransformというAPIを用意しています。 これは見たほうが早いでしょう。\n\n\nCode\ndef mlp_v2(observation: JaxArray) -&gt; JaxArray:\n    w1 = hk.get_parameter(\"w1\", shape=[observation.shape[1], 3], init=jnp.ones)\n    b1 = hk.get_parameter(\"b1\", shape=[3], init=jnp.zeros)\n    w2 = hk.get_parameter(\"w2\", shape=[3, 3], init=jnp.ones)\n    b2 = hk.get_parameter(\"b2\", shape=[3], init=jnp.zeros)\n    w3 = hk.get_parameter(\"w3\", shape=[3, 2], init=jnp.ones)\n    b3 = hk.get_parameter(\"b3\", shape=[2], init=jnp.zeros)\n    x = jnp.dot(observation, w1) + b1\n    x = jnp.tanh(x)\n    x = jnp.dot(x, w2) + b2\n    x = jnp.tanh(x)\n    return jnp.dot(x, w3) + b3\n\nprng_seq = hk.PRNGSequence(0)  # これをグローバル変数にするのは良くないです。真似しないで\ninit, apply = hk.transform(mlp_v2)  # transformする\n# initは乱数シード・インプットを受け取って、初期化したパラメタを返す関数\nparams = init(next(prng_seq), jnp.zeros((10, 2)))\nprint(params)\n# applyはパラメタ・乱数シード・インプットを受け取って、出力を返す関数\noutput = apply(params, next(prng_seq), jnp.zeros((10, 2)))\n\n\nFlatMap({\n  '~': FlatMap({\n         'w1': DeviceArray([[1., 1., 1.],\n                            [1., 1., 1.]], dtype=float32),\n         'b1': DeviceArray([0., 0., 0.], dtype=float32),\n         'w2': DeviceArray([[1., 1., 1.],\n                            [1., 1., 1.],\n                            [1., 1., 1.]], dtype=float32),\n         'b2': DeviceArray([0., 0., 0.], dtype=float32),\n         'w3': DeviceArray([[1., 1.],\n                            [1., 1.],\n                            [1., 1.]], dtype=float32),\n         'b3': DeviceArray([0., 0.], dtype=float32),\n       }),\n})\n\n\nこんな感じになります。 まとめると、 - transform(f)は二つの関数init、applyをかえす - transformはfを、fの中でhaiku.get_parameterを使って呼び出されたパラメタを入力とする関数に変換する - initはパラメタを初期化して返す。パラメタはFlatMapというオブジェクトだがこれはほとんどdictと同じ - applyは与えられたパラメタを使って所望の計算を行う という感じですね。\nついでに、上の例ではhk.PRNGSequenceというPRNGKeyの更新を勝手にやってくれるものを使っています。\nしかし、これでもまだ面倒ですね。 実際のところ、よく使うネットワークはモジュールとしてまとまっているので、これを使えばいいです。\n\n\nCode\ndef mlp_v3(output_size: int, observation: JaxArray) -&gt; JaxArray:\n    x = hk.Linear(64)(observation)\n    x = jnp.tanh(x)\n    x = hk.Linear(64)(observation)\n    x = jnp.tanh(x)\n    return hk.Linear(output_size)(observation)\n\n\nこれを使って、PPOのネットワークを書いてみましょう。 方策は標準偏差が状態に依存しない正規分布にします。\n\n\nCode\nclass NetworkOutput(t.NamedTuple):\n    mean: JaxArray\n    stddev: JaxArray\n    value: JaxArray\n\n\ndef policy_and_value(action_size: int, observation: JaxArray) -&gt; NetworkOutput:\n    mean = mlp_v3(output_size=action_size, observation=observation)\n    value = mlp_v3(output_size=1, observation=observation)\n    logstd = hk.get_parameter(\"logstd\", (1, action_size), init=jnp.zeros)\n    stddev = jnp.ones_like(mean) * jnp.exp(logstd + 1e-8)\n    return NetworkOutput(mean, stddev, value)\n\n\nこれだけです。デフォルトでは、ネットワークの重みはTruncatedNormalで初期化されます。今回は全部デフォルトのままにしました。\n次に、これを使って、環境とインタラクトするコードを書いてみます。 いま、braxの利点を活かすために、 1. ネットワークから次のアクションをサンプルして 2. シミュレータで次の状態をシミュレート という過程をすべてjax.jitの中でやるのが理想ですよね。\nですから、たとえばこんな感じにやればいいです。\n\n\nCode\nAction = JaxArray\n\n\ndef make_step_function(\n    env: brax.envs.Env,\n) -&gt; t.Tuple[t.Callable[..., t.Any], t.Callable[..., t.Any]]:\n    def step(state: brax.envs.State) -&gt; t.Tuple[brax.envs.State, NetworkOutput, Action]:\n        out = policy_and_value(env.action_size, state.obs)\n        policy = distrax.MultivariateNormalDiag(out.mean, out.stddev)\n        action = policy.sample(seed=hk.next_rng_key())  # transformするとこれが使えます\n        state = env.step(state, jnp.tanh(action))\n        return state, out, action\n\n    init, apply = hk.transform(step)\n    return jax.jit(init), jax.jit(apply)\n\n\nここで、行動のサンプルにはdistraxというライブラリを使いました。 平均値にノイズをいれるだけなので、ライブラリを使ってもあまり変わらないのですが…。 いま、各ジョイントに対して加える力が、それぞれ独立な正規分布からサンプリングされると仮定しているので、MutliVariateNormDiag(共分散行列が対角行列になる多変量正規分布）を使ってモデリングしています。 distrax.Independentとdistrax.Normalを使っても同じことができます。 行動は一応tanhで\\([-1, 1]\\)の範囲にならしています。\nちょっと試してみましょう。\n\n\nCode\nant = brax.envs.create(env_name=\"ant\", batch_size=1)\ninit, step = make_step_function(ant)\ninitial_state = jax.jit(ant.reset)(next(prng_seq))\nparams = init(next(prng_seq), initial_state)\n_next_state, out, action = step(params, next(prng_seq), initial_state)\n# chexはテスト用のライブラリです\nchex.assert_shape((out.mean, out.stddev, action), (1, ant.action_size))\n\n\nというわけで無事にstepをJITコンパイルして高速化できました。 resetはほとんど呼ばないので別にコンパイルしなくてもいいのですが、jitしないとbraxがjnp.DeviceArrayのかわりにnumpyを使いたがって少し面倒なのでjitしています。\nあとはPPOを実装していきますが、時間の都合で手短かにいきます。 まずはGAEですね。 普通に書くとjax.jitがループアンローリングを行ってコンパイル時間が激遅になるので、jax.lax.fori_loopという黒魔術を使います。 コンパイル時定数はstatic_argnumsで指定します。 vmapで各ワーカー用に並列化します。\n\n\nCode\nimport functools\n\n@functools.partial(jax.jit, static_argnums=2)\ndef gae(\n    r_t: JaxArray,\n    discount_t: JaxArray,\n    lambda_: float,\n    values: JaxArray,\n) -&gt; chex.Array:\n    chex.assert_rank([r_t, values, discount_t], 1)\n    chex.assert_type([r_t, values, discount_t], float)\n    lambda_ = jnp.ones_like(discount_t) * lambda_\n    delta_t = r_t + discount_t * values[1:] - values[:-1]\n    n = delta_t.shape[0]\n\n    def update(i: int, advantage_t: JaxArray) -&gt; JaxArray:\n        t_ = n - i - 1\n        adv_t = delta_t[t_] + lambda_[t_] * discount_t[t_] * advantage_t[t_ + 1]\n        return jax.ops.index_update(advantage_t, t_, adv_t)\n\n    advantage_t = jax.lax.fori_loop(0, n, update, jnp.zeros(n + 1))\n    return advantage_t[:-1]\n\n\nbatched_gae = jax.vmap(gae, in_axes=(1, 1, None, 1), out_axes=1)\n\n\nなんかブログで書くには黒魔術すぎる気もしますが…。\n次に学習データのバッチを構成する部分ですね。 これは、普通にPyTorchとかと変わらないです。\n\n\nCode\nimport dataclasses\n\n\n@chex.dataclass\nclass RolloutResult:\n    \"\"\"\n    Required experiences for PPO.\n    \"\"\"\n\n    observations: t.List[JaxArray]\n    actions: t.List[JaxArray] = dataclasses.field(default_factory=list)\n    rewards: t.List[JaxArray] = dataclasses.field(default_factory=list)\n    terminals: t.List[JaxArray] = dataclasses.field(default_factory=list)\n    outputs: t.List[NetworkOutput] = dataclasses.field(default_factory=list)\n\n    def append(\n        self,\n        *,\n        observation: JaxArray,\n        action: JaxArray,\n        reward: JaxArray,\n        output: NetworkOutput,\n        terminal: JaxArray,\n    ) -&gt; None:\n        self.observations.append(observation)\n        self.actions.append(action)\n        self.rewards.append(reward)\n        self.outputs.append(output)\n        self.terminals.append(terminal)\n\n    def clear(self) -&gt; None:\n        self.observations = [self.observations[-1]]\n        self.actions.clear()\n        self.rewards.clear()\n        self.outputs.clear()\n        self.terminals.clear()\n\n\nclass Batch(t.NamedTuple):\n    \"\"\"Batch for PPO, also used as minibatch by indexing.\"\"\"\n\n    observation: JaxArray\n    action: JaxArray\n    reward: JaxArray\n    advantage: JaxArray\n    value_target: JaxArray\n    log_prob: JaxArray\n\n    def __getitem__(self, idx: Array) -&gt; \"Batch\":\n        return self.__class__(\n            observation=self.observation[idx],\n            action=self.action[idx],\n            reward=self.reward[idx],\n            advantage=self.advantage[idx],\n            value_target=self.value_target[idx],\n            log_prob=self.log_prob[idx],\n        )\n\n\n@jax.jit\ndef make_batch(rollout: RolloutResult, next_value: JaxArray) -&gt; Batch:\n    action = jnp.concatenate(rollout.actions)\n    mean, stddev, value = jax.tree_map(lambda *x: jnp.concatenate(x), *rollout.outputs)\n    log_prob = distrax.MultivariateNormalDiag(mean, stddev).log_prob(action)\n    reward = jnp.stack(rollout.rewards)\n    mask = 1.0 - jnp.stack(rollout.terminals)\n    value = jnp.concatenate(\n        (value.reshape(reward.shape), next_value.reshape(1, -1)),\n        axis=0,\n    )\n    advantage = batched_gae(reward, mask * 0.99, 0.95, value)\n    value_target = advantage + value[:-1]\n    return Batch(\n        observation=jnp.concatenate(rollout.observations[:-1]),\n        action=action,\n        reward=jnp.ravel(reward),\n        advantage=jnp.ravel(advantage),\n        value_target=jnp.ravel(value_target),\n        log_prob=log_prob,\n    )\n\n\n普通のdataclassesはjitできないので、chex.dataclassを使います。 さっき少しだけ触れましたが、chex.dataclassは作成したdataclassをPyTreeとしてjaxに登録してくれます。 実はflax.struct.dataclassというだいたい同じものもあって、braxの内部ではこれを使っているようです。 また\\(\\gamma = 0.99, \\lambda = 0.95\\)としました。\nいよいよ学習の部分ですね。 まず、損失関数をjax.gradできるように書きます。\n\n\nCode\ndef ppo_loss(action_size: int, batch: Batch) -&gt; JaxArray:\n    mean, stddev, value = policy_and_value(action_size, batch.observation)\n    # Policy loss\n    policy = distrax.MultivariateNormalDiag(mean, stddev)\n    log_prob = policy.log_prob(batch.action)\n    prob_ratio = jnp.exp(log_prob - batch.log_prob)\n    clipped_ratio = jnp.clip(prob_ratio, 0.8, 1.2)\n    clipped_obj = jnp.fmin(prob_ratio * batch.advantage, clipped_ratio * batch.advantage)\n    policy_loss = -jnp.mean(clipped_obj)\n    # Value loss\n    value_loss = jnp.mean(0.5 * (value - batch.value_target) ** 2)\n    # Entropy regularization\n    entropy_mean = jnp.mean(policy.entropy(), axis=-1)\n    return policy_loss + value_loss - 0.001 * entropy_mean\n\n\n\\(\\epsilon = 0.2\\)で固定しているので、\\([1 - 0.2, 1 + 0.2]\\)の範囲でクリップします。\nではこれを使って、今度はパラメタの更新を全部jitにつっこんでみましょう。 パラメタの更新にはoptaxというライブラリを使います。色々なSGDのバリアントを実装しているライブラリですが、僕はほとんどAdamしか使いません…。\n\n\nCode\nimport optax\n\n\ndef make_update_function(\n    action_size: int,\n    opt_update: optax.TransformUpdateFn,\n) -&gt; t.Callable[..., t.Any]:\n    # hk.Paramsを使い回すのでinitは捨てていい\n    # 行動をサンプルしないので、without_apply_rngが使える\n    _, loss_fn = hk.without_apply_rng(hk.transform(lambda batch: ppo_loss(action_size, batch)))\n    grad_fn = jax.grad(loss_fn)\n\n    # ここでjitしていい\n    @jax.jit\n    def update(\n        params: hk.Params,\n        opt_state: optax.OptState,\n        batch: Batch,\n    ) -&gt; t.Tuple[hk.Params, Batch]:\n        grad = grad_fn(params, batch)\n        updates, new_opt_state = opt_update(grad, opt_state)\n        return optax.apply_updates(params, updates), new_opt_state\n\n    return update\n\n\nさて、ここまで来たらあと一歩ですね。 次に面倒ですが次の状態のvalueをとってきてバッチを作る部分を書きます。\n\n\nCode\ndef make_next_value_function(action_size: int) -&gt; Batch:\n    def next_value_fn(obs: JaxArray) -&gt; JaxArray:\n        output = policy_and_value(action_size, obs)\n        return output.value\n\n    _, next_value_fn = hk.without_apply_rng(hk.transform(next_value_fn))\n    return jax.jit(next_value_fn)\n\n\n速度を求めるなら、これはmake_batchと一緒にjitしてしまってもいいですが、まあ面倒なのでこれでもいいでしょう。\nでは材料がそろったのでメインループを書いていきましょう。 面倒ですが、評価用のenvironmentも別に作ります。\n\n\nCode\ntry:\n    import tqdm\nexcept ImportError as _e:\n    ! pip install tqdm\n    import tqdm\n    clear_output()\n\n\n\n\nCode\nimport datetime\n\nfrom tqdm.notebook import trange\n\n\ndef sample_minibatch_indices(\n    n_instances: int,\n    n_minibatches: int,\n    prng_key: chex.PRNGKey,\n) -&gt; t.Iterable[JaxArray]:\n    indices = jax.random.permutation(prng_key, n_instances)\n    minibatch_size = n_instances // n_minibatches\n    for start in range(0, n_instances, minibatch_size):\n        yield indices[start : start + minibatch_size]\n\n\ndef train_ppo(\n    env_name: str = \"ant\",\n    n_workers: int = 32,\n    n_steps: int = 2048,\n    n_training_steps: int = 10000000,\n    n_optim_epochs: int = 10,\n    n_minibatches: int = 64,\n    eval_freq: int = 20,\n    eval_workers: int = 16,\n    seed: int = 0,\n) -&gt; HTML:\n    # 環境と、環境を含んだstep関数を作る\n    env = brax.envs.create(env_name=env_name, episode_length=1000, batch_size=n_workers)\n    eval_env = brax.envs.create(\n        env_name=env_name,\n        episode_length=1000,\n        batch_size=eval_workers,\n    )\n    network_init, step = make_step_function(env)\n    _, eval_step = make_step_function(eval_env)\n    eval_reset = jax.jit(eval_env.reset)\n    # 乱数\n    prng_seq = hk.PRNGSequence(seed)\n    # 初期状態\n    state = jax.jit(env.reset)(rng=next(prng_seq))\n    rollout = RolloutResult(observations=[state.obs])\n    # Optimizerとパラメタを初期化する\n    optim = optax.chain(optax.clip_by_global_norm(1.0), optax.adam(3e-4, eps=1e-4))\n    update = make_update_function(env.action_size, optim.update)\n    params = network_init(next(prng_seq), state)\n    opt_state = optim.init(params)\n    # next_value\n    next_value_fn = make_next_value_function(env.action_size)\n    n_instances = n_workers * n_steps\n\n    def evaluate(step: int) -&gt; None:\n        eval_state = eval_reset(rng=next(prng_seq))\n        return_ = jnp.zeros(eval_workers)\n        done = jnp.zeros(eval_workers, dtype=bool)\n        for _ in range(1000):\n            eval_state, _, _ = eval_step(params, next(prng_seq), eval_state)\n            return_ = return_ + eval_state.reward * (1.0 - done)\n            done = jnp.bitwise_or(done, eval_state.done.astype(bool))\n        print(f\"Step: {step} Avg. ret: {jnp.mean(return_).item()}\")\n\n    for i in trange(n_training_steps // n_instances):\n        for _ in range(n_steps):\n            state, output, action = step(params, next(prng_seq), state)\n            rollout.append(\n                observation=state.obs,\n                action=action,\n                reward=state.reward,\n                output=output,\n                terminal=state.done,\n            )\n        next_value = next_value_fn(params, state.obs)\n        batch = make_batch(rollout, next_value)\n        rollout.clear()\n        # Batchを作ったので、ミニバッチサンプリングして学習\n        for _ in range(n_optim_epochs):\n            for idx in sample_minibatch_indices(\n                n_instances,\n                n_minibatches,\n                next(prng_seq),\n            ):\n                minibatch = batch[idx]\n                params, opt_state = update(params, opt_state, minibatch)\n\n        # 時々評価する\n        if (i + 1) % eval_freq == 0:\n            evaluate(i + 1)\n\n    evaluate(i + 1)\n    # Visualize\n    eval_state = eval_reset(rng=next(prng_seq))\n    qps = []\n    while eval_state.done[0] == 0.0:\n        eval_state, _, _ = eval_step(params, next(prng_seq), eval_state)\n        qps.append(jax.tree_map(lambda qp: qp[0], eval_state.qp))\n    return render_html(eval_env.sys, qps)\n\n\nstart_time = datetime.datetime.now()\nhtml = train_ppo()\nelapsed = datetime.datetime.now() - start_time\nprint(f\"Train completed after {elapsed.total_seconds() / 60:.2f} min.\")\nhtml\n\n\n\n\n\nStep: 20 Avg. ret: -286.34039306640625\nStep: 40 Avg. ret: -273.5491943359375\nStep: 60 Avg. ret: -193.0821990966797\nStep: 80 Avg. ret: -84.20954132080078\nStep: 100 Avg. ret: -45.654090881347656\nStep: 120 Avg. ret: -20.323640823364258\nStep: 140 Avg. ret: -42.74524688720703\nStep: 152 Avg. ret: -5.514527320861816\nTrain completed after 41.34 min.\n\n\n\n\n  \n    brax visualizer\n    \n  \n  \n    \n    \n    \n  \n\n\n\n100万ステップの訓練が41分で終わりました。速いですねやっぱり。 なんか前に跳ねすぎている微妙なのがビジュアライズされていますが…。\n\n\n6. まとめ\nというわけで、このブログではJax、Brax、Haikuを使って、GPUだけでシミュレータ上のロボットを訓練する例を示しました。 かなり駆け足の解説になりましたが、なんとなくプログラムの組み方を理解していただけたのではないかと思います。\n総括すると、Jaxはかなり広い範囲のNumPy演算をGPU/TPU上で高速に動作するコードに変換できる、非常に強力なライブラリです。 今回紹介したvmapは、例えば一つのGPU上で演算を並列化する機能ですが、他にもpmapによるデバイスをまたいだ並列化もできます。 ですから特に、 - ✔ CPUとGPUの通信オーバーヘッドが気になるとき - ✔ 大規模に並列なシミュレーションを行いたいとき\nは、Jaxが効果を発揮すると思います。また、jax.lax.fori_loopを使って - ✔ CythonやC++/Rustなど他の言語を使わずにPythonのループを高速化したいとき\nにも使えます。 一方で、単に深層学習を高速化したい場合、例えば - 🔺 PyTorchモデルの訓練がボトルネックになっている場合\nなどは、JaxやHaiku/Flaxを使うことによる高速化の恩恵はあまりないと思います。 うまくJitを使えばJaxの方が速いと思いますが、PyTorchのCUDAコードはかなり速いですからね。また、PyTorchと比較した際、 - 🔺 学習コストについてもJaxの方が大きい\nのではないかと思います。 なので、個人的には学習以外の部分でベクトル並列化・Jitコンパイルによる並列化の余地がある場合に、Jaxは便利に使えるのかなあと思います。 ただDeepmindはAlphaFold2を始め、多くのJax+Haiku製深層学習コードをリリースしていますし、一応読める程度に親しんでおくだけでもある程度のメリットはあると思います。\nさて、冒頭の大規模にシミュレーションしてSim2Realを頑張るのか、実機のデータで頑張るのかという話に戻りますが、シミュレーションをスケールさせたいのであれば大規模にシミュレーションしたいならBraxのように「シミュレーターをJaxでコンパイルできるように作る」というアプローチは面白いと思います。分子動力学計算など、物理演算以外のシミュレーションへの活用も期待されます（と聞いたことがあります。僕は分子動力学計算が何なのかよくわかりません…）。 一方で、シミュレータが微分可能であるという利点をどう活かすのかも興味深いテーマです。僕も以前PFNさんのインターンで、報酬が微分可能なシミュレータを使って、\\(\\sum_{t = 0}^T\\frac{\\partial r_t}{\\partial \\theta}\\)についての山登り法で方策を更新するのを試したことがあるのですが、報酬が遠いとなかなか難しいなあという印象でした。うまい方法があればいいのですが…。意外と勾配降下だけでなく進化計算などのメタヒューリスティクスと組み合わせると面白いかもしれないです。\nさて、僕はもう一つアドベントカレンダーの記事を書く予定があったのですが、時間がないので他の人に代わってもらうかもしれません…。出たらそちらもよろしくお願いします。"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html",
    "href": "posts/understanding-what-makes-rl-difficult.html",
    "title": "より良い問題設計へ向けて： 何が強化学習を難しくするのかを理解しよう",
    "section": "",
    "text": "強化学習苦手の会 Advent Calendar 2020 20日目 # 1. はじめに\n強化学習は、逐次的に意思決定する問題を定義するためのフレームワークです。 従来は大変だった1 ニューラルネットワークの訓練が簡単になったことや、 Alpha GOなど深層強化学習(Deep RL)の成功を背景に、対話システム・経済 など、様々なアプリケーションで強化学習の利用が試みられているように思います。 僕個人としても、強化学習は汎用的で面白いツールだと思うので、将来的には色々な応用分野で広く活用されるといいな、と思います。\n一方で、強化学習を何か特定の問題に応用してみよう、という場面では、 その汎用性ゆえかえってとっつきにくい・扱いづらい面があるように思います。 実際に、苦手の会などで応用研究をされている方から、 - 問題を定義するのがそもそも大変 - 色々な手法があって、何がなんだかよくわからない\nなどの意見を観測できました。\nでは応用研究に対する「ツール」として強化学習を扱う上で何が大事なのだろう、と考えたとき、 僕は簡単な問題を設計することこそが大事だという仮説に思いいたりました。 簡単な問題を設計するためには、強化学習の中でもどういう問題が難しいのか、 ということをきちんと理解しておく必要があるように思います。\nそこでこのブログ記事では、強化学習の中でも「難しい問題」がどういうものなのか、 そういう問題はなぜ難しいのかについて、例を通してなるべく直感的に説明することを試みます。 強化学習の難しさがわかった暁には、きっと - そもそも強化学習を使わないという選択ができるし、 - なるべく簡単に解けるような強化学習の問題を設計できるし、 - 問題に合わせて手法を選択できる\nことでしょう。\n記事の構成として、強化学習の難しさについて「場合分け」を行い、 - MDPを解くことの難しさ - データを収集することの難しさ\nという2つの観点から整理していきます。\n前提知識について、初心者の方でも読めるように、 強化学習についての知識についてはなるべく記事の中で補足します。 しかし、すごく雑に書くので、詳細はReinforcement Learning: An Introduction などの教科書を参照されるといいと思います。 また、プログラムを見たほうがイメージしやすい（方もいる）かと思ってPythonのコード例をたまに出しています。 コード例では、\\(\\sum_{s} f(s) g(s, a)\\)のようにテンソルの適当な軸で掛け算して足し込む演算に numpy.einsum を多用しているので、知っていたほうが読みやすいかもしれません。"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html#vpicdot-vを評価する難しさ",
    "href": "posts/understanding-what-makes-rl-difficult.html#vpicdot-vを評価する難しさ",
    "title": "より良い問題設計へ向けて： 何が強化学習を難しくするのかを理解しよう",
    "section": "2.1: \\(V^\\pi\\cdot V^*\\)を評価する難しさ",
    "text": "2.1: \\(V^\\pi\\cdot V^*\\)を評価する難しさ\nでは、もう少し難しいMDPを考えてみましょう。\n\n\nCode\nmdp3 = ChainMDP(\n    [[1.0, 0.0], [0.8, 1.0], [1.0, 0.9]], [[0.0, 0.0], [0.5, 0.0], [0.0, 1.0]]\n)\n_ = mdp3.show(\"MDP3\")\n\n\n\n\n\n今度は、State 1で右に、State 2で左に行けば良さそうです。 \\[\n\\begin{aligned}\nV^* (1) = 0.5 + \\gamma (0.1 * V^*(1) + 0.9 * V^*(2)) \\\\\nV^* (2) = 1.0 + \\gamma (0.8 * V^*(1) + 0.2 * V^*(2))\n\\end{aligned}\n\\]\n先ほどの問題と違って1も2も吸引状態ではないので、\\(V(1)\\)と\\(V(2)\\)がお互いに依存する面倒な 方程式が出てきてしまいました。 このようなループの存在が、強化学習を難しくしている要素の一つです。\nとはいえ、コンピューターで数値的に解くのは簡単です。 状態\\(s\\)にいて、あと\\(n\\)回行動できる時の価値関数を\\(V_n^\\pi(s)\\)と書きます。 任意の\\(s\\)について、\\(V_0^\\pi(s) = 0\\)です（1回も行動できないので!）。 \\(V_i^\\pi\\) から \\(V_{i + 1}^\\pi\\) を求めるには、1ステップだけ先読みすればいいので、 \\[\nV_{i + 1}^\\pi(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s) \\left( r(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s'|s, a) V_{i}^\\pi(s') \\right)\n\\] で計算できます。\\(\\gamma &lt; 1\\)によりこの反復計算は収束し、\\(V^\\pi\\) が求まります。 実際にプログラムで書いてみましょう。\n\n\nCode\nMAX_ITER_V_PI: int = int(1e5)\n\n\ndef v_pi(\n    r: Array2,\n    p: Array3,\n    pi: Array2,\n    gamma: float,\n    epsilon: float,\n) -&gt; Tuple[Array1, int]:\n    v = np.zeros(r.shape[0])  # Vπ\n    r_pi = np.einsum(\"sa,sa-&gt;s\", pi, r)  # |S|, πを使ったときに貰う報酬のベクトル\n    p_pi = np.einsum(\"saS,sa-&gt;sS\", p, pi)  # |S| x |S|, πを使ったときの状態遷移確率\n    for n_iter in range(MAX_ITER_V_PI):\n        v_next = r_pi + gamma * np.einsum(\"s,sS\", v, p_pi.T)\n        if np.all(np.absolute(v_next - v) &lt; epsilon):\n            return v_next, n_iter + 1\n        v = v_next\n    # 理論的には必ず収束するので、バグ予防\n    raise RuntimeError(\"Policy Evaluation did not converge &gt;_&lt;\")\n\n\npi_star = np.array([[1.0, 0.0], [1.0, 0.0], [0.0, 1.0]])\nv_star_mdp3, n_iter = v_pi(mdp3.r, mdp3.p, pi_star, gamma=0.9, epsilon=1e-4)\nprint(f\"反復回数: {n_iter}\")\nprint(\" \".join([f\"V({i}): {v:.3}\" for i, v in enumerate(v_star_mdp3)]))\n\n\n反復回数: 86\nV(0): 6.49 V(1): 7.21 V(2): 7.51\n\n\n86回この計算を反復した後、なんかそれっぽい数字が出てきました。 この反復回数は、何によって決まるのでしょうか？\n任意の \\(s\\) について \\(|V_{i+1}^\\pi(s) - V_i^\\pi(s)| &lt; \\epsilon\\) なら計算終わり、とします4。 \\(V_n^\\pi(s)\\)は「あと\\(n\\)ステップ行動できる時の状態価値の期待値」なので、\\(i\\) ステップ目にもらった報酬を \\(R_i\\)とすると、 \\[\nV_n^\\pi(s) = \\mathbb{E}_{s, \\pi} \\left[ R_1 + \\gamma R_2 + \\gamma^2 R_3 + ... \\right]\n\\] と書けます。 なので、報酬の範囲が\\(0 \\leq R_t &lt; R_\\textrm{max}\\)だと仮定すると、 \\(\\gamma^{k - 1} R_\\textrm{max} &lt; \\epsilon\\)ならこの数値計算が収束することがわかります。 簡単のため\\(R_\\textrm{max}=1\\)としてみると、\\(k\\)が満たすべき条件は \\[\n\\gamma^{k-1} &lt; \\epsilon\n\\Leftrightarrow\nk &lt; \\frac{\\log\\epsilon}{\\log\\gamma} + 1\n\\] となります。 コードの中で \\(\\gamma = 0.9, \\epsilon = 0.0001\\) としたので、たかだか89回の反復で収束することがわかります。 実験結果では86回だったので、だいたい同じくらいですね。\nよって、\\(V^\\pi\\)を反復法により評価した時、その反復回数は報酬・\\(\\epsilon\\)・\\(\\gamma\\)に依存することがわかりました。 報酬と\\(\\epsilon\\)には\\(\\log\\)のオーダーでしか依存しないのに対し、\\(\\gamma\\)に対しては \\(O((-\\log\\gamma)^{-1})\\)のオーダーで依存していることに注意してください。 試しに、\\(\\epsilon=0.0001\\)の時の\\(\\frac{\\log\\epsilon}{\\log\\gamma}\\)をプロットしてみましょう。\n\n\nCode\n#hide_input\n_, ax = plt.subplots(1, 1)\nx = np.logspace(-0.1, -0.001, 1000)\ny = np.log(1e-4) / np.log(x)\nax.set_xlabel(\"γ\", fontsize=16)\nax.set_ylabel(\"log(ε)/log(γ)\", fontsize=16)\n_ = ax.plot(x, y, \"b-\", lw=3, alpha=0.7)\n\n\n\n\n\nこのように、\\(\\gamma\\)が大きくなると一気に反復回数が増えることがわかります。 また、この数値計算が収束した時、真の\\(V^\\pi\\)との差が \\[\n\\begin{aligned}\nV^\\pi(s) - V_k^\\pi(s) &= \\mathbb{E}_{s, \\pi} \\left[ \\gamma^k R_{k + 1} + \\gamma^{k + 1} R_{k + 2} ... \\right] \\\\\n&&lt; \\frac{\\gamma^k R_\\textrm{max}}{1 - \\gamma} &lt; \\frac{\\gamma \\epsilon}{1 - \\gamma}\n\\end{aligned}\n\\] で抑えられることもわかります。\n次は、いきなり\\(V^*\\)を求めてみましょう。 \\(V^\\pi\\)を求めた時と同じように、状態\\(s\\)にいて、 あと\\(n\\)回行動できる時の最適価値関数を\\(V_n^*(s)\\)と書きます。 先ほどと同様に、\\(V_i^*\\)から1ステップ先読みして\\(V^*_{i + 1}\\)を求めます。 残り\\(i + 1\\)ステップある時、 \\(r(s, a) + \\sum_{s' \\in \\mathcal{S}} P(s'|s, a) V_i^*(s)\\) が最大になるような行動\\(a\\)を選ぶのが最適です。 ですから、\\(V^*_{i + 1}\\)は \\[\nV_{i + 1}^*(s) = \\max_a \\left( r(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s'|s, a) V_{i}^\\pi(s') \\right)\n\\] で求められます。 さっきより簡単な式になってしまいました。 プログラムで書いてみます。\n\n\nCode\nMAX_ITER_VI: int = int(1e6)\n\n\ndef value_iteration(\n    r: Array2,\n    p: Array3,\n    gamma: float,\n    epsilon: float,\n) -&gt; Tuple[Array1, int]:\n    v = np.zeros(p.shape[0])\n    for n_iter in range(MAX_ITER_VI):\n        # これ↓はQ Valueとも言います\n        r_plus_gamma_pv = r + gamma * np.einsum(\"saS,S-&gt;sa\", p, v)\n        v_next = r_plus_gamma_pv.max(axis=1)\n        if np.all(np.absolute(v_next - v) &lt; epsilon):\n            return v_next, n_iter + 1\n        v = v_next\n    raise RuntimeError(\"Value Iteration did not converge &gt;_&lt;\")\n\n\nv_star_mdp3_vi, n_iter = value_iteration(mdp3.r, mdp3.p, 0.9, 1e-4)\nprint(f\"反復回数: {n_iter}\")\nprint(\" \".join([f\"V({i}): {v:.3}\" for i, v in enumerate(v_star_mdp3_vi)]))\n\n\n反復回数: 86\nV(0): 6.49 V(1): 7.21 V(2): 7.51\n\n\n先程と同じく、86回の反復で\\(V^*\\)が求まりました。 この反復回数も、先ほどの\\(V^\\pi\\)と同じように\\(\\gamma,\\epsilon\\)を用いて抑えられます。\nしかし、\\(\\epsilon\\)は人手で設定するパラメタです。 最適方策が求まれば\\(V^*\\)は大して正確でなくとも困らないという場合は、もっと\\(\\epsilon\\)を大きくして、 計算を早く終わらせたい気がします。 では、「どんな場合に\\(\\epsilon\\)を大きくできるか」を考えてみましょう。\n簡単のため、 \\(Q^\\pi(s, a) = r(s, a) + \\gamma \\sum_{s' in \\mathcal{S}} P(s'|s, a) V^\\pi(s')\\) (QはQualityのQらしい)を導入します。 残り\\(k\\)ステップある時の最適行動を\\(a_k^* = \\textrm{argmax}_a Q_k^*(s, a)\\)とします。 すると、\\(k+1\\)ステップ目以降の割引報酬和は \\(\\frac{\\gamma^{k}R_\\textrm{max}}{1 -\\gamma}\\)で抑えられるので、 \\[\nQ_k^*(s, a_k^*) - \\max_{a \\neq a_k^*} Q_k^*(s, a) &lt; \\frac{\\gamma^k R_\\textrm{max}}{1 -\\gamma}\n\\] が成り立つなら、\\(a_k^*\\)が今後他の行動に逆転されることはありません。 なので\\(a_k^*\\)が最適でいいよね、はいこの話終わり、ということになります。 以下略記して \\(A_\\textrm{min}^*(s, a_k) = Q_k^*(s, a_k^*) - \\max_{a \\neq a_k^*} Q_k^*(s, a)\\) と書きます（他の行動に対するアドバンテージの最小値という意味）。 いま\\(\\gamma^{k-1} R_\\textrm{max}&lt;\\epsilon\\)が終了条件なので、 \\[\nA_\\textrm{min}^*(s, a_k) &lt; \\frac{\\epsilon\\gamma}{1 -\\gamma}\n\\Leftrightarrow\n\\frac{A_\\textrm{min}^*(s, a_k)(1 - \\gamma)}{\\gamma}&lt; \\epsilon\n\\] が成り立ちます。 これが意味するのは、\\(V*\\)と二番目にいい\\(Q^*(s, a)\\)との差が大きいほど\\(\\epsilon\\)を大きくできるということです。\nここまでの議論から、計算量の観点では、 - \\(\\gamma\\)が大きいほどMDPを解くのが難しい - 最適解と二番目にいい解との差が小さいほどMDPを解くのが難しい\nという2点が言えそうですね。"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html#方策最適化の難しさ",
    "href": "posts/understanding-what-makes-rl-difficult.html#方策最適化の難しさ",
    "title": "より良い問題設計へ向けて： 何が強化学習を難しくするのかを理解しよう",
    "section": "2.2: 方策最適化の難しさ",
    "text": "2.2: 方策最適化の難しさ\n前節で用いた再帰的な数値計算は動的計画法(DP)と呼ばれるものです。 Q学習など、多くの強化学習アルゴリズムがDPをもとにしています。 一方で、単に強化学習をブラックボックス最適化だと考えることもできます。 特に、方策パラメタ\\(\\theta\\)を最適化して解く方法を方策最適化と呼びます。\nいま、\\(\\pi(0|s) = \\theta(s), \\pi(1|s) = 1.0 - \\theta(s)\\)によって\\(\\pi\\)をパラメタ\\(\\theta\\)により表すことにします （これをdirect parameterizationと呼びます）。 ためしに、先ほどのMDP3で\\(\\pi(0|0)=1.0\\)を固定して、\\(\\theta(1), \\theta(2)\\)を動かした時の\\(\\sum_{s \\in \\mathcal{S}} V^\\pi(s)\\)の変動をプロットしてみましょう。\n\n\nCode\ndef v_pi_sum_2dim(\n    r: Array2,\n    p: Array3,\n    gamma: float,\n    epsilon: float,\n    initial_pi: Array2,\n    states: Tuple[int, int],\n    n_discretization: int,\n) -&gt; Array2:\n    res = []\n    for i2 in range(n_discretization + 1):\n        p2 = (1.0 / n_discretization) * i2\n        for i1 in range(n_discretization + 1):\n            p1 = (1.0 / n_discretization) * i1\n            pi = initial_pi.copy()\n            pi[states[0]] = p1, 1 - p1\n            pi[states[1]] = p2, 1 - p2\n            res.append(v_pi(r, p, pi, gamma, epsilon)[0].sum())\n    return np.array(res).reshape(n_discretization + 1, -1)\n\n\ndef plot_piv_heatmap(\n    data: Array2,\n    xlabel: str = \"\",\n    ylabel: str = \"\",\n    title: str = \"\",\n    ax: Optional[Axes] = None,\n) -&gt; Axes:\n    from matplotlib.ticker import LinearLocator\n\n    if ax is None:\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection=\"3d\")\n    n_discr = data.shape[0]\n    x, y = np.meshgrid(np.linspace(0, 1, n_discr), np.linspace(0, 1, n_discr))\n    ax.plot_surface(x, y, data, cmap=\"inferno\", linewidth=0, antialiased=False)\n    ax.zaxis.set_major_locator(LinearLocator(10))\n    ax.zaxis.set_major_formatter('{x:.01f}')\n    ax.set_xlabel(xlabel, fontsize=14)\n    ax.set_ylabel(ylabel, fontsize=14)\n    ax.set_zlabel(\"∑Vπ\", fontsize=14, horizontalalignment=\"right\")\n    ax.set_title(title, fontsize=15)\n    return ax\n\n\ninitial_pi = np.array([[1.0, 0.0], [0.5, 0.5], [0.5, 0.5]])\nv_pi_sums = v_pi_sum_2dim(mdp3.r, mdp3.p, 0.9, 1e-4, initial_pi, (1, 2), 20)\nax = plot_piv_heatmap(v_pi_sums, \"θ(1)\", \"θ(2)\", \"MDP3\")\n_ = ax.set_xlim(tuple(reversed(ax.get_xlim())))\n_ = ax.set_ylim(tuple(reversed(ax.get_ylim())))\n\n\n\n\n\nなんかいい感じに山になっていますね。 この問題の場合は、山登り法（勾配上昇法）で\\(\\theta\\)を更新していけば大域解 \\(\\theta(1) = 0.0, \\theta(2) = 1.0\\)にたどり着きそうです5。\nしかし、\\(f(\\theta) = \\sum_{s\\in\\mathcal{S}} V^{\\pi_\\theta}(s)\\)は、いつもこのような性質 のいい関数になっているのでしょうが？ 結論から言うとそうではないです。 例えば、以下のようなMDPではどうでしょうか？(\\(\\gamma=0.95\\)にしています)\n\n\nCode\nmdp4 = ChainMDP(\n    [[1.0, 0.0], [0.6, 0.9], [0.9, 0.6], [1.0, 1.0]],\n    [[0.0, 1.0], [0.0, 0.0], [0.0, 0.0], [0.9, 0.0]],\n)\nwidth, height = mdp4.figure_shape()\nfig = plt.figure(\"MDP4-pi-vis\", (width * 1.25, height))\nmdp_ax = fig.add_axes([0.42, 0.0, 1.0, 1.0])\n_ = mdp4.show(\"MDP4\", ax=mdp_ax)\npi_ax = fig.add_axes([0.0, 0.0, 0.4, 1.0], projection=\"3d\")\ninitial_pi = np.array([[0.0, 1.0], [0.5, 0.5], [0.5, 0.5], [1.0, 0.0]])\nv_pi_sums = v_pi_sum_2dim(mdp4.r, mdp4.p, 0.95, 1e-4, initial_pi, (1, 2), 24)\n_ = plot_piv_heatmap(v_pi_sums, \"θ(1)\", \"θ(2)\", ax=pi_ax)\nprint(\n    f\"f(θ(1) = 0.0, θ(2) = 0.0): {v_pi_sums[0][0]}\\n\"\n    f\"f(θ(1) = 0.5, θ(2) = 0.5): {v_pi_sums[12][12]}\\n\"\n    f\"f(θ(1) = 1.0, θ(2) = 1.0): {v_pi_sums[24][24]}\"\n)\n\n\nf(θ(1) = 0.0, θ(2) = 0.0): 74.25901721830479\nf(θ(1) = 0.5, θ(2) = 0.5): 72.01388270994806\nf(θ(1) = 1.0, θ(2) = 1.0): 70.6327625115528\n\n\n\n\n\n一番右だと永遠に0.9がもらえて、一番左だと1.0がもらえるので、より最適方策を見分けるのが難しそうな感じがします。\nプロットしてみると、\\(f(\\theta)\\)は先程とは逆に谷のような形になっていて、山登り法で解いても 必ずしも大域解に収束しなそうに見えます。 これをもっと専門的な言葉で言うと、\\(f(0.0) + f(1.0) &gt; 2 * f(0.5)\\)よりこれは凹関数ではありません。 あまり詳しく説明しませんが、凹関数だと山登り法が大域解に収束するなど嬉しい点があるので、 これは最適化する上で厄介な特徴だと言えます。\n以上より、方策最適化で問題を解く時は\\(\\sum_{s\\in\\mathcal{S}} V(s)\\)が凹関数かどうかが、 問題の難しさに影響を与えそうだということがわかりました。"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html#a-方策反復法の難しさ",
    "href": "posts/understanding-what-makes-rl-difficult.html#a-方策反復法の難しさ",
    "title": "より良い問題設計へ向けて： 何が強化学習を難しくするのかを理解しよう",
    "section": "2.A 方策反復法の難しさ",
    "text": "2.A 方策反復法の難しさ\n\nNote: この節は特に内容がないのでアペンディクス扱いになっています。飛ばしても問題ありません。\n\nところで2.1で\\(V^*\\)を求めたときに使った手法を価値反復法と言います。 もう一つ、方策反復法という手法で\\(V^*\\)を求めることができます。\n\\(\\pi^*\\)が満たすべき性質について考えてみます。 \\(\\pi\\)が最適であるとき、 \\[\nV^\\pi(s) \\geq \\max_{a \\in \\mathcal{A}} r(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s'|s,a) V^\\pi(s')\n\\] が成り立ちます。 これが成り立たないとすると、 \\[\n\\pi'(s, a) = \\begin{cases}\n1.0 &(\\textrm{if}~a = \\textrm{argmax}_a r(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s'|s,a) V^\\pi(s')) \\\\\n0.0 &(\\textrm{otherwise})\n\\end{cases}\n\\] の方が性能が良くなり、\\(\\pi\\)が最適であることと矛盾します。\nでは、この性質が成り立つまで方策を改善し続けるというアルゴリズムを試してみましょう。 さっき書いたv_pi関数を使って実装できます。\n\n\nCode\nMAX_ITER_PI: int = 10000\n\ndef policy_iteration(\n    r: Array2,\n    p: Array3,\n    gamma: float,\n    epsilon: float,\n) -&gt; Tuple[Array1, Array2, int]:\n    pi = np.zeros(p.shape[:2])  # |S| x |A|\n    pi[:, 1] = 1.0  # 最初の方策は決定的ならなんでもいいが、行動1を選ぶ方策にしてみる\n    state_indices = np.arange(0, p.shape[0], dtype=np.uint)\n    for n_iter in range(MAX_ITER_PI):\n        v_pi_, _ = v_pi(r, p, pi, gamma, epsilon)\n        q_pi = r + gamma * np.einsum(\"saS,S-&gt;sa\", p, v_pi_)\n        greedy_actions = np.argmax(q_pi, axis=1)\n        pi_next = np.zeros_like(pi)\n        pi_next[state_indices, greedy_actions] = 1.0\n        # pi == pi_next なら収束\n        if np.linalg.norm(pi - pi_next) &lt; 1.0:\n            return v_pi_, pi_next, n_iter + 1\n        pi = pi_next\n    raise RuntimeError(\"Policy Iteration did not converge &gt;_&lt;\")\n\nv_star_mdp3_vi, _, n_iter = policy_iteration(mdp3.r, mdp3.p, 0.9, 1e-4)\nprint(f\"反復回数: {n_iter}\")\nprint(\" \".join([f\"V({i}): {v:.3}\" for i, v in enumerate(v_star_mdp3_vi)]))\n\n\n反復回数: 2\nV(0): 6.49 V(1): 7.21 V(2): 7.51\n\n\nなんか2回反復しただけで求まってしまいましたが…。 このアルゴリズムは方策反復法と呼ばれ、なんやかんやで最適方策に収束することが知られています。 では、この反復回数は、何によって決まるのでしょうか？ 方策の組み合わせは\\(|A|^{|S|}\\)通りありますが、上の実験だとずっと速く収束しているので、もっといいバウンドがありそうに思えます。 しかし、実際のところ最悪ケースでは指数時間かかることが知られています。 この記事では、この方策反復法が難しくなる場合についても解説したかったのですが、 理解できなかったので、諦めました。ヾ(｡&gt;﹏&lt;｡)ﾉ"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html#b-参考文献など",
    "href": "posts/understanding-what-makes-rl-difficult.html#b-参考文献など",
    "title": "より良い問題設計へ向けて： 何が強化学習を難しくするのかを理解しよう",
    "section": "2.B 参考文献など",
    "text": "2.B 参考文献など\n\nOn the Complexity of Solving Markov Decision Problems\nCournell CS 6789: Foundations of Reinforcement Learning\n\n参考文献では\\(\\frac{1}{1 - \\gamma}\\)で反復回数を抑えているじゃないか、話が違うじゃないか、という気が一見してしまいます。 これは有名不等式\\(\\log x \\leq x - 1\\) からなんやかんやで\\(\\frac{1}{1 - \\gamma} \\geq -\\frac{1}{\\log\\gamma}\\) だから〜という感じで考えればなんとかなると思います。 この不等式は\\(x=1\\)で等号なので、よく使う\\(\\gamma=0.99\\)とかの設定ならかなり差は近くなります。"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html#報酬なし探査の問題",
    "href": "posts/understanding-what-makes-rl-difficult.html#報酬なし探査の問題",
    "title": "より良い問題設計へ向けて： 何が強化学習を難しくするのかを理解しよう",
    "section": "3.1 報酬なし探査の問題",
    "text": "3.1 報酬なし探査の問題\nというわけで、とりあえず別に学習しなくていいので、環境から情報を集めてこよう、という問題を考えてみましょう。\n\n\nCode\nfrom matplotlib.figure import Figure\nfrom matplotlib.image import FigureImage\n\n\nclass GridMDP:\n    from matplotlib.colors import ListedColormap\n\n    #: Up, Down, Left, Right\n    ACTIONS = np.array([[-1, 0], [+1, 0], [0, -1], [0, +1]])\n    #: Symbols\n    EMPTY, BLOCK, START, GOAL = range(4)\n    DESCRIPTIONS = [\"Empty\", \"Block\", \"Start\", \"Goal\"]\n    #: Colormap for visualizing the map\n    CM = ListedColormap([\"w\", \".75\", \"xkcd:leaf green\", \"xkcd:vermillion\"])\n    REWARD_COLORS = [\"xkcd:light royal blue\", \"xkcd:vermillion\"]\n    FIG_AREA = 28\n\n    # Returns PIL.Image\n    def __download_agent_image():\n        from io import BytesIO\n        from urllib import request\n\n        from PIL import Image\n\n        fd = BytesIO(\n            request.urlopen(\n                \"https://2.bp.blogspot.com/-ZwYKR5Zu28s/U6Qo2qAjsqI\"\n                + \"/AAAAAAAAhkM/HkbDZEJwvPs/s400/omocha_robot.png\"\n            ).read()\n        )\n        return Image.open(fd)\n\n    AGENT_IMAGE = __download_agent_image()\n\n    def __init__(\n        self,\n        map_array: Sequence[Sequence[int]],\n        reward_array: Optional[Sequence[Sequence[float]]] = None,\n        action_noise: float = 0.1,\n        horizon: Optional[int] = None,\n        seed: int = 123456789,\n        legend_loc: str = \"upper right\",\n    ) -&gt; None:\n        def add_padding(seq: Sequence[Sequence[T]], value: T) -&gt; list:\n            width = len(seq[0]) + 2\n            ret_list = [[value for _ in range(width)]]\n            for col in seq:\n                ret_list.append([value] + list(col) + [value])\n            ret_list.append([value for _ in range(width)])\n            return ret_list\n\n        self.map_array = np.array(add_padding(map_array, 1), dtype=np.uint8)\n        assert self.map_array.max() &lt;= 3\n        assert 0 &lt;= self.map_array.min()\n        self.rows, self.cols = self.map_array.shape\n\n        if reward_array is None:\n            self.reward_array = np.zeros((self.rows, self.cols), np.float64)\n        else:\n            self.reward_array = np.array(\n                add_padding(reward_array, 0.0), np.float64\n            )\n\n        self.action_noise = action_noise\n        self.horizon = horizon\n        self.start_positions = np.argwhere(self.map_array == self.START)\n        if len(self.start_positions) == 0:\n            raise ValueError(\"map_array needs at least one start posiiton\")\n        self.random_state = np.random.RandomState(seed)\n        _ = self.reset()\n\n        # Visualization stuffs\n        self.legend_loc = legend_loc\n        self.map_fig, self.map_ax, self.map_img = None, None, None\n        self.agent_img, self.agent_fig_img = None, None\n\n    def n_states(self) -&gt; int:\n        return np.prod(self.map_array.shape)\n\n    @staticmethod\n    def n_actions() -&gt; int:\n        return 4\n\n    def reset(self) -&gt; Array1:\n        idx = self.random_state.randint(self.start_positions.shape[0])\n        self.state = self.start_positions[idx]\n        self.n_steps = 0\n        return self.state.copy()\n\n    def state_index(self, state: Array1) -&gt; int:\n        y, x = state\n        return y * self.map_array.shape[1] + x\n\n    def _load_agent_img(self, fig_height: float) -&gt; None:\n        from io import BytesIO\n        from urllib import request\n\n        fd = BytesIO(request.urlopen(self.ROBOT).read())\n        img = Image.open(fd)\n        scale = fig_height / img.height\n        self.agent_img = img.resize((int(img.width * scale), int(img.height * scale)))\n\n    def _fig_inches(self) -&gt; Tuple[int, int]:\n        prod = self.rows * self.cols\n        scale = np.sqrt(self.FIG_AREA / prod)\n        return self.cols * scale, self.rows * scale\n\n    def _is_valid_state(self, *args) -&gt; bool:\n        if len(args) == 2:\n            y, x = args\n        else:\n            y, x = args[0]\n        return 0 &lt;= y &lt; self.rows and 0 &lt;= x &lt; self.cols\n\n    def _possible_actions(self) -&gt; Array1:\n        possible_actions = []\n        for i, act in enumerate(self.ACTIONS):\n            y, x = self.state + act\n            if self._is_valid_state(y, x) and self.map_array[y, x] != self.BLOCK:\n                possible_actions.append(i)\n        return np.array(possible_actions)\n\n    def _reward(self, next_state: Array1) -&gt; float:\n        y, x = next_state\n        return self.reward_array[y, x]\n\n    def _is_terminal(self) -&gt; bool:\n        if self.horizon is not None and self.n_steps &gt; self.horizon:\n            return True\n        y, x = self.state\n        return self.map_array[y, x] == self.GOAL\n\n    def step(self, action: int) -&gt; Tuple[Tuple[int, int], float, bool]:\n        self.n_steps += 1\n        possible_actions = self._possible_actions()\n        if self.random_state.random_sample() &lt; self.action_noise:\n            action = self.random_state.choice(possible_actions)\n\n        if action in possible_actions:\n            next_state = self.state + self.ACTIONS[action]\n        else:\n            next_state = self.state.copy()\n\n        reward = self._reward(next_state)\n        self.state = next_state\n        is_terminal = self._is_terminal()\n        return next_state.copy(), reward, is_terminal\n\n    def _draw_agent(self, fig: Figure) -&gt; FigureImage:\n        unit = self.map_img.get_window_extent().y1 / self.rows\n        y, x = self.state\n        return fig.figimage(\n            self.agent_img,\n            unit * (x + 0.3),\n            unit * (self.rows - 0.8 - y),\n        )\n\n    def _draw_rewards(self) -&gt; None:\n        for y in range(self.rows):\n            for x in range(self.cols):\n                rew = self.reward_array[y, x]\n                if abs(rew) &lt; 1e-3:\n                    continue\n                if self.map_array[y, x] == self.GOAL:\n                    color = \"w\"\n                else:\n                    color = self.REWARD_COLORS[int(rew &gt;= 0)]\n                self.map_ax.text(\n                    x + 0.1,\n                    y + 0.5,\n                    f\"{rew:+.2}\",\n                    color=color,\n                    fontsize=12,\n                )\n\n    def show(self, title: str = \"\", explicit: bool = False) -&gt; Axes:\n        if self.map_fig is None:\n            self.map_fig = plt.figure(title or \"GridMDP\", self._fig_inches())\n            ax = self.map_fig.add_axes([0, 0, 1, 1])\n            ax.set_aspect(\"equal\")\n            ax.set_xticks([])\n            ax.set_yticks([])\n            self.map_img = ax.imshow(\n                self.map_array,\n                cmap=self.CM,\n                extent=(0, self.cols, self.rows, 0),\n                vmin=0,\n                vmax=4,\n                alpha=0.6,\n            )\n            for i in range(1, 4):\n                if np.any(self.map_array == i):\n                    ax.plot([0.0], [0.0], color=self.CM(i), label=self.DESCRIPTIONS[i])\n            ax.legend(fontsize=12, loc=self.legend_loc)\n            ax.text(0.1, 0.8, title or \"GridMDP\", fontsize=16)\n            self.map_ax = ax\n\n            imw, imh = self.AGENT_IMAGE.width, self.AGENT_IMAGE.height\n            scale = (self.map_img.get_window_extent().y1 / self.rows) / imh\n            self.agent_img = self.AGENT_IMAGE.resize(\n                (int(imw * scale), int(imh * scale))\n            )\n\n            if np.linalg.norm(self.reward_array) &gt; 1e-3:\n                self._draw_rewards()\n        if self.agent_fig_img is not None:\n            self.agent_fig_img.remove()\n        self.agent_fig_img = self._draw_agent(self.map_fig)\n        if explicit:\n            from IPython.display import display\n\n            self.map_fig.canvas.draw()\n            display(self.map_fig)\n        return self.map_ax\n\n\n\n\nCode\ngrid_mdp1 = GridMDP(\n    [[0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0], \n     [0, 0, 2, 0, 0],\n     [0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0]],\n    horizon=50,\n)\n_ = grid_mdp1.show(\"GridMDP1\")\n\n\n\n\n\nGridMDPと題されたこちらが、今回使用する「環境」になります。 環境の中で行動する主体をエージェントと呼びます。 今回は、いらすとや様のロボットの画像を使用させていただきました。 各マス目の中で、エージェントは行動は上下左右に移動の4種類の行動を選択できます。 行動は時々失敗して、一様ランダムな状態遷移が発生します。 ここで、前章で用いなかったいくつかの新しい概念を導入します。\n\n「スタート地点」の存在\n\nロボットくんは、決められた場所から行動を始めなくてはなりません。この問題では、初期状態はあらかじめ決まられたいくつかのスタート地点から均等に選びます。理論的なフレームワークでは、初期状態分布\\(\\mu: \\mathcal{S} \\rightarrow \\mathbb{R}\\)として表現すればいいです。\n\n「終了地点」の存在\n\nロボットくんは、いくつかの決められた場所に到達したら強制的にスタートまで戻されます。\n\nエピソード\n\nスタートから終了するまでの一連の流れをエピソードと呼びます。\n\n「エピソード長さ（ホライゾン）」の存在\n\n一定のターンが経過した時、ロボットくんはスタート地点まで戻されます。強化学習ではしばしば、シミュレーターを何度もリセットして学習します。理論を実際に近づけるため、MDPのフレームワークにもこれが導入される場合があります。\n\n\nではさっそく、適当に行動してもらいましょう。 エージェントをランダムに行動させて、訪問した場所に色をつけていきます。 なお、エピソード長さは50とします。\n\n\nCode\nfrom abc import ABC, abstractmethod\nfrom typing import Callable\n\n\nclass VisitationHeatmap:\n    def __init__(\n        self,\n        map_shape: Tuple[int, int],\n        figsize: Tuple[float, float],\n        ax: Optional[Axes] = None,\n        max_visit: int = 1000,\n        title: str = \"\",\n    ) -&gt; None:\n        from matplotlib import colors as mc\n        from mpl_toolkits.axes_grid1 import make_axes_locatable\n\n        self.counter = np.zeros(map_shape, np.int64)\n        self.title = title\n        self.fig = plt.figure(self.title, figsize, facecolor=\"w\")\n        self.ax = self.fig.add_axes([0, 0, 1, 1])\n        self.ax.set_aspect(\"equal\")\n        self.ax.set_xticks([])\n        self.ax.set_yticks([])\n        r, g, b = mc.to_rgb(\"xkcd:fuchsia\")\n        cdict = {\n            \"red\": [(0.0, r, r), (1.0, r, r)],\n            \"green\": [(0.0, g, g), (1.0, g, g)],\n            \"blue\": [(0.0, b, b), (1.0, b, b)],\n            \"alpha\": [(0.0, 0.0, 0.0), (1.0, 1.0, 1.0)],\n        }\n        self.img = self.ax.imshow(\n            np.zeros(map_shape),\n            cmap=mc.LinearSegmentedColormap(\"visitation\", cdict),\n            extent=(0, map_shape[1], map_shape[0], 0),\n            vmin=0,\n            vmax=max_visit,\n        )\n\n        divider = make_axes_locatable(self.ax)\n        cax = divider.append_axes(\"right\", size=\"4%\", pad=0.1)\n        self.fig.colorbar(self.img, cax=cax, orientation=\"vertical\")\n        cax.set_ylabel(\"Num. Visitation\", rotation=0, position=(1.0, 1.1), fontsize=14)\n\n        self._update_text()\n        self.agent = None\n\n    def _update_text(self) -&gt; None:\n        self.text = self.ax.text(\n            0.1,\n            -0.5,\n            f\"{self.title} After {self.counter.sum()} steps\",\n            fontsize=16,\n        )\n\n    def _draw_agent(self, draw: Callable[[Figure], FigureImage]) -&gt; None:\n        if self.agent is not None:\n            self.agent.remove()\n        self.agent = draw(self.fig)\n\n    def visit(self, state: Array1) -&gt; int:\n        y, x = state\n        res = self.counter[y, x]\n        self.counter[y, x] += 1\n        self.img.set_data(self.counter)\n        self.text.remove()\n        self._update_text()\n        return res\n\n    def show(self) -&gt; None:\n        from IPython.display import display\n\n        display(self.fig)\n\n\ndef do_nothing(\n    _state: int,\n    _action: int,\n    _next_state: int,\n    _reward: float,\n    _is_terminal: bool,\n) -&gt; None:\n    return\n\n\ndef simulation(\n    mdp: GridMDP,\n    n: int,\n    act: Callable[[int], int],\n    learn: Callable[[int, int, int, float, bool], None] = do_nothing,\n    max_visit: Optional[int] = None,\n    vis_freq: Optional[int] = None,\n    vis_last: bool = False,\n    title: str = \"\",\n) -&gt; None:\n    visitation = VisitationHeatmap(\n        mdp.map_array.shape,\n        mdp._fig_inches(),\n        max_visit=max_visit or n // 10,\n        title=title,\n    )\n    state = mdp.reset()\n    visitation.visit(state)\n    vis_interval = n + 1 if vis_freq is None else n // vis_freq\n    for i in range(n):\n        if (i + 1) % vis_interval == 0 and (vis_last or i &lt; n - 1):\n            visitation._draw_agent(mdp._draw_agent)\n            visitation.show()\n        action = act(mdp.state_index(state))\n        next_state, reward, terminal = mdp.step(action)\n        visitation.visit(next_state)\n        learn(\n            mdp.state_index(state),\n            action,\n            mdp.state_index(next_state),\n            reward,\n            terminal,\n        )\n        if terminal:\n            state = mdp.reset()\n        else:\n            state = next_state\n    visitation._draw_agent(mdp._draw_agent)\n\n\nsimulation(grid_mdp1, 1000, lambda _: np.random.randint(4), vis_freq=2)\n\n\n\n\n\n\n\n\nランダムに行動させただけですが、それなりにまんべんなく色が塗られていて、まあまあいいのではないか、という気がします。 しかし、もっと広い環境ではどうでしょうか。\n\n\nCode\ngrid_mdp2_map = [[0] * 15 for _ in range(15)]\ngrid_mdp2_map[7][7] = 2\ngrid_mdp2 = GridMDP(grid_mdp2_map, horizon=50)\n_ = grid_mdp2.show()\nrandom_state = np.random.RandomState(1)\nsimulation(\n    grid_mdp2,\n    5000,\n    lambda _: random_state.randint(4),\n    max_visit=100,\n    title=\"Random Exploration\",\n    vis_freq=None,\n)\n\n\n\n\n\n\n\n\nなんか駄目っぽい感じですね。 場所によっては全く色がついていません。 環境が広いと、ランダムに歩き回るのでは、効率よく情報を集めてこれないようです。 具体的にどのくらい難しいのかと言うと、平均一回訪問するのにかかる時間が、だいたい - 一方通行の直線: \\(O(|S|)\\) - 二次元ランダムウォーク: \\(O(|S|^2)\\)? (参考: plane上のランダムウォーク） - 最悪ケース: \\(O(2^{|S|})\\)\nくらいになります。 一方通行なのはなんとなくわかりますね。 ランダムウォークの場合、同じ場所を行ったりきたりできるので、そのぶん時間がかかってしまいます。 最悪ケースは、以下のように構成すればいいです。\n\n\nCode\n# hide-input\nclass WorstCaseMDP(ChainMDP):\n    def __init__(self, n: int) -&gt; None:\n        self.n_states = n\n        # For plotting\n        self.circles = []\n        self.cached_ax = None\n\n    def show(self, title: str = \"\", ax: Optional[Axes] = None) -&gt; Axes:\n        # だいたいChainMDPからコピペ...\n        if self.cached_ax is not None:\n            return self.cached_ax\n\n        from matplotlib.patches import Circle\n\n        width, height = self.figure_shape()\n        circle_position = height / 2 - height / 10\n        if ax is None:\n            fig = plt.figure(title or \"ChainMDP\", (width, height))\n            ax = fig.add_axes([0, 0, 1, 1], aspect=1.0)\n        ax.set_xlim(0, width)\n        ax.set_ylim(0, height)\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n        def xi(si: int) -&gt; float:\n            return self.OFFSET + (1.0 + self.INTERVAL) * si + 0.5\n\n        self.circles = [\n            Circle((xi(i), circle_position), 0.5, fc=\"w\", ec=\"k\")\n            for i in range(self.n_states)\n        ]\n        for i in range(self.n_states):\n            x = self.OFFSET + (1.0 + self.INTERVAL) * i + 0.1\n            ax.text(x, height * 0.85, f\"State {i}\", fontsize=16)\n\n        def annon(act: int, *args, **kwargs) -&gt; None:\n            # We don't hold references to annotations (i.e., we treat them immutable)\n            a_to_b(\n                ax,\n                *args,\n                **kwargs,\n                arrowcolor=self.ACT_COLORS[act],\n                text=f\"P: 1.0\",\n                fontsize=11,\n            )\n\n        for si in range(self.n_states):\n            ax.add_patch(self.circles[si])\n            x = xi(si)\n            # Action 0:\n            y = circle_position + self.SHIFT\n            if si &lt; self.n_states - 1:\n                annon(\n                    0,\n                    (x + self.SHIFT, y),\n                    (xi(si + 1) - self.SHIFT * 1.2, y - self.SHIFT * 0.3),\n                    verticalalignment=\"center_baseline\",\n                )\n            else:\n                annon(\n                    0,\n                    (x - self.SHIFT * 1.2, y),\n                    (x + self.SHIFT * 0.5, y - self.SHIFT * 0.1),\n                    style=\"self\",\n                    verticalalignment=\"bottom\",\n                )\n            # Action 1:\n            y = circle_position - self.SHIFT\n            if si &gt; 0:\n                annon(\n                    1,\n                    (x - self.SHIFT * 1.6, y),\n                    (xi(0), y - self.SHIFT * 0.6),\n                    style=\"arc3,rad=-0.15\",\n                    verticalalignment=\"top\",\n                )\n            else:\n                annon(\n                    1,\n                    (x + self.SHIFT * 0.4, y),\n                    (x - self.SHIFT * 0.45, y + self.SHIFT * 0.1),\n                    style=\"self\",\n                    verticalalignment=\"top\",\n                )\n\n        for i in range(2):\n            ax.plot([0.0], [0.0], color=self.ACT_COLORS[i], label=f\"Action {i}\")\n        ax.legend(fontsize=11, loc=\"upper right\")\n        if len(title) &gt; 0:\n            ax.text(0.06, height * 0.9, title, fontsize=18)\n        self.cached_ax = ax\n        return ax\n\n\n_ = WorstCaseMDP(6).show()\n\n\n\n\n\nこの環境で状態\\(0\\)からランダムに行動すると、右端にたどり着くまでに平均\\(2^5\\)くらいのステップ数がかかります。 そんなんありかよ…って感じですが。\nこの結果から、最悪の場合だと指数時間かかるから賢くデータ収集しないといけないよね、 思うこともできます。 その一方で、ランダムウォークのように遷移の対称性がある環境なら、 ランダムに行動してもそんなに悪くないんじゃないかな、とも思えます。\nさてその話は一旦おいておいて、もっと効率よくデータを集める方法を考えてみましょう。\n\n訪問した場所を覚えておいて、訪問していない場所を優先して探査する\n状態と状態の間に距離が定義できると仮定して、遠くに行くようにする\n環境がユークリッド空間だと仮定してSLAMで自己位置推定する\n\nなど、色々な方法が考えられると思いますが、ここでは1の方法を使ってみましょう。\n以下のようなアルゴリズムを考えます。 1. 適当な方策\\(\\pi_0\\)から開始する 2. 状態行動訪問回数\\(n(s, a)\\)、状態行動次状態訪問回数\\(n(s, a, s')\\)を記録しておく - ただし、初期値は\\(n_0(s, a) = 1.0, n_0(s, a, s) = \\frac{1}{|S|}\\)とする(0除算防止のため) 3. エピソードが終わったとき、以下のように方策を更新する 1. 状態遷移関数の推定値\\(\\hat{P}(s'|s, a) = \\frac{n(s, a, s')}{n(s, a}\\)、疑似報酬\\(r_k(s, a)=\\frac{1}{n(s, a)}\\)、適当な\\(\\gamma\\)から成るMDP\\(\\mathcal{M}_k\\)を解く 2. \\(\\mathcal{M}_k\\)の最適価値関数\\(V^*_k,Q^*_k\\)から以下のように方策\\(pi_{k+1}\\)を構成する - \\(V^*_k(s) &lt; \\frac{1}{|S|}\\sum_{s'\\in\\mathcal{S}}V^*_k(s')\\) なら \\(\\pi_{k+1}(s)\\)は\\(Q^*_k\\)に基づく貪欲行動 - それ以外の場合、\\(\\pi_{k+1}(s)\\)は一様ランダムな行動をとる (=方策を緩和する)\n疑似報酬\\(r_k=\\frac{1}{n(s, a)}\\)を使用してプランニングするのが、最も重要なポイントです。 この値は、一度も状態行動ペア\\((s,a)\\)を経験していないなら\\(1\\)、一度経験したら\\(1/2\\)、2回経験したら\\(1/3\\)のように減衰します。 これを報酬とするMDPを解くことで、あまり経験していない状態行動ペアをとろうとする方策が得られます。 完全な貪欲方策ではなく緩和をいれているのは、高い報酬の状態をループしないようにするためです。 では、やってみましょう。\n\n\nCode\nclass RewardFreeExplore:\n    def __init__(\n        self,\n        n_states: int,\n        n_actions: int,\n        seed: int = 1,\n        gamma: float = 0.95,\n    ) -&gt; None:\n        self.sa_count = np.ones((n_states, n_actions))\n        self.sas_count = np.ones((n_states, n_actions, n_states)) / n_states\n        self.pi = np.ones((n_states, n_actions)) / n_actions\n        self.random_state = np.random.RandomState(seed)\n        self.n_states, self.n_actions = n_states, n_actions\n        self.n_updates = 0\n        self.gamma = gamma\n        self.pi_indices = np.arange(n_states, dtype=np.uint32)\n\n    def learn(\n        self,\n        state: int,\n        action: int,\n        next_state: int,\n        reward: float,\n        is_terminal: bool,\n    ) -&gt; None:\n        # 訪問記録を更新する\n        self.sa_count[state, action] += 1\n        if is_terminal:\n            # エピソードが終わったら、Value Iterationを解いて方策を更新する\n            r = 1.0 / self.sa_count\n            p = self.sas_count / np.expand_dims(self.sa_count, axis=-1)\n            v, _n_iter = value_iteration(r, p, self.gamma, 1e-2)\n            v_is_larger_than_mean = v &gt; v.mean()\n            q = r + self.gamma * np.einsum(\"saS,S-&gt;sa\", p, v)\n            self.pi.fill(0.0)\n            for state in range(self.n_states):\n                # Vが大きい場所では方策を緩和する\n                if v_is_larger_than_mean[state]:\n                    self.pi[state] = 1.0 / self.n_actions\n                # そうでない場合は貪欲\n                else:\n                    self.pi[state][q[state].argmax()] = 1.0\n            self.n_updates += 1\n        else:\n            self.sas_count[state, action, next_state] += 1\n\n    def act(self, state: int) -&gt; int:\n        return self.random_state.choice(self.n_actions, p=self.pi[state])\n\n\nagent = RewardFreeExplore(grid_mdp2.n_states(), grid_mdp2.n_actions())\nsimulation(\n    grid_mdp2,\n    5000,\n    agent.act,\n    learn=agent.learn,\n    max_visit=100,\n    title=\"Strategic Exploration\",\n    vis_freq=None,\n)\n\n\n\n\n\nさっきよりも満遍なく、色々な状態を訪問してくれるようになりましたね。"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html#報酬あり探査",
    "href": "posts/understanding-what-makes-rl-difficult.html#報酬あり探査",
    "title": "より良い問題設計へ向けて： 何が強化学習を難しくするのかを理解しよう",
    "section": "3.2 報酬あり探査",
    "text": "3.2 報酬あり探査\n次は、環境から報酬が与えられるので、なるべく早く学習を終わらせたい、という問題を考えます。 訪問していない場所に積極的にいけばいい、という方針はさっきと変わりません。 一方で、あまりに報酬がもらえなさそうな状態はとっとと諦めることをしなくてはいけない点が異なっています。\n例えば、報酬の推定値を\\(\\hat{r}(s, a)\\)とするとき、先ほどのアルゴリズムの疑似報酬を \\(r_k(s, a)=\\hat{r}(s, a)+\\frac{\\beta}{\\sqrt{n(s, a)}}\\)とすればいいです。 これを、単に\\(r_k(s, a)=\\hat{r}(s, a)\\)とするアルゴリズム(Approximate Value Iteration)と比較してみましょう。 こちらは、確率\\(\\epsilon\\)で一様分布から行動をサンプリングし、\\(1-\\epsilon\\)で\\(Q^*_k\\)が一番大きい行動を選択するという行動方策を使ってみましょう（\\(\\epsilon\\)-Greedyと言います）。 今回は\\(\\epsilon=0.9 \\rightarrow 0.4\\)とします。\n\n\nCode\ngrid_mdp3 = GridMDP(\n    [[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n    [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9.0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0, 0]],\n    horizon=100,\n    legend_loc=\"lower right\"\n)\n_ = grid_mdp3.show(\"GridMDP3\")\n\n\n\n\n\nまず、こちらの環境で実験してみます。素直に\\(0.1\\)の報酬→\\(9.0\\)の報酬を目指せばいい感じです。 また、\\(\\gamma=0.99\\)とします。\n\n\nCode\nclass EpsgApproxVI:\n    def __init__(\n        self,\n        n_states: int,\n        n_actions: int,\n        seed: int = 1,\n        gamma: float = 0.99,\n        epsilon: float = 0.9,\n        epsilon_delta: float = 0.0001,\n    ) -&gt; None:\n        self.sa_count = np.ones((n_states, n_actions))\n        self.sas_count = np.ones((n_states, n_actions, n_states)) / n_states\n        self.r_sum = np.zeros((n_states, n_actions))\n        self.pi = np.ones((n_states, n_actions)) / n_actions\n        self.random_state = np.random.RandomState(seed)\n        self.n_states, self.n_actions = n_states, n_actions\n        self.n_updates = 0\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.epsilon_delta = epsilon_delta\n        self.pi_indices = np.arange(n_states, dtype=np.uint32)\n\n    def learn(\n        self,\n        state: int,\n        action: int,\n        next_state: int,\n        reward: float,\n        is_terminal: bool,\n    ) -&gt; None:\n        self.sa_count[state, action] += 1\n        self.r_sum[state, action] += reward\n        if is_terminal:\n            r = self.r_sum / self.sa_count\n            p = self.sas_count / np.expand_dims(self.sa_count, axis=-1)\n            v, _n_iter = value_iteration(r, p, self.gamma, 1e-2)\n            q = r + self.gamma * np.einsum(\"saS,S-&gt;sa\", p, v)\n            self.pi.fill(0)\n            for state in range(self.n_states):\n                self.pi[state][q[state].argmax()] = 1.0\n            self.n_updates += 1\n        else:\n            self.sas_count[state, action, next_state] += 1\n\n    def act(self, state: int) -&gt; int:\n        if self.random_state.rand() &lt; self.epsilon:\n            self.epsilon -= self.epsilon_delta\n            return self.random_state.choice(self.n_actions)\n        else:\n            return self.random_state.choice(self.n_actions, p=self.pi[state])\n\n\nclass MBIB_EB:\n    def __init__(\n        self,\n        n_states: int,\n        n_actions: int,\n        seed: int = 1,\n        gamma: float = 0.99,\n        beta: float = 0.1,\n    ) -&gt; None:\n        self.sa_count = np.ones((n_states, n_actions))\n        self.sas_count = np.ones((n_states, n_actions, n_states)) / n_states\n        self.r_sum = np.zeros((n_states, n_actions))\n        self.pi = np.ones((n_states, n_actions)) / n_actions\n        self.random_state = np.random.RandomState(seed)\n        self.n_states, self.n_actions = n_states, n_actions\n        self.n_updates = 0\n        self.gamma = gamma\n        self.beta = beta\n        self.pi_indices = np.arange(n_states, dtype=np.uint32)\n    \n    def learn(\n        self,\n        state: int,\n        action: int,\n        next_state: int,\n        reward: float,\n        is_terminal: bool,\n    ) -&gt; None:\n        self.sa_count[state, action] += 1\n        self.r_sum[state, action] += reward\n        if is_terminal:\n            r = self.r_sum / self.sa_count + self.beta / np.sqrt(self.sa_count)\n            p = self.sas_count / np.expand_dims(self.sa_count, axis=-1)\n            v, _n_iter = value_iteration(r, p, self.gamma, 1e-2)\n            v_is_larger_than_mean = v &gt; v.mean()\n            q = r + self.gamma * np.einsum(\"saS,S-&gt;sa\", p, v)\n            self.pi.fill(0.0)\n            for state in range(self.n_states):\n                if v_is_larger_than_mean[state]:\n                    self.pi[state] = 1.0 / self.n_actions\n                else:\n                    self.pi[state][q[state].argmax()] = 1.0\n            self.n_updates += 1\n        else:\n            self.sas_count[state, action, next_state] += 1\n\n    def act(self, state: int) -&gt; int:\n        return self.random_state.choice(self.n_actions, p=self.pi[state])\n\n\nepsg_vi = EpsgApproxVI(grid_mdp3.n_states(), grid_mdp3.n_actions())\nsimulation(\n    grid_mdp3,\n    10000,\n    epsg_vi.act,\n    learn=epsg_vi.learn,\n    max_visit=100,\n    title=\"ε-Greedy\",\n    vis_freq=None,\n)\nmbib_eb = MBIB_EB(grid_mdp3.n_states(), grid_mdp3.n_actions())\nsimulation(\n    grid_mdp3,\n    10000,\n    mbib_eb.act,\n    learn=mbib_eb.learn,\n    max_visit=100,\n    title=\"Startegic Exploration\",\n    vis_freq=None,\n)\n\n\n\n\n\n\n\n\n両方とも、いい感じに探査してくれているように見えます。 \\(\\epsilon\\)-Greedyの方が、\\(R_t=9.0\\)がもらえるゴールの周辺を多く探査していて、 良さそうに見えます。 一方で、もう少し意地悪な環境の場合はどうでしょうか？\n\n\nCode\ngrid_mdp4 = GridMDP(\n    [[0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 3],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n    [[0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 9.0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n    horizon=100,\n    legend_loc=\"lower right\"\n)\n_ = grid_mdp4.show(\"GridMDP3\")\n\n\n\n\n\nこの環境では、\\(+5.0\\)とかいう邪魔くさい報酬があります。 しかもここはゴールなので、ここに行くとまたリセットしてやり直しです。 ここを目指すように学習してしまうと、なかなか\\(+9.0\\)の方に行くのは厳しそうに見えます。 実験してみましょう。\n\n\nCode\nepsg_vi = EpsgApproxVI(grid_mdp4.n_states(), grid_mdp4.n_actions())\nsimulation(\n    grid_mdp4,\n    10000,\n    epsg_vi.act,\n    learn=epsg_vi.learn,\n    max_visit=100,\n    title=\"ε-Greedy\",\n    vis_freq=None,\n)\nmbib_eb = MBIB_EB(grid_mdp4.n_states(), grid_mdp4.n_actions())\nsimulation(\n    grid_mdp4,\n    10000,\n    mbib_eb.act,\n    learn=mbib_eb.learn,\n    max_visit=100,\n    title=\"Startegic Exploration\",\n    vis_freq=None,\n)\n\n\n\n\n\n\n\n\n予想通り、\\(\\epsilon\\)-Greedyの方は右上ばかり行ってしまってイマイチな感じになりました。\n以上の結果から、 - 邪魔がなく遷移関数が対称な状態空間（ランダムウォークのように考えられるもの）では、わりあい簡単にデータ収集ができる - 邪魔な報酬がない環境では、わりあい簡単にデータ収集ができる\nという2点が言えるかと思います。 ワーストケースを考えると探査が難しいのも事実ですが、実用上は難しいケースを考えるより邪魔な報酬を排除する ことを考えるのが重要です。"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html#a-参考文献など",
    "href": "posts/understanding-what-makes-rl-difficult.html#a-参考文献など",
    "title": "より良い問題設計へ向けて： 何が強化学習を難しくするのかを理解しよう",
    "section": "3.A 参考文献など",
    "text": "3.A 参考文献など\n\nOn the Sample Complexity of Reinforcement Learning\nReward-Free Exploration for Reinforcement Learning\nSample Complexity Bounds of Exploration\nAn analysis of model-based Interval Estimation for Markov Decision Processes\n\n3.1で紹介したアルゴリズムは一応2.の文献を参考にしていますが、僕がさっき適当に考えた(は？)ものです。 理論保証があるかはあやしいと思います。 3.2のやつはMBIB-EB(4.)に似ていますが、方策の緩和が入っている点が違います。 緩和も僕が適当に考えたものなのですが、入れた方が性能が良かったので入れてみました。 良い子の皆さんは真似しないでください。"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html#footnotes",
    "href": "posts/understanding-what-makes-rl-difficult.html#footnotes",
    "title": "より良い問題設計へ向けて： 何が強化学習を難しくするのかを理解しよう",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSuttonも誤差逆伝播を使うのにはトリッキーな工夫が必要だと言っています。↩︎\nこの報酬関数は最も簡単な定義です。他に\\(r: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow \\mathbb{R}\\)(遷移先に依存)、\\(r: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{E}[R_{s, a}]\\)（確率的）があります。↩︎\n初期状態分布(雑に言うと、スタート地点の分布)を\\(\\mu(s)\\)とすると、$ {s} (s)V(s)$ がエージェントが獲得する割引報酬和の期待値です。\\(V_\\pi(s)\\) が最大ならこれも最大になります。↩︎\nこの\\(\\epsilon\\)は\\(\\epsilon\\)-Optimal Policyの\\(\\epsilon\\)ではありません。↩︎\nこの記事では勾配の導出については一切触れないので、別途資料などを参照してください。↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "RLog2: RL blog 2",
    "section": "",
    "text": "Blog posts on reinforcement learning and other technical stuff with some code"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RLog2",
    "section": "",
    "text": "Exercise 5.12 Racetrack from the Reinforcement Learning textbook\n\n\n\n\n\n\n\nen\n\n\nRL\n\n\nbasic\n\n\n\n\n\n\n\n\n\n\n\nJul 3, 2022\n\n\n\n\n\n\n  \n\n\n\n\nJax・Brax・HaikuでGPU引きこもり学習\n\n\n\n\n\n\n\nja\n\n\nRL\n\n\ndeep\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2022\n\n\n\n\n\n\n  \n\n\n\n\nより良い問題設計へ向けて： 何が強化学習を難しくするのかを理解しよう\n\n\n\n\n\n\n\nja\n\n\nRL\n\n\nbasic\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2021\n\n\n\n\n\n\nNo matching items"
  }
]