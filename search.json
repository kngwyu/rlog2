[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RLog2",
    "section": "",
    "text": "Gemma3でRLHFを試してみる\n\n\n\nja\n\nNLP\n\ndeep\n\n\n\n\n\n\n\n\n\nAug 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGemma3でRLHFを試してみる\n\n\n\nen\n\nNLP\n\ndeep\n\n\n\n\n\n\n\n\n\nAug 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nQuick Python basic course for OIST new students\n\n\n\nen\n\neducation\n\n\n\n\n\n\n\n\n\nSep 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nImplement fast 2D physics simulation with Jax\n\n\n\nen\n\nRL\n\nphysics\n\n\n\n\n\n\n\n\n\nNov 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nJaxで高速な2D物理シミュレーションを実装してみる\n\n\n\nja\n\nRL\n\nphysics\n\n\n\n\n\n\n\n\n\nSep 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTry reinforcement learning with equinox\n\n\n\nja\n\nRL\n\ndeep\n\n\n\n\n\n\n\n\n\nJul 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nequinoxで強化学習してみる\n\n\n\nja\n\nRL\n\ndeep\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding what self-attention is doing\n\n\n\nen\n\nNLP\n\ndeep\n\n\n\n\n\n\n\n\n\nMay 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nAttentionが何をやっているのか理解しよう\n\n\n\nja\n\nNLP\n\ndeep\n\n\n\n\n\n\n\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 5.12 Racetrack from the Reinforcement Learning textbook\n\n\n\nen\n\nRL\n\nbasic\n\n\n\n\n\n\n\n\n\nJul 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nJax・Brax・HaikuでGPU引きこもり学習\n\n\n\nja\n\nRL\n\ndeep\n\n\n\n\n\n\n\n\n\nDec 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nより良い問題設計へ向けて： 何が強化学習を難しくするのかを理解しよう\n\n\n\nja\n\nRL\n\nbasic\n\n\n\n\n\n\n\n\n\nDec 22, 2020\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/try_gemma3_rlhf-en.html",
    "href": "posts/try_gemma3_rlhf-en.html",
    "title": "Gemma3でRLHFを試してみる",
    "section": "",
    "text": "I was shocked to see that my last entry was almost two years ago…\nPutting that aside, web services that allow interaction with large language models, starting with ChatGPT, have become very popular recently. According to a survey by Anthropic, they are particularly widespread in computer-using professions like programming. The technology supporting this is called RLHF (Reinforcement Learning from Human Feedback). I believe it was developed to formulate a problem for training a base LLM to interact nicely according to human preferences. In reality, I think it’s a contextual bandit problem (within reinforcement learning), but since it has ‘RL’ in its name, I felt I should give it a try. I hadn’t done anything about it, so I finally decided to get to it.\nSo, in this blog post, after a brief overview of RLHF, I will try to actually train a publicly available model with RLHF. I’m using a machine with four NVIDIA RTX4090s, so any model that can be trained with about 20GB of GPU memory would be fine. However, since I like Jax, I decided to use the code for Gemma 3, which was recently released by Google. As I’ll mention later, I think this decision was quite a mistake."
  },
  {
    "objectID": "posts/try_gemma3_rlhf-en.html#rlhfs-objective-function",
    "href": "posts/try_gemma3_rlhf-en.html#rlhfs-objective-function",
    "title": "Gemma3でRLHFを試してみる",
    "section": "2.1: RLHF’s Objective Function",
    "text": "2.1: RLHF’s Objective Function\nSo, what do we optimize using preference data? In RLHF, preferences are first converted into rewards based on a model called the Bradley-Terry model. This model uses \\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\) and assumes the existence of a real-valued function \\(r\\) such that \\(p(y \\succ y' | x) = \\sigma(r(x, y) - r(x, y'))\\). Then, for a dataset \\(\\mathcal{D} = (x_i, y_{w,i} \\succ y_{l, i})^N_{i=1}\\), \\(r\\) can be learned through logistic regression with the loss function \\(L(r)= -\\mathbb{E}_{(x, y_w, y_l)~D} \\left[ \\log ( \\sigma(r(x, y_w) - r(x, y_l)) ) \\right]\\).\nUnder this reward function, the objective function of RLHF is to maximize the constrained expected reward sum \\(J(\\pi) = \\mathbb{E}_\\pi [r(x, y)] − \\tau D_\\textbf{KL}(\\pi || \\pi_\\textbf{ref})\\). The term \\(\\tau D_\\textbf{KL}(\\pi || \\pi_\\textbf{ref})\\) is a constraint on the policy, and it seems common to use a pre-trained model as \\(\\pi_\\textbf{ref}\\) to prevent the policy from changing too drastically."
  },
  {
    "objectID": "posts/try_gemma3_rlhf-en.html#dpos-objective-function",
    "href": "posts/try_gemma3_rlhf-en.html#dpos-objective-function",
    "title": "Gemma3でRLHFを試してみる",
    "section": "2.2: DPO’s Objective Function",
    "text": "2.2: DPO’s Objective Function\nTo maximize the objective function in 2.1, it is necessary to first learn \\(r\\). DPO (Direct Preference Optimization) is a formulation that optimizes this directly. In DPO, the following minimization term is used as the objective function:\n\\(\\min_{\\pi} \\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}} \\left[ - \\log \\sigma \\left( \\tau \\log \\frac{\\pi(y_w|x)}{\\pi(y_l|x)} - \\tau \\log \\frac{\\pi_{ref}(y_w|x)}{\\pi_{ref}(y_l|x)}  \\right) \\right]\\)\nIt’s a bit hard to understand, so let’s plot \\(L_\\textbf{simple}= - \\log \\sigma(\\log \\frac{\\pi(y_w|x)}{\\pi(y_l|x)})\\), ignoring the constraint term and constants, with \\(\\frac{\\pi(y_w|x)}{\\pi(y_l|x)}\\) as the x-axis.\n\n\nCode\nimport numpy as np\nimport seaborn.objects as so\n\n\ndef sigmoid(x):\n    return 1.0 / (1.0 + np.exp(-x))\n\n\nx = np.linspace(0.1, 10.0, 100)\ny = -np.log(sigmoid(np.log(x)))\n\n(\n    so.Plot(\n        data={\"x\": x, \"y\": y},\n        x=\"x\",\n        y=\"y\",\n    )\n    .add(\n        so.Line(),\n        orient=\"y\",\n    )\n    .label(x=r\"$\\frac{\\pi(y_w|x)}{\\pi(y_l|x)}$\", y=r\"$L_\\text{simple}$\")\n)\n\n\n\n\n\n\n\n\n\nTherefore, the larger the probability density ratio of generating the more preferred \\(y_w\\) to the less preferred \\(y_l\\), the smaller this loss function becomes. It can be shown that, under certain assumptions, such as the validity of the Bradley-Terry model, the objective functions of DPO and RLHF are equivalent."
  },
  {
    "objectID": "posts/try_equinox_for_rl_jp.html",
    "href": "posts/try_equinox_for_rl_jp.html",
    "title": "equinoxで強化学習してみる",
    "section": "",
    "text": "最近equinoxというjaxベースの深層学習モデルを定義するライブラリを使ってみたのですが、これが中々いいと思ったので紹介ついでに強化学習してみます。他のjaxベースのライブラリにはDeepmindのhaikuやGoogle Researchのflaxがあります。この2つのライブラリは実際のところあまり変わりはありません。というのも、jaxには試験的に書かれたstaxという深層学習ライブラリのリファレンス実装があり、haikuもflaxもstaxをベースにオブジェクト志向的なModuleを採用したものだからです。あるいは、haikuやflaxは「staxをPyTorchっぽくしたもの」と言ってもいいかもしれません。equinoxのドキュメントにあるCompatibility with init-apply librariesというページでは、これらのライブラリのやり方を「init-applyアプローチ」と呼んで軽く説明しています。これについてざっと見てみましょう。"
  },
  {
    "objectID": "posts/try_equinox_for_rl_jp.html#環境",
    "href": "posts/try_equinox_for_rl_jp.html#環境",
    "title": "equinoxで強化学習してみる",
    "section": "環境",
    "text": "環境\n一通りequinoxの特徴を紹介したところで、これを使って強化学習してみます。せっかくjaxを使っているのとgymのAPIが変わりまくった上にgymnasiumに変わって全然ついていけないので、jax製の環境を使ってみましょう。ここではjumanjiというライブラリのMazeを使ってみます。\n\n\nCode\nimport jumanji\nfrom jumanji.wrappers import AutoResetWrapper\nfrom IPython.display import HTML\n\nenv = jumanji.make(\"Maze-v0\")\nenv = AutoResetWrapper(env)\nn_actions = env.action_spec().num_values\nkey, *keys = jax.random.split(jax.random.PRNGKey(20230720), 11)\nstate, timestep = env.reset(key)\nstates = [state]\nfor key in keys:\n    action = jax.random.randint(key=key, minval=0, maxval=n_actions, shape=())\n    state, timestep = env.step(state, action)\n    states.append(state)\nanim = env.animate(states)\nHTML(anim.to_html5_video().replace('=\"1000\"', '=\"640\"'))  # Change video size\n\n\n\n\n\n\n\n\n\n  \n  Your browser does not support the video tag.\n\n\n\n使いやすそうですね。ただデフォルトだと動画がハチャメチャなサイズだったので適当にHTMLのタグを書き換えて小さくしてます。 実際に学習するときにはvmapやjitと組み合わせて使えるようです。"
  },
  {
    "objectID": "posts/try_equinox_for_rl_jp.html#ppoを実装してみる",
    "href": "posts/try_equinox_for_rl_jp.html#ppoを実装してみる",
    "title": "equinoxで強化学習してみる",
    "section": "PPOを実装してみる",
    "text": "PPOを実装してみる\nということで、この環境を学習してみましょう。ここでは定番かつ学習が高速なPPOを実装してみます。\n\n入力\nそれぞれ壁の位置、エージェントの位置、ゴールの位置をそれぞれ10x10のバイナリ画像で表現し、3x10x10の配列として入力します。\n\n\nCode\nfrom jumanji.environments.routing.maze.types import Observation, State\n\ndef obs_to_image(obs: Observation) -&gt; jax.Array:\n    walls = obs.walls.astype(jnp.float32)\n    agent = jnp.zeros_like(walls).at[obs.agent_position].set(1.0)\n    target = jnp.zeros_like(walls).at[obs.target_position].set(1.0)\n    return jnp.stack([walls, agent, target])\n\n\n\n\nネットワーク\nたたみこんでからReLU + 線形レイヤを2回というシンプルな構成にします。途中までは価値関数と方策は共通でいいでしょう。方策はカテゴリカル分布とします。面倒なので、入力サイズ\\(3 \\times 10 \\times 10\\)、行動数\\(4\\)として入出力サイズをベタ書きしてしまいます。\n\n\nCode\nfrom typing import NamedTuple\n\nfrom jax.nn.initializers import orthogonal\n\n\nclass PPONetOutput(NamedTuple):\n    policy_logits: jax.Array\n    value: jax.Array\n\n\nclass SoftmaxPPONet(eqx.Module):\n    torso: list\n    value_head: eqx.nn.Linear\n    policy_head: eqx.nn.Linear\n\n    def __init__(self, key: jax.Array) -&gt; None:\n        key1, key2, key3, key4, key5 = jax.random.split(key, 5)\n        # Common layers\n        self.torso = [\n            eqx.nn.Conv2d(3, 1, kernel_size=3, key=key1),\n            jax.nn.relu,\n            jnp.ravel,\n            eqx.nn.Linear(64, 64, key=key2),\n            jax.nn.relu,\n        ]\n        self.value_head = eqx.nn.Linear(64, 1, key=key3)\n        policy_head = eqx.nn.Linear(64, 4, key=key4)\n        # Use small value for policy initialization\n        self.policy_head = eqx.tree_at(\n            lambda linear: linear.weight,\n            policy_head,\n            orthogonal(scale=0.01)(key5, policy_head.weight.shape),\n        )\n\n    def __call__(self, x: jax.Array) -&gt; PPONetOutput:\n        for layer in self.torso:\n            x = layer(x)\n        value = self.value_head(x)\n        policy_logits = self.policy_head(x)\n        return PPONetOutput(policy_logits=policy_logits, value=value)\n\n    def value(self, x: jax.Array) -&gt; jax.Array:\n        for layer in self.torso:\n            x = layer(x)\n        return self.value_head(x)\n\n\n\n\nロールアウト\nPPOの実装では1000~8000ステップ程度環境で行動した履歴を集めてそれを使って何度かネットワークを更新するのが普通です。ここではjax.lax.scanを使って、Pythonのループを使うより高速なロールアウトを実装します。scanの速度面での恩恵は大きいですが、使い方には少し注意が必要です。特に、1ステップ進める関数の二番目の出力をresults: list[Result] とすると、最終的に返ってくるのがResult(member1=stack([m1 for m1 in results.member1]), ...)になることは把握しておきましょう。 また、exec_rolloutの引数がeqx.ModuleのインスタンスであるSoftmaxPPONetを含んでいるので、eqx.filter_jitでjitしてあげるとうまいことjitできない値を除外してjitしてくれます。 行動は方策ネットワークの出力をsoftmaxしてサンプルしますが、Observationに迷路で移動可能な方向を教えてくれるaction_maskが入っているので、これを使って取れない行動には-infをかけてマスクしておきましょう。簡単な環境ならこれをやらなくても大丈夫だと思うのですが、壁を多いときはこれがないとけっこう難しいです。\n\n\nCode\nimport chex\n\n\n@chex.dataclass\nclass Rollout:\n    \"\"\"Rollout buffer that stores the entire history of one rollout\"\"\"\n\n    observations: jax.Array\n    actions: jax.Array\n    action_masks: jax.Array\n    rewards: jax.Array\n    terminations: jax.Array\n    values: jax.Array\n    policy_logits: jax.Array\n\n\ndef mask_logits(policy_logits: jax.Array, action_mask: jax.Array) -&gt; jax.Array:\n    return jax.lax.select(\n        action_mask,\n        policy_logits,\n        jnp.ones_like(policy_logits) * -jnp.inf,\n    )\n\n\nvmapped_obs2i = jax.vmap(obs_to_image)\n\n\n@eqx.filter_jit\ndef exec_rollout(\n    initial_state: State,\n    initial_obs: Observation,\n    env: jumanji.Environment,\n    network: SoftmaxPPONet,\n    prng_key: jax.Array,\n    n_rollout_steps: int,\n) -&gt; tuple[State, Rollout, Observation, jax.Array]:\n    def step_rollout(\n        carried: tuple[State, Observation],\n        key: jax.Array,\n    ) -&gt; tuple[tuple[State, jax.Array], Rollout]:\n        state_t, obs_t = carried\n        obs_image = vmapped_obs2i(obs_t)\n        net_out = jax.vmap(network)(obs_image)\n        masked_logits = mask_logits(net_out.policy_logits, obs_t.action_mask)\n        actions = jax.random.categorical(key, masked_logits, axis=-1)\n        state_t1, timestep = jax.vmap(env.step)(state_t, actions)\n        rollout = Rollout(\n            observations=obs_image,\n            actions=actions,\n            action_masks=obs_t.action_mask,\n            rewards=timestep.reward,\n            terminations=1.0 - timestep.discount,\n            values=net_out.value,\n            policy_logits=masked_logits,\n        )\n        return (state_t1, timestep.observation), rollout\n\n    (state, obs), rollout = jax.lax.scan(\n        step_rollout,\n        (initial_state, initial_obs),\n        jax.random.split(prng_key, n_rollout_steps),\n    )\n    next_value = jax.vmap(network.value)(vmapped_obs2i(obs))\n    return state, rollout, obs, next_value\n\n\nテストしてみましょう。jax.vmapで簡単に環境をベクトル並列化できるのがjax製環境の利点なので、今回は16並列で動かしてみます。resetをvmapしてPRNGKeyを16個突っ込むと勝手に16並列のStateがでてきます。\n\n\nCode\nkey, net_key, reset_key, rollout_key = jax.random.split(key, 4)\npponet = SoftmaxPPONet(net_key)\ninitial_state, initial_timestep = jax.vmap(env.reset)(jax.random.split(reset_key, 16))\nnext_state, rollout, next_obs, next_value = exec_rollout(\n    initial_state,\n    initial_timestep.observation,\n    env,\n    pponet,\n    rollout_key,\n    512,\n)\nrollout.rewards.shape\n\n\n(512, 16)\n\n\n入力が16並列になっていること、Rolloutの各メンバがステップ数x環境数x…の大きさになっていることが確認できました。\n\n\n学習\nデータが集められたので後はネットワークを更新するコードを書きましょう。まずGAEを計算します。意外とボトルネックになるのでfori_loopで高速化しておきましょう。\n\n\nCode\n@chex.dataclass(frozen=True, mappable_dataclass=False)\nclass Batch:\n    \"\"\"Batch for PPO, indexable to get a minibatch.\"\"\"\n\n    observations: jax.Array\n    action_masks: jax.Array\n    onehot_actions: jax.Array\n    rewards: jax.Array\n    advantages: jax.Array\n    value_targets: jax.Array\n    log_action_probs: jax.Array\n\n    def __getitem__(self, idx: jax.Array):\n        return self.__class__(  # type: ignore\n            observations=self.observations[idx],\n            action_masks=self.action_masks[idx],\n            onehot_actions=self.onehot_actions[idx],\n            rewards=self.rewards[idx],\n            advantages=self.advantages[idx],\n            value_targets=self.value_targets[idx],\n            log_action_probs=self.log_action_probs[idx],\n        )\n\n\ndef compute_gae(\n    r_t: jax.Array,\n    discount_t: jax.Array,\n    values: jax.Array,\n    lambda_: float = 0.95,\n) -&gt; jax.Array:\n    \"\"\"Efficiently compute generalized advantage estimator (GAE)\"\"\"\n\n    gamma_lambda_t = discount_t * lambda_\n    delta_t = r_t + discount_t * values[1:] - values[:-1]\n    n = delta_t.shape[0]\n\n    def update(i: int, advantage_t: jax.Array) -&gt; jax.Array:\n        t = n - i - 1\n        adv_t = delta_t[t] + gamma_lambda_t[t] * advantage_t[t + 1]\n        return advantage_t.at[t].set(adv_t)\n\n    advantage_t = jax.lax.fori_loop(0, n, update, jnp.zeros_like(values))\n    return advantage_t[:-1]\n\n\n@eqx.filter_jit\ndef make_batch(\n    rollout: Rollout,\n    next_value: jax.Array,\n    gamma: float,\n    gae_lambda: float,\n) -&gt; Batch:\n    all_values = jnp.concatenate(\n        [jnp.squeeze(rollout.values), next_value.reshape(1, -1)]\n    )\n    advantages = compute_gae(\n        rollout.rewards,\n        # Set γ = 0 when the episode terminates\n        (1.0 - rollout.terminations) * gamma,\n        all_values,\n        gae_lambda,\n    )\n    value_targets = advantages + all_values[:-1]\n    onehot_actions = jax.nn.one_hot(rollout.actions, 4)\n    _, _, *obs_shape = rollout.observations.shape\n    log_action_probs = jnp.sum(\n        jax.nn.log_softmax(rollout.policy_logits) * onehot_actions,\n        axis=-1,\n    )\n    return Batch(\n        observations=rollout.observations.reshape(-1, *obs_shape),\n        action_masks=rollout.action_masks.reshape(-1, 4),\n        onehot_actions=onehot_actions.reshape(-1, 4),\n        rewards=rollout.rewards.ravel(),\n        advantages=advantages.ravel(),\n        value_targets=value_targets.ravel(),\n        log_action_probs=log_action_probs.ravel(),\n    )\n\n\n\n\nCode\nbatch = make_batch(rollout, next_value, 0.99, 0.95)\nbatch.advantages.shape, batch.onehot_actions.shape, batch.log_action_probs.shape\n\n\n((8192,), (8192, 4), (8192,))\n\n\n\\(512 \\times 16 = 8192\\)なので大丈夫そうですね。あとはこれで作ったBatchからミニバッチをサンプルして、損失関数を最小化するように勾配降下で更新します。jax界隈では定番のoptaxを使いましょう。この時、以下の3点に注意します。\n\n前節で説明したように、jax.gradのかわりにeqx.filter_gradを使う\nSoftmaxPPONetはjax.nn.reluなどjax.jitの引数として使えない型を持っているので、jax.lax.scanの引数にする前にeqx.partitionで分解する\n同様に、SoftmaxPPONetはそのままoptaxの初期化・アップデート関数の引数にできないので、eqx.partitionで分解するかeqx.filterでjax.Array以外のメンバを除外しておく\n\nまた、ミニバッチ更新のループでjax.lax.scanを使いたい場合、いくつか方法があると思うのですが、ここではミニバッチサイズ\\(N\\)、更新回数\\(M\\)、更新エポック数\\(K\\)回、全体のバッチサイズ\\(N \\times M\\)として、\n\n\\(0, 1, 2, ..., NM - 1\\)の順列を\\(K\\)個作る\nバッチの各要素を1で作った順列のもと並び替えたものを\\(K\\)個作る\n各メンバをjnp.concatenateでくっつけて大きさ\\(MK \\times N \\times ...\\)の配列にreshapeする\n\nという方法を使いました。ちょっと面倒ですし、正直ここまで高速化しなくてもいいかもしれませんね。メモリ使用量が不安な場合は\\(K\\)のループをPythonで書くのもアリかなと思いますが、今回は入力が\\(3\\times 10\\times 10\\)なので大丈夫そうですね。\n\n\nCode\nimport optax\n\n\ndef loss_function(\n    network: SoftmaxPPONet,\n    batch: Batch,\n    ppo_clip_eps: float,\n) -&gt; jax.Array:\n    net_out = jax.vmap(network)(batch.observations)\n    # Policy loss\n    log_pi = jax.nn.log_softmax(\n        jax.lax.select(\n            batch.action_masks,\n            net_out.policy_logits,\n            jnp.ones_like(net_out.policy_logits * -jnp.inf),\n        )\n    )\n    log_action_probs = jnp.sum(log_pi * batch.onehot_actions, axis=-1)\n    policy_ratio = jnp.exp(log_action_probs - batch.log_action_probs)\n    clipped_ratio = jnp.clip(policy_ratio, 1.0 - ppo_clip_eps, 1.0 + ppo_clip_eps)\n    clipped_objective = jnp.fmin(\n        policy_ratio * batch.advantages,\n        clipped_ratio * batch.advantages,\n    )\n    policy_loss = -jnp.mean(clipped_objective)\n    # Value loss\n    value_loss = jnp.mean(0.5 * (net_out.value - batch.value_targets) ** 2)\n    # Entropy regularization\n    entropy = jnp.mean(-jnp.exp(log_pi) * log_pi)\n    return policy_loss + value_loss - 0.01 * entropy\n\n\nvmapped_permutation = jax.vmap(jax.random.permutation, in_axes=(0, None), out_axes=0)\n\n\n@eqx.filter_jit\ndef update_network(\n    batch: Batch,\n    network: SoftmaxPPONet,\n    optax_update: optax.TransformUpdateFn,\n    opt_state: optax.OptState,\n    prng_key: jax.Array,\n    minibatch_size: int,\n    n_epochs: int,\n    ppo_clip_eps: float,\n) -&gt; tuple[optax.OptState, SoftmaxPPONet]:\n    # Prepare update function\n    dynamic_net, static_net = eqx.partition(network, eqx.is_array)\n\n    def update_once(\n        carried: tuple[optax.OptState, SoftmaxPPONet],\n        batch: Batch,\n    ) -&gt; tuple[tuple[optax.OptState, SoftmaxPPONet], None]:\n        opt_state, dynamic_net = carried\n        network = eqx.combine(dynamic_net, static_net)\n        grad = eqx.filter_grad(loss_function)(network, batch, ppo_clip_eps)\n        updates, new_opt_state = optax_update(grad, opt_state)\n        dynamic_net = optax.apply_updates(dynamic_net, updates)\n        return (new_opt_state, dynamic_net), None\n\n    # Prepare minibatches\n    batch_size = batch.observations.shape[0]\n    permutations = vmapped_permutation(jax.random.split(prng_key, n_epochs), batch_size)\n    minibatches = jax.tree_map(\n        # Here, x's shape is [batch_size, ...]\n        lambda x: x[permutations].reshape(-1, minibatch_size, *x.shape[1:]),\n        batch,\n    )\n    # Update network n_epochs x n_minibatches times\n    (opt_state, updated_dynet), _ = jax.lax.scan(\n        update_once,\n        (opt_state, dynamic_net),\n        minibatches,\n    )\n    return opt_state, eqx.combine(updated_dynet, static_net)\n\n\nというわけで、部品が全部できたので学習を回してみましょう。まず何も壁がない簡単な迷路で試してみます。簡単な迷路を作りましょう。 junmanjiのデフォルトの迷路ジェネレーターをコピペして適当に書き換えました。また、この環境ではゴールでのみ報酬が与えられるので、単純に報酬和/終了したエピソードの数 でエピソードあたりの平均リターンが求められます。これを訓練状況の指標として出力しておきましょう。ハイパーパラメータはわりと勘で決めました。\n\n\nCode\nfrom jumanji.environments.routing.maze.generator import Generator\nfrom jumanji.environments.routing.maze.types import Position, State\n\n\nclass TestGenerator(Generator):\n    def __init__(self) -&gt; None:\n        super().__init__(num_rows=10, num_cols=10)\n\n    def __call__(self, key: chex.PRNGKey) -&gt; State:\n        walls = jnp.zeros((10, 10), dtype=bool)\n        agent_position = Position(row=0, col=0)\n        target_position = Position(row=9, col=9)\n\n        # Build the state.\n        return State(\n            agent_position=agent_position,\n            target_position=target_position,\n            walls=walls,\n            action_mask=None,\n            key=key,\n            step_count=jnp.array(0, jnp.int32),\n        )\n\n\n\n\nCode\ndef run_training(\n    key: jax.Array,\n    adam_lr: float = 3e-4,\n    adam_eps: float = 1e-7,\n    gamma: float = 0.99,\n    gae_lambda: float = 0.95,\n    n_optim_epochs: int = 10,\n    minibatch_size: int = 1024,\n    n_agents: int = 16,\n    n_rollout_steps: int = 512,\n    n_total_steps: int = 16 * 512 * 100,\n    ppo_clip_eps: float = 0.2,\n    **env_kwargs,\n) -&gt; SoftmaxPPONet:\n    key, net_key, reset_key = jax.random.split(key, 3)\n    pponet = SoftmaxPPONet(net_key)\n    env = AutoResetWrapper(jumanji.make(\"Maze-v0\", **env_kwargs))\n    adam_init, adam_update = optax.adam(adam_lr, eps=adam_eps)\n    opt_state = adam_init(eqx.filter(pponet, eqx.is_array))\n    env_state, timestep = jax.vmap(env.reset)(jax.random.split(reset_key, 16))\n    obs = timestep.observation\n\n    n_loop = n_total_steps // (n_agents * n_rollout_steps)\n    return_reporting_interval = 1 if n_loop &lt; 10 else n_loop // 10\n    n_episodes, reward_sum = 0.0, 0.0\n    for i in range(n_loop):\n        key, rollout_key, update_key = jax.random.split(key, 3)\n        env_state, rollout, obs, next_value = exec_rollout(\n            env_state,\n            obs,\n            env,\n            pponet,\n            rollout_key,\n            n_rollout_steps,\n        )\n        batch = make_batch(rollout, next_value, gamma, gae_lambda)\n        opt_state, pponet = update_network(\n            batch,\n            pponet,\n            adam_update,\n            opt_state,\n            update_key,\n            minibatch_size,\n            n_optim_epochs,\n            ppo_clip_eps,\n        )\n        n_episodes += jnp.sum(rollout.terminations).item()\n        reward_sum += jnp.sum(rollout.rewards).item()\n        if i &gt; 0 and (i % return_reporting_interval == 0):\n            print(f\"Mean episodic return: {reward_sum / n_episodes}\")\n            n_episodes = 0.0\n            reward_sum = 0.0\n    return pponet\n\n\n\n\nCode\nimport datetime\n\nstarted = datetime.datetime.now()\nkey, training_key = jax.random.split(key)\ntrained_net = run_training(training_key, n_total_steps=16 * 512 * 10, generator=TestGenerator())\nelapsed = datetime.datetime.now() - started\nprint(f\"Elapsed time: {elapsed.total_seconds():.2}s\")\n\n\nMean episodic return: 0.40782122905027934\nMean episodic return: 0.967741935483871\nMean episodic return: 1.0\nMean episodic return: 1.0\nMean episodic return: 1.0\nMean episodic return: 1.0\nMean episodic return: 1.0\nMean episodic return: 1.0\nMean episodic return: 1.0\nElapsed time: 3.2s\n\n\n約8万ステップが3秒ちょっとで学習できました。速いですね。学習したエージェントがどんなもんか見てみましょう。\n\n\nCode\n@eqx.filter_jit\ndef visualization_rollout(\n    key: jax.random.PRNGKey,\n    pponet: SoftmaxPPONet,\n    env: jumanji.Environment,\n    n_steps: int,\n) -&gt; list[State]:\n    def step_rollout(\n        carried: tuple[State, Observation],\n        key: jax.Array,\n    ) -&gt; tuple[tuple[State, jax.Array], State]:\n        state_t, obs_t = carried\n        obs_image = obs_to_image(obs_t)\n        net_out = pponet(obs_image)\n        action, _ = sample_action(key, net_out.policy_logits, obs_t.action_mask)\n        state_t1, timestep = env.step(state_t, action)\n        return (state_t1, timestep.observation), state_t1\n\n    initial_state, timestep = env.reset(key)\n    _, states = jax.lax.scan(\n        step_rollout,\n        (initial_state, timestep.observation),\n        jax.random.split(key, n_steps),\n    )\n    leaves, treedef = jax.tree_util.tree_flatten(states)\n    return [initial_state] + [treedef.unflatten(leaf) for leaf in zip(*leaves)]\n\n\n\n\nCode\nenv = AutoResetWrapper(jumanji.make(\"Maze-v0\", generator=TestGenerator()))\nkey, eval_key = jax.random.split(key)\nstates = visualization_rollout(eval_key, trained_net, env, 40)\nanim = env.animate(states)\nHTML(anim.to_html5_video().replace('=\"1000\"', '=\"640\"'))\n\n\n\n\n\n\n\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nさすがにこのくらいなら簡単そうですね。次はもう少し難しくしてみたいところです。デフォルトの完全ランダム生成される迷路はやたら遅かったので、適当に自作してみましょう。スタート、ゴールをそれぞれ3箇所から同確率でサンプルし、合計9通りの組み合わせを解きます。まあこのくらいならたぶんいけるでしょう。10倍の80万ステップ学習させてみます。\n\n\nCode\nclass MedDifficultyGenerator(Generator):\n    WALLS = [\n        [0, 0, 1, 0, 0, 1, 0, 1, 1, 0],\n        [0, 0, 1, 0, 0, 1, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 1, 1, 0, 1, 1],\n        [1, 1, 1, 1, 0, 0, 0, 0, 1, 0],\n        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],\n        [0, 0, 1, 0, 1, 1, 0, 0, 0, 0],\n        [1, 0, 1, 0, 1, 1, 0, 1, 1, 1],\n        [0, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n        [1, 1, 1, 0, 0, 1, 0, 1, 1, 0],\n        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n    ]\n    def __init__(self) -&gt; None:\n        super().__init__(num_rows=10, num_cols=10)\n\n    def __call__(self, key: chex.PRNGKey) -&gt; State:\n        key, config_key = jax.random.split(key)\n        walls = jnp.array(self.WALLS).astype(bool)\n        agent_cfg, target_cfg = jax.random.randint(config_key, (2,), 0, 2)\n        agent_position = jax.lax.switch(\n            agent_cfg,\n            [\n                lambda: Position(row=0, col=0),\n                lambda: Position(row=9, col=0),\n                lambda: Position(row=0, col=9),\n            ]\n        )\n        target_position = jax.lax.switch(\n            target_cfg,\n            [\n                lambda: Position(row=3, col=9),\n                lambda: Position(row=7, col=8),\n                lambda: Position(row=7, col=0),\n            ]\n        )\n        # Build the state.\n        return State(\n            agent_position=agent_position,\n            target_position=target_position,\n            walls=walls,\n            action_mask=None,\n            key=key,\n            step_count=jnp.array(0, jnp.int32),\n        )\n\n\n\n\nCode\nimport datetime\n\nstarted = datetime.datetime.now()\nkey, training_key = jax.random.split(key)\ntrained_net = run_training(\n    training_key,\n    n_total_steps=16 * 512 * 100,\n    generator=MedDifficultyGenerator(),\n)\nelapsed = datetime.datetime.now() - started\nprint(f\"Elapsed time: {elapsed.total_seconds():.2}s\")\n\n\nMean episodic return: 0.2812202097235462\nMean episodic return: 0.4077407740774077\nMean episodic return: 0.5992010652463382\nMean episodic return: 0.7285136501516684\nMean episodic return: 0.75\nMean episodic return: 0.8620564808110065\nMean episodic return: 0.9868290258449304\nMean episodic return: 0.9977788746298124\nMean episodic return: 0.9970540974825924\nElapsed time: 1.2e+01s\n\n\n\n\nCode\nenv = AutoResetWrapper(jumanji.make(\"Maze-v0\", generator=MedDifficultyGenerator()))\nkey, eval_key = jax.random.split(key)\nstates = visualization_rollout(eval_key, trained_net, env, 100)\nanim = env.animate(states)\nHTML(anim.to_html5_video().replace('=\"1000\"', '=\"640\"'))\n\n\n\n\n\n\n\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nやや逡巡が見られますがゴールには行けてるっぽいですね。"
  },
  {
    "objectID": "posts/sandb-exercise-racetrack.html",
    "href": "posts/sandb-exercise-racetrack.html",
    "title": "Exercise 5.12 Racetrack from the Reinforcement Learning textbook",
    "section": "",
    "text": "Here I demonstrate the execise 5.12 of the textbook Reinforcement Learning: An Introduction by Richard Sutton and Andrew G. Barto, using both of planning method and Monte Carlo method. Basic knowledge of Python (&gt;= 3.7) and NumPy are assumed. Some konwledge of matplotlib and Python typing library also helps.\nContact: yuji.kanagawa@oist.jp"
  },
  {
    "objectID": "posts/sandb-exercise-racetrack.html#modeling-the-problem-in-code",
    "href": "posts/sandb-exercise-racetrack.html#modeling-the-problem-in-code",
    "title": "Exercise 5.12 Racetrack from the Reinforcement Learning textbook",
    "section": "Modeling the problem in code",
    "text": "Modeling the problem in code\nLet’s start from writing the problem in code. What are important in this phase? Here, I’d like to emphasize the importantness of looking back at the definition of the environment. I.e., in reinforcement learning (RL), environments are modelled by Markov decision process (MDP), consisting of states, actions, transition function, and reward function. So first let’s check the definition of states and actions in the problem statement. It’s (somehow) not very straightforward, but we can find &gt; In our simplified racetrack, the car is at one of a discrete set of grid positions, the cells in the diagram. The velocity is also discrete, a number of grid cells moved horizontally and vertically per time step.\nSo, a state consists of position and velocity of the car (a.k.a. agent). What about actions?\n\nThe actions are increments to the velocity components. Each may be changed by +1, −1, or 0 in each step, for a total of nine (\\(3 \\times 3\\)) actions.\n\nSo there are 9 actions for each direction (↓↙←↖↑↗→↘ or no acceleration). Here, we can also notice that the total number of states is given by \\(\\textrm{Num. positions} \\times \\textrm{Num. choices of velocity}\\). And the texbook also says &gt; Both velocity components are restricted to be nonnegative and less than 5, and they cannot both be zero except at the starting line.\nSo there are 24 possible velocity at the non-starting positions:\n\n\nCode\nimport itertools\nlist(itertools.product(range(5), range(5)))[1:]\n\n\n[(0, 1),\n (0, 2),\n (0, 3),\n (0, 4),\n (1, 0),\n (1, 1),\n (1, 2),\n (1, 3),\n (1, 4),\n (2, 0),\n (2, 1),\n (2, 2),\n (2, 3),\n (2, 4),\n (3, 0),\n (3, 1),\n (3, 2),\n (3, 3),\n (3, 4),\n (4, 0),\n (4, 1),\n (4, 2),\n (4, 3),\n (4, 4)]\n\n\nAgain, number of states is given by (roughly) \\(24 \\times \\textrm{Num. positions}\\) and number of actions is \\(9\\). Sounds not very easy problem with many positions.\nSo, let’s start the coding from representing the state and actions. There are multiple ways, but I prefer to NumPy array for representing everything.\nLet’s consider a ASCII representation of the map (or track) like this:\n\n\nCode\nSMALL_TRACK = \"\"\"\n###      F\n##       F\n##       F\n#      ###\n#      ###\n#      ###\n#SSSSS####\n\"\"\"\n\n\nHere, S denotes a starting positiona, F denotes a finishing position, # denotes a wall, and  denotes a road. We have this track as a 2D NumPy Array, and encode agent’s position as an index of this array.\n\n\nCode\nimport numpy as np\n\ndef ascii_to_array(ascii_track: str) -&gt; np.ndarray:\n    \"\"\"Convert the ascii (string) map to a NumPy array.\"\"\"\n\n    lines = [line for line in ascii_track.split(\"\\n\") if len(line) &gt; 0]\n    byte_lines = [list(bytes(line, encoding=\"utf-8\")) for line in lines]\n    return np.array(byte_lines, dtype=np.uint8)\n\ntrack = ascii_to_array(SMALL_TRACK)\nprint(track)\nposition = np.array([0, 0])\ntrack[tuple(position)] == int.from_bytes(b'#', \"big\")\n\n\n[[35 35 35 32 32 32 32 32 32 70]\n [35 35 32 32 32 32 32 32 32 70]\n [35 35 32 32 32 32 32 32 32 70]\n [35 32 32 32 32 32 32 35 35 35]\n [35 32 32 32 32 32 32 35 35 35]\n [35 32 32 32 32 32 32 35 35 35]\n [35 83 83 83 83 83 35 35 35 35]]\n\n\nTrue\n\n\nThen, agent’s velocity and acceleration are also naturally represented by an array. And, we represent an action as an index of an array consisting of all possible acceleration vetors:\n\n\nCode\nnp.array(list(itertools.product([-1, 0, 1], [-1, 0, 1])))\n\n\narray([[-1, -1],\n       [-1,  0],\n       [-1,  1],\n       [ 0, -1],\n       [ 0,  0],\n       [ 0,  1],\n       [ 1, -1],\n       [ 1,  0],\n       [ 1,  1]])\n\n\nThe next step is to represent a transition function as a black box simulator. Note that we will visit another representation by transition matrix, but implementing the simulator is easier. Basically, the simulator should take an agent’s action and current state, and then return the next state. Let’s call this function step. However, let’s make it return some other things to make the implementation easier. Reward function sounds fairly easy to implement given the agent’s position. &gt; The rewards are −1 for each step until the car crosses the finish line.\nAlso, we have to handle the termination of the episode. &gt; Each episode begins in one of the randomly selected start states with both velocity components zero and ends when the car crosses the finish line.\nSo the resulting step function should return a tuple (state, reward, termination). The below cell contains my implementation of the simulator with matplotlib visualization. The step function is so complicated to handle the case where the agent goes through a wall, so readers are encouraged to just run their eyes through.\n\n\nCode\n# collapse-hide\n\nfrom typing import List, NamedTuple, Optional, Tuple\n\nfrom IPython.display import display\nfrom matplotlib import pyplot as plt\nfrom matplotlib.axes import Axes\nfrom matplotlib.colors import ListedColormap\n\n\nclass State(NamedTuple):\n    position: np.ndarray\n    velocity: np.ndarray\n\n\nclass RacetrackEnv:\n    \"\"\"Racetrack environment\"\"\"\n\n    EMPTY = int.from_bytes(b\" \", \"big\")\n    WALL = int.from_bytes(b\"#\", \"big\")\n    START = int.from_bytes(b\"S\", \"big\")\n    FINISH = int.from_bytes(b\"F\", \"big\")\n\n    def __init__(\n        self,\n        ascii_track: str,\n        noise_prob: float = 0.1,\n        seed: int = 0,\n    ) -&gt; None:\n        self._track = ascii_to_array(ascii_track)\n        self._max_height, self._max_width = self._track.shape\n        self._noise_prob = noise_prob\n        self._actions = np.array(list(itertools.product([-1, 0, 1], [-1, 0, 1])))\n        self._no_accel = 4\n        self._random_state = np.random.RandomState(seed=seed)\n        self._start_positions = np.argwhere(self._track == self.START)\n        self._state_indices = None\n        self._ax = None\n        self._agent_fig = None\n        self._arrow_fig = None\n        \n    def state_index(self, state: State) -&gt; int:\n        \"\"\"Returns a state index\"\"\"\n        (y, x), (vy, vx) = state\n        return y * self._max_width * 25 + x * 25 + vy * 5 + vx\n        \n\n    def _all_passed_positions(\n        self,\n        start: np.ndarray,\n        velocity: np.ndarray,\n    ) -&gt; Tuple[List[np.ndarray], bool]:\n        \"\"\"\n        List all positions that the agent passes over.\n        Here we assume that the y-directional velocity is already flipped by -1.\n        \"\"\"\n\n        maxv = np.max(np.abs(velocity))\n        if maxv == 0:\n            return [start], False\n        one_step_vector = velocity / maxv\n        pos = start + 0.0\n        traj = []\n        for i in range(maxv):\n            pos = pos + one_step_vector\n            ceiled = np.ceil(pos).astype(int)\n            if self._is_out(ceiled):\n                return traj, True\n            traj.append(ceiled)\n        # To prevent numerical issue\n        traj[-1] = start + velocity\n        return traj, False\n\n    def _is_out(self, position: np.ndarray) -&gt; bool:\n        \"\"\"Returns whether the given position is out of the map.\"\"\"\n        y, x = position\n        return y &lt; 0 or x &gt;= self._max_width\n\n    def step(self, state: State, action: int) -&gt; Tuple[State, float, bool]:\n        \"\"\"\n        Taking the current state and an agents' action, returns the next state,\n        reward and a boolean flag that indicates that the current episode terminates.\n        \"\"\"\n        position, velocity = state\n        if self._random_state.rand() &lt; self._noise_prob:\n            accel = self._actions[self._no_accel]\n        else:\n            accel = self._actions[action]\n        # velocity is clipped so that only ↑→ directions are possible\n        next_velocity = np.clip(velocity + accel, a_min=0, a_max=4)\n        # If both of velocity is 0, cancel the acceleration\n        if np.sum(next_velocity) == 0:\n            next_velocity = velocity\n        # List up trajectory. y_velocity is flipped to adjust the coordinate system.\n        traj, went_out = self._all_passed_positions(\n            position,\n            next_velocity * np.array([-1, 1]),\n        )\n        passed_wall, passed_finish = False, False\n        for track in map(lambda pos: self._track[tuple(pos)], traj):\n            passed_wall |= track == self.WALL\n            passed_finish |= track == self.FINISH\n        if not passed_wall and passed_finish:  # Goal!\n            return State(traj[-1], next_velocity), 0, True\n        elif passed_wall or went_out:  # Crasshed to the wall or run outside\n            return self.reset(), -1.0, False\n        else:\n            return State(traj[-1], next_velocity), -1, False\n\n    def reset(self) -&gt; State:\n        \"\"\"Randomly assigns a start position of the agent.\"\"\"\n        n_starts = len(self._start_positions)\n        initial_pos_idx = self._random_state.choice(n_starts)\n        initial_pos = self._start_positions[initial_pos_idx]\n        initial_velocity = np.array([0, 0])\n        return State(initial_pos, initial_velocity)\n\n    def render(\n        self,\n        state: Optional[State] = None,\n        movie: bool = False,\n        ax: Optional[Axes] = None,\n    ) -&gt; Axes:\n        \"\"\"Render the map and (optinally) the agents' position and velocity.\"\"\"\n        if self._ax is None or ax is not None:\n            if ax is None:\n                _, ax = plt.subplots(1, 1, figsize=(8, 8))\n            ax.set_aspect(\"equal\")\n            ax.set_xticks([])\n            ax.set_yticks([])\n            # Map the track to one of [0, 1, 2, 3] to that simple colormap works\n            map_array = np.zeros_like(track)\n            symbols = [self.EMPTY, self.WALL, self.START, self.FINISH]\n            for i in range(track.shape[0]):\n                for j in range(track.shape[1]):\n                    map_array[i, j] = symbols.index(self._track[i, j])\n            cm = ListedColormap(\n                [\"w\", \".75\", \"xkcd:reddish orange\", \"xkcd:kermit green\"]\n            )\n            map_img = ax.imshow(\n                map_array,\n                cmap=cm,\n                vmin=0,\n                vmax=4,\n                alpha=0.8,\n            )\n            if ax.get_legend() is None:\n                descriptions = [\"Empty\", \"Wall\", \"Start\", \"Finish\"]\n                for i in range(1, 4):\n                    if np.any(map_array == i):\n                        ax.plot([0.0], [0.0], color=cm(i), label=descriptions[i])\n                ax.legend(fontsize=12, loc=\"lower right\")\n            self._ax = ax\n        if state is not None:\n            if not movie and self._agent_fig is not None:\n                self._agent_fig.remove()\n            if not movie and self._arrow_fig is not None:\n                self._arrow_fig.remove()\n            pos, vel = state\n            self._agent_fig = self._ax.plot(pos[1], pos[0], \"k^\", markersize=20)[0]\n            # Show velocity\n            self._arrow_fig = self._ax.annotate(\n                \"\",\n                xy=(pos[1], pos[0] + 0.2),\n                xycoords=\"data\",\n                xytext=(pos[1] - vel[1], pos[0] + vel[0] + 0.2),\n                textcoords=\"data\",\n                arrowprops={\"color\": \"xkcd:blueberry\", \"alpha\": 0.6, \"width\": 2},\n            )\n        return self._ax\n\n\nsmalltrack = RacetrackEnv(SMALL_TRACK)\nstate = smalltrack.reset()\nprint(state)\ndisplay(smalltrack.render(state=state).get_figure())\nnext_state, reward, termination = smalltrack.step(state, 7)\nprint(next_state)\nsmalltrack.render(state=next_state)\n\n\nState(position=array([6, 5]), velocity=array([0, 0]))\n\n\n\n\n\n\n\n\n\nState(position=array([5, 5]), velocity=array([1, 0]))\n\n\n\n\n\n\n\n\n\nNote that the vertical velocity is negative, so that we can simply represent the coordinate by an array index."
  },
  {
    "objectID": "posts/sandb-exercise-racetrack.html#solve-a-small-problem-by-dynamic-programming",
    "href": "posts/sandb-exercise-racetrack.html#solve-a-small-problem-by-dynamic-programming",
    "title": "Exercise 5.12 Racetrack from the Reinforcement Learning textbook",
    "section": "Solve a small problem by dynamic programming",
    "text": "Solve a small problem by dynamic programming\nOK, now we have a simulator, so let’s solve the problem! However, before stepping into reinforcement learning, it’s better to compute an optimal policy in a small problem for sanity check. To do so, we need a transition matrix p, which is a \\(|\\mathcal{S}| \\times |\\mathcal{A}| \\times |\\mathcal{S}|\\) NumPy array where p[i][j][k] is the probability of transiting from i to k when action j is taken. Also, we need a \\(|\\mathcal{S}| \\times |\\mathcal{A}| \\times |\\mathcal{S}|\\) reward matrix r. Note that this representation is not general as \\(R_t\\) can be stochastic, but since the only stochasticity of rewards is the noise to actions in this problem, this notion suffices. It is often not very straightforward to get p and r from the problem definition, but basically we need to give 0-indexd indices to each state (0, 1, 2, ...) and fill the array. Here, I index each state by \\(\\textrm{Idx(S)} = y \\times 25 \\times W + x \\times 25 + vy \\times 5 + vx\\), where \\((x, y)\\) is a position, \\((vx, vy\\)) is a velocity, and \\(W\\) is the width of the map.\n\n\nCode\n# collapse-hide\nfrom typing import Iterable\n\n\ndef get_p_and_r(env: RacetrackEnv) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Taking RacetrackEnv, returns the transition probability p and reward fucntion r of the env.\"\"\"\n    n_states = env._max_height * env._max_width * 25\n    n_actions = len(env._actions)\n    p = np.zeros((n_states, n_actions, n_states))\n    r = np.ones((n_states, n_actions, n_states)) * -1\n    noise = env._noise_prob\n\n    def state_prob(*indices):\n        \"\"\"Returns a |S| length zero-initialized array where specified elements are filled\"\"\"\n        prob = 1.0 / len(indices)\n        res = np.zeros(n_states)\n        for idx in indices:\n            res[idx] = prob\n        return res\n\n    # List up all states and memonize starting states\n    states = []\n    starting_states = []\n    for y in range(env._max_height):\n        for x in range(env._max_width):\n            track = env._track[y][x]\n            for y_velocity in range(5):\n                for x_velocity in range(5):\n                    state = State(np.array([y, x]), np.array([y_velocity, x_velocity]))\n                    states.append(state)\n                    if track == env.START:\n                        starting_states.append(env.state_index(state))\n\n    for state in states:\n        position, velocity = state\n        i = env.state_index(state)\n        track = env._track[tuple(position)]\n        # At a terminating state or unreachable, the agent cannot move\n        if (\n            track == env.FINISH\n            or track == env.WALL\n            or (np.sum(velocity) == 0 and track != env.START)\n        ):\n            r[i] = 0\n            p[i, :] = state_prob(i)\n        # Start or empty states\n        else:\n            # First, compute next state probs without noise\n            next_state_prob = []\n            for j, action in enumerate(env._actions):\n                next_velocity = np.clip(velocity + action, a_min=0, a_max=4)\n                if np.sum(next_velocity) == 0:\n                    next_velocity = velocity\n                traj, went_out = env._all_passed_positions(\n                    position,\n                    next_velocity * np.array([-1, 1]),\n                )\n                passed_wall, passed_finish = False, False\n                for track in map(lambda pos: env._track[tuple(pos)], traj):\n                    passed_wall |= track == env.WALL\n                    passed_finish |= track == env.FINISH\n                if passed_wall or (went_out and not passed_finish):\n                    #  Run outside or crasheed to the wall\n                    next_state_prob.append(state_prob(*starting_states))\n                else:\n                    next_state_idx = env.state_index(State(traj[-1], next_velocity))\n                    next_state_prob.append(state_prob(next_state_idx))\n                    if passed_finish:\n                        r[i, j, next_state_idx] = 0.0\n            # Then linearly mix the transition probs with noise\n            for j in range(n_actions):\n                p[i][j] = (\n                    noise * next_state_prob[env._no_accel]\n                    + (1.0 - noise) * next_state_prob[j]\n                )\n\n    return p, r\n\n\nThen, let’s compute the optimal Q value by value iteration. So far, we only learned dynamic programming with discount factor \\(\\gamma\\), so let’s use \\(\\gamma =0.95\\) that is sufficiently large for this small problem. \\(\\epsilon = 0.000001\\) is used as a convergence threshold. Let’s show the elapsed time and the required number of iteration.\n\n\nCode\nimport datetime\n\n\nclass ValueIterationResult(NamedTuple):\n    q: np.ndarray\n    v: np.ndarray\n    elapsed: datetime.timedelta\n    n_iterations: int\n\n\ndef value_iteration(\n    p: np.ndarray,\n    r: np.ndarray,\n    discount: float,\n    epsilon: float = 1e-6,\n) -&gt; ValueIterationResult:\n    n_states, n_actions, _ = p.shape\n    q = np.zeros((n_states, n_actions))\n    v = np.zeros(n_states)\n    n_iterations = 0\n    start = datetime.datetime.now()\n    while True:\n        n_iterations += 1\n        v_old = v.copy()\n        for s in range(n_states):\n            # Q(s, a) = ∑ p(s, a, s') * (r(s, a, s') + γ v(s')\n            for a in range(n_actions):\n                q[s, a] = np.dot(p[s, a], r[s, a] + discount * v)\n            # V(s) = max_a Q(s, a)\n            v[s] = np.max(q[s])\n        if np.linalg.norm(v - v_old, ord=np.inf) &lt; epsilon:\n            break\n    return ValueIterationResult(q, v, datetime.datetime.now() - start, n_iterations)\n\n\np, r = get_p_and_r(smalltrack)\nvi_result = value_iteration(p, r, discount=0.95)\nprint(f\"Elapsed: {vi_result.elapsed.total_seconds()} n_iter: {vi_result.n_iterations}\")\n\n\nElapsed: 4.235676 n_iter: 10\n\n\nIt took longer that a second on my laptop, even for this small problem. Some technical notes on value iteration (\\(R_\\textrm{max} = 1.0\\) is assumed for simplicity): - Each iteration takes \\(O(|\\mathcal{S}| ^ 2 |\\mathcal{A}|)\\) time - The required iteration number is bounded by \\(\\frac{\\log \\epsilon}{\\gamma - 1}\\) - In our case, \\(\\frac{\\log \\epsilon}{\\gamma - 1} \\approx 270\\), so our computation converged quicker than theory - Thus the total computation time is bounded by \\(O(|\\mathcal{S}| ^ 2 |\\mathcal{A}|\\frac{\\log \\epsilon}{\\gamma - 1})\\) - For a convergence threshold \\(\\epsilon\\), \\(\\max |V(s) - V^*(s)| &lt; \\frac{\\gamma\\epsilon}{1 - \\gamma}\\) is guranteed - In our case, \\(\\frac{\\gamma\\epsilon}{1 - \\gamma} \\approx 0.00002\\) - This is called relative error\nLet’s visualize the V value and an optimal trajectory. celluloid) is used for making an animation.\n\n\nCode\nfrom typing import Callable\n\nfrom IPython.display import HTML\n\ntry:\n    from celluloid import Camera\nexcept ImportError as _e:\n    ! pip install celluloid --user\n    from celluloid import Camera\n\nPolicy = Callable[[int], int]\n\n\ndef smalltrack_optimal_policy(state_idx: int) -&gt; int:\n    return np.argmax(vi_result.q[state_idx])\n\n\ndef show_rollout(\n    env: RacetrackEnv,\n    policy: Policy,\n    v: np.ndarray = vi_result.v,\n    title: Optional[str] = None,\n) -&gt; HTML:\n    state = env.reset()\n    prev_termination = False\n    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n    camera = Camera(fig)\n    initial = True\n    while True:\n        env.render(state=state, movie=True, ax=ax)\n        state_idx = env.state_index(state)\n        ax.text(3, 0.5, f\"V(s): {v[state_idx]:02}\", c=\"red\")\n        camera.snap()\n        if prev_termination:\n            break\n        state, _, prev_termination = env.step(state, policy(state_idx))\n    if title is not None:\n        ax.text(3, 0.1, title, c=\"k\")\n    return HTML(camera.animate(interval=1000).to_jshtml())\n\n\nshow_rollout(smalltrack, smalltrack_optimal_policy)\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nThe computed optimal policy and value seems correct."
  },
  {
    "objectID": "posts/sandb-exercise-racetrack.html#monte-carlo-prediction",
    "href": "posts/sandb-exercise-racetrack.html#monte-carlo-prediction",
    "title": "Exercise 5.12 Racetrack from the Reinforcement Learning textbook",
    "section": "Monte-Carlo prediction",
    "text": "Monte-Carlo prediction\nThen let’s try ‘reinforcement learning’. First, I implemeted ‘First visit Monte-Carlo prediction’, which evaluates a (Markovian) policy \\(\\pi\\) by doing a simulation multiple times, and calculates the average of received returns. Here, I evaluate the optimal policy \\(\\pi^*\\) obtained by value iteration.\n\n\nCode\n# collapse-hide\n\nfrom typing import Union\n\n\ndef first_visit_mc_prediction(\n    policy: Policy,\n    env: RacetrackEnv,\n    n_episodes: int,\n    discount: float = 0.95,\n    record_all_values: bool = False,\n) -&gt; Tuple[np.ndarray, List[np.ndarray]]:\n    \"\"\"Predict value function corresponding to the policy by First-visit MC prediction\"\"\"\n    n_states = env._max_width * env._max_height * 25\n    v = np.zeros(n_states)\n    all_values = []\n    # Note that we have to have a list of returns for each state!\n    # So the maximum memory usage would be Num.States x Num.Episodes\n    all_returns = [[] for _ in range(n_states)]\n    for i in range(n_episodes):\n        if record_all_values:\n            all_values.append(v.copy())\n        state = env.reset()\n        visited_states = [env.state_index(state)]\n        received_rewards = []\n        # Rollout the policy until the episode ends\n        while True:\n            # Sample an action from the policy\n            action = policy(env.state_index(state))\n            # Step the simulator\n            state, reward, termination = env.step(state, action)\n            visited_states.append(env.state_index(state))\n            received_rewards.append(reward)\n            if termination:\n                break\n        # Compute return\n        traj_len = len(received_rewards)\n        returns = np.zeros(traj_len)\n        # Gt = Rt when t = T\n        returns[-1] = received_rewards[-1]\n        # Iterating from T - 2, T - 1, ..., to 0\n        for t in reversed(range(traj_len - 1)):\n            # Gt = Rt + γGt+1\n            returns[t] = received_rewards[t] + discount * returns[t + 1]\n        updated = set()\n        # Update the value\n        for i, state in enumerate(visited_states[: -1]):\n            # If the state is already visited, skip it\n            if state in updated:\n                continue\n            updated.add(state)\n            all_returns[state].append(returns[i].item())\n            # V(St) ← average(Returns(St))\n            v[state] = np.mean(all_returns[state])\n    return v, all_values\n\n\nv, all_values = first_visit_mc_prediction(\n    smalltrack_optimal_policy,\n    smalltrack,\n    1000,\n    record_all_values=True,\n)\nvalue_diff = []\nfor i, mc_value in enumerate(all_values + [v]):\n    value_diff.append(np.mean(np.abs(mc_value - vi_result.v)))\nplt.plot(value_diff)\nplt.xlabel(\"Num. Episodes\")\nplt.ylabel(\"Avg. |V* - V|\")\n\n\nText(0, 0.5, 'Avg. |V* - V|')\n\n\n\n\n\n\n\n\n\nIt looks like that the difference between \\(V^*\\) and the value function estimated by MC prediction converges after 600 steps, but it’s still larger than \\(0\\), because \\(\\pi^*\\) doesn’t visit all states. Let’s plot the difference between value functions only on starting states.\n\n\nCode\nstart_states = []\nfor x in range(1, 6):\n    idx = smalltrack.state_index(State(position=np.array([6, x]), velocity=np.array([0, 0])))\n    start_states.append(idx)\nstart_values = []\nfor i, mc_value in enumerate(all_values + [v]):\n    start_values.append(np.mean(np.abs(mc_value - vi_result.v)[start_states]))\nplt.plot(start_values)\nplt.xlabel(\"Num. Episodes\")\nplt.ylabel(\"|V*(start) - V(start)| only on possible states\")\nplt.ylim((0.0, 0.5))\n\n\n\n\n\n\n\n\n\nHere, we can confirm that the estimated value certainly converged close to 0.0, while fractuating a bit. Note that the magnitude of fractuation is larger than the relative error we allowed for value iteration (\\(0.00002\\)), implying the difficulty of convergence."
  },
  {
    "objectID": "posts/sandb-exercise-racetrack.html#monte-carlo-control",
    "href": "posts/sandb-exercise-racetrack.html#monte-carlo-control",
    "title": "Exercise 5.12 Racetrack from the Reinforcement Learning textbook",
    "section": "Monte-Carlo Control",
    "text": "Monte-Carlo Control\nNow we successfully estimate \\(V^\\pi\\) using Monte Carlo method, so then let’s try to learn a sub-optimal \\(\\pi\\) directly using Monte Carlo method. In the textbook, three methods are introduced: - Monte Carlo ES (Exploring Starts) - On-policy first visit Monte Carlo Control - Off-policy first visit Monte Carlo Control\nHere, let’s try all three methods and compare the resulting value functions. However, we cannot naively implement the pseudo code in the textbook, due to a ‘loop’ problem. Since the car that crashed into the wall is returned to a starting point, the episode length can be infinitte depending on a policy. As a remedy for this problem, I limit the length of the episode as \\(H\\). Supposing that we ignore the future rewards smaller than \\(\\epsilon\\), how to set \\(H\\)? Just by solving \\(\\gamma^H R &lt; \\epsilon\\), we get \\(H &gt; \\frac{\\log \\epsilon}{\\log \\gamma}\\), which is about \\(270\\) in case \\(\\gamma = 0.95\\) and \\(\\epsilon = 0.000001\\).\nBelow are the implementation of three methods. A few notes about implementation: - Monte Carlo ES requires a set of all possible states, which is implemented in valid_states function. - For On-Policy MC, \\(\\epsilon\\) is decreased from 0.5 to 0.01 - We can use arbitary policy in Off-Policy MC, but I used the same \\(\\epsilon\\)-soft policy as On-Policy MC.\n\n\nCode\n# collapse-hide\n\ndef valid_states(env: RacetrackEnv) -&gt; List[State]:\n    states = []\n    for y in range(env._max_height):\n        for x in range(env._max_width):\n            track = env._track[y][x]\n            if track == env.WALL:\n                continue\n            for y_velocity in range(5):\n                for x_velocity in range(5):\n                    state = State(np.array([y, x]), np.array([y_velocity, x_velocity]))\n                    if track != env.START and (x_velocity &gt; 0 or y_velocity &gt; 0):\n                        states.append(state)\n    return states\n\n\ndef mc_es(\n    env: RacetrackEnv,\n    n_episodes: int,\n    discount: float = 0.95,\n    record_all_values: bool = False,\n    seed: int = 999,\n) -&gt; Tuple[np.ndarray, List[np.ndarray]]:\n    \"\"\"Monte-Carlo Control with Exploring Starts\"\"\"\n    n_states = env._max_width * env._max_height * 25\n    n_actions = len(env._actions)\n    random_state = np.random.RandomState(seed=seed)\n    q = np.zeros((n_states, n_actions))\n    pi = random_state.randint(9, size=n_states)\n    all_values = []\n    all_returns = [[[] for _ in range(n_actions)] for _ in range(n_states)]\n    possible_starts = valid_states(env)\n    max_episode_length = int(np.ceil(np.log(1e-6) / np.log(discount)))\n    for i in range(n_episodes):\n        if record_all_values:\n            all_values.append(q.copy())\n        state = possible_starts[random_state.choice(len(possible_starts))]\n        visited_states = [env.state_index(state)]\n        taken_actions = []\n        received_rewards = []\n        initial = True\n        for _ in range(max_episode_length):\n            if initial:\n                # Randomly sample the first action\n                action = random_state.randint(9)\n                initial = False\n            else:\n                # Take an action following the policy\n                action = pi[env.state_index(state)]\n            taken_actions.append(action)\n            # Step the simulator\n            state, reward, termination = env.step(state, action)\n            visited_states.append(env.state_index(state))\n            received_rewards.append(reward)\n            if termination:\n                break\n        # Compute return\n        traj_len = len(received_rewards)\n        returns = np.zeros(traj_len)\n        # Gt = Rt when t = T\n        returns[-1] = received_rewards[-1]\n        # Iterating from T - 2, T - 1, ..., to 0\n        for t in reversed(range(traj_len - 1)):\n            # Gt = Rt + γGt+1\n            returns[t] = received_rewards[t] + discount * returns[t + 1]\n        updated = set()\n        # Update the value\n        for i, (state, action) in enumerate(zip(visited_states[:-1], taken_actions)):\n            # If the state is already visited, skip it\n            if (state, action) in updated:\n                continue\n            updated.add((state, action))\n            all_returns[state][action].append(returns[i].item())\n            # Q(St, At) ← average(Returns(St, At))\n            q[state, action] = np.mean(all_returns[state][action])\n            pi[state] = np.argmax(q[state])\n    return q, all_values\n\n\ndef on_policy_fist_visit_mc(\n    env: RacetrackEnv,\n    n_episodes: int,\n    discount: float = 0.95,\n    epsilon: float = 0.1,\n    epsilon_final: float = 0.1,\n    record_all_values: bool = False,\n    seed: int = 999,\n) -&gt; Tuple[np.ndarray, List[np.ndarray]]:\n    \"\"\"On-policy first visit Monte-Carlo\"\"\"\n\n    n_states = env._max_width * env._max_height * 25\n    n_actions = len(env._actions)\n    random_state = np.random.RandomState(seed=seed)\n    q = np.zeros((n_states, n_actions))\n    pi = random_state.randint(9, size=n_states)\n    all_values = []\n    all_returns = [[[] for _ in range(n_actions)] for _ in range(n_states)]\n    possible_starts = valid_states(env)\n    max_episode_length = int(np.ceil(np.log(1e-6) / np.log(discount)))\n    epsilon_decay = (epsilon - epsilon_final) / n_episodes\n    for i in range(n_episodes):\n        if record_all_values:\n            all_values.append(q.copy())\n        state = env.reset()\n        visited_states = [env.state_index(state)]\n        taken_actions = []\n        received_rewards = []\n        for _ in range(max_episode_length):\n            # ε-soft policy\n            if random_state.rand() &lt; epsilon:\n                action = random_state.randint(9)\n            else:\n                action = pi[env.state_index(state)]\n            taken_actions.append(action)\n            # Step the simulator\n            state, reward, termination = env.step(state, action)\n            visited_states.append(env.state_index(state))\n            received_rewards.append(reward)\n            if termination:\n                break\n        # Below code is the same as mc_es\n        # Compute return\n        traj_len = len(received_rewards)\n        returns = np.zeros(traj_len)\n        # Gt = Rt when t = T\n        returns[-1] = received_rewards[-1]\n        # Iterating from T - 2, T - 1, ..., to 0\n        for t in reversed(range(traj_len - 1)):\n            # Gt = Rt + γGt+1\n            returns[t] = received_rewards[t] + discount * returns[t + 1]\n        updated = set()\n        # Update the value\n        for i, (state, action) in enumerate(zip(visited_states[:-1], taken_actions)):\n            # If the state is already visited, skip it\n            if (state, action) in updated:\n                continue\n            updated.add((state, action))\n            all_returns[state][action].append(returns[i].item())\n            # Q(St, At) ← average(Returns(St, At))\n            q[state, action] = np.mean(all_returns[state][action])\n            pi[state] = np.argmax(q[state])\n        epsilon -= epsilon_decay\n    return q, all_values\n\n\ndef off_policy_mc(\n    env: RacetrackEnv,\n    n_episodes: int,\n    discount: float = 0.95,\n    record_all_values: bool = False,\n    epsilon: float = 0.1,\n    epsilon_final: float = 0.1,\n    seed: int = 999,\n) -&gt; Tuple[np.ndarray, List[np.ndarray]]:\n    \"\"\"Off-policy MC control\"\"\"\n\n    n_states = env._max_width * env._max_height * 25\n    n_actions = len(env._actions)\n    random_state = np.random.RandomState(seed=seed)\n    q = np.zeros((n_states, n_actions))\n    c = np.zeros_like(q)\n    pi = np.argmax(q, axis=1)\n    all_values = []\n    possible_starts = valid_states(env)\n    max_episode_length = int(np.ceil(np.log(1e-6) / np.log(discount)))\n    epsilon_decay = (epsilon - epsilon_final) / n_episodes\n    for i in range(n_episodes):\n        if record_all_values:\n            all_values.append(q.copy())\n        state = env.reset()\n        visited_states = [env.state_index(state)]\n        taken_actions = []\n        received_rewards = []\n        acted_optimally = []\n        for _ in range(max_episode_length):\n            # ε-soft policy\n            if random_state.rand() &lt; epsilon:\n                action = random_state.randint(9)\n            else:\n                action = pi[env.state_index(state)]\n            acted_optimally.append(action == pi[env.state_index(state)])\n            taken_actions.append(action)\n            # Step the simulator\n            state, reward, termination = env.step(state, action)\n            visited_states.append(env.state_index(state))\n            received_rewards.append(reward)\n            if termination:\n                break\n        g = 0\n        w = 1.0\n        for i, (state, action) in enumerate(zip(visited_states[:-1], taken_actions)):\n            g = discount * g + received_rewards[i]\n            c[state, action] += w\n            q[state, action] += w / c[state, action] * (g - q[state, action])\n            pi[state] = np.argmax(q[state])\n            if action == pi[state]:\n                break\n            else:\n                if acted_optimally[i]:\n                    w *= 1.0 - epsilon + epsilon / n_actions\n                else:\n                    w *= epsilon / n_actions\n        epsilon -= epsilon_decay\n    return q, all_values\n\n\nmces_result = mc_es(smalltrack, 3000, record_all_values=True)\non_mc_result = on_policy_fist_visit_mc(\n    smalltrack,\n    3000,\n    epsilon=0.5,\n    epsilon_final=0.01,\n    record_all_values=True,\n)\noff_mc_result = off_policy_mc(\n    smalltrack,\n    3000,\n    epsilon=0.5,\n    epsilon_final=0.01,\n    record_all_values=True,\n)\n\n\nLet’s plot the results. Here I plotted the difference from the optimal value function and the number of states that the policy choices the optimal action.\n\n\nCode\ndef value_diff(q_values: List[np.ndarray]) -&gt; List[float]:\n    diffs = []\n    for i, q in enumerate(q_values):\n        diff = np.abs(np.max(q, axis=-1) - vi_result.v)[start_states]\n        diffs.append(np.mean(diff))\n    return diffs\n\n\ndef n_optimal_actions(q_values: List[np.ndarray]) -&gt; List[int]:\n    n_optimal = []\n    optimal_actions = np.argmax(vi_result.q, axis=-1)\n    for i, q in enumerate(q_values):\n        greedy = np.argmax(q, axis=-1)\n        n_optimal.append(np.sum(greedy == optimal_actions))\n    return n_optimal\n\n\n_, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\nmc_es_value_diff = value_diff([mces_result[0]] + mces_result[1])\non_mc_value_diff = value_diff([on_mc_result[0]] + on_mc_result[1])\noff_mc_value_diff = value_diff([off_mc_result[0]] + off_mc_result[1])\nax1.plot(mc_es_value_diff, label=\"MC-ES\")\nax1.plot(on_mc_value_diff, label=\"On-Policy\")\nax1.plot(off_mc_value_diff, label=\"Off-Policy\")\nax1.set_xlabel(\"Num. Episodes\")\nax1.set_ylabel(\"Avg. |V* - V|\")\nax1.set_title(\"Diff. from V*\")\nmc_es_nopt = n_optimal_actions([mces_result[0]] + mces_result[1])\non_mc_nopt =  n_optimal_actions([on_mc_result[0]] + on_mc_result[1])\noff_mc_nopt =  n_optimal_actions([off_mc_result[0]] + off_mc_result[1])\nax1.legend(fontsize=12, loc=\"upper right\")\nax2.plot(mc_es_nopt, label=\"MC-ES\")\nax2.plot(on_mc_nopt, label=\"On-Policy\")\nax2.plot(off_mc_nopt, label=\"Off-Policy\")\nax2.set_xlabel(\"Num. Episodes\")\nax2.set_ylabel(\"Num. Optimal Actions\")\nax2.legend(fontsize=12, loc=\"upper right\")\nax2.set_title(\"Num. of optimal actions\")\n\n\nText(0.5, 1.0, 'Num. of optimal actions')\n\n\n\n\n\n\n\n\n\nSome observations from the results: - On-Policy MC converges to the optimal policy the fastest, though the convergence of its value function is the slowest - MC-ES struggles to distinguish optimal and non-optimal actions at some states, probably because of the lack of exploration during an episode. - Compared to MC-ES and On-Policy MC, the peak of value differences of Off-Policy MC is much milder. - A randomly initialized policy is often caught in a loop and cannot reach to the goal. The value of such a policy is really small (\\(-1  -1 * 0.95 - 1 * 0.95^2 - ... \\approx -20\\)). However, Off-Policy MC uses important sampling to decay the rewards by uncertain actions, resulting the smaller value differences.\nHere I visualized sampled plays from all three methods. On-Policy MC looks the most efficient.\n\n\nCode\nfor q, name in zip([mces_result[0], on_mc_result[0], off_mc_result[0]], [\"MC-ES\", \"On-Policy\", \"Off-Policy\"]):\n    display(show_rollout(smalltrack, lambda i: np.argmax(q[i]), np.argmax(q, axis=-1), name))\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/jax-brax-haiku.html",
    "href": "posts/jax-brax-haiku.html",
    "title": "Jax・Brax・HaikuでGPU引きこもり学習",
    "section": "",
    "text": "強化学習若手の会 Advent Calendar 2021 18日目\n\n\n0. 時勢のあいさつとか\n\nNote: このブログは強化学習若手の会 Advent Calendar 2021 18日目の記事として書かれました\n\nこんにちは。 コロナ禍も終わりが見えつつある（と思ったらオミクロン株が…）2021年もあとわずか。寒さも厳しくなってきましたが、皆さん如何お過ごしでしょうか。 とは言ったものの、僕は沖縄にいるので、それほど寒くはないのですが…。\n若手の会のアドベントカレンダーということで、国内でのコミュニティの活動について、最初に総括してみたいと思います。\n\n若手の会では模倣学習の勉強会をしましたが、結局2回しか続きませんでした。\n苦手の会のもくもく会はフリスビーやマラソンの練習とバッティングしてやらなくなってしまいましたが、最近日程を変えて、火曜日の夜に始めました。暇な方一緒にもくもくしましょう。\n強化学習の講義資料の翻訳をしています。難しいですが、けっこう勉強になります。有限サンプルでのバウンドを、初めて勉強しました。興味がある方は、ぜひ一緒にやりましょう。\n昨年のブログを書いてからはや一年、せっかく専用のブログを作ったので今年もいい感じにmatplotlib芸強化学習の記事を書いていきたいと思っていましたが、結局何も書きませんでした。\n\n最近は人工進化や人工生命の研究も始めたので、もはや「強化学習の人」と名乗っていいのかよくわかりませんが、今後も国内のコミュニティに何か貢献できればと思います。\n今年は強化学習に対する楽観論も悲観論も多く目にした一年でした。 David SilverやSuttonはReward is Enoughという強気な論文を出し、知的なシステムはおよそ全て報酬最大化で作れると主張しました。 さすがに強気すぎると思いますが、その後Reward is enough for convex MDPsやOn the Expressivity of Markov Rewardといったテクニカルな論文が出てきたのは面白いです。 また、オフライン強化学習・教師なし強化学習の論文が増えてきたと思います。 ざっくり、オフライン強化学習 = 強化学習 - 探索、教師なし強化学習 = 強化学習 - 報酬と思ってもらって問題ないでしょう。 何を隠そう僕の修士論文も単なる「階層型強化学習」だったのですが、リジェクト諸般の事情により教師なしに魔改造して再投稿しました。 Sergey LevineにいたってはUnderstanding the World Through Actionというタイトルが強い論文の中で、「大量にデータを集めてオフライン教師なし強化学習をすれば世界を理解できる（≒世界を理解しているのと同等のシステムが作れる？）」と言っています。面白い方向性だと思います。 一方で、みんな大好きルービックキューブ論文を出したOpen AIのロボティクスチームは、「とりあえず今データがある領域に注力する」とのことで解散してしまいました。 このブログを書いている最中にWebGPTの論文を目にしましたが、今後は言語モデル＋強化学習で色々やっていくのでしょうか。品川さんは喜びそうですが、僕なんかはこういう到底自分でできないものは「テレビの中の研究」という感じがして一歩引いてしまいます（最近は、テレビとかたとえに使うと古いのかな…）。 Open AIのロボティクスは、Sim2Realにこだわりすぎたのでは？という意見を某所でお聞きしました。実際そうなのかは知りませんが、大規模にシミュレーションしてSim2Realを頑張るのか、実機のデータで頑張るのかというのは、面白い視点ですよね。\nOpen AIが今までほど強化学習に注力しなくなったことで、Open AI gymをはじめ強化学習研究で使われてきたソフトウェア群にも、色々と情勢の変化がありそうです。 1. OpenAI Gymのメンテナが変わりました。これからはOpen AIではなくメリーランド大学の学生さんがメンテナになるようです。mujoco-pyなど関連するライブラリについては相変わらず放置されています。 2. DeepmindがMuJoCoを買い取って無料にしました。今後ソースコードも公開されるようです。 3. Googleから新しくbraxというシミュレータが公開されました。\nそんなわけで、僕はこれまでmujoco-py + gymで作成したカスタム環境でたくさん実験をやってきましたが、MuJoCoを使うにしてもdm_controlを使うとか、はたまたbraxにしてしまうとか、別の選択肢を検討したくなってきました。 このブログでは、とりあえずbraxを試してみようと思います。\n\n\n1. はじめに: シミュレーション・謎ロボット・GPU\n本題に入りますが、ざっくり、強化学習とは、報酬から行動を学習する枠組みだと言うことができます。 では何の行動を学習させたいのでしょうか。 ゲームのAIだったり、チャットボットだったり、色々な選択肢があると思いますが、どういうわけかシミュレータ上で動く謎ロボットというのがポピュラーな選択肢です。\nこのブログをごらんの方の中には、こういったナナフシのような謎ロボットの画像を目にしたことがある方も多いのではないでしょうか。\n\n\n\nHalfCheetah\n\n\nこれはOpen AI gymのHalfCheetahというロボットです。足が2本なのでハーフなのだと思いますが、なんとも残酷なネーミングです。愛玩されるため病気のまま品種改良されてきた犬猫のような哀愁が漂います。\nMuJoCoシミュレーターに「こことここがジョイントで、可動域はこうです。床は白黒でお願いします」みたいなXMLファイルを渡すと、こういうロボットを作ってくれます。 もしくは、dm_controlなどのPythonライブラリにXMLを作らせることもできます。 このような謎ロボットが実験で広く使われている要因として、 - みんなが使っているから - Atariなどのゲームより高速 - ジョイントの速さ・位置などの完全な内部状態が手に入る - マルコフ性について心配しなくてもいい - 色々カスタマイズできて便利だから - 普通のロボットを訓練するためのテストにちょうどいいから\nなどの理由があると思いますが、なんだかんだみんなが使っているからというのが大きい気がします。\nところで、このMuJoCoシミュレータというのは非常に高速に動作するのですが、CPU上でしか動作しません。 今日使われている深層学習のコードは、その計算量のほとんどを占める行列演算がベクトル並列化ととても相性がいいため、ネットワークやバッチサイズが大きくなればなるほどGPU上で高速に動作します。 となると、GPUで学習を回している場合、どうしてもCPUからGPUにデータを転送するボトルネックが発生し、高速化の妨げになります。 そこで、GPU上でシミュレーションを行えるようにしたのが、今回紹介するbraxというシミュレータです。\n\n\n2. Jaxでnumpy演算を高速化してみる\nでは、braxはCUDAか何かで書かれているのかな？と思ったかもしれませんが、なんと全てPythonで書かれているのです。 その鍵となるのがjaxというライブラリです。 おもむろに、インストールしてみましょう。\n\n\nCode\n! pip install jax\n\n\nドキュメントの冒頭に’JAX is Autograd and XLA’とありますが、Jaxは - Numpy演算をXLAに変換するコンパイラ(Tensorflow) - jax.jit - XLAはTensorflowのバックエンドとして開発された中間言語で、GPU/TPU用にすごく速いコードを生成できる - Numpy演算を追跡して勾配を計算する機能 - jax.grad/jax.vjp など\nの2つのコア機能を核とするライブラリです。 この節では、ひとまず前者の「XLAに変換するコンパイラ」としての機能に焦点を当ててみます。\nコンパイラはJIT方式で実装されており、 1. jax.jitに関数fを渡す (f_compiled = jax.jit(f)） 2. コンパイルされる関数f_compiledを最初に呼び出したとき、jaxはPythonの関数をXLAにコンパイルする 3. 2回目以降関数呼び出しが高速になる という処理の流れになります。\nでは、さっそく何かシミュレーションしてみましょう。 適当に天井からボールを落としてみましょう。\n\n\nCode\nimport typing as t\n\nimport numpy as np\nfrom IPython.display import HTML, clear_output\ntry:\n    import pandas as pd\n    import seaborn as sns\n    from celluloid import Camera\n    from matplotlib import pyplot as plt\n    from matplotlib.animation import ArtistAnimation\nexcept ImportError as _e:\n    ! pip isntall pandas seaborn celluloid\n    clear_output()\n    \n    import pandas as pd\n    import seaborn as sns\n    from celluloid import Camera\n    from matplotlib import pyplot as plt\n    from matplotlib.animation import ArtistAnimation\n\nsns.set_theme(style=\"darkgrid\")\n\nArray = np.ndarray\nGRAVITY = -9.8\n\n\ndef move_balls(\n    ball_positions: Array,\n    ball_velocities: Array,\n    delta_t: float = 0.1,\n) -&gt; Array:\n    accel_x = np.zeros(ball_positions.shape[0])\n    accel_y = np.ones(ball_positions.shape[0]) * GRAVITY * delta_t  # y方向にGΔt加速\n    new_velocities = np.stack((accel_x, accel_y), axis=1) + ball_velocities\n    new_positions = ball_positions + delta_t * new_velocities\n    return new_positions, new_velocities\n\n\ndef simulate_balls(\n    n_balls: int,\n    n_steps: int = 100,\n    forward: t.Callable[[Array], Array] = move_balls,\n) -&gt; t.List[Array]:\n    p = np.random.uniform(size=(n_balls, 2), low=-50, high=50.0)\n    v = np.random.randn(n_balls, 2)\n    results = [p]\n    for _ in range(n_steps):\n        p, v = forward(p, v)\n        results.append(p)\n    return results\n\n\n適当にボールを20個落としてみます。\n\n\nCode\ndef ball_animation(balls: t.Iterable[Array]) -&gt; ArtistAnimation:\n    fig = plt.figure(figsize=(8, 8))\n    ax = fig.add_subplot()\n    ax.set_xlim(-50, 50)\n    ax.set_ylim(-50, 50)\n    camera = Camera(fig)\n    for ball_batch in balls:\n        ax.scatter(ball_batch[:, 0], ball_batch[:, 1], color=\"red\", alpha=0.7)\n        camera.snap()\n    return camera.animate()\n\n\nHTML(ball_animation(simulate_balls(20, 40)).to_jshtml())\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nでは、このシミュレーションをするのに、どれくらい時間がかかるでしょうか。ボールの数を変えてベンチマークしてみましょう。\n\n\nCode\ndef bench(\n    f: t.Callable[..., t.Any],\n    inputs: t.Iterable[t.Any],\n    number: int = 10,\n) -&gt; t.List[float]:\n    import timeit\n\n    return [timeit.Timer(lambda: f(x)).timeit(number=number) for x in inputs]\n\n\ndef bench_and_plot(f: t.Callable[..., t.Any], title: str) -&gt; pd.DataFrame:\n    inputs = [4000, 8000, 16000, 32000, 64000]\n    result = pd.DataFrame({\"x\": inputs, \"y\": bench(f, inputs)})\n    result[\"Method\"] = [title] * len(inputs)\n    ax = sns.lineplot(data=result, x=\"x\", y=\"y\")\n    ax.set_title(title)\n    ax.set_xlabel(\"Num. of balls\")\n    ax.set_ylabel(\"Time (sec.)\")\n    return result\n\n\nnumpy_result = bench_and_plot(simulate_balls, \"NumPy\")\n\n\n\n\n\n\n\n\n\nおおむね線形に実行時間が増えていることがわかります。このコードを、jaxを使って高速化してみましょう。 基本的にはnumpyをjax.numpyに置き換えればいいです。\n\n\nCode\nimport jax\nimport jax.numpy as jnp\n\nJaxArray = jnp.DeviceArray\n\ndef move_balls_jax(\n    ball_positions: JaxArray,\n    ball_velocities: JaxArray,\n    delta_t: float = 0.1,\n) -&gt; JaxArray:\n    accel_x = jnp.zeros(ball_positions.shape[0])\n    accel_y = jnp.ones(ball_positions.shape[0]) * GRAVITY * delta_t\n    new_velocities = jnp.stack((accel_x, accel_y), axis=1) + ball_velocities\n    new_positions = ball_positions + delta_t * new_velocities\n    return new_positions, new_velocities\n\n\nでは同じようにベンチマークをとってみましょう。\n\n\nCode\njax_nojit_result = bench_and_plot(\n    lambda n: simulate_balls(n, forward=move_balls_jax),\n    \"JAX (without JIT)\",\n)\n\n\n\n\n\n\n\n\n\n謎の挙動を見せているし、すごく遅いですね。今度はJITコンパイルしてみましょう。 jax.jit(f, backend=\"cpu\")で関数をCPU上で動くXLAコードにコンパイルできます。\n\n\nCode\njax_cpu_result = bench_and_plot(\n    lambda n: simulate_balls(n, forward=jax.jit(move_balls_jax, backend=\"cpu\")),\n    \"JAX (with JIT on CPU)\",\n)\n\n\n\n\n\n\n\n\n\nすごく速くなりました。今度はGPUでやってみます。\n\n\nCode\njax_gpu_result = bench_and_plot(\n    lambda n: simulate_balls(n, forward=jax.jit(move_balls_jax, backend=\"gpu\")),\n    \"JAX (with JIT for GPU)\",\n)\n\n\n\n\n\n\n\n\n\n圧倒的に速いですね。一応線形に実行時間が増えてはいますが…。 なお、今回は学内スパコンのNVIDIA P100 GPUを使用しています。\n\n\nCode\nax = sns.lineplot(\n    data=pd.concat(\n        [numpy_result, jax_nojit_result, jax_cpu_result, jax_gpu_result],\n        ignore_index=True,\n    ),\n    x=\"x\",\n    y=\"y\",\n    style=\"Method\",\n    hue=\"Method\",\n)\nax.set_title(\"Ball benchmark\")\nax.set_xlabel(\"Num. of balls\")\nax.set_ylabel(\"Time (sec.)\")\nNone\n\n\n\n\n\n\n\n\n\nこのボールの数だとGPUは線形に計算時間が増えているように見えませんね。 まあ何はともあれ、GPU用にJITコンパイルしてあげると速そうだなあ、という感じがします。\n\n\n3. Jaxで勾配を計算してみる\nJaxは単に速いNumPyとしての機能に加え、自動微分によって、関数\\(f(x, y, z, ...)\\)の各\\(x, y, z,...\\)による偏微分\\(\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}, \\frac{\\partial f}{\\partial z}, ...\\)を計算する機能を持っています。ここではjax.gradによる勾配の計算だけを紹介します。\nなんか、適当に関数を最適化してみましょう。まずは、適当に関数を決めてみます。 \\(z = x^2 + y^2 + y\\) にしました。\n\n\nCode\ndef f(x, y):\n    return x ** 2 + y ** 2 + y\n\n\ndef plot_f(traj: t.Optional[Array] = None) -&gt; None:\n    x, y = np.meshgrid(np.linspace(-5, 5, 100), np.linspace(-5, 5, 100))\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection=\"3d\")\n    ax.plot_surface(\n        x,\n        y,\n        f(x, y),\n        cmap=sns.color_palette(\"flare\", as_cmap=True),\n        alpha=0.8,\n        linewidth=0,\n    )\n    if traj is not None:\n        ax.plot(traj[:, 0], traj[:, 1], traj[:, 2], color=\"blue\")\n    ax.set_xlabel(\"x\", fontsize=14)\n    ax.set_ylabel(\"y\", fontsize=14)\n    ax.set_zlabel(\"z\", fontsize=14, horizontalalignment=\"right\")\n    ax.set_title(\"f\", fontsize=15)\n\n\nplot_f()\n\n\n\n\n\n\n\n\n\n\\((x, y) = (5, 5)\\)でのこの関数の勾配を計算してみます。勾配を計算してほしい引数をjax.grad(argnums=...)で指定します。\n\n\nCode\njax.grad(f, argnums=(0, 1))(jnp.array(5.0), jnp.array(5.0))\n\n\n(DeviceArray(10., dtype=float32, weak_type=True),\n DeviceArray(11., dtype=float32, weak_type=True))\n\n\n\\(\\frac{\\partial z}{\\partial x}, \\frac{\\partial z}{\\partial y}\\)を計算してくれました。 せっかくなので、最急降下法してみましょう。\n\n\nCode\ndef steepest_descent(alpha: float = 0.01) -&gt; JaxArray:\n    f_grad = jax.grad(f, argnums=(0, 1))\n    x, y = jnp.array(5.0), jnp.array(5.0)\n    traj = []\n    while True:\n        traj.append((x, y, f(x, y)))\n        x_grad, y_grad = f_grad(x, y)\n        if jnp.linalg.norm(jnp.array([x_grad, y_grad])) &lt; 0.05:\n            break\n        x -= alpha * x_grad\n        y -= alpha * y_grad\n    return jnp.array(traj)\n\nplot_f(steepest_descent())\n\n\n\n\n\n\n\n\n\n最急降下方向に進んでくれているように見えます。 ところで、gradはトップダウン型リバースモード自動微分（誤差逆伝播法の難しい言い方です）を採用しているので、リバースモードでVector Jacobian Productを計算するvjpという関数が使われています。 フォーワードモードで計算するjvpという関数もあります。 このあたりの機能は、ただネットワークを学習させたいだけならほとんど使いませんが、一応やってみましょう。\n\n\nCode\nprimals, f_vjp = jax.vjp(f, 5.0, 5.0)\nprint(f\"VJP value: {primals.item()} grad: {[x.item() for x in f_vjp(1.0)]}\")\nvalue, grad = jax.jvp(f, (5.0, 5.0), (1.0, 1.0))\nprint(f\"JVP value: {value.item()} grad: {grad.item()}\")\n\n\nVJP value: 55.0 grad: [10.0, 11.0]\nJVP value: 55.0 grad: 21.0\n\n\nフォーワードモードの場合勾配となんかのベクトルvとの内積がでてきます。 このあたり、色々な教科書に書いてあると思いますが、Forward modeとReverse modeの違いなど、Probabilistic Machine Learning: An Introductionの13章が特にわかりやすいと思います。興味がある方は参考にしてみてください。\n\n\n4. Braxを使ってみる\nじゃあMuJoCoみたいな物理シミュレーターもJaxで書いてしまえば勝手にGPU上で動いて速いんじゃない？というモチベーションで作られたのがbraxです。 簡単に特徴をまとめてみます。\n\nJaxで記述されているため、jitで高速化できる\nProtocol Bufferでシステムを定義 (cf. MuJoCoはXML）\ndataclassQPを使った簡潔な状態記述\n\nQは正準座標、Pは運動量らしい\n\nOpenAI gym風のEnv APIやAnt・Halfcheetahなどの謎ロボット\n\nおもむろにインストールしてみます。\n\n\nCode\ntry:\n    import brax\nexcept ImportError:\n    !pip install git+https://github.com/google/brax.git@main\n    clear_output()\n    import brax\n\n\nさっきと同じ、ボールを動かしてみましょう。さっきはxy座標で動かしましたが、brax\n\n\nCode\ndef make_ball() -&gt; None:\n    config = brax.Config(dt=0.1, substeps=4)\n    # ボールを追加\n    ball = config.bodies.add(name=\"ball\", mass=1)\n    capsule = ball.colliders.add().capsule\n    capsule.radius = 0.5\n    # y座標に重力\n    config.gravity.y = GRAVITY\n    return config\n\n\ndef make_qp(p, v) -&gt; brax.QP:\n    return brax.QP(\n        pos=jnp.array([[p[0], p[1], 0.0]]),  # position\n        vel=jnp.array([[v[0], v[1], 0.0]]),  # velocity\n        rot=jnp.zeros((1,4)),  # rotation\n        ang=jnp.zeros((1, 3)),  # angular velocity\n    )\n\n\ndef simulate_one_ball_brax(n_steps: int = 100) -&gt; t.List[Array]:\n    sys = brax.System(make_ball())\n    qp = make_qp([0.0, 48.0], [1.0, 0.0])\n    results = []\n    for _ in range(n_steps):\n        qp, _ = sys.step(qp, [])\n        results.append(qp.pos[:2])\n    return results\n\n\nHTML(ball_animation(simulate_one_ball_brax(40)).to_jshtml())\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nここで、4つのAPIを使いました。 - brax.Configでシステムを定義 - brax.System(config)でシステムを作成 - brax.QPで初期位置・速度・アングル等を作成 - brax.System.step(qp, ...)で1ステップシミュレーションした結果を取得\nボールが一つだとなんとなく物足りないですね。増やしてみましょう。そのためには、jax.vmapでsys.stepをベクトル化します。 デフォルトで、vmapは引数のテンソルに対する演算をaxis=0でバッチ化します。 このあたりはin_axes=(1, 0, ...)とかやれば調節できますが、今回はデフォルトでOKです。\n[make_qp(*pv) for pv in zip(p, v)]で、List[brax.QP]を作っていますが、これをjax.tree_mapでもう一回QPに戻しています。\nList[QP(p=(0, 0), v(0, 0)), QP(..), ...] \nが\nQP(\n    p=[(0, 0), (0.1, 0.2),. ...], \n    v=[(0, 0), (1, 2), ...],\n)\nに変換される感じです。 このジャーゴンは便利なので覚えてもいいと思います。 ちなみに、treemapのノードが葉かどうかはオブジェクトがPyTreeか否かによります。 これは「以上のデータ構造をJaxは暗に木構造だとみなします。不足なら自分で登録してください」という話なので、最初は面食らうと思います。 これを陽なAPIでやろうにするとRustやScalaにあるtraitが必要なので、悪い設計ではないと思いますが。 というわけで、コードはこんな感じになります。\n\n\nCode\ndef simulate_balls_brax(n_balls: int, n_steps: int = 100) -&gt; t.List[Array]:\n    sys = brax.System(make_ball())\n    p = np.random.uniform(size=(n_balls, 2), low=-50, high=50.0)\n    v = np.random.randn(n_balls, 2)\n    qps = [make_qp(*pv) for pv in zip(p, v)]\n    qps = jax.tree_map(lambda *args: jnp.stack(args), *qps)\n    # ここで\n    step_vmap = jax.jit(jax.vmap(lambda qp: sys.step(qp, [])))\n    results = []\n    for _ in range(n_steps):\n        qps, _ = step_vmap(qps)\n        results.append(qps.pos[:, 0, :2])\n    return results\n\nHTML(ball_animation(simulate_balls_brax(20, 40)).to_jshtml())\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\njitを使わないとbraxがなぜかnumpyの関数を呼ぼうとしてエラーになったので、jitも併用しています。\n\n\n5. Haikuで謎のロボットを学習させてみる\nというわけで、braxの使い方をざっと見てみましたが、毎回自分でロボットを考えるのは大変だし査読者にも文句を言われるなので、今回は謎ロボットを学習させてみましょう。 open AI gym風のbrax.envs.Envがサポートされています。今回はAntを訓練してみましょう。 gym.makeに相当するのがbrax.envs.createです。 stepのAPIはgymと違い内部状態・報酬などが入ったbrax.envs.Stateというクラスを渡して次のStateを受け取るというインターフェイスです。\n\n\nCode\nimport brax.envs\n\n\ndef render_html(sys: brax.System, qps: t.List[brax.QP]) -&gt; HTML:\n    import uuid\n    import brax.io.html\n\n    html = brax.io.html.render(sys, qps)\n    # A weired trick to show multiple brax viewers...\n    html = html.replace(\"brax-viewer\", f\"brax-viewer-{uuid.uuid4()}\")\n    return HTML(html)\n\n\ndef random_ant() -&gt; HTML:\n    env = brax.envs.create(env_name=\"ant\")\n    prng_key = jax.random.PRNGKey(0)\n    state = env.reset(prng_key)\n    qps = [state.qp]\n    step_jit = jax.jit(env.step)\n    for i in range(10):\n        prng_key, action_key = jax.random.split(prng_key)\n        action = jax.random.normal(action_key, shape=(env.action_size,))\n        state = step_jit(state, action)\n        qps.append(state.qp)\n    return render_html(env.sys, qps)\n\n\nrandom_ant()\n\n\n\n\n  \n    brax visualizer\n    \n  \n  \n    \n    \n    \n  \n\n\n\nなんか、跳ねていますね。嬉しそう。 では、さっそく学習させてみましょう。 今回は、深層強化学習の代表的な手法であるPPOを使ってみます。 本当はSACも用意したかったのですが、時間がなかったので諦めました。 とりあえず、三層MLPを用意しましょう。 例えば、こんな感じのものがあればいいです。\n\n\nCode\ndef mlp_v1(\n    observation: JaxArray,\n    w1: JaxArray,\n    b1: JaxArray,\n    w2: JaxArray,\n    b2: JaxArray,\n    w3: JaxArray,\n    b3: JaxArray,\n) -&gt; JaxArray:\n    x = jnp.dot(observation, w1) + b1\n    x = jnp.tanh(x)\n    x = jnp.dot(x, w2) + b2\n    x = jnp.tanh(x)\n    return jnp.dot(x, w3) + b3\n\n\nこれをjitしてgradをとってAdamか何かでパラメタを更新して…とやればMLPが動くわけですが、パラメタが多すぎてちょっと面倒ですね。 そこでここでは、jaxでニューラルネットワークを訓練する際、パラメタの管理などをやってくれるライブラリであるHaikuを使ってみます。 なお、brax公式のexamplesではFlaxを使っています。 正直HaikuもFlaxもそこまで変わらないのですが、Flaxの方がややAPIの押しが強い（PyTorchでいうnn.Module相当のものがdataclassでないといけなかったりとか）印象があります。 また、HaikuはDeepmindが、FlaxはGoogleが開発しているライブラリとなります。 とりあえずインストールしてみましょう。\n\n\nCode\ntry:\n    import haiku as hk\n    import optax\n    import chex\n    import distrax\nexcept ImportError as e:\n    ! pip install git+https://github.com/deepmind/dm-haiku \\\n        git+https://github.com/deepmind/optax \\\n        git+https://github.com/deepmind/chex \\\n        git+https://github.com/deepmind/distrax\n    \n    import haiku as hk\n    import optax\n    import chex\n    import distrax\n    \n    clear_output()\n\n\nニューラルネットワークを定義するためのPythonライブラリはtheano、tensorflowと色々ありましたが、最近はtorch.nn.Moduleやchainer.Linkのように、ネットワークの重み・forwardの出力・隣接しているノードを記録したオブジェクトを使って、動的に計算グラフを構築するものが多いかと思います。 しかし、Haikuによるそれは少し異なります。ポイントは、勾配を計算する部分はJaxが担当するので、Haikuはただ「ネットワークのパラメタを管理するだけ」でいいということです。 そのために、HaikuはtransformというAPIを用意しています。 これは見たほうが早いでしょう。\n\n\nCode\ndef mlp_v2(observation: JaxArray) -&gt; JaxArray:\n    w1 = hk.get_parameter(\"w1\", shape=[observation.shape[1], 3], init=jnp.ones)\n    b1 = hk.get_parameter(\"b1\", shape=[3], init=jnp.zeros)\n    w2 = hk.get_parameter(\"w2\", shape=[3, 3], init=jnp.ones)\n    b2 = hk.get_parameter(\"b2\", shape=[3], init=jnp.zeros)\n    w3 = hk.get_parameter(\"w3\", shape=[3, 2], init=jnp.ones)\n    b3 = hk.get_parameter(\"b3\", shape=[2], init=jnp.zeros)\n    x = jnp.dot(observation, w1) + b1\n    x = jnp.tanh(x)\n    x = jnp.dot(x, w2) + b2\n    x = jnp.tanh(x)\n    return jnp.dot(x, w3) + b3\n\nprng_seq = hk.PRNGSequence(0)  # これをグローバル変数にするのは良くないです。真似しないで\ninit, apply = hk.transform(mlp_v2)  # transformする\n# initは乱数シード・インプットを受け取って、初期化したパラメタを返す関数\nparams = init(next(prng_seq), jnp.zeros((10, 2)))\nprint(params)\n# applyはパラメタ・乱数シード・インプットを受け取って、出力を返す関数\noutput = apply(params, next(prng_seq), jnp.zeros((10, 2)))\n\n\nFlatMap({\n  '~': FlatMap({\n         'w1': DeviceArray([[1., 1., 1.],\n                            [1., 1., 1.]], dtype=float32),\n         'b1': DeviceArray([0., 0., 0.], dtype=float32),\n         'w2': DeviceArray([[1., 1., 1.],\n                            [1., 1., 1.],\n                            [1., 1., 1.]], dtype=float32),\n         'b2': DeviceArray([0., 0., 0.], dtype=float32),\n         'w3': DeviceArray([[1., 1.],\n                            [1., 1.],\n                            [1., 1.]], dtype=float32),\n         'b3': DeviceArray([0., 0.], dtype=float32),\n       }),\n})\n\n\nこんな感じになります。 まとめると、 - transform(f)は二つの関数init、applyをかえす - transformはfを、fの中でhaiku.get_parameterを使って呼び出されたパラメタを入力とする関数に変換する - initはパラメタを初期化して返す。パラメタはFlatMapというオブジェクトだがこれはほとんどdictと同じ - applyは与えられたパラメタを使って所望の計算を行う という感じですね。\nついでに、上の例ではhk.PRNGSequenceというPRNGKeyの更新を勝手にやってくれるものを使っています。\nしかし、これでもまだ面倒ですね。 実際のところ、よく使うネットワークはモジュールとしてまとまっているので、これを使えばいいです。\n\n\nCode\ndef mlp_v3(output_size: int, observation: JaxArray) -&gt; JaxArray:\n    x = hk.Linear(64)(observation)\n    x = jnp.tanh(x)\n    x = hk.Linear(64)(observation)\n    x = jnp.tanh(x)\n    return hk.Linear(output_size)(observation)\n\n\nこれを使って、PPOのネットワークを書いてみましょう。 方策は標準偏差が状態に依存しない正規分布にします。\n\n\nCode\nclass NetworkOutput(t.NamedTuple):\n    mean: JaxArray\n    stddev: JaxArray\n    value: JaxArray\n\n\ndef policy_and_value(action_size: int, observation: JaxArray) -&gt; NetworkOutput:\n    mean = mlp_v3(output_size=action_size, observation=observation)\n    value = mlp_v3(output_size=1, observation=observation)\n    logstd = hk.get_parameter(\"logstd\", (1, action_size), init=jnp.zeros)\n    stddev = jnp.ones_like(mean) * jnp.exp(logstd + 1e-8)\n    return NetworkOutput(mean, stddev, value)\n\n\nこれだけです。デフォルトでは、ネットワークの重みはTruncatedNormalで初期化されます。今回は全部デフォルトのままにしました。\n次に、これを使って、環境とインタラクトするコードを書いてみます。 いま、braxの利点を活かすために、 1. ネットワークから次のアクションをサンプルして 2. シミュレータで次の状態をシミュレート という過程をすべてjax.jitの中でやるのが理想ですよね。\nですから、たとえばこんな感じにやればいいです。\n\n\nCode\nAction = JaxArray\n\n\ndef make_step_function(\n    env: brax.envs.Env,\n) -&gt; t.Tuple[t.Callable[..., t.Any], t.Callable[..., t.Any]]:\n    def step(state: brax.envs.State) -&gt; t.Tuple[brax.envs.State, NetworkOutput, Action]:\n        out = policy_and_value(env.action_size, state.obs)\n        policy = distrax.MultivariateNormalDiag(out.mean, out.stddev)\n        action = policy.sample(seed=hk.next_rng_key())  # transformするとこれが使えます\n        state = env.step(state, jnp.tanh(action))\n        return state, out, action\n\n    init, apply = hk.transform(step)\n    return jax.jit(init), jax.jit(apply)\n\n\nここで、行動のサンプルにはdistraxというライブラリを使いました。 平均値にノイズをいれるだけなので、ライブラリを使ってもあまり変わらないのですが…。 いま、各ジョイントに対して加える力が、それぞれ独立な正規分布からサンプリングされると仮定しているので、MutliVariateNormDiag(共分散行列が対角行列になる多変量正規分布）を使ってモデリングしています。 distrax.Independentとdistrax.Normalを使っても同じことができます。 行動は一応tanhで\\([-1, 1]\\)の範囲にならしています。\nちょっと試してみましょう。\n\n\nCode\nant = brax.envs.create(env_name=\"ant\", batch_size=1)\ninit, step = make_step_function(ant)\ninitial_state = jax.jit(ant.reset)(next(prng_seq))\nparams = init(next(prng_seq), initial_state)\n_next_state, out, action = step(params, next(prng_seq), initial_state)\n# chexはテスト用のライブラリです\nchex.assert_shape((out.mean, out.stddev, action), (1, ant.action_size))\n\n\nというわけで無事にstepをJITコンパイルして高速化できました。 resetはほとんど呼ばないので別にコンパイルしなくてもいいのですが、jitしないとbraxがjnp.DeviceArrayのかわりにnumpyを使いたがって少し面倒なのでjitしています。\nあとはPPOを実装していきますが、時間の都合で手短かにいきます。 まずはGAEですね。 普通に書くとjax.jitがループアンローリングを行ってコンパイル時間が激遅になるので、jax.lax.fori_loopという黒魔術を使います。 コンパイル時定数はstatic_argnumsで指定します。 vmapで各ワーカー用に並列化します。\n\n\nCode\nimport functools\n\n@functools.partial(jax.jit, static_argnums=2)\ndef gae(\n    r_t: JaxArray,\n    discount_t: JaxArray,\n    lambda_: float,\n    values: JaxArray,\n) -&gt; chex.Array:\n    chex.assert_rank([r_t, values, discount_t], 1)\n    chex.assert_type([r_t, values, discount_t], float)\n    lambda_ = jnp.ones_like(discount_t) * lambda_\n    delta_t = r_t + discount_t * values[1:] - values[:-1]\n    n = delta_t.shape[0]\n\n    def update(i: int, advantage_t: JaxArray) -&gt; JaxArray:\n        t_ = n - i - 1\n        adv_t = delta_t[t_] + lambda_[t_] * discount_t[t_] * advantage_t[t_ + 1]\n        return jax.ops.index_update(advantage_t, t_, adv_t)\n\n    advantage_t = jax.lax.fori_loop(0, n, update, jnp.zeros(n + 1))\n    return advantage_t[:-1]\n\n\nbatched_gae = jax.vmap(gae, in_axes=(1, 1, None, 1), out_axes=1)\n\n\nなんかブログで書くには黒魔術すぎる気もしますが…。\n次に学習データのバッチを構成する部分ですね。 これは、普通にPyTorchとかと変わらないです。\n\n\nCode\nimport dataclasses\n\n\n@chex.dataclass\nclass RolloutResult:\n    \"\"\"\n    Required experiences for PPO.\n    \"\"\"\n\n    observations: t.List[JaxArray]\n    actions: t.List[JaxArray] = dataclasses.field(default_factory=list)\n    rewards: t.List[JaxArray] = dataclasses.field(default_factory=list)\n    terminals: t.List[JaxArray] = dataclasses.field(default_factory=list)\n    outputs: t.List[NetworkOutput] = dataclasses.field(default_factory=list)\n\n    def append(\n        self,\n        *,\n        observation: JaxArray,\n        action: JaxArray,\n        reward: JaxArray,\n        output: NetworkOutput,\n        terminal: JaxArray,\n    ) -&gt; None:\n        self.observations.append(observation)\n        self.actions.append(action)\n        self.rewards.append(reward)\n        self.outputs.append(output)\n        self.terminals.append(terminal)\n\n    def clear(self) -&gt; None:\n        self.observations = [self.observations[-1]]\n        self.actions.clear()\n        self.rewards.clear()\n        self.outputs.clear()\n        self.terminals.clear()\n\n\nclass Batch(t.NamedTuple):\n    \"\"\"Batch for PPO, also used as minibatch by indexing.\"\"\"\n\n    observation: JaxArray\n    action: JaxArray\n    reward: JaxArray\n    advantage: JaxArray\n    value_target: JaxArray\n    log_prob: JaxArray\n\n    def __getitem__(self, idx: Array) -&gt; \"Batch\":\n        return self.__class__(\n            observation=self.observation[idx],\n            action=self.action[idx],\n            reward=self.reward[idx],\n            advantage=self.advantage[idx],\n            value_target=self.value_target[idx],\n            log_prob=self.log_prob[idx],\n        )\n\n\n@jax.jit\ndef make_batch(rollout: RolloutResult, next_value: JaxArray) -&gt; Batch:\n    action = jnp.concatenate(rollout.actions)\n    mean, stddev, value = jax.tree_map(lambda *x: jnp.concatenate(x), *rollout.outputs)\n    log_prob = distrax.MultivariateNormalDiag(mean, stddev).log_prob(action)\n    reward = jnp.stack(rollout.rewards)\n    mask = 1.0 - jnp.stack(rollout.terminals)\n    value = jnp.concatenate(\n        (value.reshape(reward.shape), next_value.reshape(1, -1)),\n        axis=0,\n    )\n    advantage = batched_gae(reward, mask * 0.99, 0.95, value)\n    value_target = advantage + value[:-1]\n    return Batch(\n        observation=jnp.concatenate(rollout.observations[:-1]),\n        action=action,\n        reward=jnp.ravel(reward),\n        advantage=jnp.ravel(advantage),\n        value_target=jnp.ravel(value_target),\n        log_prob=log_prob,\n    )\n\n\n普通のdataclassesはjitできないので、chex.dataclassを使います。 さっき少しだけ触れましたが、chex.dataclassは作成したdataclassをPyTreeとしてjaxに登録してくれます。 実はflax.struct.dataclassというだいたい同じものもあって、braxの内部ではこれを使っているようです。 また\\(\\gamma = 0.99, \\lambda = 0.95\\)としました。\nいよいよ学習の部分ですね。 まず、損失関数をjax.gradできるように書きます。\n\n\nCode\ndef ppo_loss(action_size: int, batch: Batch) -&gt; JaxArray:\n    mean, stddev, value = policy_and_value(action_size, batch.observation)\n    # Policy loss\n    policy = distrax.MultivariateNormalDiag(mean, stddev)\n    log_prob = policy.log_prob(batch.action)\n    prob_ratio = jnp.exp(log_prob - batch.log_prob)\n    clipped_ratio = jnp.clip(prob_ratio, 0.8, 1.2)\n    clipped_obj = jnp.fmin(prob_ratio * batch.advantage, clipped_ratio * batch.advantage)\n    policy_loss = -jnp.mean(clipped_obj)\n    # Value loss\n    value_loss = jnp.mean(0.5 * (value - batch.value_target) ** 2)\n    # Entropy regularization\n    entropy_mean = jnp.mean(policy.entropy(), axis=-1)\n    return policy_loss + value_loss - 0.001 * entropy_mean\n\n\n\\(\\epsilon = 0.2\\)で固定しているので、\\([1 - 0.2, 1 + 0.2]\\)の範囲でクリップします。\nではこれを使って、今度はパラメタの更新を全部jitにつっこんでみましょう。 パラメタの更新にはoptaxというライブラリを使います。色々なSGDのバリアントを実装しているライブラリですが、僕はほとんどAdamしか使いません…。\n\n\nCode\nimport optax\n\n\ndef make_update_function(\n    action_size: int,\n    opt_update: optax.TransformUpdateFn,\n) -&gt; t.Callable[..., t.Any]:\n    # hk.Paramsを使い回すのでinitは捨てていい\n    # 行動をサンプルしないので、without_apply_rngが使える\n    _, loss_fn = hk.without_apply_rng(hk.transform(lambda batch: ppo_loss(action_size, batch)))\n    grad_fn = jax.grad(loss_fn)\n\n    # ここでjitしていい\n    @jax.jit\n    def update(\n        params: hk.Params,\n        opt_state: optax.OptState,\n        batch: Batch,\n    ) -&gt; t.Tuple[hk.Params, Batch]:\n        grad = grad_fn(params, batch)\n        updates, new_opt_state = opt_update(grad, opt_state)\n        return optax.apply_updates(params, updates), new_opt_state\n\n    return update\n\n\nさて、ここまで来たらあと一歩ですね。 次に面倒ですが次の状態のvalueをとってきてバッチを作る部分を書きます。\n\n\nCode\ndef make_next_value_function(action_size: int) -&gt; Batch:\n    def next_value_fn(obs: JaxArray) -&gt; JaxArray:\n        output = policy_and_value(action_size, obs)\n        return output.value\n\n    _, next_value_fn = hk.without_apply_rng(hk.transform(next_value_fn))\n    return jax.jit(next_value_fn)\n\n\n速度を求めるなら、これはmake_batchと一緒にjitしてしまってもいいですが、まあ面倒なのでこれでもいいでしょう。\nでは材料がそろったのでメインループを書いていきましょう。 面倒ですが、評価用のenvironmentも別に作ります。\n\n\nCode\ntry:\n    import tqdm\nexcept ImportError as _e:\n    ! pip install tqdm\n    import tqdm\n    clear_output()\n\n\n\n\nCode\nimport datetime\n\nfrom tqdm.notebook import trange\n\n\ndef sample_minibatch_indices(\n    n_instances: int,\n    n_minibatches: int,\n    prng_key: chex.PRNGKey,\n) -&gt; t.Iterable[JaxArray]:\n    indices = jax.random.permutation(prng_key, n_instances)\n    minibatch_size = n_instances // n_minibatches\n    for start in range(0, n_instances, minibatch_size):\n        yield indices[start : start + minibatch_size]\n\n\ndef train_ppo(\n    env_name: str = \"ant\",\n    n_workers: int = 32,\n    n_steps: int = 2048,\n    n_training_steps: int = 10000000,\n    n_optim_epochs: int = 10,\n    n_minibatches: int = 64,\n    eval_freq: int = 20,\n    eval_workers: int = 16,\n    seed: int = 0,\n) -&gt; HTML:\n    # 環境と、環境を含んだstep関数を作る\n    env = brax.envs.create(env_name=env_name, episode_length=1000, batch_size=n_workers)\n    eval_env = brax.envs.create(\n        env_name=env_name,\n        episode_length=1000,\n        batch_size=eval_workers,\n    )\n    network_init, step = make_step_function(env)\n    _, eval_step = make_step_function(eval_env)\n    eval_reset = jax.jit(eval_env.reset)\n    # 乱数\n    prng_seq = hk.PRNGSequence(seed)\n    # 初期状態\n    state = jax.jit(env.reset)(rng=next(prng_seq))\n    rollout = RolloutResult(observations=[state.obs])\n    # Optimizerとパラメタを初期化する\n    optim = optax.chain(optax.clip_by_global_norm(1.0), optax.adam(3e-4, eps=1e-4))\n    update = make_update_function(env.action_size, optim.update)\n    params = network_init(next(prng_seq), state)\n    opt_state = optim.init(params)\n    # next_value\n    next_value_fn = make_next_value_function(env.action_size)\n    n_instances = n_workers * n_steps\n\n    def evaluate(step: int) -&gt; None:\n        eval_state = eval_reset(rng=next(prng_seq))\n        return_ = jnp.zeros(eval_workers)\n        done = jnp.zeros(eval_workers, dtype=bool)\n        for _ in range(1000):\n            eval_state, _, _ = eval_step(params, next(prng_seq), eval_state)\n            return_ = return_ + eval_state.reward * (1.0 - done)\n            done = jnp.bitwise_or(done, eval_state.done.astype(bool))\n        print(f\"Step: {step} Avg. ret: {jnp.mean(return_).item()}\")\n\n    for i in trange(n_training_steps // n_instances):\n        for _ in range(n_steps):\n            state, output, action = step(params, next(prng_seq), state)\n            rollout.append(\n                observation=state.obs,\n                action=action,\n                reward=state.reward,\n                output=output,\n                terminal=state.done,\n            )\n        next_value = next_value_fn(params, state.obs)\n        batch = make_batch(rollout, next_value)\n        rollout.clear()\n        # Batchを作ったので、ミニバッチサンプリングして学習\n        for _ in range(n_optim_epochs):\n            for idx in sample_minibatch_indices(\n                n_instances,\n                n_minibatches,\n                next(prng_seq),\n            ):\n                minibatch = batch[idx]\n                params, opt_state = update(params, opt_state, minibatch)\n\n        # 時々評価する\n        if (i + 1) % eval_freq == 0:\n            evaluate(i + 1)\n\n    evaluate(i + 1)\n    # Visualize\n    eval_state = eval_reset(rng=next(prng_seq))\n    qps = []\n    while eval_state.done[0] == 0.0:\n        eval_state, _, _ = eval_step(params, next(prng_seq), eval_state)\n        qps.append(jax.tree_map(lambda qp: qp[0], eval_state.qp))\n    return render_html(eval_env.sys, qps)\n\n\nstart_time = datetime.datetime.now()\nhtml = train_ppo()\nelapsed = datetime.datetime.now() - start_time\nprint(f\"Train completed after {elapsed.total_seconds() / 60:.2f} min.\")\nhtml\n\n\n\n\n\nStep: 20 Avg. ret: -286.34039306640625\nStep: 40 Avg. ret: -273.5491943359375\nStep: 60 Avg. ret: -193.0821990966797\nStep: 80 Avg. ret: -84.20954132080078\nStep: 100 Avg. ret: -45.654090881347656\nStep: 120 Avg. ret: -20.323640823364258\nStep: 140 Avg. ret: -42.74524688720703\nStep: 152 Avg. ret: -5.514527320861816\nTrain completed after 41.34 min.\n\n\n\n\n  \n    brax visualizer\n    \n  \n  \n    \n    \n    \n  \n\n\n\n100万ステップの訓練が41分で終わりました。速いですねやっぱり。 なんか前に跳ねすぎている微妙なのがビジュアライズされていますが…。\n\n\n6. まとめ\nというわけで、このブログではJax、Brax、Haikuを使って、GPUだけでシミュレータ上のロボットを訓練する例を示しました。 かなり駆け足の解説になりましたが、なんとなくプログラムの組み方を理解していただけたのではないかと思います。\n総括すると、Jaxはかなり広い範囲のNumPy演算をGPU/TPU上で高速に動作するコードに変換できる、非常に強力なライブラリです。 今回紹介したvmapは、例えば一つのGPU上で演算を並列化する機能ですが、他にもpmapによるデバイスをまたいだ並列化もできます。 ですから特に、 - ✔ CPUとGPUの通信オーバーヘッドが気になるとき - ✔ 大規模に並列なシミュレーションを行いたいとき\nは、Jaxが効果を発揮すると思います。また、jax.lax.fori_loopを使って - ✔ CythonやC++/Rustなど他の言語を使わずにPythonのループを高速化したいとき\nにも使えます。 一方で、単に深層学習を高速化したい場合、例えば - 🔺 PyTorchモデルの訓練がボトルネックになっている場合\nなどは、JaxやHaiku/Flaxを使うことによる高速化の恩恵はあまりないと思います。 うまくJitを使えばJaxの方が速いと思いますが、PyTorchのCUDAコードはかなり速いですからね。また、PyTorchと比較した際、 - 🔺 学習コストについてもJaxの方が大きい\nのではないかと思います。 なので、個人的には学習以外の部分でベクトル並列化・Jitコンパイルによる並列化の余地がある場合に、Jaxは便利に使えるのかなあと思います。 ただDeepmindはAlphaFold2を始め、多くのJax+Haiku製深層学習コードをリリースしていますし、一応読める程度に親しんでおくだけでもある程度のメリットはあると思います。\nさて、冒頭の大規模にシミュレーションしてSim2Realを頑張るのか、実機のデータで頑張るのかという話に戻りますが、シミュレーションをスケールさせたいのであれば大規模にシミュレーションしたいならBraxのように「シミュレーターをJaxでコンパイルできるように作る」というアプローチは面白いと思います。分子動力学計算など、物理演算以外のシミュレーションへの活用も期待されます（と聞いたことがあります。僕は分子動力学計算が何なのかよくわかりません…）。 一方で、シミュレータが微分可能であるという利点をどう活かすのかも興味深いテーマです。僕も以前PFNさんのインターンで、報酬が微分可能なシミュレータを使って、\\(\\sum_{t = 0}^T\\frac{\\partial r_t}{\\partial \\theta}\\)についての山登り法で方策を更新するのを試したことがあるのですが、報酬が遠いとなかなか難しいなあという印象でした。うまい方法があればいいのですが…。意外と勾配降下だけでなく進化計算などのメタヒューリスティクスと組み合わせると面白いかもしれないです。\nさて、僕はもう一つアドベントカレンダーの記事を書く予定があったのですが、時間がないので他の人に代わってもらうかもしれません…。出たらそちらもよろしくお願いします。"
  },
  {
    "objectID": "posts/fast_2d_physics_in_jax.html",
    "href": "posts/fast_2d_physics_in_jax.html",
    "title": "Jaxで高速な2D物理シミュレーションを実装してみる",
    "section": "",
    "text": "GPU上での高速な物理シミュレーションは、(RLHFやOffline RLに押され気味とはいえ)強化学習界隈では話題のトピックですよね。また単純に、GPU上で爆速でシミュレーションが終わるのはなかなか楽しいものです。 NVIDIA IsaacSymもありますが、jaxで強化学習パイプライン全体を高速化したいならbraxが便利です。以前紹介するブログも書きましたが、現在のバージョンではより精度のいい手法が選べるようになっていて、普通にMuJoCoの代わりに使えそうな感じです。しかし、最近単純な2次元物理シミュレーションでbraxが使えないかな？と思って検討してみたところ、無理ではないのだけれどどうにも使いづらいな…という印象でした。また、二次元物理シミュレーションをするのに、三次元のボールとかで当たり判定を行うのはちょっと計算資源がもったいない気もします。なら自分で作ってしまえばいいんじゃないか？ということでやってみました。"
  },
  {
    "objectID": "posts/fast_2d_physics_in_jax.html#衝突判定を行う",
    "href": "posts/fast_2d_physics_in_jax.html#衝突判定を行う",
    "title": "Jaxで高速な2D物理シミュレーションを実装してみる",
    "section": "衝突判定を行う",
    "text": "衝突判定を行う\n次に衝突判定を実装してみましょう。円と円しかないので判定自体は簡単ですが、後で衝突後の物理状態を求めるために計算するときのため、衝突した場所やそれによって発生したインパルスといった情報を保存しておく必要があります。 また、衝突判定を行う際、全てのペアについて衝突を検出するコードをナイーブに書くと、以下のようになります。\nfor i in range(N):\n    for j in range(i + 1, N):\n        check_contact(i, j)\nですが、衝突検出は各ペアについて独立に行えるので、jax.vmapを使って並列化したいところです。そこで、このループを手動でアンロールし、あらかじめペアを生成してからvmapで並列化した衝突検出関数を呼ぶようにしました。以下のコード中のgenerate_self_pairsが全ペアを生成する関数になります。\n\n\nCode\nfrom typing import Any, Callable\n\nAxis = Sequence[int] | int\n\n\ndef safe_norm(x: jax.Array, axis: Axis | None = None) -&gt; jax.Array:\n    is_zero = jnp.allclose(x, 0.0)\n    x = jnp.where(is_zero, jnp.ones_like(x), x)\n    n = jnp.linalg.norm(x, axis=axis)\n    return jnp.where(is_zero, 0.0, n)  # pyright: ignore\n\n\ndef normalize(x: jax.Array, axis: Axis | None = None) -&gt; tuple[jax.Array, jax.Array]:\n    norm = safe_norm(x, axis=axis)\n    n = x / (norm + 1e-6 * (norm == 0.0))\n    return n, norm\n\n\ndef tree_map2(\n    f: Callable[..., Any],\n    tree: Any,\n    *rest: Any,\n    is_leaf: Callable[[Any], bool] | None = None\n) -&gt; tuple[Any, Any]:\n    \"\"\"Same as tree_map, but returns a tuple\"\"\"\n    leaves, treedef = jax.tree_util.tree_flatten(tree, is_leaf)\n    all_leaves = [leaves] + [treedef.flatten_up_to(r) for r in rest]\n    result = [f(*xs) for xs in zip(*all_leaves)]\n    a = treedef.unflatten([elem[0] for elem in result])\n    b = treedef.unflatten([elem[1] for elem in result])\n    return a, b\n\n\ndef generate_self_pairs(x: jax.Array) -&gt; tuple[jax.Array, jax.Array]:\n    \"\"\"Returns two arrays that iterate over all combination of elements in x and y.\"\"\"\n    # x.shape[0] &gt; 1\n    chex.assert_axis_dimension_gt(x, 0, 1)\n    n = x.shape[0]\n    # (a, a, a, b, b, c)\n    outer_loop = jnp.repeat(\n        x,\n        jnp.arange(n - 1, -1, -1),\n        axis=0,\n        total_repeat_length=n * (n - 1) // 2,\n    )\n    # (b, c, d, c, d, d)\n    inner_loop = jnp.concatenate([x[i:] for i in range(1, len(x))])\n    return outer_loop, inner_loop\n\n\n@chex.dataclass\nclass Contact(PyTreeOps):\n    pos: jax.Array\n    normal: jax.Array\n    penetration: jax.Array\n    elasticity: jax.Array\n    friction: jax.Array\n\n    def contact_dim(self) -&gt; int:\n        return self.pos.shape[1]\n\n@jax.vmap\ndef _circle_to_circle_impl(\n    a: Circle,\n    b: Circle,\n    a_pos: Position,\n    b_pos: Position,\n    isactive: jax.Array,\n) -&gt; Contact:\n    a2b_normal, dist = normalize(b_pos.xy - a_pos.xy)\n    penetration = a.radius + b.radius - dist\n    a_contact = a_pos.xy + a2b_normal * a.radius\n    b_contact = b_pos.xy - a2b_normal * b.radius\n    pos = (a_contact + b_contact) * 0.5\n    # Filter penetration\n    penetration = jnp.where(isactive, penetration, jnp.ones_like(penetration) * -1)\n    return Contact(\n        pos=pos,\n        normal=a2b_normal,\n        penetration=penetration,\n        elasticity=(a.elasticity + b.elasticity) * 0.5,\n        friction=(a.friction + b.friction) * 0.5,\n    )\n\ndef check_circle_to_circle(\n    space: Space,\n    position: Position,\n    is_active: jax.Array,\n) -&gt; tuple[Contact, Circle, Circle]:\n    circle1, circle2 = tree_map2(generate_self_pairs, space.circle)\n    pos1, pos2 = tree_map2(generate_self_pairs, position)\n    is_active = jnp.logical_and(*generate_self_pairs(is_active))\n    contacts = _circle_to_circle_impl(circle1, circle2, pos1, pos2, is_active)\n    return contacts, circle1, circle2\n\n\n\n\nCode\nimport seaborn as sns\nfrom matplotlib.patches import Arrow\n\nN = 5\npalette = sns.color_palette(\"husl\", N)\n\n\ncircles = Circle(\n    mass=jnp.ones(N),\n    radius=jnp.ones(N),\n    moment=jnp.ones(N) * 0.5,\n    elasticity=jnp.ones(N) * 0.5,\n    friction=jnp.ones(N) * 0.2,\n    rgba=jnp.array([p + (1.0,) for p in palette]),\n)\nspace = Space(gravity=jnp.array([0.0, -9.8]), circle=circles)\np = Position(\n    angle=jnp.zeros(N),\n    xy=jnp.array([[-3, 4.0], [0.0, 2.0], [5.0, 3], [-3, 1], [2, 0]]),\n)\nv_xy = jnp.concatenate((jnp.zeros((N - 2, 2)), jnp.array([[0, 10.0], [-2.0, 8.0]])))\nv = Velocity(angle=jnp.zeros(N), xy=v_xy)\nf = Force(angle=jnp.zeros(N), xy=jnp.zeros((N, 2)))\nstate = State(p=p, v=v, f=f, is_active=jnp.ones(N, dtype=bool))\npositions = [state.p]\ncontact_list = []\nfor i in range(10):\n    state = update_velocity(space, circles, state)\n    state = update_position(space, state)\n    positions.append(state.p)\n    contacts, _, _ = check_circle_to_circle(space, state.p, state.is_active)\n    total_index = 0\n    for j in range(N):\n        for k in range(j + 1, N):\n            if contacts.penetration[total_index] &gt; 0:\n                contact_list.append(contacts.get_slice(total_index))\n            total_index += 1\nfig, ax = plt.subplots()\nax.set_aspect(\"equal\", adjustable=\"box\")\nax.set_xlim((-5, 5))\nax.set_ylim((-5, 5))\nvisualize_balls(ax, space.circle, positions)\nfor contact in contact_list:\n    arrow = Arrow(*contact.pos, *contact.normal, width=0.2, color=\"r\")\n    ax.add_patch(arrow)\nfig\n\n\n\n\n\n\n\n\n\n衝突後の処理を実装していないので物体がすりぬけていますが、衝突自体はきちんと検出されているようです。"
  },
  {
    "objectID": "posts/fast_2d_physics_in_jax.html#衝突した後どうすればいいの",
    "href": "posts/fast_2d_physics_in_jax.html#衝突した後どうすればいいの",
    "title": "Jaxで高速な2D物理シミュレーションを実装してみる",
    "section": "衝突した後どうすればいいの",
    "text": "衝突した後どうすればいいの\n衝突を検出した後は、物体が重ならないように衝突時に発生したインパルスに関する制約つき方程式を解く必要があります。そのために色々な方法があるのですが、今回はChipmunkなど2次元物理エンジンでよく使われているSequential Impulseと呼ばれる方法を使ってみます。なお、この資料はBox2DのWebページで公開されているものです。\nでは、Sequential Impulseではどのようにして衝突を解決するのでしょうか。まず、物体の衝突時にインパルスが発生するという衝突モデルを仮定し、発生したインパルスを\\(\\mathbf{p}\\)とおきます。面倒なので角速度は一切考えないことにします。このとき、物体1の速度を\\(\\mathbf{v}_1\\)、質量を\\(m_1\\)、物体2の速度を\\(\\mathbf{v}_2\\)、質量を\\(m_2\\)とおくと、インパルスが発生した後の速度は \\[\n\\begin{align*}\n\\mathbf{v}_1 = \\mathbf{v}_1^{\\mathrm{old}} - \\mathbf{p} / m_1 \\\\\n\\mathbf{v}_2 = \\mathbf{v}_2^{\\mathrm{old}} + \\mathbf{p} / m_2\n\\end{align*}\n\\] となります。このとき、\\(\\mathbf{p}\\)の方向は衝突の法線ベクトル\\(\\mathbf{n}\\)なので\\(\\mathbf{p} = p\\mathbf{n}\\)と表せます。よって、結局\\(p\\)を求めればいいです。衝突した点における相対速度を\\(\\Delta \\mathbf{v} = \\mathbf{v}_2 - \\mathbf{v}_1\\)とおきます。このとき、\\(\\Delta \\mathbf{v} \\cdot n = 0\\)なので、上の2式と合わせて、\\(p = \\frac{-\\Delta \\mathbf{v}^{\\mathrm{old}}\\cdot \\mathbf{n}}{\\frac{1}{m_1} + \\frac{1}{m_2}}\\)となります。角速度や摩擦を考慮するともう少しややこしくなりますが、基本はこんな感じです。\nこうして計算したインパルスを全ての衝突に対して適用し、インパルスが小さくなるまで繰り返します。しかし、この手法は物体のめりこみを考慮していないので、これだけだと物体がめりこんだままになってしまうことがあります。 めり込みを減らすための手法はいくつかありますが、主に\n\nどのくらいめり込んでいるかに応じてバイアス速度\\(v_\\mathrm{bias} = \\frac{\\beta}{\\Delta t}\\max(0, \\delta - \\delta_\\mathrm{slop})\\)(\\(\\delta\\)はめりこみの長さ、\\(\\delta_\\mathrm{slop}\\)は許容されるめりこみの長さ)を加え\\(p = \\frac{-\\Delta \\mathbf{v}^{\\mathrm{old}}\\cdot \\mathbf{n} + v_\\mathrm{bias}}{\\frac{1}{m_1} + \\frac{1}{m_2}}\\)とする (Baumegarte)\n速度を更新した後にもう一回Positionに関する制約を解いて擬似的な速度を加える (Nonlinear Gauss Seidel, NGS)\n\nという2種類の手法があります。先程紹介したBox2Dの資料やChipmunk2Dでは1が、現在のBox2Dでは2が使われています。詳しくはBox2D 3.0のコメントを参照してください。 今回は若干高速な1の手法を実装しようかと思ったのですが、この方法だと2つの物体が同じ方向に進んでいる時はめりこみを解消できないので、結局2の手法を実装しました。具体的にソルバの実装としては、\n\n衝突により発生するインパルスに関する制約を解く\n弾性により発生するインパルスを加える\n位置に関する制約を解く\n\nという3つのステップに分けて実装すればいいです。\nまた、さっき並列化のため手動で全ペアに対するループをアンロールしましたが、Sequential Impulseの実装でもこれが使えます。ただし、インパルスを加えた後の速度の更新は、v_update[i][j]にi番目の物体とj番目の物体の衝突により生じるi番目の物体の速度変化が入っているとして、\nfor i in range(N):\n    for j in range(i + 1, N):\n        obj[i].velocity += v_update[i][j]\n        obj[j].velocity += v_update[j][i]\nのように各衝突により生じた速度変化を物体にフィードバックする必要があります。これもいちいちループで書くと遅くなってしまうのですが、さっきのgenerate_self_pairsで\\(0, 1, 2, ..., N - 1\\)のペアを生成しておいてインデックスにするとループなしで書けます。細かく言うと、generate_self_pairsではループが使われているのですが、jax.jitでコンパイルした時に計算結果がキャッシュされるはずなので気にしなくてもいいです。\nというわけで実装してみましょう。実装は基本的にBox2d-Liteと開発中の最新版であるBox2D 3.0を参考にしました。また、Box2D作者のErin Catto氏による講演の内容をTypescriptで実装したリポジトリがあったのでこれも参考にしました。\n\n\nCode\nimport functools\n\n\n@chex.dataclass\nclass ContactHelper:\n    tangent: jax.Array\n    mass_normal: jax.Array\n    mass_tangent: jax.Array\n    v_bias: jax.Array\n    bounce: jax.Array\n    r1: jax.Array\n    r2: jax.Array\n    inv_mass1: jax.Array\n    inv_mass2: jax.Array\n    inv_moment1: jax.Array\n    inv_moment2: jax.Array\n    local_anchor1: jax.Array\n    local_anchor2: jax.Array\n    allow_bounce: jax.Array\n\n\n@chex.dataclass\nclass VelocitySolver:\n    v1: Velocity\n    v2: Velocity\n    pn: jax.Array\n    pt: jax.Array\n    contact: jax.Array\n\n    def update(self, new_contact: jax.Array) -&gt; Self:\n        continuing_contact = jnp.logical_and(self.contact, new_contact)\n        pn = jnp.where(continuing_contact, self.pn, jnp.zeros_like(self.pn))\n        pt = jnp.where(continuing_contact, self.pt, jnp.zeros_like(self.pt))\n        return self.replace(pn=pn, pt=pt, contact=new_contact)\n\n\ndef init_solver(n: int) -&gt; VelocitySolver:\n    return VelocitySolver(\n        v1=Velocity.zeros(n),\n        v2=Velocity.zeros(n),\n        pn=jnp.zeros(n),\n        pt=jnp.zeros(n),\n        contact=jnp.zeros(n, dtype=bool),\n    )\n\n\ndef _pv_gather(\n    p1: _PositionLike,\n    p2: _PositionLike,\n    orig: _PositionLike,\n) -&gt; _PositionLike:\n    indices = jnp.arange(len(orig.angle))\n    outer, inner = generate_self_pairs(indices)\n    p1_xy = jnp.zeros_like(orig.xy).at[outer].add(p1.xy)\n    p1_angle = jnp.zeros_like(orig.angle).at[outer].add(p1.angle)\n    p2_xy = jnp.zeros_like(orig.xy).at[inner].add(p2.xy)\n    p2_angle = jnp.zeros_like(orig.angle).at[inner].add(p2.angle)\n    return p1.__class__(xy=p1_xy + p2_xy, angle=p1_angle + p2_angle)\n\n\ndef _vmap_dot(xy1: jax.Array, xy2: jax.Array) -&gt; jax.Array:\n    \"\"\"Dot product between nested vectors\"\"\"\n    chex.assert_equal_shape((xy1, xy2))\n    orig_shape = xy1.shape\n    a = xy1.reshape(-1, orig_shape[-1])\n    b = xy2.reshape(-1, orig_shape[-1])\n    return jax.vmap(jnp.dot, in_axes=(0, 0))(a, b).reshape(*orig_shape[:-1])\n\n\ndef _sv_cross(s: jax.Array, v: jax.Array) -&gt; jax.Array:\n    \"\"\"Cross product with scalar and vector\"\"\"\n    x, y = _get_xy(v)\n    return jnp.stack((y * -s, x * s), axis=-1)\n\n\ndef _dv2from1(v1: Velocity, r1: jax.Array, v2: Velocity, r2: jax.Array) -&gt; jax.Array:\n    \"\"\"Compute relative veclotiy from v2/r2 to v1/r1\"\"\"\n    rel_v1 = v1.xy + _sv_cross(v1.angle, r1)\n    rel_v2 = v2.xy + _sv_cross(v2.angle, r2)\n    return rel_v2 - rel_v1\n\n\ndef _effective_mass(\n    inv_mass: jax.Array,\n    inv_moment: jax.Array,\n    r: jax.Array,\n    n: jax.Array,\n) -&gt; jax.Array:\n    rn2 = jnp.cross(r, n) ** 2\n    return inv_mass + inv_moment * rn2\n\n\ndef init_contact_helper(\n    space: Space,\n    contact: Contact,\n    a: Shape,\n    b: Shape,\n    p1: Position,\n    p2: Position,\n    v1: Velocity,\n    v2: Velocity,\n) -&gt; ContactHelper:\n    r1 = contact.pos - p1.xy\n    r2 = contact.pos - p2.xy\n\n    inv_mass1, inv_mass2 = a.inv_mass(), b.inv_mass()\n    inv_moment1, inv_moment2 = a.inv_moment(), b.inv_moment()\n    kn1 = _effective_mass(inv_mass1, inv_moment1, r1, contact.normal)\n    kn2 = _effective_mass(inv_mass2, inv_moment2, r2, contact.normal)\n    nx, ny = _get_xy(contact.normal)\n    tangent = jnp.stack((-ny, nx), axis=-1)\n    kt1 = _effective_mass(inv_mass1, inv_moment1, r1, tangent)\n    kt2 = _effective_mass(inv_mass2, inv_moment2, r2, tangent)\n    clipped_p = jnp.clip(space.allowed_penetration - contact.penetration, a_max=0.0)\n    v_bias = -space.bias_factor / space.dt * clipped_p\n    # k_normal, k_tangent, and v_bias should have (N(N-1)/2, N_contacts) shape\n    chex.assert_equal_shape((contact.friction, kn1, kn2, kt1, kt2, v_bias))\n    # Compute elasiticity * relative_vel\n    dv = _dv2from1(v1, r1, v2, r2)\n    vn = _vmap_dot(dv, contact.normal)\n    return ContactHelper(\n        tangent=tangent,\n        mass_normal=1 / (kn1 + kn2),\n        mass_tangent=1 / (kt1 + kt2),\n        v_bias=v_bias,\n        bounce=vn * contact.elasticity,\n        r1=r1,\n        r2=r2,\n        inv_mass1=inv_mass1,\n        inv_mass2=inv_mass2,\n        inv_moment1=inv_moment1,\n        inv_moment2=inv_moment2,\n        local_anchor1=p1.inv_rotate(r1),\n        local_anchor2=p2.inv_rotate(r2),\n        allow_bounce=vn &lt;= -space.bounce_threshold,\n    )\n\n\n@jax.vmap\ndef apply_initial_impulse(\n    contact: Contact,\n    helper: ContactHelper,\n    solver: VelocitySolver,\n) -&gt; VelocitySolver:\n    \"\"\"Warm starting by applying initial impulse\"\"\"\n    p = helper.tangent * solver.pt + contact.normal * solver.pn\n    v1 = solver.v1 - Velocity(\n        angle=helper.inv_moment1 * jnp.cross(helper.r1, p),\n        xy=p * helper.inv_mass1,\n    )\n    v2 = solver.v2 + Velocity(\n        angle=helper.inv_moment2 * jnp.cross(helper.r2, p),\n        xy=p * helper.inv_mass2,\n    )\n    return solver.replace(v1=v1, v2=v2)\n\n\n@jax.vmap\ndef apply_velocity_normal(\n    contact: Contact,\n    helper: ContactHelper,\n    solver: VelocitySolver,\n) -&gt; VelocitySolver:\n    \"\"\"\n    Apply velocity constraints to the solver.\n    Suppose that each shape has (N_contact, 1) or (N_contact, 2).\n    \"\"\"\n    # Relative veclocity (from shape2 to shape1)\n    dv = _dv2from1(solver.v1, helper.r1, solver.v2, helper.r2)\n    vt = jnp.dot(dv, helper.tangent)\n    dpt = -helper.mass_tangent * vt\n    # Clamp friction impulse\n    max_pt = contact.friction * solver.pn\n    pt = jnp.clip(solver.pt + dpt, a_min=-max_pt, a_max=max_pt)\n    dpt_clamped = helper.tangent * (pt - solver.pt)\n    # Velocity update by contact tangent\n    dvt1 = Velocity(\n        angle=-helper.inv_moment1 * jnp.cross(helper.r1, dpt_clamped),\n        xy=-dpt_clamped * helper.inv_mass1,\n    )\n    dvt2 = Velocity(\n        angle=helper.inv_moment2 * jnp.cross(helper.r2, dpt_clamped),\n        xy=dpt_clamped * helper.inv_mass2,\n    )\n    # Compute Relative velocity again\n    dv = _dv2from1(solver.v1 + dvt1, helper.r1, solver.v2 + dvt2, helper.r2)\n    vn = _vmap_dot(dv, contact.normal)\n    dpn = helper.mass_normal * (-vn + helper.v_bias)\n    # Accumulate and clamp impulse\n    pn = jnp.clip(solver.pn + dpn, a_min=0.0)\n    dpn_clamped = contact.normal * (pn - solver.pn)\n    # Velocity update by contact normal\n    dvn1 = Velocity(\n        angle=-helper.inv_moment1 * jnp.cross(helper.r1, dpn_clamped),\n        xy=-dpn_clamped * helper.inv_mass1,\n    )\n    dvn2 = Velocity(\n        angle=helper.inv_moment2 * jnp.cross(helper.r2, dpn_clamped),\n        xy=dpn_clamped * helper.inv_mass2,\n    )\n    # Filter dv\n    dv1, dv2 = jax.tree_map(\n        lambda x: jnp.where(solver.contact, x, jnp.zeros_like(x)),\n        (dvn1 + dvt1, dvn2 + dvt2),\n    )\n    # Summing up dv per each contact pair\n    return VelocitySolver(\n        v1=dv1,\n        v2=dv2,\n        pn=pn,\n        pt=pt,\n        contact=solver.contact,\n    )\n\n\n@jax.vmap\ndef apply_bounce(\n    contact: Contact,\n    helper: ContactHelper,\n    solver: VelocitySolver,\n) -&gt; tuple[Velocity, Velocity]:\n    \"\"\"\n    Apply bounce (resititution).\n    Suppose that each shape has (N_contact, 1) or (N_contact, 2).\n    \"\"\"\n    # Relative veclocity (from shape2 to shape1)\n    dv = _dv2from1(solver.v1, helper.r1, solver.v2, helper.r2)\n    vn = jnp.dot(dv, contact.normal)\n    pn = -helper.mass_normal * (vn + helper.bounce)\n    dpn = contact.normal * pn\n    # Velocity update by contact normal\n    dv1 = Velocity(\n        angle=-helper.inv_moment1 * jnp.cross(helper.r1, dpn),\n        xy=-dpn * helper.inv_mass1,\n    )\n    dv2 = Velocity(\n        angle=helper.inv_moment2 * jnp.cross(helper.r2, dpn),\n        xy=dpn * helper.inv_mass2,\n    )\n    # Filter dv\n    allow_bounce = jnp.logical_and(solver.contact, helper.allow_bounce)\n    return jax.tree_map(\n        lambda x: jnp.where(allow_bounce, x, jnp.zeros_like(x)),\n        (dv1, dv2),\n    )\n\n\n@chex.dataclass\nclass PositionSolver:\n    p1: Position\n    p2: Position\n    contact: jax.Array\n    min_separation: jax.Array\n\n\n@functools.partial(jax.vmap, in_axes=(None, None, None, 0, 0, 0))\ndef correct_position(\n    bias_factor: float | jax.Array,\n    linear_slop: float | jax.Array,\n    max_linear_correction: float | jax.Array,\n    contact: Contact,\n    helper: ContactHelper,\n    solver: PositionSolver,\n) -&gt; PositionSolver:\n    \"\"\"\n    Correct positions to remove penetration.\n    Suppose that each shape in contact and helper has (N_contact, 1) or (N_contact, 2).\n    p1 and p2 should have xy: (1, 2) angle (1, 1) shape\n    \"\"\"\n    # (N_contact, 2)\n    r1 = solver.p1.rotate(helper.local_anchor1)\n    r2 = solver.p2.rotate(helper.local_anchor2)\n    ga2_ga1 = r2 - r1 + solver.p2.xy - solver.p1.xy\n    separation = jnp.dot(ga2_ga1, contact.normal) - contact.penetration\n    c = jnp.clip(\n        bias_factor * (separation + linear_slop),\n        a_min=-max_linear_correction,\n        a_max=0.0,\n    )\n    kn1 = _effective_mass(helper.inv_mass1, helper.inv_moment1, r1, contact.normal)\n    kn2 = _effective_mass(helper.inv_mass2, helper.inv_moment2, r2, contact.normal)\n    k_normal = kn1 + kn2\n    impulse = jnp.where(k_normal &gt; 0.0, -c / k_normal, jnp.zeros_like(c))\n    pn = impulse * contact.normal\n    p1 = Position(\n        angle=-helper.inv_moment1 * jnp.cross(r1, pn),\n        xy=-pn * helper.inv_mass1,\n    )\n    p2 = Position(\n        angle=helper.inv_moment2 * jnp.cross(r2, pn),\n        xy=pn * helper.inv_mass2,\n    )\n    min_sep = jnp.fmin(solver.min_separation, separation)\n    # Filter separation\n    p1, p2 = jax.tree_map(\n        lambda x: jnp.where(solver.contact, x, jnp.zeros_like(x)),\n        (p1, p2),\n    )\n    return solver.replace(p1=p1, p2=p2, min_separation=min_sep)\n\n\ndef fake_fori_loop(start, end, step, initial):\n    \"\"\"For debugging. Just replace jax.lax.fori_loop with this.\"\"\"\n    state = initial\n    for i in range(start, end):\n        state = step(i, state)\n    return state\n\n\ndef apply_seq_impulses(\n    space: Space,\n    solver: VelocitySolver,\n    p: Position,\n    v: Velocity,\n    contact: Contact,\n    a: Shape,\n    b: Shape,\n) -&gt; tuple[Velocity, Position, VelocitySolver]:\n    \"\"\"Resolve collisions by Sequential Impulse method\"\"\"\n    p1, p2 = tree_map2(generate_self_pairs, p)\n    v1, v2 = tree_map2(generate_self_pairs, v)\n    helper = init_contact_helper(space, contact, a, b, p1, p2, v1, v2)\n    solver = apply_initial_impulse(\n        contact,\n        helper,\n        solver.replace(v1=v1, v2=v2),\n    )\n\n    def vstep(\n        _n_iter: int,\n        vs: tuple[Velocity, VelocitySolver],\n    ) -&gt; tuple[Velocity, VelocitySolver]:\n        v_i, solver_i = vs\n        solver_i1 = apply_velocity_normal(contact, helper, solver_i)\n        v_i1 = _pv_gather(solver_i1.v1, solver_i1.v2, v_i) + v_i\n        v1, v2 = tree_map2(generate_self_pairs, v_i1)\n        return v_i1, solver_i1.replace(v1=v1, v2=v2)\n\n    v, solver = jax.lax.fori_loop(0, space.n_velocity_iter, vstep, (v, solver))\n    rest_v1, rest_v2 = apply_bounce(contact, helper, solver)\n    v = _pv_gather(rest_v1, rest_v2, v) + v\n\n    def pstep(\n        _n_iter: int,\n        ps: tuple[Position, PositionSolver],\n    ) -&gt; tuple[Position, PositionSolver]:\n        p_i, solver_i = ps\n        solver_i1 = correct_position(\n            space.bias_factor,\n            space.linear_slop,\n            space.max_linear_correction,\n            contact,\n            helper,\n            solver_i,\n        )\n        p_i1 = _pv_gather(solver_i1.p1, solver_i1.p2, p_i) + p_i\n        p1, p2 = tree_map2(generate_self_pairs, p_i1)\n        return p_i1, solver_i1.replace(p1=p1, p2=p2)\n\n    pos_solver = PositionSolver(\n        p1=p1,\n        p2=p2,\n        contact=solver.contact,\n        min_separation=jnp.zeros_like(p1.angle),\n    )\n    p, pos_solver = jax.lax.fori_loop(0, space.n_position_iter, pstep, (p, pos_solver))\n    return v, p, solver\n\n\nなかなか複雑になりましたが、実装できました。衝突させてみましょう。\n\n\nCode\nfrom celluloid import Camera\nfrom IPython.display import HTML\n\n\ndef animate_balls(\n    fig,\n    ax: Axes,\n    circles: Circle,\n    positions: Iterable[Position],\n) -&gt; HTML:\n    pos = list(positions)\n    camera = Camera(fig)\n    circle_list = circles.tolist()\n    for pi in pos:\n        for pij, circle in zip(pi.tolist(), circle_list):\n            circle_patch = CirclePatch(\n                xy=pij.xy,\n                radius=circle.radius,\n                fill=False,\n                color=circle.rgba.tolist(),\n            )\n            ax.add_patch(circle_patch)\n        camera.snap()\n    return HTML(camera.animate().to_jshtml())\n\n\n\n\nCode\nspace = Space(gravity=jnp.array([0.0, -9.8]), dt=0.04, bias_factor=0.2, circle=circles)\nstate = State(p=p, v=v, f=f, is_active=jnp.ones(N, dtype=bool))\npositions = [state.p]\nsolver = init_solver(N * (N - 1) // 2)\n\n\n@jax.jit\ndef step(state: State, solver: VelocitySolver) -&gt; tuple[State, VelocitySolver]:\n    state = update_velocity(space, space.circle, state)\n    contacts, c1, c2 = check_circle_to_circle(space, state.p, state.is_active)\n    v, p, solver = apply_seq_impulses(\n        space,\n        solver.update(contacts.penetration &gt;= 0),\n        state.p,\n        state.v,\n        contacts,\n        c1,\n        c2,\n    )\n    return update_position(space, state.replace(v=v, p=p)), solver\n\n\nfor i in range(30):\n    state, solver = step(state, solver)\n    positions.append(state.p)\nfig, ax = plt.subplots()\nax.set_aspect(\"equal\", adjustable=\"box\")\nax.set_xlim((-10, 10))\nax.set_ylim((-10, 10))\nanimate_balls(fig, ax, space.circle, positions)\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n最初の衝突で若干のめりこみが発生していますが、一応大丈夫そうですね。"
  },
  {
    "objectID": "posts/fast_2d_physics_in_jax.html#線分",
    "href": "posts/fast_2d_physics_in_jax.html#線分",
    "title": "Jaxで高速な2D物理シミュレーションを実装してみる",
    "section": "線分",
    "text": "線分\n円同士の衝突が実装できたところで、次は凸多角形の実装…と言いたいところですが、それは後回しにして、ゲームには欠かせない「線分」を実装してみます。現実世界にはそんなもの存在しないのですが、ゲームやシミュレーションの世界では、どうしても地面や柵といった境界を表現する必要が生じてきます。こういったものを例えば「めちゃくちゃ重い長方形」として表現することもできますが、シミュレーションを組むユーザー側からするといちいち長方形の大きさだったりを定義するのは面倒なので、「無限の質量を持つ線」として扱えたほうが楽ですよね。というわけで線分を実装してみます。線分の衝突判定は薬のカプセル💊のように両端が丸くなっているやつ(Box2Dだとカプセルと呼ばれているのでカプセルと呼びます)と実装がほぼ同じなので、カプセルも一緒に実装してしまいます。ただカプセル同士の衝突は面倒なので、とりあえず円とカプセルだけ実装しましょう。新しい図形を加えたので、Spaceも作り直す必要があります。とりあえずdataclassに全シェイプをつっこんでおいて、各シェイプの組み合わせごとに衝突判定を行い、衝突解決のときはjnp.concatenateで全部くっつけてからvmapで一度にやるという実装方針にしました。さっきのペアに対するループをアンロールするところは、インデックスのペアを持っておいて適当なオフセットを足しておけばそのまま使えます。\n\n\nCode\nfrom matplotlib.patches import Rectangle\n\n@chex.dataclass\nclass Capsule(Shape):\n    length: jax.Array\n    radius: jax.Array\n\n\n@chex.dataclass\nclass Segment(Shape):\n    length: jax.Array\n\n    def to_capsule(self) -&gt; Capsule:\n        return Capsule(\n            mass=self.mass,\n            moment=self.moment,\n            elasticity=self.elasticity,\n            friction=self.friction,\n            rgba=self.rgba,\n            length=self.length,\n            radius=jnp.zeros_like(self.length),\n        )\n\n\ndef _length_to_points(length: jax.Array) -&gt; tuple[jax.Array, jax.Array]:\n    a = jnp.stack((length * -0.5, length * 0.0), axis=-1)\n    b = jnp.stack((length * 0.5, length * 0.0), axis=-1)\n    return a, b\n\n\n@jax.vmap\ndef _capsule_to_circle_impl(\n    a: Capsule,\n    b: Circle,\n    a_pos: Position,\n    b_pos: Position,\n    isactive: jax.Array,\n) -&gt; Contact:\n    # Move b_pos to capsule's coordinates\n    pb = a_pos.inv_transform(b_pos.xy)\n    p1, p2 = _length_to_points(a.length)\n    edge = p2 - p1\n    s1 = jnp.dot(pb - p1, edge)\n    s2 = jnp.dot(p2 - pb, edge)\n    in_segment = jnp.logical_and(s1 &gt;= 0.0, s2 &gt;= 0.0)\n    ee = jnp.sum(jnp.square(edge), axis=-1, keepdims=True)\n    # Closest point\n    # s1 &lt; 0: pb is left to the capsule\n    # s2 &lt; 0: pb is right to the capsule\n    # else: pb is in between capsule\n    pa = jax.lax.select(\n        in_segment,\n        p1 + edge * s1 / ee,\n        jax.lax.select(s1 &lt; 0.0, p1, p2),\n    )\n    a2b_normal, dist = normalize(pb - pa)\n    penetration = a.radius + b.radius - dist\n    a_contact = pa + a2b_normal * a.radius\n    b_contact = pb - a2b_normal * b.radius\n    pos = a_pos.transform((a_contact + b_contact) * 0.5)\n    xy_zeros = jnp.zeros_like(b_pos.xy)\n    a2b_normal_rotated = a_pos.replace(xy=xy_zeros).transform(a2b_normal)\n    # Filter penetration\n    penetration = jnp.where(isactive, penetration, jnp.ones_like(penetration) * -1)\n    return Contact(\n        pos=pos,\n        normal=a2b_normal_rotated,\n        penetration=penetration,\n        elasticity=(a.elasticity + b.elasticity) * 0.5,\n        friction=(a.friction + b.friction) * 0.5,\n    )\n\n\n@chex.dataclass\nclass ShapeDict:\n    circle: Circle | None = None\n    segment: Segment | None = None\n    capsule: Capsule | None = None\n    \n    def concat(self) -&gt; Shape:\n        shapes = [s.to_shape() for s in self.values() if s is not None]\n        return jax.tree_map(lambda *args: jnp.concatenate(args, axis=0), *shapes)\n\n\n@chex.dataclass\nclass StateDict:\n    circle: State | None = None\n    segment: State | None = None\n    capsule: State | None = None\n\n    def concat(self) -&gt; None:\n        states = [s for s in self.values() if s is not None]\n        return jax.tree_map(lambda *args: jnp.concatenate(args, axis=0), *states)\n\n    def offset(self, key: str) -&gt; int:\n        total = 0\n        for k, state in self.items():\n            if k == key:\n                return total\n            if state is not None:\n                total += state.p.batch_size()\n        raise RuntimeError(\"Unreachable\")\n        \n    def _get(self, name: str, state: State) -&gt; State | None:\n        if self[name] is None:\n            return None\n        else:\n            start = self.offset(name)\n            end = start + self[name].p.batch_size()\n            return state.get_slice(jnp.arange(start, end))\n        \n    def update(self, statec: State) -&gt; Self:\n        circle = self._get(\"circle\", statec)\n        segment = self._get(\"segment\", statec)\n        capsule = self._get(\"capsule\", statec)\n        return self.__class__(circle=circle, segment=segment, capsule=capsule)\n\n\nContactFn = Callable[[StateDict], tuple[Contact, Shape, Shape]]\n\n\ndef _pair_outer(x: jax.Array, reps: int) -&gt; jax.Array:\n    return jnp.repeat(x, reps, axis=0, total_repeat_length=x.shape[0] * reps)\n\n\ndef _pair_inner(x: jax.Array, reps: int) -&gt; jax.Array:\n    return jnp.tile(x, (reps,) + (1,) * (x.ndim - 1))\n\n\ndef generate_pairs(x: jax.Array, y: jax.Array) -&gt; tuple[jax.Array, jax.Array]:\n    \"\"\"Returns two arrays that iterate over all combination of elements in x and y\"\"\"\n    xlen, ylen = x.shape[0], y.shape[0]\n    return _pair_outer(x, ylen), _pair_inner(y, xlen)\n\n\ndef _circle_to_circle(\n    shaped: ShapeDict,\n    stated: StateDict,\n) -&gt; tuple[Contact, Circle, Circle]:\n    circle1, circle2 = tree_map2(generate_self_pairs, shaped.circle)\n    pos1, pos2 = tree_map2(generate_self_pairs, stated.circle.p)\n    is_active = jnp.logical_and(*generate_self_pairs(stated.circle.is_active))\n    contacts = _circle_to_circle_impl(\n        circle1,\n        circle2,\n        pos1,\n        pos2,\n        is_active,\n    )\n    return contacts, circle1, circle2\n\n\ndef _capsule_to_circle(\n    shaped: ShapeDict,\n    stated: StateDict,\n) -&gt; tuple[Contact, Capsule, Circle]:\n    capsule = jax.tree_map(\n        functools.partial(_pair_outer, reps=shaped.circle.mass.shape[0]),\n        shaped.capsule,\n    )\n    circle = jax.tree_map(\n        functools.partial(_pair_inner, reps=shaped.capsule.mass.shape[0]),\n        shaped.circle,\n    )\n    pos1, pos2 = tree_map2(generate_pairs, stated.capsule.p, stated.circle.p)\n    is_active = jnp.logical_and(\n        *generate_pairs(stated.capsule.is_active, stated.circle.is_active)\n    )\n    contacts = _capsule_to_circle_impl(\n        capsule,\n        circle,\n        pos1,\n        pos2,\n        is_active,\n    )\n    return contacts, capsule, circle\n\n\ndef _segment_to_circle(\n    shaped: ShapeDict,\n    stated: StateDict,\n) -&gt; tuple[Contact, Segment, Circle]:\n    segment = jax.tree_map(\n        functools.partial(_pair_outer, reps=shaped.circle.mass.shape[0]),\n        shaped.segment,\n    )\n    circle = jax.tree_map(\n        functools.partial(_pair_inner, reps=shaped.segment.mass.shape[0]),\n        shaped.circle,\n    )\n    pos1, pos2 = tree_map2(generate_pairs, stated.segment.p, stated.circle.p)\n    is_active = jnp.logical_and(\n        *generate_pairs(stated.segment.is_active, stated.circle.is_active)\n    )\n    contacts = _capsule_to_circle_impl(\n        segment.to_capsule(),\n        circle,\n        pos1,\n        pos2,\n        is_active,\n    )\n    return contacts, segment, circle\n\n\n_CONTACT_FUNCTIONS = {\n    (\"circle\", \"circle\"): _circle_to_circle,\n    (\"capsule\", \"circle\"): _capsule_to_circle,\n    (\"segment\", \"circle\"): _segment_to_circle,\n}\n\n\n@chex.dataclass\nclass ContactWithMetadata:\n    contact: Contact\n    shape1: Shape\n    shape2: Shape\n    outer_index: jax.Array\n    inner_index: jax.Array\n\n    def gather_p_or_v(\n        self,\n        outer: _PositionLike,\n        inner: _PositionLike,\n        orig: _PositionLike,\n    ) -&gt; _PositionLike:\n        xy_outer = jnp.zeros_like(orig.xy).at[self.outer_index].add(outer.xy)\n        angle_outer = jnp.zeros_like(orig.angle).at[self.outer_index].add(outer.angle)\n        xy_inner = jnp.zeros_like(orig.xy).at[self.inner_index].add(inner.xy)\n        angle_inner = jnp.zeros_like(orig.angle).at[self.inner_index].add(inner.angle)\n        return orig.__class__(angle=angle_outer + angle_inner, xy=xy_outer + xy_inner)\n\n\n@chex.dataclass\nclass ExtendedSpace:\n    gravity: jax.Array\n    shaped: ShapeDict\n    dt: jax.Array | float = 0.1\n    linear_damping: jax.Array | float = 0.95\n    angular_damping: jax.Array | float = 0.95\n    bias_factor: jax.Array | float = 0.2\n    n_velocity_iter: int = 8\n    n_position_iter: int = 2\n    linear_slop: jax.Array | float = 0.005\n    max_linear_correction: jax.Array | float = 0.2\n    allowed_penetration: jax.Array | float = 0.005\n    bounce_threshold: float = 1.0\n\n    def check_contacts(self, stated: StateDict) -&gt; ContactWithMetadata:\n        contacts = []\n        for (n1, n2), fn in _CONTACT_FUNCTIONS.items():\n            if stated[n1] is not None and stated[n2] is not None:\n                contact, shape1, shape2 = fn(self.shaped, stated)\n                len1, len2 = stated[n1].p.batch_size(), stated[n2].p.batch_size()\n                offset1, offset2 = stated.offset(n1), stated.offset(n2)\n                if n1 == n2:\n                    outer_index, inner_index = generate_self_pairs(jnp.arange(len1))\n                else:\n                    outer_index, inner_index = generate_pairs(\n                        jnp.arange(len1),\n                        jnp.arange(len2),\n                    )\n                contact_with_meta = ContactWithMetadata(\n                    contact=contact,\n                    shape1=shape1.to_shape(),\n                    shape2=shape2.to_shape(),\n                    outer_index=outer_index + offset1,\n                    inner_index=inner_index + offset2,\n                )\n                contacts.append(contact_with_meta)\n        return jax.tree_map(lambda *args: jnp.concatenate(args, axis=0), *contacts)\n    \n    def n_possible_contacts(self) -&gt; int:\n        n = 0\n        for n1, n2 in _CONTACT_FUNCTIONS.keys():\n            if self.shaped[n1] is not None and self.shaped[n2] is not None:\n                len1, len2 = len(self.shaped[n1].mass), len(self.shaped[n2].mass)\n                if n1 == n2:\n                    n += len1 * (len1 - 1) // 2\n                else:\n                    n += len1 * len2\n        return n\n\n\ndef animate_balls_and_segments(\n    fig,\n    ax: Axes,\n    circles: Circle,\n    segments: Segment,\n    c_pos: Iterable[Position],\n    s_pos: Position,\n) -&gt; HTML:\n    camera = Camera(fig)\n    circle_list = circles.tolist()\n    # Lower left\n    segment_ll = s_pos.transform(\n        jnp.stack((-segments.length * 0.5, jnp.zeros_like(segments.length)), axis=1)\n    )\n    for pi in c_pos:\n        for pij, circle in zip(pi.tolist(), circle_list):\n            circle_patch = CirclePatch(\n                xy=pij.xy,\n                radius=circle.radius,\n                fill=False,\n                color=circle.rgba.tolist(),\n            )\n            ax.add_patch(circle_patch)\n        for ll, pj, segment in zip(segment_ll, s_pos.tolist(), segments.tolist()):\n            rect_patch = Rectangle(\n                xy=ll,\n                width=segment.length,\n                angle=(pj.angle / jnp.pi).item() * 180,\n                height=0.1,\n            )\n            ax.add_patch(rect_patch)\n        camera.snap()\n    return HTML(camera.animate().to_jshtml())\n\ndef solve_constraints(\n    space: Space,\n    solver: VelocitySolver,\n    p: Position,\n    v: Velocity,\n    contact_with_meta: ContactWithMetadata,\n) -&gt; tuple[Velocity, Position, VelocitySolver]:\n    \"\"\"Resolve collisions by Sequential Impulse method\"\"\"\n    outer, inner = contact_with_meta.outer_index, contact_with_meta.inner_index\n\n    def get_pairs(p_or_v: _PositionLike) -&gt; tuple[_PositionLike, _PositionLike]:\n        return p_or_v.get_slice(outer), p_or_v.get_slice(inner)\n\n    p1, p2 = get_pairs(p)\n    v1, v2 = get_pairs(v)\n    helper = init_contact_helper(\n        space,\n        contact_with_meta.contact,\n        contact_with_meta.shape1,\n        contact_with_meta.shape2,\n        p1,\n        p2,\n        v1,\n        v2,\n    )\n    # Warm up the velocity solver\n    solver = apply_initial_impulse(\n        contact_with_meta.contact,\n        helper,\n        solver.replace(v1=v1, v2=v2),\n    )\n\n    def vstep(\n        _n_iter: int,\n        vs: tuple[Velocity, VelocitySolver],\n    ) -&gt; tuple[Velocity, VelocitySolver]:\n        v_i, solver_i = vs\n        solver_i1 = apply_velocity_normal(contact_with_meta.contact, helper, solver_i)\n        v_i1 = contact_with_meta.gather_p_or_v(solver_i1.v1, solver_i1.v2, v_i) + v_i\n        v1, v2 = get_pairs(v_i1)\n        return v_i1, solver_i1.replace(v1=v1, v2=v2)\n\n    v, solver = jax.lax.fori_loop(0, space.n_velocity_iter, vstep, (v, solver))\n    bv1, bv2 = apply_bounce(contact_with_meta.contact, helper, solver)\n    v = contact_with_meta.gather_p_or_v(bv1, bv2, v) + v\n\n    def pstep(\n        _n_iter: int,\n        ps: tuple[Position, PositionSolver],\n    ) -&gt; tuple[Position, PositionSolver]:\n        p_i, solver_i = ps\n        solver_i1 = correct_position(\n            space.bias_factor,\n            space.linear_slop,\n            space.max_linear_correction,\n            contact_with_meta.contact,\n            helper,\n            solver_i,\n        )\n        p_i1 = contact_with_meta.gather_p_or_v(solver_i1.p1, solver_i1.p2, p_i) + p_i\n        p1, p2 = get_pairs(p_i1)\n        return p_i1, solver_i1.replace(p1=p1, p2=p2)\n\n    pos_solver = PositionSolver(\n        p1=p1,\n        p2=p2,\n        contact=solver.contact,\n        min_separation=jnp.zeros_like(p1.angle),\n    )\n    p, pos_solver = jax.lax.fori_loop(0, space.n_position_iter, pstep, (p, pos_solver))\n    return v, p, solver\n\n\ndef dont_solve_constraints(\n    _space: Space,\n    solver: VelocitySolver,\n    p: Position,\n    v: Velocity,\n    _contact_with_meta: ContactWithMetadata,\n) -&gt; tuple[Velocity, Position, VelocitySolver]:\n    return v, p, solver\n\n\nN_SEG = 3\nsegments = Segment(\n    mass=jnp.ones(N_SEG) * jnp.inf,\n    moment=jnp.ones(N_SEG) * jnp.inf,\n    elasticity=jnp.ones(N_SEG) * 0.5,\n    friction=jnp.ones(N_SEG) * 1.0,\n    rgba=jnp.ones((N_SEG, 4)),\n    length=jnp.array([4 * jnp.sqrt(2), 4, 4 * jnp.sqrt(2)]),\n)\ncpos = jnp.array([[2, 2], [4, 3], [3, 6], [6, 5], [5, 7]], dtype=jnp.float32)\nstated = StateDict(\n    circle=State(\n        p=Position(xy=cpos, angle=jnp.zeros(N)),\n        v=Velocity.zeros(N),\n        f=Force.zeros(N),\n        is_active=jnp.array([True, True, True, True, True]),\n    ),\n    segment=State(\n        p=Position(\n            xy=jnp.array([[-2.0, 2.0], [2, 0], [6, 2]], dtype=jnp.float32),\n            angle=jnp.array([jnp.pi * 1.75, 0, jnp.pi * 0.25]),\n        ),\n        v=Velocity.zeros(N_SEG),\n        f=Force.zeros(N_SEG),\n        is_active=jnp.ones(N_SEG, dtype=bool),\n    ),\n)\nspace = ExtendedSpace(\n    gravity=jnp.array([0.0, -9.8]),\n    linear_damping=1.0,\n    angular_damping=1.0,\n    dt=0.04,\n    bias_factor=0.2,\n    n_velocity_iter=6,\n    shaped=ShapeDict(circle=circles, segment=segments),\n)\n\n\n@jax.jit\ndef step(stated: StateDict, solver: VelocitySolver) -&gt; StateDict:\n    state = update_velocity(space, space.shaped.concat(), stated.concat())\n    contact_with_meta = space.check_contacts(stated.update(state))\n    # Check there's any penetration\n    contacts = contact_with_meta.contact.penetration &gt;= 0\n    v, p, solver = jax.lax.cond(\n        jnp.any(contacts),\n        solve_constraints,\n        dont_solve_constraints,\n        space,\n        solver.update(contacts),\n        state.p,\n        state.v,\n        contact_with_meta,\n    )\n    statec = update_position(space, state.replace(v=v, p=p))\n    return stated.update(statec)\n\n\nアニメーションしてみます。\n\n\nCode\npositions = [stated[\"circle\"].p]\nsolver = init_solver(space.n_possible_contacts())\n\nfor i in range(50):\n    stated = step(stated, solver)\n    positions.append(stated[\"circle\"].p)\nfig, ax = plt.subplots()\nax.set_aspect(\"equal\", adjustable=\"box\")\nax.set_xlim((-10, 10))\nax.set_ylim((0, 10))\nanimate_balls_and_segments(\n    fig,\n    ax,\n    space.shaped[\"circle\"],\n    space.shaped[\"segment\"],\n    positions,\n    stated[\"segment\"].p,\n)\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nちょっとめりこんでいますがまあ一応計算できてはいそうです。一応簡単にベンチマークしてみます。\n\n\nCode\n%timeit step(stated, solver)\n\n\nボールの数がまだ少ないですが、1ステップあたり約700マイクロ秒ということで、かなり高速にできたのではないでしょうか。"
  },
  {
    "objectID": "posts/try_equinox_for_rl_en.html",
    "href": "posts/try_equinox_for_rl_en.html",
    "title": "Try reinforcement learning with equinox",
    "section": "",
    "text": "I recently tried equinox, a jax-based library for defining and managing neural nets, and I liked it. So in this blog post I will introduce equinox and demonstrate its use with a reinforcement learning example. Other jax-based NN libraries include haiku from Deepmind and flax from Google Research. Both are actually quite similar because their designs are based on stax, which is a reference implementation by the Jax developers. In other words, I would say that both haiku and flax are libraries that have added a PyTorch-like Module to stax. An equinox document briefly summarizes the approach of haiku and flax, calling it the ‘init-apply approach’. Let’s start this blog post by paraphrasing that document."
  },
  {
    "objectID": "posts/try_equinox_for_rl_en.html#nn-parameter-management-by-init-apply-approach",
    "href": "posts/try_equinox_for_rl_en.html#nn-parameter-management-by-init-apply-approach",
    "title": "Try reinforcement learning with equinox",
    "section": "NN parameter management by init-apply approach",
    "text": "NN parameter management by init-apply approach\nIn stax, haiku, and flax, this problem of parameter management is solved by the so-called “init-apply approach”, which can be summarized as:\n\nthe model has no parameters\ninstead, the model is just a set of two functions: init and apply\n\n\ninit takes an example input, initializes the parameters, and returns the initial parameter\napply takes input and parameters, and then compute the output of the model\n\nSo the APIs for flax and haiku are as follows. While there are a number of differences (e.g., flax uses __call__ (while haiku uses forward like PyTorch (just like how Chainer and PyTorch differ), flax employs dataclasses.dataclass for Module class), their design is quite similar.\nclass Linear(Module):\n    def __call__(self, x):\n        batch_size = x.shape[0]\n        w = self.parameter(output, shape=(batch_size, self.output_size) init=self.w_init)\n        b = self.parameter(output, shape=(1, self.output_size) init=self.w_init)\n        return w @ x.T + b\n\nmodel = Linear(output_size=10)\nparams = model.init(jnp.zeros(1, 100))\nresult = model.apply(params, jnp.zeros(10, 100))\nIn the above example, I used Module to represent the abstract class for defining NN. Note that he method self.parameter in the above example is the counterpart of the parameter registration functions in flax and haiku. In the case of haiku, the params is the following dict.\n{\n    \"Linear/~/w\": [...],\n    \"Linear/~/b\": [...],\n}\nSo, it means that users need to know this naming scheme to access a specific value of parameter. stax doesn’t have Module feature. Instead, it provides function combinator to combine multiple init and apply. Then let’s summarize the pros and cons of this approach.\nPros\n\nNo need to specify the shape of the input Array when initializing the Module\n\nParameters are initialized when init is called\n\nFunctions and data are separated\n\nModule has no variables that can be changed, and is treated completely separately from its parameters\n\n\nDisadvantages.\n\n(Only haiku/flax) Module is redundant\n\nModule only has “model settings” such as output dimensions\n\nDirect access to parameter elements is cumbersome\n\nFor example, in haiku, each parameter element can be accessed with params[\"Linear/~/w\"], so we need to understand this naming scheme for keys to check parameters\n\nNot very object oriented\n\nBecause classes don’t have data\n\n(Only haiku/flax) Parameter calls in Module need to be converted to functions that can be used by jax.grad.\n\nFor example, haiku.transform converts functions that include parameter calls with haiku.get_parameter into functions that take parameters as arguments"
  },
  {
    "objectID": "posts/try_equinox_for_rl_en.html#try-reinforcement-learning",
    "href": "posts/try_equinox_for_rl_en.html#try-reinforcement-learning",
    "title": "Try reinforcement learning with equinox",
    "section": "Try reinforcement learning",
    "text": "Try reinforcement learning"
  },
  {
    "objectID": "posts/try_equinox_for_rl_en.html#environment",
    "href": "posts/try_equinox_for_rl_en.html#environment",
    "title": "Try reinforcement learning with equinox",
    "section": "Environment",
    "text": "Environment\nNow that we have introduced the features of equinox, let’s try reinforcement learning with it. Since we are using jax and since the API of Gym has changed a lot and changed to gymnasium we can’t keep up with it at all, let’s use an environment made by jax. Here I use Maze from the jumanji.\n\n\nCode\nimport jumanji\nfrom jumanji.wrappers import AutoResetWrapper\nfrom IPython.display import HTML\n\nenv = jumanji.make(\"Maze-v0\")\nenv = AutoResetWrapper(env)\nn_actions = env.action_spec().num_values\nkey, *keys = jax.random.split(jax.random.PRNGKey(20230720), 11)\nstate, timestep = env.reset(key)\nstates = [state]\nfor key in keys:\n    action = jax.random.randint(key=key, minval=0, maxval=n_actions, shape=())\n    state, timestep = env.step(state, action)\n    states.append(state)\nanim = env.animate(states)\nHTML(anim.to_html5_video().replace('=\"1000\"', '=\"640\"'))  # Change video size\n\n\n\n\n\n\n\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nIt looks easy to use. However, the default size of the video was too big, so I replaced the HTML tags to make it smaller. It seems to be usable in combination with vmap and jit for actual learning."
  },
  {
    "objectID": "posts/try_equinox_for_rl_en.html#lets-implement-ppo.",
    "href": "posts/try_equinox_for_rl_en.html#lets-implement-ppo.",
    "title": "Try reinforcement learning with equinox",
    "section": "Let’s implement PPO.",
    "text": "Let’s implement PPO.\nSo, let’s try to learn this environment. Here we will implement PPO, because it’s fast.\n\nInput\nInput as an array with 3x10x10 shape, each representing a 10x10 binary image of the position of the wall, the position of the agent, and the position of the goal, respectively.\n\n\nCode\nfrom jumanji.environments.routing.maze.types import Observation, State\n\ndef obs_to_image(obs: Observation) -&gt; jax.Array:\n    walls = obs.walls.astype(jnp.float32)\n    agent = jnp.zeros_like(walls).at[obs.agent_position].set(1.0)\n    target = jnp.zeros_like(walls).at[obs.target_position].set(1.0)\n    return jnp.stack([walls, agent, target])\n\n\n\n\nNetwork\nI use a simple network with a 2D convolution layer, followed by ReLU activations and two linear layers. Policy and value networks share the first two layers, and the policy is modeled by a categorical distribution. To make things simpler, I will write the input and output sizes as \\(3 \\times 10 \\times 10\\) and \\(4\\) for the number of actions.\n\n\nCode\nfrom typing import NamedTuple\n\nfrom jax.nn.initializers import orthogonal\n\n\nclass PPONetOutput(NamedTuple):\n    policy_logits: jax.Array\n    value: jax.Array\n\n\nclass SoftmaxPPONet(eqx.Module):\n    torso: list\n    value_head: eqx.nn.Linear\n    policy_head: eqx.nn.Linear\n\n    def __init__(self, key: jax.Array) -&gt; None:\n        key1, key2, key3, key4, key5 = jax.random.split(key, 5)\n        # Common layers\n        self.torso = [\n            eqx.nn.Conv2d(3, 1, kernel_size=3, key=key1),\n            jax.nn.relu,\n            jnp.ravel,\n            eqx.nn.Linear(64, 64, key=key2),\n            jax.nn.relu,\n        ]\n        self.value_head = eqx.nn.Linear(64, 1, key=key3)\n        policy_head = eqx.nn.Linear(64, 4, key=key4)\n        # Use small value for policy initialization\n        self.policy_head = eqx.tree_at(\n            lambda linear: linear.weight,\n            policy_head,\n            orthogonal(scale=0.01)(key5, policy_head.weight.shape),\n        )\n\n    def __call__(self, x: jax.Array) -&gt; PPONetOutput:\n        for layer in self.torso:\n            x = layer(x)\n        value = self.value_head(x)\n        policy_logits = self.policy_head(x)\n        return PPONetOutput(policy_logits=policy_logits, value=value)\n\n    def value(self, x: jax.Array) -&gt; jax.Array:\n        for layer in self.torso:\n            x = layer(x)\n        return self.value_head(x)\n\n\n\n\nRollout\nIn PPO implementations, it is common to collect a history of environmental interaction about 500~8000 steps and use it to update the network several times. Here we use jax.lax.scan to implement a rollout that is faster than native Python for loop. While scan is super fast, its usage requires a bit of care. In particular, if you set the second output of the function that moves forward one step to results: list[Result], keep in mind that the final return will be Result(member1=stack([m1 for m1 in results.member1]), ...) Also, since the argument of exec_rollout contains a SoftmaxPPONet which is an instance of eqx.Module, you can’t use jax.jit directly. Instead, you need to use eqx.filter_jit that decomposes the module to parameters and non-parameters automatically. Actions are sampled by a categorical distribution obtained by applying softmax to the output of the policy network, but since Observation contains an action_mask that tells you the direction in which you can move in the maze, you can mask the actions you cannot take with this by applying -inf to policy logits. In a simple environment, you don’t need to do this, but mazes with many walls are difficult to solve without this masking.\n\n\nCode\nimport chex\n\n\n@chex.dataclass\nclass Rollout:\n    \"\"\"Rollout buffer that stores the entire history of one rollout\"\"\"\n\n    observations: jax.Array\n    actions: jax.Array\n    action_masks: jax.Array\n    rewards: jax.Array\n    terminations: jax.Array\n    values: jax.Array\n    policy_logits: jax.Array\n\n\ndef mask_logits(policy_logits: jax.Array, action_mask: jax.Array) -&gt; jax.Array:\n    return jax.lax.select(\n        action_mask,\n        policy_logits,\n        jnp.ones_like(policy_logits) * -jnp.inf,\n    )\n\n\nvmapped_obs2i = jax.vmap(obs_to_image)\n\n\n@eqx.filter_jit\ndef exec_rollout(\n    initial_state: State,\n    initial_obs: Observation,\n    env: jumanji.Environment,\n    network: SoftmaxPPONet,\n    prng_key: jax.Array,\n    n_rollout_steps: int,\n) -&gt; tuple[State, Rollout, Observation, jax.Array]:\n    def step_rollout(\n        carried: tuple[State, Observation],\n        key: jax.Array,\n    ) -&gt; tuple[tuple[State, jax.Array], Rollout]:\n        state_t, obs_t = carried\n        obs_image = vmapped_obs2i(obs_t)\n        net_out = jax.vmap(network)(obs_image)\n        masked_logits = mask_logits(net_out.policy_logits, obs_t.action_mask)\n        actions = jax.random.categorical(key, masked_logits, axis=-1)\n        state_t1, timestep = jax.vmap(env.step)(state_t, actions)\n        rollout = Rollout(\n            observations=obs_image,\n            actions=actions,\n            action_masks=obs_t.action_mask,\n            rewards=timestep.reward,\n            terminations=1.0 - timestep.discount,\n            values=net_out.value,\n            policy_logits=masked_logits,\n        )\n        return (state_t1, timestep.observation), rollout\n\n    (state, obs), rollout = jax.lax.scan(\n        step_rollout,\n        (initial_state, initial_obs),\n        jax.random.split(prng_key, n_rollout_steps),\n    )\n    next_value = jax.vmap(network.value)(vmapped_obs2i(obs))\n    return state, rollout, obs, next_value\n\n\nLet’s test it. The advantage of the jax environment is that you can easily vector-parallelize the environment with jax.vmap, so this time we will run it in 16 parallel. If you apply vmap to reset and give it 16 PRNGKeys, you will get 16 parallel State.\n\n\nCode\nkey, net_key, reset_key, rollout_key = jax.random.split(key, 4)\npponet = SoftmaxPPONet(net_key)\ninitial_state, initial_timestep = jax.vmap(env.reset)(jax.random.split(reset_key, 16))\nnext_state, rollout, next_obs, next_value = exec_rollout(\n    initial_state,\n    initial_timestep.observation,\n    env,\n    pponet,\n    rollout_key,\n    512,\n)\nrollout.rewards.shape\n\n\n(512, 16)\n\n\nWe can confirm that the inputs are 16 in parallel, and each member of Rollout has n. steps x n. env x .... shape.\n\n\nLearning\nNow that the data has been collected, it is time to write the code to update the network. First, compute GAE. Since it is unexpectedly a bottleneck, let’s speed it up with fori_loop.\n\n\nCode\n@chex.dataclass(frozen=True, mappable_dataclass=False)\nclass Batch:\n    \"\"\"Batch for PPO, indexable to get a minibatch.\"\"\"\n\n    observations: jax.Array\n    action_masks: jax.Array\n    onehot_actions: jax.Array\n    rewards: jax.Array\n    advantages: jax.Array\n    value_targets: jax.Array\n    log_action_probs: jax.Array\n\n    def __getitem__(self, idx: jax.Array):\n        return self.__class__(  # type: ignore\n            observations=self.observations[idx],\n            action_masks=self.action_masks[idx],\n            onehot_actions=self.onehot_actions[idx],\n            rewards=self.rewards[idx],\n            advantages=self.advantages[idx],\n            value_targets=self.value_targets[idx],\n            log_action_probs=self.log_action_probs[idx],\n        )\n\n\ndef compute_gae(\n    r_t: jax.Array,\n    discount_t: jax.Array,\n    values: jax.Array,\n    lambda_: float = 0.95,\n) -&gt; jax.Array:\n    \"\"\"Efficiently compute generalized advantage estimator (GAE)\"\"\"\n\n    gamma_lambda_t = discount_t * lambda_\n    delta_t = r_t + discount_t * values[1:] - values[:-1]\n    n = delta_t.shape[0]\n\n    def update(i: int, advantage_t: jax.Array) -&gt; jax.Array:\n        t = n - i - 1\n        adv_t = delta_t[t] + gamma_lambda_t[t] * advantage_t[t + 1]\n        return advantage_t.at[t].set(adv_t)\n\n    advantage_t = jax.lax.fori_loop(0, n, update, jnp.zeros_like(values))\n    return advantage_t[:-1]\n\n\n@eqx.filter_jit\ndef make_batch(\n    rollout: Rollout,\n    next_value: jax.Array,\n    gamma: float,\n    gae_lambda: float,\n) -&gt; Batch:\n    all_values = jnp.concatenate(\n        [jnp.squeeze(rollout.values), next_value.reshape(1, -1)]\n    )\n    advantages = compute_gae(\n        rollout.rewards,\n        # Set γ = 0 when the episode terminates\n        (1.0 - rollout.terminations) * gamma,\n        all_values,\n        gae_lambda,\n    )\n    value_targets = advantages + all_values[:-1]\n    onehot_actions = jax.nn.one_hot(rollout.actions, 4)\n    _, _, *obs_shape = rollout.observations.shape\n    log_action_probs = jnp.sum(\n        jax.nn.log_softmax(rollout.policy_logits) * onehot_actions,\n        axis=-1,\n    )\n    return Batch(\n        observations=rollout.observations.reshape(-1, *obs_shape),\n        action_masks=rollout.action_masks.reshape(-1, 4),\n        onehot_actions=onehot_actions.reshape(-1, 4),\n        rewards=rollout.rewards.ravel(),\n        advantages=advantages.ravel(),\n        value_targets=value_targets.ravel(),\n        log_action_probs=log_action_probs.ravel(),\n    )\n\n\n\n\nCode\nbatch = make_batch(rollout, next_value, 0.99, 0.95)\nbatch.advantages.shape, batch.onehot_actions.shape, batch.log_action_probs.shape\n\n\n((8192,), (8192, 4), (8192,))\n\n\nLooks OK. Now we can sample a mini-batch from the Batch we created with this and update it with gradient descent to minimize the loss function. Here I use optax, which is a standard in the jax community. At this point, note the following three points.\n\nUse eqx.filter_grad instead of jax.grad as explained in the previous sections\nSoftmaxPPONet has types that cannot be used as jax.jit arguments, such as jax.nn.relu, so use eqx.partition to break them up before using them as jax.lax.scan arguments\nSimilarly, SoftmaxPPONet cannot be used as an argument to optax initialization/update functions as is, so it should be broken down with eqx.partition or eqx.filter to exclude members other than jax.Array.\n\nAlso, if you want to use jax.lax.scan in a mini-batch update loop, there are several ways to do it. What I do here is that, assuming mini-batch size \\(N\\), number of updates \\(M\\), number of update epochs \\(K\\) times, overall batch size \\(N \\times M\\),\n\nMake permutations of \\(0, 1, 2, ... , NM - 1\\) \\(K\\) times.\nMake \\(K\\) copies of Batch shuffled by the permutations we made\nConcatenate each member with jnp.concatenate and reshape to an array with size \\(MK \\times N \\times ...\\).\n\nIt’s a bit difficult, and you may not need to speed it up this fast. If you are worried about memory usage, you could write a \\(K\\) loop in Python, but in this case, the input is \\(3\\times 10\\times 10\\), so it seems to be ok.\n\n\nCode\nimport optax\n\n\ndef loss_function(\n    network: SoftmaxPPONet,\n    batch: Batch,\n    ppo_clip_eps: float,\n) -&gt; jax.Array:\n    net_out = jax.vmap(network)(batch.observations)\n    # Policy loss\n    log_pi = jax.nn.log_softmax(\n        jax.lax.select(\n            batch.action_masks,\n            net_out.policy_logits,\n            jnp.ones_like(net_out.policy_logits * -jnp.inf),\n        )\n    )\n    log_action_probs = jnp.sum(log_pi * batch.onehot_actions, axis=-1)\n    policy_ratio = jnp.exp(log_action_probs - batch.log_action_probs)\n    clipped_ratio = jnp.clip(policy_ratio, 1.0 - ppo_clip_eps, 1.0 + ppo_clip_eps)\n    clipped_objective = jnp.fmin(\n        policy_ratio * batch.advantages,\n        clipped_ratio * batch.advantages,\n    )\n    policy_loss = -jnp.mean(clipped_objective)\n    # Value loss\n    value_loss = jnp.mean(0.5 * (net_out.value - batch.value_targets) ** 2)\n    # Entropy regularization\n    entropy = jnp.mean(-jnp.exp(log_pi) * log_pi)\n    return policy_loss + value_loss - 0.01 * entropy\n\n\nvmapped_permutation = jax.vmap(jax.random.permutation, in_axes=(0, None), out_axes=0)\n\n\n@eqx.filter_jit\ndef update_network(\n    batch: Batch,\n    network: SoftmaxPPONet,\n    optax_update: optax.TransformUpdateFn,\n    opt_state: optax.OptState,\n    prng_key: jax.Array,\n    minibatch_size: int,\n    n_epochs: int,\n    ppo_clip_eps: float,\n) -&gt; tuple[optax.OptState, SoftmaxPPONet]:\n    # Prepare update function\n    dynamic_net, static_net = eqx.partition(network, eqx.is_array)\n\n    def update_once(\n        carried: tuple[optax.OptState, SoftmaxPPONet],\n        batch: Batch,\n    ) -&gt; tuple[tuple[optax.OptState, SoftmaxPPONet], None]:\n        opt_state, dynamic_net = carried\n        network = eqx.combine(dynamic_net, static_net)\n        grad = eqx.filter_grad(loss_function)(network, batch, ppo_clip_eps)\n        updates, new_opt_state = optax_update(grad, opt_state)\n        dynamic_net = optax.apply_updates(dynamic_net, updates)\n        return (new_opt_state, dynamic_net), None\n\n    # Prepare minibatches\n    batch_size = batch.observations.shape[0]\n    permutations = vmapped_permutation(jax.random.split(prng_key, n_epochs), batch_size)\n    minibatches = jax.tree_map(\n        # Here, x's shape is [batch_size, ...]\n        lambda x: x[permutations].reshape(-1, minibatch_size, *x.shape[1:]),\n        batch,\n    )\n    # Update network n_epochs x n_minibatches times\n    (opt_state, updated_dynet), _ = jax.lax.scan(\n        update_once,\n        (opt_state, dynamic_net),\n        minibatches,\n    )\n    return opt_state, eqx.combine(updated_dynet, static_net)\n\n\nSo now that we have all the components, let’s start learning. First, let’s try a simple maze with no walls. I copied and pasted junmanji’s default maze generator and rewrote it. In this environment, the reward is given only at the goal, so the average return per episode is simply calculated by \\(\\frac{\\sum R}{\\mathrm{Num.~episodes}}\\), which is used as a progress indicator. I selected each hyperparameter from my experience.\n\n\nCode\nfrom jumanji.environments.routing.maze.generator import Generator\nfrom jumanji.environments.routing.maze.types import Position, State\n\n\nclass TestGenerator(Generator):\n    def __init__(self) -&gt; None:\n        super().__init__(num_rows=10, num_cols=10)\n\n    def __call__(self, key: chex.PRNGKey) -&gt; State:\n        walls = jnp.zeros((10, 10), dtype=bool)\n        agent_position = Position(row=0, col=0)\n        target_position = Position(row=9, col=9)\n\n        # Build the state.\n        return State(\n            agent_position=agent_position,\n            target_position=target_position,\n            walls=walls,\n            action_mask=None,\n            key=key,\n            step_count=jnp.array(0, jnp.int32),\n        )\n\n\n\n\nCode\ndef run_training(\n    key: jax.Array,\n    adam_lr: float = 3e-4,\n    adam_eps: float = 1e-7,\n    gamma: float = 0.99,\n    gae_lambda: float = 0.95,\n    n_optim_epochs: int = 10,\n    minibatch_size: int = 1024,\n    n_agents: int = 16,\n    n_rollout_steps: int = 512,\n    n_total_steps: int = 16 * 512 * 100,\n    ppo_clip_eps: float = 0.2,\n    **env_kwargs,\n) -&gt; SoftmaxPPONet:\n    key, net_key, reset_key = jax.random.split(key, 3)\n    pponet = SoftmaxPPONet(net_key)\n    env = AutoResetWrapper(jumanji.make(\"Maze-v0\", **env_kwargs))\n    adam_init, adam_update = optax.adam(adam_lr, eps=adam_eps)\n    opt_state = adam_init(eqx.filter(pponet, eqx.is_array))\n    env_state, timestep = jax.vmap(env.reset)(jax.random.split(reset_key, 16))\n    obs = timestep.observation\n\n    n_loop = n_total_steps // (n_agents * n_rollout_steps)\n    return_reporting_interval = 1 if n_loop &lt; 10 else n_loop // 10\n    n_episodes, reward_sum = 0.0, 0.0\n    for i in range(n_loop):\n        key, rollout_key, update_key = jax.random.split(key, 3)\n        env_state, rollout, obs, next_value = exec_rollout(\n            env_state,\n            obs,\n            env,\n            pponet,\n            rollout_key,\n            n_rollout_steps,\n        )\n        batch = make_batch(rollout, next_value, gamma, gae_lambda)\n        opt_state, pponet = update_network(\n            batch,\n            pponet,\n            adam_update,\n            opt_state,\n            update_key,\n            minibatch_size,\n            n_optim_epochs,\n            ppo_clip_eps,\n        )\n        n_episodes += jnp.sum(rollout.terminations).item()\n        reward_sum += jnp.sum(rollout.rewards).item()\n        if i &gt; 0 and (i % return_reporting_interval == 0):\n            print(f\"Mean episodic return: {reward_sum / n_episodes}\")\n            n_episodes = 0.0\n            reward_sum = 0.0\n    return pponet\n\n\n\n\nCode\nimport datetime\n\nstarted = datetime.datetime.now()\nkey, training_key = jax.random.split(key)\ntrained_net = run_training(training_key, n_total_steps=16 * 512 * 10, generator=TestGenerator())\nelapsed = datetime.datetime.now() - started\nprint(f\"Elapsed time: {elapsed.total_seconds():.2}s\")\n\n\nMean episodic return: 0.40782122905027934\nMean episodic return: 0.967741935483871\nMean episodic return: 1.0\nMean episodic return: 1.0\nMean episodic return: 1.0\nMean episodic return: 1.0\nMean episodic return: 1.0\nMean episodic return: 1.0\nMean episodic return: 1.0\nElapsed time: 3.2s\n\n\nAbout 80,000 steps were learned in a little over 3 seconds. That’s fast. Let’s see what the trained agent looks like.\n\n\nCode\n@eqx.filter_jit\ndef visualization_rollout(\n    key: jax.random.PRNGKey,\n    pponet: SoftmaxPPONet,\n    env: jumanji.Environment,\n    n_steps: int,\n) -&gt; list[State]:\n    def step_rollout(\n        carried: tuple[State, Observation],\n        key: jax.Array,\n    ) -&gt; tuple[tuple[State, jax.Array], State]:\n        state_t, obs_t = carried\n        obs_image = obs_to_image(obs_t)\n        net_out = pponet(obs_image)\n        action, _ = sample_action(key, net_out.policy_logits, obs_t.action_mask)\n        state_t1, timestep = env.step(state_t, action)\n        return (state_t1, timestep.observation), state_t1\n\n    initial_state, timestep = env.reset(key)\n    _, states = jax.lax.scan(\n        step_rollout,\n        (initial_state, timestep.observation),\n        jax.random.split(key, n_steps),\n    )\n    leaves, treedef = jax.tree_util.tree_flatten(states)\n    return [initial_state] + [treedef.unflatten(leaf) for leaf in zip(*leaves)]\n\n\n\n\nCode\nenv = AutoResetWrapper(jumanji.make(\"Maze-v0\", generator=TestGenerator()))\nkey, eval_key = jax.random.split(key)\nstates = visualization_rollout(eval_key, trained_net, env, 40)\nanim = env.animate(states)\nHTML(anim.to_html5_video().replace('=\"1000\"', '=\"640\"'))\n\n\n\n\n\n\n\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nAs expected, this one looks easy enough. Next time, we would like to make it a little more difficult. The default maze, which is generated completely at random, is very slow, so let’s make our own maze. The maze samples the start and the goal from 3 different locations with the same probability, and solves a total of 9 combinations. Let’s try to train the maze with 800,000 steps, which is 10 times as many as the default maze.\n\n\nCode\nclass MedDifficultyGenerator(Generator):\n    WALLS = [\n        [0, 0, 1, 0, 0, 1, 0, 1, 1, 0],\n        [0, 0, 1, 0, 0, 1, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 1, 1, 0, 1, 1],\n        [1, 1, 1, 1, 0, 0, 0, 0, 1, 0],\n        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],\n        [0, 0, 1, 0, 1, 1, 0, 0, 0, 0],\n        [1, 0, 1, 0, 1, 1, 0, 1, 1, 1],\n        [0, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n        [1, 1, 1, 0, 0, 1, 0, 1, 1, 0],\n        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n    ]\n    def __init__(self) -&gt; None:\n        super().__init__(num_rows=10, num_cols=10)\n\n    def __call__(self, key: chex.PRNGKey) -&gt; State:\n        key, config_key = jax.random.split(key)\n        walls = jnp.array(self.WALLS).astype(bool)\n        agent_cfg, target_cfg = jax.random.randint(config_key, (2,), 0, 2)\n        agent_position = jax.lax.switch(\n            agent_cfg,\n            [\n                lambda: Position(row=0, col=0),\n                lambda: Position(row=9, col=0),\n                lambda: Position(row=0, col=9),\n            ]\n        )\n        target_position = jax.lax.switch(\n            target_cfg,\n            [\n                lambda: Position(row=3, col=9),\n                lambda: Position(row=7, col=8),\n                lambda: Position(row=7, col=0),\n            ]\n        )\n        # Build the state.\n        return State(\n            agent_position=agent_position,\n            target_position=target_position,\n            walls=walls,\n            action_mask=None,\n            key=key,\n            step_count=jnp.array(0, jnp.int32),\n        )\n\n\n\n\nCode\nimport datetime\n\nstarted = datetime.datetime.now()\nkey, training_key = jax.random.split(key)\ntrained_net = run_training(\n    training_key,\n    n_total_steps=16 * 512 * 100,\n    generator=MedDifficultyGenerator(),\n)\nelapsed = datetime.datetime.now() - started\nprint(f\"Elapsed time: {elapsed.total_seconds():.2}s\")\n\n\nMean episodic return: 0.2812202097235462\nMean episodic return: 0.4077407740774077\nMean episodic return: 0.5992010652463382\nMean episodic return: 0.7285136501516684\nMean episodic return: 0.75\nMean episodic return: 0.8620564808110065\nMean episodic return: 0.9868290258449304\nMean episodic return: 0.9977788746298124\nMean episodic return: 0.9970540974825924\nElapsed time: 1.2e+01s\n\n\n\n\nCode\nenv = AutoResetWrapper(jumanji.make(\"Maze-v0\", generator=MedDifficultyGenerator()))\nkey, eval_key = jax.random.split(key)\nstates = visualization_rollout(eval_key, trained_net, env, 100)\nanim = env.animate(states)\nHTML(anim.to_html5_video().replace('=\"1000\"', '=\"640\"'))\n\n\n\n\n\n\n\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nThere is some hesitation, but it looks like we are getting to the goal."
  },
  {
    "objectID": "posts/understanding-attention-en.html",
    "href": "posts/understanding-attention-en.html",
    "title": "Understanding what self-attention is doing",
    "section": "",
    "text": "ChatGPT is getting a lot of buzz these days. I don’t use it much because I don’t like to worry about prompts, but my friend uses it to write papers, and my mom uses it to just talk, which makes me feel a little sorry for being an unfriendly son… A neural network called Transformer is the success of language generative models like ChatGPT. A layer called Multihead Attention is repeatedly applied to a sequence of input tokens to form a complex model. In this blog we will focus on a simplified version of Multihead Attention, Singlehead Self-Attention, study what it does, and try to write some code to run it."
  },
  {
    "objectID": "posts/understanding-attention-en.html#sequence-of-tokens",
    "href": "posts/understanding-attention-en.html#sequence-of-tokens",
    "title": "Understanding what self-attention is doing",
    "section": "Sequence of tokens",
    "text": "Sequence of tokens\nA token sequence is literally a sequence consisting of tokens. A token is an element of a finite set. For practical use, this includes substrings obtained by byte pair encoding, but you don’t need to worry about that for now. Let \\(V\\) be a set of tokens, and number them \\([Nv] := {1, ... , Nv}\\). Write \\(x = x[1: l]\\) for the token sequence. Also, let \\(L\\) be the maximum length of the token sequence."
  },
  {
    "objectID": "posts/understanding-attention-en.html#from-a-token-to-a-vector",
    "href": "posts/understanding-attention-en.html#from-a-token-to-a-vector",
    "title": "Understanding what self-attention is doing",
    "section": "From a token to a vector",
    "text": "From a token to a vector\nUsing a \\(d_e \\times Nv\\)-dimensional matrix \\(W_e\\), the token embedding is obtained from the \\(v\\)th token by \\(e = W_e[:, v]\\). This will be a \\(d_e\\)-dimensional vector. Note that we write \\(W[i, :]\\) for the \\(i\\)-th row vector and \\(W[:, j]\\) for the \\(j\\)-th column vector in the numpy style. This matrix \\(W_e\\) seems to be learned by gradient descent."
  },
  {
    "objectID": "posts/understanding-attention-en.html#position-is-also-embedded",
    "href": "posts/understanding-attention-en.html#position-is-also-embedded",
    "title": "Understanding what self-attention is doing",
    "section": "Position is also embedded",
    "text": "Position is also embedded\nUsing a \\(d_p \\times L\\)-dimensional matrix \\(W_p\\), a positional embedding is obtained by \\(p = W_p[:, l]\\) from the information that there is a token at \\(l\\)th place in the token sequence. This is also a vector with length \\(d_e\\). To be honest, I am not sure what it means, but we can add this to the token embedding described earlier to obtain the embedding for the \\(t\\)th token \\(x[t]\\) in the token sequence \\(x\\) by \\(e = W_e[:, x[t]] + W_p[:, t]\\). Is it safe to add this? I don’t know. The position embedding may be learned, but in the paper Attention Is All You Need, where Transformer was first proposed, it is constructed as follows.\n\\[\n\\begin{align*}\nW_p[2i - 1, t] &= \\sin (\\frac{t}{L^{2i / d_e}}) \\\\\nW_p[2i, t] &= \\cos (\\frac{t}{L^{2i / d_e}}) \\\\\n&~~~~~(0 &lt; 2i \\leq d_e)\n\\end{align*}\n\\] Let’s visualize it with\\(L=50, d_e = 5\\).\n\n\nCode\nimport matplotlib\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nmatplotlib.font_manager.fontManager.addfont(\"NotoEmoji-Medium.ttf\")\n\nL = 50\nd_e = 5\nx = np.arange(L)\nfor i in range(1, 1 + d_e):\n    if i % 2 == 0:\n        w_p = np.sin(x / L ** (i / d_e))\n    else:\n        w_p = np.cos(x / L ** ((i - 1) / d_e))\n    _ = plt.plot(x, w_p, label=f\"i={i}\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nSo this embedding seems to embed words at different frequencies for each component. I suspect that this allows us to consider the position in a short context at the same time."
  },
  {
    "objectID": "posts/understanding-attention-en.html#training-a-markov-model",
    "href": "posts/understanding-attention-en.html#training-a-markov-model",
    "title": "Understanding what self-attention is doing",
    "section": "Training a Markov model",
    "text": "Training a Markov model\nLet’s start with a simple model to generate the weather. Let’s assume that the next day’s weather is stochastically determined based on the previous day’s weather. Note that 🌧️, ☁️, and ☀️ are multibyte characters, and implement the following.\n\n\nCode\nimport dataclasses\n\n_GEN = np.random.Generator(np.random.PCG64(20230508))\n_MARKOV = {\n    \"\": [0.3, 0.4, 0.3],\n    \"🌧️\": [0.6, 0.3, 0.1],\n    \"☁️\": [0.3, 0.4, 0.3],\n    \"☀️\": [0.2, 0.3, 0.5],\n}\n\ndef markov(prev: str) -&gt; str:\n    prob = _MARKOV[prev[-2:]]\n    return prev + _GEN.choice(WEATHERS, p=prob)\n\n\ndef generate(f, n: int, init: str = \"\"):\n    value = init\n    for _ in range(n):\n        value = f(value)\n    return value\n\n\n@dataclasses.dataclass\nclass Dataset:\n    weathers: list[str]\n    embeddings: jax.Array\n    next_weather_indices: jax.Array\n    \n    def __len__(self) -&gt; int:\n        return len(self.weathers)\n\n\ndef make_dataset(f, seq_len, size) -&gt; Dataset:\n    w_list, e_list, nw_list = [], [], []\n    for _ in range(size):\n        weathers = generate(f, seq_len + 1)\n        e = jnp.array(get_embedding(weathers[:-2]))\n        w_list.append(weathers)\n        e_list.append(e)\n        nw_list.append(WEATHERS.index(weathers[-2:]))\n    return Dataset(w_list, jnp.stack(e_list), jnp.array(nw_list))\n\n\ngenerated = generate(markov, 10)\ngenerated, get_embedding(generated)\n\n\n('🌧️🌧️🌧️☀️🌧️☁️🌧️🌧️☀️☀️',\n array([[1.  , 1.  , 1.  , 0.  , 1.  , 0.  , 1.  , 1.  , 0.  , 0.  ],\n        [0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  ],\n        [0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 1.  , 1.  ],\n        [0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 ]]))\n\n\nThe generated weathers look like this. Since we now only want to predict the weather for the next day, the output of the model should be a probability distribution over a set {🌧️, ☁️, ☀️}. Since Self-Attention will return a \\(d_\\textrm{out} \\times T\\) matrix for an embedded column of length \\(T\\), we set \\(d_\\textrm{out} = 3\\) and apply the softmax function to Attention’s output \\(\\tilde{V}\\) to obtain \\(P_t = \\textrm{softmax}(\\tilde{V}[:, t])\\). We model each element of \\(P_t\\) as representing the probability that it will be on the next day 🌧️, ☁️, or ☀️. Let this be trained to maximize the sum of log-likelihood \\(\\sum_t \\log P_t(\\textrm{next weather})\\).\n\n\nCode\nfrom typing import Callable\n\nimport optax\n\n\ndef attn_neglogp(model: eqx.Module, seq: jax.Array, next_w: jax.Array) -&gt; jax.Array:\n    batch_size = seq.shape[0]\n    tilde_v = jax.vmap(model)(seq)  # B x OUT x SEQ_LEN\n    logp = jax.nn.log_softmax(tilde_v, axis=1)  # B x OUT x SEQ_LEN\n    logp_masked = logp * jax.nn.one_hot(next_w, num_classes=3).reshape(-1, 3, 1)\n    return -jnp.mean(jnp.sum(logp_masked.reshape(batch_size, -1), axis=-1))\n\ndef train(\n    n_total_epochs: int,\n    minibatch_size: int,\n    model: eqx.Module,\n    ds: Dataset,\n    test_ds: Dataset,\n    key: jax.Array,\n    learning_rate: float = 1e-2,\n    loss_fn: Callable[[eqx.Module, jax.Array, jax.Array], jax.Array] = attn_neglogp,\n) -&gt; tuple[eqx.Module, jax.Array, list[float], list[float]]:\n    n_data = len(ds)\n    optim = optax.adam(learning_rate)\n\n    @eqx.filter_jit\n    def train_1step(\n        model: eqx.Module,\n        seq: jax.Array,\n        next_w: jax.Array,\n        opt_state: optax.OptState,\n    ) -&gt; tuple[jax.Array, eqx.Module, optax.OptState]:\n        loss, grads = eqx.filter_value_and_grad(loss_fn)(model, seq, next_w)\n        updates, opt_state = optim.update(grads, opt_state)\n        model = eqx.apply_updates(model, updates)\n        return loss, model, opt_state\n\n    opt_state = optim.init(model)\n    n_optim_epochs = n_data // minibatch_size\n    loss_list, eval_list = [], []\n    for epoch in range(n_total_epochs // n_optim_epochs):\n        key, perm_key = jax.random.split(key)\n        indices = jax.random.permutation(perm_key, n_data, independent=True)\n        for _ in range(n_optim_epochs):\n            e = ds.embeddings[indices]\n            next_w = ds.next_weather_indices[indices]\n            loss, model, opt_state = train_1step(model, e, next_w, opt_state)\n            loss_list.append(loss.item())\n            test_loss = jax.jit(loss_fn)(\n                model,\n                test_ds.embeddings,\n                test_ds.next_weather_indices,\n            )\n            eval_list.append(test_loss.item())\n    return model, key, loss_list, eval_list\n\n\nLet’s run it. I use \\(6\\) for \\(d_textrm{attn}\\) and \\(10\\) for the sequence length \\(T\\).\n\n\nCode\nD_ATTN = 6\nSEQ_LEN = 10\nkey = jax.random.PRNGKey(1234)\nmodel = MaskedAttention(4, D_ATTN, 3, key)\nds = make_dataset(markov, SEQ_LEN, 1000)\ntest_ds = make_dataset(markov, SEQ_LEN, 1000)\nmodel, key, loss_list, eval_list = train(500, 100, model, ds, test_ds, key, 1e-2)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on Markov model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\n@jax.jit\ndef accuracy(model: eqx.Module, seq: jax.Array, next_w: jax.Array) -&gt; float:\n    tilde_v = jax.vmap(model)(seq)  # B x OUT x SEQ_LEN\n    inferred = jnp.argmax(tilde_v[:, :, 0], axis=1)\n    n_correct = jnp.sum(inferred == next_w)\n    return n_correct / seq.shape[0]\n\nf\"Accuracy: {accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.49800002574920654'\n\n\n\n\n\n\n\n\n\nThe loss is no longer dropping around 100 epochs, so it seemes to have converged. Let’s see what has actually been learned. For now, let’s try generating weather. This is not very meaningful in this case, but I thought it would be good to learn the generative process. It seems that the beam search is often used, but since it’s complex, I use a simpler method this time. Starting from ☁️, we sample the next weather from the categorical distribution, and keep adding to it.\n\n\nCode\ndef generate_from_model(\n    model: eqx.Module,\n    key: jax.Array,\n    seq_len: int,\n    init: str = \"☁️\",\n) -&gt; tuple[str, jax.Array]:\n    @jax.jit\n    def step(\n        model: eqx.Module,\n        seq: jax.Array,\n        key: jax.Array,\n    ) -&gt; tuple[jax.Array, jax.Array]:\n        sample_key, key = jax.random.split(key)\n        tilde_v = model(seq)  # 3 x len(seq)\n        sampled = jax.random.categorical(key=sample_key, logits=tilde_v[:, 0])\n        return sampled, key\n\n    generated = init\n    for _ in range(seq_len):\n        next_w, key = step(model, get_embedding(generated), key)\n        generated += WEATHERS[next_w.item()]\n    return generated, key\n\n\ngenerated, key = generate_from_model(model, key, 20)\ngenerated\n\n\n'☁️🌧️🌧️☀️🌧️☁️🌧️☀️☀️☀️☁️☁️☀️☀️☀️☀️☀️☁️☁️☀️☁️'\n\n\nThis is what the predicted weathers look like. Of course, this doesn’t tell us anything. Next, let’s visualize the contents of Self-Attention for some data in the test data.\n\n\nCode\n@jax.jit\ndef get_attn(model: eqx.Module, seq: jax.Array) -&gt; jax.Array:\n    q = model.w_q @ seq + model.b_q\n    k = model.w_k @ seq + model.b_k\n    score = causal_mask(q.T @ k) / model.sqrt_d_attn\n    return jax.nn.softmax(score, axis=-1)\n\n\ndef visualize_attn(ax, model: eqx.Module, ds: Dataset, index: int = 0) -&gt; None:\n    attn = np.array(get_attn(model, ds.embeddings[index]))\n    im = ax.imshow(attn)\n    ax.set_xticks(\n        np.arange(10),\n        labels=[ds.weathers[index][i * 2] for i in range(10)],\n        fontname=\"Noto Emoji\",\n    )\n    ax.set_yticks(\n        np.arange(10),\n        labels=[ds.weathers[index][i * 2] for i in range(10)],\n        fontname=\"Noto Emoji\",\n    )\n    for i in [np.argmin(attn), np.argmax(attn)]:\n        # Show min and max values\n        im.axes.text(i % 10, i // 10, f\"{attn.flatten()[i]:.1f}\", color=\"gray\")\n\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\nvisualize_attn(ax1, model, test_ds, 1)\nvisualize_attn(ax2, model, test_ds, 2)\n\n\n\n\n\n\n\n\n\nNote that I could not use color emojis in matplotlib. We observe that: 1. ‘last day -&gt; last day’ has the largest attention 2. ‘other days -&gt; last day’ also has larger Attention 3. the other factors are almost irrelevant\nThe first is natural since we trained a weather sequence generated from a Markov model. The attentions from other days to last days are actually unnecessary, but also were taken."
  },
  {
    "objectID": "posts/understanding-attention-en.html#when-future-events-depend-on-multiple-independently-occurring-past-events",
    "href": "posts/understanding-attention-en.html#when-future-events-depend-on-multiple-independently-occurring-past-events",
    "title": "Understanding what self-attention is doing",
    "section": "When future events depend on multiple independently occurring past events",
    "text": "When future events depend on multiple independently occurring past events\nNext, let’s train some more complex data. This time, we will generate 11 days of weather in the following way: 1. Generate weather for days 1, 4, and 8 independently 2. Generate the weather for days 2 and 3 using a Markov chain with the weather for day 1 as the initial condition; generate the weather for days 5, 6, 7, 9, and 10 in the same way, based on the weather for days 4 and 8. 3. Generate the weather for day 11 stochastically based on the weather for days 1, 4, and 8.\nLet’s see if the self-attention layer can learn this.\n\n\nCode\ndef _make_table() -&gt; dict[str, list[float]]:\n    candidates = []\n    for i in range(1, 9):\n        for j in range(1, 9):\n            for k in range(1, 9):\n                if i + j + k == 10:\n                    candidates.append((i, j, k))\n    table = {}\n    for i in WEATHERS:\n        for j in WEATHERS:\n            for k in WEATHERS:\n                table[i + j + k] = [p / 10 for p in _GEN.choice(candidates)]\n    return table\n\n_ONE_FOUR_8_TABLE = _make_table()\n\ndef one_four_8(prev: str) -&gt; str:\n    length = len(prev) // 2\n    if length == 10:\n        p = _ONE_FOUR_8_TABLE[prev[0: 2] + prev[6: 8] + prev[14: 16]]\n        return prev + _GEN.choice(WEATHERS, p=p)\n    elif length == 4 or length == 8:\n        return prev + _GEN.choice(WEATHERS, p=_MARKOV[\"\"])\n    else:\n        return markov(prev)\n    \ngenerate(one_four_8, 11)\n\n\n'☀️☁️☁️☀️🌧️🌧️☁️☀️🌧️🌧️🌧️'\n\n\nOK, let’s do it.\n\n\nCode\nmodel = MaskedAttention(4, D_ATTN, 3, key)\nds = make_dataset(one_four_8, SEQ_LEN, 5000)\ntest_ds = make_dataset(one_four_8, SEQ_LEN, 1000)\nmodel, key, loss_list, eval_list = train(500, 100, model, ds, test_ds, key, 1e-2)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on 1-4-8 model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\nvisualize_attn(ax1, model, test_ds, 1)\nvisualize_attn(ax2, model, test_ds, 2)\n\nf\"Accuracy: {accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.4020000100135803'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt converged, but the accuracy is poor and the attention is also not very effective. Attentions are given to days 1, 4, and 8, but as in the previous experiment, the last day’s attention is larger."
  },
  {
    "objectID": "posts/understanding-attention-en.html#do-we-need-attention",
    "href": "posts/understanding-attention-en.html#do-we-need-attention",
    "title": "Understanding what self-attention is doing",
    "section": "Do we need attention?",
    "text": "Do we need attention?\nAs smart readers may have noticed, we don’t need self-attention to represent the two weather sequences we have learned so far. This is because the internal correlation of the input weather sequence has no bearing on the task at all, since the first one determines the weather of the previous day (day 10) and the next one determines the weather of days 1, 4, 8 to 11. So, let’s train with a linear model + softmax (the so-called multinomial logistic regression).\n\n\nCode\nclass LinearModel(eqx.Module):\n    w: jax.Array\n    b: jax.Array\n\n    def __init__(self, d_in: int, d_out: int, key: jax.Array) -&gt; None:\n        w_key, b_key = jax.random.split(key)\n        self.w = jax.random.normal(w_key, (d_out, d_in))\n        self.b = jax.random.normal(b_key, (d_out,))\n\n    def __call__(self, seq: jax.Array) -&gt; jax.Array:\n        return self.w @ seq.flatten() + self.b\n\n\ndef linear_neglogp(model: eqx.Module, seq: jax.Array, next_w: jax.Array) -&gt; jax.Array:\n    logp = jax.nn.log_softmax(jax.vmap(model)(seq), axis=1)  # B x OUT\n    logp_masked = logp * jax.nn.one_hot(next_w, num_classes=3)\n    return -jnp.mean(jnp.sum(logp_masked, axis=1))\n\n\nmodel = LinearModel(4 * SEQ_LEN, 3, key)\nmodel, key, loss_list, eval_list = train(\n    500, 100, model, ds, test_ds, key, 1e-2, linear_neglogp\n)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on 1-4-8 model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\n\n@jax.jit\ndef linear_accuracy(model: eqx.Module, seq: jax.Array, next_w: jax.Array) -&gt; float:\n    tilde_v = jax.vmap(model)(seq)  # B x OUT\n    inferred = jnp.argmax(tilde_v, axis=1)\n    n_correct = jnp.sum(inferred == next_w)\n    return n_correct / seq.shape[0]\n\n\nf\"Accuracy: {linear_accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.44200003147125244'\n\n\n\n\n\n\n\n\n\nThis looks better. So, when is the self-attention useful?\n\n(Compared to MLP, etc.) when you don’t want to make the number of parameters depend on the length \\(L\\) of the token sequence\n\nNote that in self-attention the number of parameters is \\((d_\\textrm{in} + 1)(2d_\\textrm{attn} + d_\\textrm{out})\\), while in the linear model it is \\((d_\\textrm{in}L + 1)d_\\textrm{out}\\). In the linear model, the number of parameters increases linearly with the length of the token sequence. Note, however, that self-attention requires \\(O(L^2)\\) memory usage for \\(q^\\top k\\), although Self-attention Does Not Need \\(O(n^2)\\) Memory shows an efficient \\(O(\\sqrt{L})\\) implementation. Still, it may be consume more memory thatn simple RNN or CNN.\n\n(Compared to RNN, CNN, etc.) when there is a long-term dependency in the token series\n\nCompared to CNN and RNN, the advantage of self-attention is that \\(q^\\top k\\) can represent arbitrary dependencies between tokens in one layer. However, since \\(q^\\top k[i, j]\\) is obtained only by linear operations on the two embeddings \\(e[i], e[j]\\), if the two embeddings are dependent via some nonlinear function, the relationship cannot be represented by a single self-attention layer.\n\n(Compared to RNN) when you want to do fast and parallel batch training\n\nThe operation of computing self-attention, namely the computation of \\(\\textrm{softmax}(q^\\top k)\\) can be parallelized per query. This is useful when you want to get a parallelized implementation that works fast on single or many GPUs.\nSo, although it has the advantage of not depending on \\(L\\) for the number of parameters compared to one linear layer, I am not sure if the self-attention can actually be more expressive or efficient. Let another blog post do some more theoretical stuff, I will try some more."
  },
  {
    "objectID": "posts/understanding-attention-en.html#when-there-are-hidden-variables",
    "href": "posts/understanding-attention-en.html#when-there-are-hidden-variables",
    "title": "Understanding what self-attention is doing",
    "section": "When there are hidden variables",
    "text": "When there are hidden variables\nGenerate a weather sequence in the following way. Look at the weather for the past \\(n\\) days, and if 🌧️ has appeared \\(k\\) times, let \\(\\frac{n - k}{2n}\\) be the probability that the weather for the next day will be 🌧️. Assign probabilities for ☁️ and ☀️ in the same way. Generate a long weather sequence in this way and create a dataset by gathering randomly sampled subsequences.\n\n\nCode\nfrom functools import partial\n\ndef ndays_model(prev: str, n: int = 10) -&gt; str:\n    counts = np.zeros(3)\n    prev_n = prev[-2 * n: ]\n    for i in range(n):\n        prev_w_i = prev_n[i * 2: i * 2 + 2]\n        counts[WEATHERS.index(prev_w_i)] += 1\n    prob = (n - counts) / (n * 2)\n    return prev + _GEN.choice(WEATHERS, p=prob)\n\ngenerate(ndays_model, 100, generate(markov, 10))                \n\n\n'☁️🌧️🌧️🌧️🌧️🌧️🌧️🌧️☁️☀️☀️☁️☁️🌧️☁️🌧️☀️🌧️☀️☁️☀️☁️☀️☁️☀️☁️☀️🌧️☀️☁️🌧️☀️🌧️☀️☀️🌧️☀️🌧️☁️☁️🌧️☁️☁️☁️☀️🌧️☀️☀️☀️☀️☁️☁️🌧️🌧️🌧️🌧️🌧️☁️☀️☁️☀️☀️☀️☀️🌧️🌧️🌧️🌧️🌧️☀️☁️🌧️☁️☀️☀️☀️☁️☁️☀️🌧️☁️🌧️☁️☀️☀️☁️☁️☁️☁️☁️🌧️☁️🌧️☀️🌧️🌧️☀️☀️☁️🌧️☀️☀️🌧️☀️☁️☀️☀️☁️☁️☀️'\n\n\nThe generated weather sequence looks like this. First, let’s traing the linear model on a 10-day model. In this case there is no hidden variable.\n\n\nCode\ndef make_ndays_dataset(seq_len, size, n: int = 10) -&gt; Dataset:\n    weathers = generate(partial(ndays_model, n=n), seq_len * size * 2, generate(markov, n * 2))\n    w_list, e_list, nw_list = [], [], []\n    for _ in range(size):\n        start = _GEN.integers(0, seq_len * size * 2 - 11)\n        w = weathers[start * 2 : start * 2 + (seq_len + 1) * 2]\n        e = jnp.array(get_embedding(w[:-2]))\n        w_list.append(w)\n        e_list.append(e)\n        nw_list.append(WEATHERS.index(w[-2:]))\n    return Dataset(w_list, jnp.stack(e_list), jnp.array(nw_list))\n\n\nds = make_ndays_dataset(SEQ_LEN, 5000, n=10)\ntest_ds = make_ndays_dataset(SEQ_LEN, 1000, n=10)\n\nmodel = LinearModel(4 * SEQ_LEN, 3, key)\nmodel, key, loss_list, eval_list = train(\n    500, 100, model, ds, test_ds, key, 1e-2, linear_neglogp\n)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on 10days model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nf\"Accuracy: {linear_accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.41600000858306885'\n\n\n\n\n\n\n\n\n\nTrain the self-attention layer next.\n\n\nCode\nmodel = MaskedAttention(4, D_ATTN, 3, key)\nmodel, key, loss_list, eval_list = train(500, 100, model, ds, test_ds, key, 1e-2)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on 10days model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\nvisualize_attn(ax1, model, test_ds, 1)\nvisualize_attn(ax2, model, test_ds, 2)\n\nf\"Accuracy: {accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.3630000054836273'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe self-attention is now worse than the linear model. Next, let’s turn the 10-day model into a 15-day model with hidden variables. First, we train the linear layer.\n\n\nCode\nds = make_ndays_dataset(SEQ_LEN, 5000, n=15)\ntest_ds = make_ndays_dataset(SEQ_LEN, 1000, n=15)\n\nmodel = LinearModel(4 * SEQ_LEN, 3, key)\nmodel, key, loss_list, eval_list = train(\n    500, 100, model, ds, test_ds, key, 1e-2, linear_neglogp\n)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on 15days model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nf\"Accuracy: {linear_accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.34200000762939453'\n\n\n\n\n\n\n\n\n\nThen train the self-attention layer.\n\n\nCode\nmodel = MaskedAttention(4, D_ATTN, 3, key)\nmodel, key, loss_list, eval_list = train(500, 100, model, ds, test_ds, key, 1e-2)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on 15days model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\nvisualize_attn(ax1, model, test_ds, 1)\nvisualize_attn(ax2, model, test_ds, 2)\n\nf\"Accuracy: {accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.35600000619888306'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttention is a bit better, but there is no meaningful difference in accuracy."
  },
  {
    "objectID": "posts/understanding-attention-en.html#what-about-non-linear",
    "href": "posts/understanding-attention-en.html#what-about-non-linear",
    "title": "Understanding what self-attention is doing",
    "section": "What about non-linear?",
    "text": "What about non-linear?\nIf the linear model performs better even with hidden variables, it probably means that the task is still linearly solvable. So let’s consider more difficult nonlinear data. Let \\(y\\) be a vector created by assigning 0, 1, and 2 to 🌧️, ☁️, and ☀️ in the 10-day weather sequence, respectively. Also, let \\(\\beta = (0, 1, 2, 3, 2, 1, 0, 1, 2, 3)^\\top\\). Let \\((y(2 - y)\\cdot \\beta)\\mod 3\\) be the weather for the next day. To make the data a bit stohcastic, let’s assign other weathers 2% probability.\n\n\nCode\n_BETA = np.tile([0, 1, 2, 3, 2, 1], (10,))\n\ndef dotmod_model(prev: str, n: int =10) -&gt; str:\n    y = np.zeros(n, dtype=int)\n    prev_n = prev[-2 * n:]\n    for i in range(n):\n        prev_w_i = prev_n[i * 2: i * 2 + 2]\n        y[i] = WEATHERS.index(prev_w_i) + 1\n    prob = [0.02, 0.02, 0.02]\n    prob[np.dot(y * (2 - y), _BETA[: n]) % 3] = 0.96\n    return prev + _GEN.choice(WEATHERS, p=prob)\n\ndef make_dotmod_dataset(seq_len, size, n: int = 10) -&gt; Dataset:\n    weathers = generate(partial(dotmod_model, n=n), seq_len * size * 2, generate(markov, n * 2))\n    w_list, e_list, nw_list = [], [], []\n    for _ in range(size):\n        start = _GEN.integers(0, seq_len * size * 2 - 11)\n        w = weathers[start * 2 : start * 2 + (seq_len + 1) * 2]\n        e = jnp.array(get_embedding(w[:-2]))\n        w_list.append(w)\n        e_list.append(e)\n        nw_list.append(WEATHERS.index(w[-2:]))\n    return Dataset(w_list, jnp.stack(e_list), jnp.array(nw_list))\n\ngenerate(dotmod_model, 100, generate(markov, 10))\n\n\n'☁️☁️☀️☀️☀️☀️☀️☀️☁️☁️☀️🌧️🌧️☀️🌧️☁️🌧️☁️☁️☁️☀️☀️☁️☁️☀️☁️🌧️🌧️☀️🌧️☁️🌧️☁️☁️🌧️☀️☁️☀️☁️🌧️🌧️☀️☀️☀️☁️🌧️☀️☁️☁️☁️☁️☀️🌧️☀️🌧️☁️☀️☀️☀️☁️☁️☁️☀️☁️☁️🌧️🌧️☀️🌧️☁️🌧️☁️☁️☁️☀️☀️☁️☁️☀️☁️🌧️🌧️☀️🌧️☁️🌧️☁️☁️☁️☀️☀️☁️☁️☀️☁️🌧️🌧️☀️☁️☁️☁️🌧️☀️☁️☁️☁️☁️☀️🌧️☀️'\n\n\nWe were able to generate a weather sequence that at a quick glance does not seem to be legal. Let’s try to train it. Let’s start with a linear model.\n\n\nCode\nds = make_dotmod_dataset(SEQ_LEN, 5000)\ntest_ds = make_dotmod_dataset(SEQ_LEN, 1000)\n\nmodel = LinearModel(4 * SEQ_LEN, 3, key)\nmodel, key, loss_list, eval_list = train(\n    500, 100, model, ds, test_ds, key, 1e-2, linear_neglogp\n)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on Dotmod model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nf\"Accuracy: {linear_accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.7440000176429749'\n\n\n\n\n\n\n\n\n\nSuprsingly, the accuracy is quite high. Then let’s train the self-attention layer.\n\n\nCode\nmodel = MaskedAttention(4, D_ATTN, 3, key)\nmodel, key, loss_list, eval_list = train(500, 100, model, ds, test_ds, key, 1e-2)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on Dotmod model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\nvisualize_attn(ax1, model, test_ds, 1)\nvisualize_attn(ax2, model, test_ds, 2)\n\nf\"Accuracy: {accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.4350000321865082'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgain, the self-attention was not better. So maybe we can at least say that the self-attention itself is not very good at approximating nonlinear function like modulo."
  },
  {
    "objectID": "posts/fast_2d_physics_in_jax_en.html",
    "href": "posts/fast_2d_physics_in_jax_en.html",
    "title": "Implement fast 2D physics simulation with Jax",
    "section": "",
    "text": "※ This article is translated from Japanese version with some improvements on code.\nMaking simulation fast utilizing the power of GPU/TPU is a hot topic in the reinforcement learning community. LLM and RLHF things are getting ‘the thing’ in the community, though. Anyway, it is quite fun to speed up simulation. NVIDIA IsaacSym is quite solid, but if you want to speed up the entire RL pipeline, Jax-based libraries such as brax is useful.\nYou don’t know Jax? It’s just a NumPy-like library, but it can be compiled into a fast vectorized machine code that works really fast on GPU and TPU. It’s even fast on CPUs. I have written a blog post (Japanese only) before, but the current version of brax is farther improved. It allows you to choose a more accurate method, and it will be tightly integrated with MuJoCo XLA.\nHowever, recently, I wanted a simple 2-D physics simulation, and tried brax. My impression was that not only brax is a complete overkill for my usecase, but also its API is quite difficult to set up a new environment. Loading MJCF is easy, but that was the only easy way to set up the stuff. I wanted to make my environment with some lines of Python code like game-physics engines such as pymunk. So, anyway, I tried to make my own."
  },
  {
    "objectID": "posts/fast_2d_physics_in_jax_en.html#check-contacts",
    "href": "posts/fast_2d_physics_in_jax_en.html#check-contacts",
    "title": "Implement fast 2D physics simulation with Jax",
    "section": "Check contacts",
    "text": "Check contacts\nNext, let’s implement collision detection. It’s pretty easy because we have only circles now. We just need to store some information such as location and overwrap to resolve collisions later.\nThe other thing I need to care is vectorization. Say, this is a naive Python code to check contacts.\nfor i in range(N):\n    for j in range(i + 1, N):\n        check_contact(objects[i], objects[j])\nIt has \\(O(N^2)\\) loop! However, you might be smart enough to point out that each operation in this loop has no dependency, or the order of computation doesn’t matter. Here, jax.vmap is for you to vectorize those operations. Then, let’s unrole this loop by hand, make pairs, and then call check_contact. Like,\nobject_1, object_2 = make_pair(objects)\njax.vmap(check_contact)(object1, object2)\nHow to make pairs? There are various ways, what I found the simplest is triu_indices, which is a function that produces the indices of upper triangular matrix with some offsets.\n\n\nCode\nfrom typing import Any, Callable\n\nAxis = Sequence[int] | int\n\n\ndef normalize(x: jax.Array, axis: Axis | None = None) -&gt; tuple[jax.Array, jax.Array]:\n    norm = jnp.linalg.norm(x, axis=axis)\n    n = x / jnp.clip(norm, a_min=1e-6)\n    return n, norm\n\n\n@chex.dataclass\nclass Contact(PyTreeOps):\n    pos: jax.Array\n    normal: jax.Array\n    penetration: jax.Array\n    elasticity: jax.Array\n    friction: jax.Array\n\n    def contact_dim(self) -&gt; int:\n        return self.pos.shape[1]\n\n@jax.vmap\ndef _circle_to_circle_impl(\n    a: Circle,\n    b: Circle,\n    a_pos: Position,\n    b_pos: Position,\n    isactive: jax.Array,\n) -&gt; Contact:\n    a2b_normal, dist = normalize(b_pos.xy - a_pos.xy)\n    penetration = a.radius + b.radius - dist\n    a_contact = a_pos.xy + a2b_normal * a.radius\n    b_contact = b_pos.xy - a2b_normal * b.radius\n    pos = (a_contact + b_contact) * 0.5\n    # Filter penetration\n    penetration = jnp.where(isactive, penetration, jnp.ones_like(penetration) * -1)\n    return Contact(\n        pos=pos,\n        normal=a2b_normal,\n        penetration=penetration,\n        elasticity=(a.elasticity + b.elasticity) * 0.5,\n        friction=(a.friction + b.friction) * 0.5,\n    )\n\n\ndef check_circle_to_circle(\n    space: Space,\n    position: Position,\n    is_active: jax.Array,\n) -&gt; tuple[Contact, Circle, Circle]:\n    idx1, idx2 = jnp.triu_indices(is_active.shape[0], 1)\n    circle1, circle2 = space.circle.get_slice(idx1), space.circle.get_slice(idx2)\n    pos1, pos2 = position.get_slice(idx1), position.get_slice(idx2)\n    is_active = jnp.logical_and(is_active[idx1], is_active[idx2])\n    contacts = _circle_to_circle_impl(circle1, circle2, pos1, pos2, is_active)\n    return contacts, circle1, circle2\n\n\n\n\nCode\nimport seaborn as sns\nfrom matplotlib.patches import Arrow\n\nN = 5\npalette = sns.color_palette(\"husl\", N)\n\n\ncircles = Circle(\n    mass=jnp.ones(N),\n    radius=jnp.ones(N),\n    moment=jnp.ones(N) * 0.5,\n    elasticity=jnp.ones(N) * 0.5,\n    friction=jnp.ones(N) * 0.2,\n    rgba=jnp.array([p + (1.0,) for p in palette]),\n)\nspace = Space(gravity=jnp.array([0.0, -9.8]), circle=circles)\np = Position(\n    angle=jnp.zeros(N),\n    xy=jnp.array([[-3, 4.0], [0.0, 2.0], [5.0, 3], [-3, 1], [2, 0]]),\n)\nv_xy = jnp.concatenate((jnp.zeros((N - 2, 2)), jnp.array([[0, 10.0], [-2.0, 8.0]])))\nv = Velocity(angle=jnp.zeros(N), xy=v_xy)\nf = Force(angle=jnp.zeros(N), xy=jnp.zeros((N, 2)))\nstate = State(p=p, v=v, f=f, is_active=jnp.ones(N, dtype=bool))\npositions = [state.p]\ncontact_list = []\nfor i in range(10):\n    state = update_velocity(space, circles, state)\n    state = update_position(space, state)\n    positions.append(state.p)\n    contacts, _, _ = check_circle_to_circle(space, state.p, state.is_active)\n    total_index = 0\n    for j in range(N):\n        for k in range(j + 1, N):\n            if contacts.penetration[total_index] &gt; 0:\n                contact_list.append(contacts.get_slice(total_index))\n            total_index += 1\nfig, ax = plt.subplots()\nax.set_aspect(\"equal\", adjustable=\"box\")\nax.set_xlim((-5, 5))\nax.set_ylim((-5, 5))\nvisualize_balls(ax, space.circle, positions)\nfor contact in contact_list:\n    arrow = Arrow(*contact.pos, *contact.normal, width=0.2, color=\"r\")\n    ax.add_patch(arrow)\nfig\n\n\n\n\n\n\n\n\n\nObjects are overlapping since I haven’t implemented resolving collision, but the dected collision looks correct."
  },
  {
    "objectID": "posts/fast_2d_physics_in_jax_en.html#what-to-do-after-collision-detection",
    "href": "posts/fast_2d_physics_in_jax_en.html#what-to-do-after-collision-detection",
    "title": "Implement fast 2D physics simulation with Jax",
    "section": "What to do after collision detection",
    "text": "What to do after collision detection\nI’m not quite familiar with physics actually, but in the real world, collisions are automatically resolved by each small molecule. But in the game physics engine, we want to get results faster by enforcing physically acurate constraints, such as:\n\nSpeed changes by collision\nNo overlap between objects\n\nSo, I’m just solving these constraints. There are many ways to solve these, but here, I employ a common approach called Sequential Impulse, which is used in a famous physics engines including Chipmunk and Box2D. Other approaches includes recently trending position-based dynamics, but sequatinal impulse caught my eyes because of its simplicity.\nAnyway, let me explain how sequential impulse method resolves collision. First, let’s assume a collision model in which collision produces impulse, and let the generated impulse be \\(\\mathbf{p}\\). I don’t use angular velocity here for simplicity. Let the velocity and mass of object 1 be \\(\\mathbf{v}_1\\) and \\(m_1\\), and the velocity and mass of object 2 be \\(\\mathbf{v}_2\\) and \\(m_2\\), respectively, then the velocity after the impulse occurs is\n\\[\n\\begin{align*}\n\\mathbf{v}_1 = \\mathbf{v}_1^{\\mathrm{old}} - \\mathbf{p} / m_1 \\\\\n\\mathbf{v}_2 = \\mathbf{v}_2^{\\mathrm{old}} + \\mathbf{p} / m_2\n\\end{align*}\n\\]\nHere, because the direction of \\(\\mathbf{p}\\) is the normal vector of collision \\(\\mathbf{n}\\), we can write that \\(\\mathbf{p} = p\\mathbf{n}\\). Hence, we just need to compute \\(p\\). Let the relative velocity at the collision point be \\(\\Delta \\mathbf{v} = \\mathbf{v}_2 - \\mathbf{v}_1\\). Here combining the avove two equalities with \\(\\Delta \\mathbf{v} \\cdot n = 0\\), \\(p = \\frac{-\\Delta \\mathbf{v}^{\\mathrm{old}}\\cdot \\mathbf{n}}{\\frac{1}{m_1} + \\frac{1}{m_2}}\\). Angle makes this a bit more complicated, but the core equation is just like this.\nこうして計算したインパルスを全ての衝突に対して適用し、インパルスが小さくなるまで繰り返します。しかし、この手法は物体のめりこみを考慮していないので、これだけだと物体がめりこんだままになってしまうことがあります。 めり込みを減らすための手法はいくつかありますが、主に\n\nどのくらいめり込んでいるかに応じてバイアス速度\\(v_\\mathrm{bias} = \\frac{\\beta}{\\Delta t}\\max(0, \\delta - \\delta_\\mathrm{slop})\\)(\\(\\delta\\)はめりこみの長さ、\\(\\delta_\\mathrm{slop}\\)は許容されるめりこみの長さ)を加え\\(p = \\frac{-\\Delta \\mathbf{v}^{\\mathrm{old}}\\cdot \\mathbf{n} + v_\\mathrm{bias}}{\\frac{1}{m_1} + \\frac{1}{m_2}}\\)とする (Baumegarte)\n速度を更新した後にもう一回Positionに関する制約を解いて擬似的な速度を加える (Nonlinear Gauss Seidel, NGS)\n\nという2種類の手法があります。先程紹介したBox2Dの資料やChipmunk2Dでは1が、現在のBox2Dでは2が使われています。詳しくはBox2D 3.0のコメントを参照してください。 今回は若干高速な1の手法を実装しようかと思ったのですが、この方法だと2つの物体が同じ方向に進んでいる時はめりこみを解消できないので、結局2の手法を実装しました。具体的にソルバの実装としては、\n\n衝突により発生するインパルスに関する制約を解く\n弾性により発生するインパルスを加える\n位置に関する制約を解く\n\nという3つのステップに分けて実装すればいいです。\nまた、さっき並列化のため手動で全ペアに対するループをアンロールしましたが、Sequential Impulseの実装でもこれが使えます。ただし、インパルスを加えた後の速度の更新は、v_update[i][j]にi番目の物体とj番目の物体の衝突により生じるi番目の物体の速度変化が入っているとして、\nfor i in range(N):\n    for j in range(i + 1, N):\n        obj[i].velocity += v_update[i][j]\n        obj[j].velocity += v_update[j][i]\nのように各衝突により生じた速度変化を物体にフィードバックする必要があります。これもいちいちループで書くと遅くなってしまうのですが、さっきのgenerate_self_pairsで\\(0, 1, 2, ..., N - 1\\)のペアを生成しておいてインデックスにするとループなしで書けます。細かく言うと、generate_self_pairsではループが使われているのですが、jax.jitでコンパイルした時に計算結果がキャッシュされるはずなので気にしなくてもいいです。\nというわけで実装してみましょう。実装は基本的にBox2d-Liteと開発中の最新版であるBox2D 3.0を参考にしました。また、Box2D作者のErin Catto氏による講演の内容をTypescriptで実装したリポジトリがあったのでこれも参考にしました。\n\n\nCode\nimport functools\n\n\n@chex.dataclass\nclass ContactHelper:\n    tangent: jax.Array\n    mass_normal: jax.Array\n    mass_tangent: jax.Array\n    v_bias: jax.Array\n    bounce: jax.Array\n    r1: jax.Array\n    r2: jax.Array\n    inv_mass1: jax.Array\n    inv_mass2: jax.Array\n    inv_moment1: jax.Array\n    inv_moment2: jax.Array\n    local_anchor1: jax.Array\n    local_anchor2: jax.Array\n    allow_bounce: jax.Array\n\n\n@chex.dataclass\nclass VelocitySolver:\n    v1: Velocity\n    v2: Velocity\n    pn: jax.Array\n    pt: jax.Array\n    contact: jax.Array\n\n    def update(self, new_contact: jax.Array) -&gt; Self:\n        continuing_contact = jnp.logical_and(self.contact, new_contact)\n        pn = jnp.where(continuing_contact, self.pn, jnp.zeros_like(self.pn))\n        pt = jnp.where(continuing_contact, self.pt, jnp.zeros_like(self.pt))\n        return self.replace(pn=pn, pt=pt, contact=new_contact)\n\n\ndef init_solver(n: int) -&gt; VelocitySolver:\n    return VelocitySolver(\n        v1=Velocity.zeros(n),\n        v2=Velocity.zeros(n),\n        pn=jnp.zeros(n),\n        pt=jnp.zeros(n),\n        contact=jnp.zeros(n, dtype=bool),\n    )\n\n\ndef _pv_gather(\n    p1: _PositionLike,\n    p2: _PositionLike,\n    orig: _PositionLike,\n) -&gt; _PositionLike:\n    indices = jnp.arange(len(orig.angle))\n    outer, inner = generate_self_pairs(indices)\n    p1_xy = jnp.zeros_like(orig.xy).at[outer].add(p1.xy)\n    p1_angle = jnp.zeros_like(orig.angle).at[outer].add(p1.angle)\n    p2_xy = jnp.zeros_like(orig.xy).at[inner].add(p2.xy)\n    p2_angle = jnp.zeros_like(orig.angle).at[inner].add(p2.angle)\n    return p1.__class__(xy=p1_xy + p2_xy, angle=p1_angle + p2_angle)\n\n\ndef _vmap_dot(xy1: jax.Array, xy2: jax.Array) -&gt; jax.Array:\n    \"\"\"Dot product between nested vectors\"\"\"\n    chex.assert_equal_shape((xy1, xy2))\n    orig_shape = xy1.shape\n    a = xy1.reshape(-1, orig_shape[-1])\n    b = xy2.reshape(-1, orig_shape[-1])\n    return jax.vmap(jnp.dot, in_axes=(0, 0))(a, b).reshape(*orig_shape[:-1])\n\n\ndef _sv_cross(s: jax.Array, v: jax.Array) -&gt; jax.Array:\n    \"\"\"Cross product with scalar and vector\"\"\"\n    x, y = _get_xy(v)\n    return jnp.stack((y * -s, x * s), axis=-1)\n\n\ndef _dv2from1(v1: Velocity, r1: jax.Array, v2: Velocity, r2: jax.Array) -&gt; jax.Array:\n    \"\"\"Compute relative veclotiy from v2/r2 to v1/r1\"\"\"\n    rel_v1 = v1.xy + _sv_cross(v1.angle, r1)\n    rel_v2 = v2.xy + _sv_cross(v2.angle, r2)\n    return rel_v2 - rel_v1\n\n\ndef _effective_mass(\n    inv_mass: jax.Array,\n    inv_moment: jax.Array,\n    r: jax.Array,\n    n: jax.Array,\n) -&gt; jax.Array:\n    rn2 = jnp.cross(r, n) ** 2\n    return inv_mass + inv_moment * rn2\n\n\ndef init_contact_helper(\n    space: Space,\n    contact: Contact,\n    a: Shape,\n    b: Shape,\n    p1: Position,\n    p2: Position,\n    v1: Velocity,\n    v2: Velocity,\n) -&gt; ContactHelper:\n    r1 = contact.pos - p1.xy\n    r2 = contact.pos - p2.xy\n\n    inv_mass1, inv_mass2 = a.inv_mass(), b.inv_mass()\n    inv_moment1, inv_moment2 = a.inv_moment(), b.inv_moment()\n    kn1 = _effective_mass(inv_mass1, inv_moment1, r1, contact.normal)\n    kn2 = _effective_mass(inv_mass2, inv_moment2, r2, contact.normal)\n    nx, ny = _get_xy(contact.normal)\n    tangent = jnp.stack((-ny, nx), axis=-1)\n    kt1 = _effective_mass(inv_mass1, inv_moment1, r1, tangent)\n    kt2 = _effective_mass(inv_mass2, inv_moment2, r2, tangent)\n    clipped_p = jnp.clip(space.allowed_penetration - contact.penetration, a_max=0.0)\n    v_bias = -space.bias_factor / space.dt * clipped_p\n    # k_normal, k_tangent, and v_bias should have (N(N-1)/2, N_contacts) shape\n    chex.assert_equal_shape((contact.friction, kn1, kn2, kt1, kt2, v_bias))\n    # Compute elasiticity * relative_vel\n    dv = _dv2from1(v1, r1, v2, r2)\n    vn = _vmap_dot(dv, contact.normal)\n    return ContactHelper(\n        tangent=tangent,\n        mass_normal=1 / (kn1 + kn2),\n        mass_tangent=1 / (kt1 + kt2),\n        v_bias=v_bias,\n        bounce=vn * contact.elasticity,\n        r1=r1,\n        r2=r2,\n        inv_mass1=inv_mass1,\n        inv_mass2=inv_mass2,\n        inv_moment1=inv_moment1,\n        inv_moment2=inv_moment2,\n        local_anchor1=p1.inv_rotate(r1),\n        local_anchor2=p2.inv_rotate(r2),\n        allow_bounce=vn &lt;= -space.bounce_threshold,\n    )\n\n\n@jax.vmap\ndef apply_initial_impulse(\n    contact: Contact,\n    helper: ContactHelper,\n    solver: VelocitySolver,\n) -&gt; VelocitySolver:\n    \"\"\"Warm starting by applying initial impulse\"\"\"\n    p = helper.tangent * solver.pt + contact.normal * solver.pn\n    v1 = solver.v1 - Velocity(\n        angle=helper.inv_moment1 * jnp.cross(helper.r1, p),\n        xy=p * helper.inv_mass1,\n    )\n    v2 = solver.v2 + Velocity(\n        angle=helper.inv_moment2 * jnp.cross(helper.r2, p),\n        xy=p * helper.inv_mass2,\n    )\n    return solver.replace(v1=v1, v2=v2)\n\n\n@jax.vmap\ndef apply_velocity_normal(\n    contact: Contact,\n    helper: ContactHelper,\n    solver: VelocitySolver,\n) -&gt; VelocitySolver:\n    \"\"\"\n    Apply velocity constraints to the solver.\n    Suppose that each shape has (N_contact, 1) or (N_contact, 2).\n    \"\"\"\n    # Relative veclocity (from shape2 to shape1)\n    dv = _dv2from1(solver.v1, helper.r1, solver.v2, helper.r2)\n    vt = jnp.dot(dv, helper.tangent)\n    dpt = -helper.mass_tangent * vt\n    # Clamp friction impulse\n    max_pt = contact.friction * solver.pn\n    pt = jnp.clip(solver.pt + dpt, a_min=-max_pt, a_max=max_pt)\n    dpt_clamped = helper.tangent * (pt - solver.pt)\n    # Velocity update by contact tangent\n    dvt1 = Velocity(\n        angle=-helper.inv_moment1 * jnp.cross(helper.r1, dpt_clamped),\n        xy=-dpt_clamped * helper.inv_mass1,\n    )\n    dvt2 = Velocity(\n        angle=helper.inv_moment2 * jnp.cross(helper.r2, dpt_clamped),\n        xy=dpt_clamped * helper.inv_mass2,\n    )\n    # Compute Relative velocity again\n    dv = _dv2from1(solver.v1 + dvt1, helper.r1, solver.v2 + dvt2, helper.r2)\n    vn = _vmap_dot(dv, contact.normal)\n    dpn = helper.mass_normal * (-vn + helper.v_bias)\n    # Accumulate and clamp impulse\n    pn = jnp.clip(solver.pn + dpn, a_min=0.0)\n    dpn_clamped = contact.normal * (pn - solver.pn)\n    # Velocity update by contact normal\n    dvn1 = Velocity(\n        angle=-helper.inv_moment1 * jnp.cross(helper.r1, dpn_clamped),\n        xy=-dpn_clamped * helper.inv_mass1,\n    )\n    dvn2 = Velocity(\n        angle=helper.inv_moment2 * jnp.cross(helper.r2, dpn_clamped),\n        xy=dpn_clamped * helper.inv_mass2,\n    )\n    # Filter dv\n    dv1, dv2 = jax.tree_map(\n        lambda x: jnp.where(solver.contact, x, jnp.zeros_like(x)),\n        (dvn1 + dvt1, dvn2 + dvt2),\n    )\n    # Summing up dv per each contact pair\n    return VelocitySolver(\n        v1=dv1,\n        v2=dv2,\n        pn=pn,\n        pt=pt,\n        contact=solver.contact,\n    )\n\n\n@jax.vmap\ndef apply_bounce(\n    contact: Contact,\n    helper: ContactHelper,\n    solver: VelocitySolver,\n) -&gt; tuple[Velocity, Velocity]:\n    \"\"\"\n    Apply bounce (resititution).\n    Suppose that each shape has (N_contact, 1) or (N_contact, 2).\n    \"\"\"\n    # Relative veclocity (from shape2 to shape1)\n    dv = _dv2from1(solver.v1, helper.r1, solver.v2, helper.r2)\n    vn = jnp.dot(dv, contact.normal)\n    pn = -helper.mass_normal * (vn + helper.bounce)\n    dpn = contact.normal * pn\n    # Velocity update by contact normal\n    dv1 = Velocity(\n        angle=-helper.inv_moment1 * jnp.cross(helper.r1, dpn),\n        xy=-dpn * helper.inv_mass1,\n    )\n    dv2 = Velocity(\n        angle=helper.inv_moment2 * jnp.cross(helper.r2, dpn),\n        xy=dpn * helper.inv_mass2,\n    )\n    # Filter dv\n    allow_bounce = jnp.logical_and(solver.contact, helper.allow_bounce)\n    return jax.tree_map(\n        lambda x: jnp.where(allow_bounce, x, jnp.zeros_like(x)),\n        (dv1, dv2),\n    )\n\n\n@chex.dataclass\nclass PositionSolver:\n    p1: Position\n    p2: Position\n    contact: jax.Array\n    min_separation: jax.Array\n\n\n@functools.partial(jax.vmap, in_axes=(None, None, None, 0, 0, 0))\ndef correct_position(\n    bias_factor: float | jax.Array,\n    linear_slop: float | jax.Array,\n    max_linear_correction: float | jax.Array,\n    contact: Contact,\n    helper: ContactHelper,\n    solver: PositionSolver,\n) -&gt; PositionSolver:\n    \"\"\"\n    Correct positions to remove penetration.\n    Suppose that each shape in contact and helper has (N_contact, 1) or (N_contact, 2).\n    p1 and p2 should have xy: (1, 2) angle (1, 1) shape\n    \"\"\"\n    # (N_contact, 2)\n    r1 = solver.p1.rotate(helper.local_anchor1)\n    r2 = solver.p2.rotate(helper.local_anchor2)\n    ga2_ga1 = r2 - r1 + solver.p2.xy - solver.p1.xy\n    separation = jnp.dot(ga2_ga1, contact.normal) - contact.penetration\n    c = jnp.clip(\n        bias_factor * (separation + linear_slop),\n        a_min=-max_linear_correction,\n        a_max=0.0,\n    )\n    kn1 = _effective_mass(helper.inv_mass1, helper.inv_moment1, r1, contact.normal)\n    kn2 = _effective_mass(helper.inv_mass2, helper.inv_moment2, r2, contact.normal)\n    k_normal = kn1 + kn2\n    impulse = jnp.where(k_normal &gt; 0.0, -c / k_normal, jnp.zeros_like(c))\n    pn = impulse * contact.normal\n    p1 = Position(\n        angle=-helper.inv_moment1 * jnp.cross(r1, pn),\n        xy=-pn * helper.inv_mass1,\n    )\n    p2 = Position(\n        angle=helper.inv_moment2 * jnp.cross(r2, pn),\n        xy=pn * helper.inv_mass2,\n    )\n    min_sep = jnp.fmin(solver.min_separation, separation)\n    # Filter separation\n    p1, p2 = jax.tree_map(\n        lambda x: jnp.where(solver.contact, x, jnp.zeros_like(x)),\n        (p1, p2),\n    )\n    return solver.replace(p1=p1, p2=p2, min_separation=min_sep)\n\n\ndef fake_fori_loop(start, end, step, initial):\n    \"\"\"For debugging. Just replace jax.lax.fori_loop with this.\"\"\"\n    state = initial\n    for i in range(start, end):\n        state = step(i, state)\n    return state\n\n\ndef apply_seq_impulses(\n    space: Space,\n    solver: VelocitySolver,\n    p: Position,\n    v: Velocity,\n    contact: Contact,\n    a: Shape,\n    b: Shape,\n) -&gt; tuple[Velocity, Position, VelocitySolver]:\n    \"\"\"Resolve collisions by Sequential Impulse method\"\"\"\n    p1, p2 = tree_map2(generate_self_pairs, p)\n    v1, v2 = tree_map2(generate_self_pairs, v)\n    helper = init_contact_helper(space, contact, a, b, p1, p2, v1, v2)\n    solver = apply_initial_impulse(\n        contact,\n        helper,\n        solver.replace(v1=v1, v2=v2),\n    )\n\n    def vstep(\n        _n_iter: int,\n        vs: tuple[Velocity, VelocitySolver],\n    ) -&gt; tuple[Velocity, VelocitySolver]:\n        v_i, solver_i = vs\n        solver_i1 = apply_velocity_normal(contact, helper, solver_i)\n        v_i1 = _pv_gather(solver_i1.v1, solver_i1.v2, v_i) + v_i\n        v1, v2 = tree_map2(generate_self_pairs, v_i1)\n        return v_i1, solver_i1.replace(v1=v1, v2=v2)\n\n    v, solver = jax.lax.fori_loop(0, space.n_velocity_iter, vstep, (v, solver))\n    rest_v1, rest_v2 = apply_bounce(contact, helper, solver)\n    v = _pv_gather(rest_v1, rest_v2, v) + v\n\n    def pstep(\n        _n_iter: int,\n        ps: tuple[Position, PositionSolver],\n    ) -&gt; tuple[Position, PositionSolver]:\n        p_i, solver_i = ps\n        solver_i1 = correct_position(\n            space.bias_factor,\n            space.linear_slop,\n            space.max_linear_correction,\n            contact,\n            helper,\n            solver_i,\n        )\n        p_i1 = _pv_gather(solver_i1.p1, solver_i1.p2, p_i) + p_i\n        p1, p2 = tree_map2(generate_self_pairs, p_i1)\n        return p_i1, solver_i1.replace(p1=p1, p2=p2)\n\n    pos_solver = PositionSolver(\n        p1=p1,\n        p2=p2,\n        contact=solver.contact,\n        min_separation=jnp.zeros_like(p1.angle),\n    )\n    p, pos_solver = jax.lax.fori_loop(0, space.n_position_iter, pstep, (p, pos_solver))\n    return v, p, solver\n\n\nなかなか複雑になりましたが、実装できました。衝突させてみましょう。\n\n\nCode\nfrom celluloid import Camera\nfrom IPython.display import HTML\n\n\ndef animate_balls(\n    fig,\n    ax: Axes,\n    circles: Circle,\n    positions: Iterable[Position],\n) -&gt; HTML:\n    pos = list(positions)\n    camera = Camera(fig)\n    circle_list = circles.tolist()\n    for pi in pos:\n        for pij, circle in zip(pi.tolist(), circle_list):\n            circle_patch = CirclePatch(\n                xy=pij.xy,\n                radius=circle.radius,\n                fill=False,\n                color=circle.rgba.tolist(),\n            )\n            ax.add_patch(circle_patch)\n        camera.snap()\n    return HTML(camera.animate().to_jshtml())\n\n\n\n\nCode\nspace = Space(gravity=jnp.array([0.0, -9.8]), dt=0.04, bias_factor=0.2, circle=circles)\nstate = State(p=p, v=v, f=f, is_active=jnp.ones(N, dtype=bool))\npositions = [state.p]\nsolver = init_solver(N * (N - 1) // 2)\n\n\n@jax.jit\ndef step(state: State, solver: VelocitySolver) -&gt; tuple[State, VelocitySolver]:\n    state = update_velocity(space, space.circle, state)\n    contacts, c1, c2 = check_circle_to_circle(space, state.p, state.is_active)\n    v, p, solver = apply_seq_impulses(\n        space,\n        solver.update(contacts.penetration &gt;= 0),\n        state.p,\n        state.v,\n        contacts,\n        c1,\n        c2,\n    )\n    return update_position(space, state.replace(v=v, p=p)), solver\n\n\nfor i in range(30):\n    state, solver = step(state, solver)\n    positions.append(state.p)\nfig, ax = plt.subplots()\nax.set_aspect(\"equal\", adjustable=\"box\")\nax.set_xlim((-10, 10))\nax.set_ylim((-10, 10))\nanimate_balls(fig, ax, space.circle, positions)\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n最初の衝突で若干のめりこみが発生していますが、一応大丈夫そうですね。"
  },
  {
    "objectID": "posts/fast_2d_physics_in_jax_en.html#線分",
    "href": "posts/fast_2d_physics_in_jax_en.html#線分",
    "title": "Implement fast 2D physics simulation with Jax",
    "section": "線分",
    "text": "線分\n円同士の衝突が実装できたところで、次は凸多角形の実装…と言いたいところですが、それは後回しにして、ゲームには欠かせない「線分」を実装してみます。現実世界にはそんなもの存在しないのですが、ゲームやシミュレーションの世界では、どうしても地面や柵といった境界を表現する必要が生じてきます。こういったものを例えば「めちゃくちゃ重い長方形」として表現することもできますが、シミュレーションを組むユーザー側からするといちいち長方形の大きさだったりを定義するのは面倒なので、「無限の質量を持つ線」として扱えたほうが楽ですよね。というわけで線分を実装してみます。線分の衝突判定は薬のカプセル💊のように両端が丸くなっているやつ(Box2Dだとカプセルと呼ばれているのでカプセルと呼びます)と実装がほぼ同じなので、カプセルも一緒に実装してしまいます。ただカプセル同士の衝突は面倒なので、とりあえず円とカプセルだけ実装しましょう。新しい図形を加えたので、Spaceも作り直す必要があります。とりあえずdataclassに全シェイプをつっこんでおいて、各シェイプの組み合わせごとに衝突判定を行い、衝突解決のときはjnp.concatenateで全部くっつけてからvmapで一度にやるという実装方針にしました。さっきのペアに対するループをアンロールするところは、インデックスのペアを持っておいて適当なオフセットを足しておけばそのまま使えます。\n\n\nCode\nfrom matplotlib.patches import Rectangle\n\n@chex.dataclass\nclass Capsule(Shape):\n    length: jax.Array\n    radius: jax.Array\n\n\n@chex.dataclass\nclass Segment(Shape):\n    length: jax.Array\n\n    def to_capsule(self) -&gt; Capsule:\n        return Capsule(\n            mass=self.mass,\n            moment=self.moment,\n            elasticity=self.elasticity,\n            friction=self.friction,\n            rgba=self.rgba,\n            length=self.length,\n            radius=jnp.zeros_like(self.length),\n        )\n\n\ndef _length_to_points(length: jax.Array) -&gt; tuple[jax.Array, jax.Array]:\n    a = jnp.stack((length * -0.5, length * 0.0), axis=-1)\n    b = jnp.stack((length * 0.5, length * 0.0), axis=-1)\n    return a, b\n\n\n@jax.vmap\ndef _capsule_to_circle_impl(\n    a: Capsule,\n    b: Circle,\n    a_pos: Position,\n    b_pos: Position,\n    isactive: jax.Array,\n) -&gt; Contact:\n    # Move b_pos to capsule's coordinates\n    pb = a_pos.inv_transform(b_pos.xy)\n    p1, p2 = _length_to_points(a.length)\n    edge = p2 - p1\n    s1 = jnp.dot(pb - p1, edge)\n    s2 = jnp.dot(p2 - pb, edge)\n    in_segment = jnp.logical_and(s1 &gt;= 0.0, s2 &gt;= 0.0)\n    ee = jnp.sum(jnp.square(edge), axis=-1, keepdims=True)\n    # Closest point\n    # s1 &lt; 0: pb is left to the capsule\n    # s2 &lt; 0: pb is right to the capsule\n    # else: pb is in between capsule\n    pa = jax.lax.select(\n        in_segment,\n        p1 + edge * s1 / ee,\n        jax.lax.select(s1 &lt; 0.0, p1, p2),\n    )\n    a2b_normal, dist = normalize(pb - pa)\n    penetration = a.radius + b.radius - dist\n    a_contact = pa + a2b_normal * a.radius\n    b_contact = pb - a2b_normal * b.radius\n    pos = a_pos.transform((a_contact + b_contact) * 0.5)\n    xy_zeros = jnp.zeros_like(b_pos.xy)\n    a2b_normal_rotated = a_pos.replace(xy=xy_zeros).transform(a2b_normal)\n    # Filter penetration\n    penetration = jnp.where(isactive, penetration, jnp.ones_like(penetration) * -1)\n    return Contact(\n        pos=pos,\n        normal=a2b_normal_rotated,\n        penetration=penetration,\n        elasticity=(a.elasticity + b.elasticity) * 0.5,\n        friction=(a.friction + b.friction) * 0.5,\n    )\n\n\n@chex.dataclass\nclass ShapeDict:\n    circle: Circle | None = None\n    segment: Segment | None = None\n    capsule: Capsule | None = None\n    \n    def concat(self) -&gt; Shape:\n        shapes = [s.to_shape() for s in self.values() if s is not None]\n        return jax.tree_map(lambda *args: jnp.concatenate(args, axis=0), *shapes)\n\n\n@chex.dataclass\nclass StateDict:\n    circle: State | None = None\n    segment: State | None = None\n    capsule: State | None = None\n\n    def concat(self) -&gt; None:\n        states = [s for s in self.values() if s is not None]\n        return jax.tree_map(lambda *args: jnp.concatenate(args, axis=0), *states)\n\n    def offset(self, key: str) -&gt; int:\n        total = 0\n        for k, state in self.items():\n            if k == key:\n                return total\n            if state is not None:\n                total += state.p.batch_size()\n        raise RuntimeError(\"Unreachable\")\n        \n    def _get(self, name: str, state: State) -&gt; State | None:\n        if self[name] is None:\n            return None\n        else:\n            start = self.offset(name)\n            end = start + self[name].p.batch_size()\n            return state.get_slice(jnp.arange(start, end))\n        \n    def update(self, statec: State) -&gt; Self:\n        circle = self._get(\"circle\", statec)\n        segment = self._get(\"segment\", statec)\n        capsule = self._get(\"capsule\", statec)\n        return self.__class__(circle=circle, segment=segment, capsule=capsule)\n\n\nContactFn = Callable[[StateDict], tuple[Contact, Shape, Shape]]\n\n\ndef _pair_outer(x: jax.Array, reps: int) -&gt; jax.Array:\n    return jnp.repeat(x, reps, axis=0, total_repeat_length=x.shape[0] * reps)\n\n\ndef _pair_inner(x: jax.Array, reps: int) -&gt; jax.Array:\n    return jnp.tile(x, (reps,) + (1,) * (x.ndim - 1))\n\n\ndef generate_pairs(x: jax.Array, y: jax.Array) -&gt; tuple[jax.Array, jax.Array]:\n    \"\"\"Returns two arrays that iterate over all combination of elements in x and y\"\"\"\n    xlen, ylen = x.shape[0], y.shape[0]\n    return _pair_outer(x, ylen), _pair_inner(y, xlen)\n\n\ndef _circle_to_circle(\n    shaped: ShapeDict,\n    stated: StateDict,\n) -&gt; tuple[Contact, Circle, Circle]:\n    circle1, circle2 = tree_map2(generate_self_pairs, shaped.circle)\n    pos1, pos2 = tree_map2(generate_self_pairs, stated.circle.p)\n    is_active = jnp.logical_and(*generate_self_pairs(stated.circle.is_active))\n    contacts = _circle_to_circle_impl(\n        circle1,\n        circle2,\n        pos1,\n        pos2,\n        is_active,\n    )\n    return contacts, circle1, circle2\n\n\ndef _capsule_to_circle(\n    shaped: ShapeDict,\n    stated: StateDict,\n) -&gt; tuple[Contact, Capsule, Circle]:\n    capsule = jax.tree_map(\n        functools.partial(_pair_outer, reps=shaped.circle.mass.shape[0]),\n        shaped.capsule,\n    )\n    circle = jax.tree_map(\n        functools.partial(_pair_inner, reps=shaped.capsule.mass.shape[0]),\n        shaped.circle,\n    )\n    pos1, pos2 = tree_map2(generate_pairs, stated.capsule.p, stated.circle.p)\n    is_active = jnp.logical_and(\n        *generate_pairs(stated.capsule.is_active, stated.circle.is_active)\n    )\n    contacts = _capsule_to_circle_impl(\n        capsule,\n        circle,\n        pos1,\n        pos2,\n        is_active,\n    )\n    return contacts, capsule, circle\n\n\ndef _segment_to_circle(\n    shaped: ShapeDict,\n    stated: StateDict,\n) -&gt; tuple[Contact, Segment, Circle]:\n    segment = jax.tree_map(\n        functools.partial(_pair_outer, reps=shaped.circle.mass.shape[0]),\n        shaped.segment,\n    )\n    circle = jax.tree_map(\n        functools.partial(_pair_inner, reps=shaped.segment.mass.shape[0]),\n        shaped.circle,\n    )\n    pos1, pos2 = tree_map2(generate_pairs, stated.segment.p, stated.circle.p)\n    is_active = jnp.logical_and(\n        *generate_pairs(stated.segment.is_active, stated.circle.is_active)\n    )\n    contacts = _capsule_to_circle_impl(\n        segment.to_capsule(),\n        circle,\n        pos1,\n        pos2,\n        is_active,\n    )\n    return contacts, segment, circle\n\n\n_CONTACT_FUNCTIONS = {\n    (\"circle\", \"circle\"): _circle_to_circle,\n    (\"capsule\", \"circle\"): _capsule_to_circle,\n    (\"segment\", \"circle\"): _segment_to_circle,\n}\n\n\n@chex.dataclass\nclass ContactWithMetadata:\n    contact: Contact\n    shape1: Shape\n    shape2: Shape\n    outer_index: jax.Array\n    inner_index: jax.Array\n\n    def gather_p_or_v(\n        self,\n        outer: _PositionLike,\n        inner: _PositionLike,\n        orig: _PositionLike,\n    ) -&gt; _PositionLike:\n        xy_outer = jnp.zeros_like(orig.xy).at[self.outer_index].add(outer.xy)\n        angle_outer = jnp.zeros_like(orig.angle).at[self.outer_index].add(outer.angle)\n        xy_inner = jnp.zeros_like(orig.xy).at[self.inner_index].add(inner.xy)\n        angle_inner = jnp.zeros_like(orig.angle).at[self.inner_index].add(inner.angle)\n        return orig.__class__(angle=angle_outer + angle_inner, xy=xy_outer + xy_inner)\n\n\n@chex.dataclass\nclass ExtendedSpace:\n    gravity: jax.Array\n    shaped: ShapeDict\n    dt: jax.Array | float = 0.1\n    linear_damping: jax.Array | float = 0.95\n    angular_damping: jax.Array | float = 0.95\n    bias_factor: jax.Array | float = 0.2\n    n_velocity_iter: int = 8\n    n_position_iter: int = 2\n    linear_slop: jax.Array | float = 0.005\n    max_linear_correction: jax.Array | float = 0.2\n    allowed_penetration: jax.Array | float = 0.005\n    bounce_threshold: float = 1.0\n\n    def check_contacts(self, stated: StateDict) -&gt; ContactWithMetadata:\n        contacts = []\n        for (n1, n2), fn in _CONTACT_FUNCTIONS.items():\n            if stated[n1] is not None and stated[n2] is not None:\n                contact, shape1, shape2 = fn(self.shaped, stated)\n                len1, len2 = stated[n1].p.batch_size(), stated[n2].p.batch_size()\n                offset1, offset2 = stated.offset(n1), stated.offset(n2)\n                if n1 == n2:\n                    outer_index, inner_index = generate_self_pairs(jnp.arange(len1))\n                else:\n                    outer_index, inner_index = generate_pairs(\n                        jnp.arange(len1),\n                        jnp.arange(len2),\n                    )\n                contact_with_meta = ContactWithMetadata(\n                    contact=contact,\n                    shape1=shape1.to_shape(),\n                    shape2=shape2.to_shape(),\n                    outer_index=outer_index + offset1,\n                    inner_index=inner_index + offset2,\n                )\n                contacts.append(contact_with_meta)\n        return jax.tree_map(lambda *args: jnp.concatenate(args, axis=0), *contacts)\n    \n    def n_possible_contacts(self) -&gt; int:\n        n = 0\n        for n1, n2 in _CONTACT_FUNCTIONS.keys():\n            if self.shaped[n1] is not None and self.shaped[n2] is not None:\n                len1, len2 = len(self.shaped[n1].mass), len(self.shaped[n2].mass)\n                if n1 == n2:\n                    n += len1 * (len1 - 1) // 2\n                else:\n                    n += len1 * len2\n        return n\n\n\ndef animate_balls_and_segments(\n    fig,\n    ax: Axes,\n    circles: Circle,\n    segments: Segment,\n    c_pos: Iterable[Position],\n    s_pos: Position,\n) -&gt; HTML:\n    camera = Camera(fig)\n    circle_list = circles.tolist()\n    # Lower left\n    segment_ll = s_pos.transform(\n        jnp.stack((-segments.length * 0.5, jnp.zeros_like(segments.length)), axis=1)\n    )\n    for pi in c_pos:\n        for pij, circle in zip(pi.tolist(), circle_list):\n            circle_patch = CirclePatch(\n                xy=pij.xy,\n                radius=circle.radius,\n                fill=False,\n                color=circle.rgba.tolist(),\n            )\n            ax.add_patch(circle_patch)\n        for ll, pj, segment in zip(segment_ll, s_pos.tolist(), segments.tolist()):\n            rect_patch = Rectangle(\n                xy=ll,\n                width=segment.length,\n                angle=(pj.angle / jnp.pi).item() * 180,\n                height=0.1,\n            )\n            ax.add_patch(rect_patch)\n        camera.snap()\n    return HTML(camera.animate().to_jshtml())\n\ndef solve_constraints(\n    space: Space,\n    solver: VelocitySolver,\n    p: Position,\n    v: Velocity,\n    contact_with_meta: ContactWithMetadata,\n) -&gt; tuple[Velocity, Position, VelocitySolver]:\n    \"\"\"Resolve collisions by Sequential Impulse method\"\"\"\n    outer, inner = contact_with_meta.outer_index, contact_with_meta.inner_index\n\n    def get_pairs(p_or_v: _PositionLike) -&gt; tuple[_PositionLike, _PositionLike]:\n        return p_or_v.get_slice(outer), p_or_v.get_slice(inner)\n\n    p1, p2 = get_pairs(p)\n    v1, v2 = get_pairs(v)\n    helper = init_contact_helper(\n        space,\n        contact_with_meta.contact,\n        contact_with_meta.shape1,\n        contact_with_meta.shape2,\n        p1,\n        p2,\n        v1,\n        v2,\n    )\n    # Warm up the velocity solver\n    solver = apply_initial_impulse(\n        contact_with_meta.contact,\n        helper,\n        solver.replace(v1=v1, v2=v2),\n    )\n\n    def vstep(\n        _n_iter: int,\n        vs: tuple[Velocity, VelocitySolver],\n    ) -&gt; tuple[Velocity, VelocitySolver]:\n        v_i, solver_i = vs\n        solver_i1 = apply_velocity_normal(contact_with_meta.contact, helper, solver_i)\n        v_i1 = contact_with_meta.gather_p_or_v(solver_i1.v1, solver_i1.v2, v_i) + v_i\n        v1, v2 = get_pairs(v_i1)\n        return v_i1, solver_i1.replace(v1=v1, v2=v2)\n\n    v, solver = jax.lax.fori_loop(0, space.n_velocity_iter, vstep, (v, solver))\n    bv1, bv2 = apply_bounce(contact_with_meta.contact, helper, solver)\n    v = contact_with_meta.gather_p_or_v(bv1, bv2, v) + v\n\n    def pstep(\n        _n_iter: int,\n        ps: tuple[Position, PositionSolver],\n    ) -&gt; tuple[Position, PositionSolver]:\n        p_i, solver_i = ps\n        solver_i1 = correct_position(\n            space.bias_factor,\n            space.linear_slop,\n            space.max_linear_correction,\n            contact_with_meta.contact,\n            helper,\n            solver_i,\n        )\n        p_i1 = contact_with_meta.gather_p_or_v(solver_i1.p1, solver_i1.p2, p_i) + p_i\n        p1, p2 = get_pairs(p_i1)\n        return p_i1, solver_i1.replace(p1=p1, p2=p2)\n\n    pos_solver = PositionSolver(\n        p1=p1,\n        p2=p2,\n        contact=solver.contact,\n        min_separation=jnp.zeros_like(p1.angle),\n    )\n    p, pos_solver = jax.lax.fori_loop(0, space.n_position_iter, pstep, (p, pos_solver))\n    return v, p, solver\n\n\ndef dont_solve_constraints(\n    _space: Space,\n    solver: VelocitySolver,\n    p: Position,\n    v: Velocity,\n    _contact_with_meta: ContactWithMetadata,\n) -&gt; tuple[Velocity, Position, VelocitySolver]:\n    return v, p, solver\n\n\nN_SEG = 3\nsegments = Segment(\n    mass=jnp.ones(N_SEG) * jnp.inf,\n    moment=jnp.ones(N_SEG) * jnp.inf,\n    elasticity=jnp.ones(N_SEG) * 0.5,\n    friction=jnp.ones(N_SEG) * 1.0,\n    rgba=jnp.ones((N_SEG, 4)),\n    length=jnp.array([4 * jnp.sqrt(2), 4, 4 * jnp.sqrt(2)]),\n)\ncpos = jnp.array([[2, 2], [4, 3], [3, 6], [6, 5], [5, 7]], dtype=jnp.float32)\nstated = StateDict(\n    circle=State(\n        p=Position(xy=cpos, angle=jnp.zeros(N)),\n        v=Velocity.zeros(N),\n        f=Force.zeros(N),\n        is_active=jnp.array([True, True, True, True, True]),\n    ),\n    segment=State(\n        p=Position(\n            xy=jnp.array([[-2.0, 2.0], [2, 0], [6, 2]], dtype=jnp.float32),\n            angle=jnp.array([jnp.pi * 1.75, 0, jnp.pi * 0.25]),\n        ),\n        v=Velocity.zeros(N_SEG),\n        f=Force.zeros(N_SEG),\n        is_active=jnp.ones(N_SEG, dtype=bool),\n    ),\n)\nspace = ExtendedSpace(\n    gravity=jnp.array([0.0, -9.8]),\n    linear_damping=1.0,\n    angular_damping=1.0,\n    dt=0.04,\n    bias_factor=0.2,\n    n_velocity_iter=6,\n    shaped=ShapeDict(circle=circles, segment=segments),\n)\n\n\n@jax.jit\ndef step(stated: StateDict, solver: VelocitySolver) -&gt; StateDict:\n    state = update_velocity(space, space.shaped.concat(), stated.concat())\n    contact_with_meta = space.check_contacts(stated.update(state))\n    # Check there's any penetration\n    contacts = contact_with_meta.contact.penetration &gt;= 0\n    v, p, solver = jax.lax.cond(\n        jnp.any(contacts),\n        solve_constraints,\n        dont_solve_constraints,\n        space,\n        solver.update(contacts),\n        state.p,\n        state.v,\n        contact_with_meta,\n    )\n    statec = update_position(space, state.replace(v=v, p=p))\n    return stated.update(statec)\n\n\nアニメーションしてみます。\n\n\nCode\npositions = [stated[\"circle\"].p]\nsolver = init_solver(space.n_possible_contacts())\n\nfor i in range(50):\n    stated = step(stated, solver)\n    positions.append(stated[\"circle\"].p)\nfig, ax = plt.subplots()\nax.set_aspect(\"equal\", adjustable=\"box\")\nax.set_xlim((-10, 10))\nax.set_ylim((0, 10))\nanimate_balls_and_segments(\n    fig,\n    ax,\n    space.shaped[\"circle\"],\n    space.shaped[\"segment\"],\n    positions,\n    stated[\"segment\"].p,\n)\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nちょっとめりこんでいますがまあ一応計算できてはいそうです。一応簡単にベンチマークしてみます。\n\n\nCode\n%timeit step(stated, solver)\n\n\nボールの数がまだ少ないですが、1ステップあたり約700マイクロ秒ということで、かなり高速にできたのではないでしょうか。"
  },
  {
    "objectID": "posts/understanding-attention.html",
    "href": "posts/understanding-attention.html",
    "title": "Attentionが何をやっているのか理解しよう",
    "section": "",
    "text": "ChatGPTが大バズリしている昨今です。僕はプロンプトを考えるのが面倒なので（ええ…)あまり使わないのですが、友人が論文を書くのに使っていたり、僕の母親が話し相手に使っていたりするようです。親不孝な息子でごめんなさいという感じもします。 ところで、ChatGPTのような言語生成モデルでしばしば利用されているのが、Transformerと呼ばれるニューラルネットワークの構成です。Transformerでは、Multihead Attentionと呼ばれるレイヤーを入力のトークン列に対し繰り返し適用し複雑なモデルを構成します。このブログでは、その簡略版である1レイヤーのAttention(Multiheadではない)に着目し、これが何をしているのかを勉強し、ついでにコードを書いて動かしてみます。"
  },
  {
    "objectID": "posts/understanding-attention.html#最初に列があった",
    "href": "posts/understanding-attention.html#最初に列があった",
    "title": "Attentionが何をやっているのか理解しよう",
    "section": "最初に列があった",
    "text": "最初に列があった\nトークン列というのは文字通りトークンからなる列のことです。トークンは有限集合の要素です。実用上はbyte pair encodingにより得られた部分文字列などがこれに該当しますが、とりあえず気にしなくていいです。トークンの集合を\\(V\\)とし、\\([Nv] := {1, ..., Nv}\\)と番号付けしておきます。トークン列を\\(x = x[1: l]\\)と書きます。また、トークン列の最大の長さを\\(L\\)とします。トークンとして連続値や無限集合は扱えないと思いますが、素人なので何か抜け道があるかどうかは知りません。"
  },
  {
    "objectID": "posts/understanding-attention.html#トークンからベクトルに",
    "href": "posts/understanding-attention.html#トークンからベクトルに",
    "title": "Attentionが何をやっているのか理解しよう",
    "section": "トークンからベクトルに",
    "text": "トークンからベクトルに\n適当な\\(d_e \\times Nv\\)次元の行列\\(W_e\\)を使って、\\(v\\)番目のトークンから埋め込み（Token embedding）を \\(e = W_e[:, v]\\)により得ます。これは\\(d_e\\)次元のベクトルになります。なお、numpy風に\\(i\\)番目の行ベクトルを\\(W[i, :]\\)、\\(j\\)番目の列ベクトルを\\(W[:, j]\\)と書いています。この行列\\(W_e\\)は勾配降下により学習されるようです。"
  },
  {
    "objectID": "posts/understanding-attention.html#ついでに位置もベクトルに",
    "href": "posts/understanding-attention.html#ついでに位置もベクトルに",
    "title": "Attentionが何をやっているのか理解しよう",
    "section": "ついでに位置もベクトルに",
    "text": "ついでに位置もベクトルに\n適当な\\(d_p \\times L\\)次元の行列\\(W_p\\)を使って、トークン列中の\\(l\\)番目にトークンがあるという情報から、位置埋め込み（Positional embedding）を \\(p = W_p[:, l]\\)により得ます。これも\\(d_e\\)次元のベクトルになります。正直なんの意味があるのかよくわからないのですが、これを先程のトークン埋め込みに足してトークン列\\(x\\)の\\(t\\)番目のトークン\\(x[t]\\)に対する埋め込みを\\(e = W_e[:, x[t]] + W_p[:, t]\\)によって得ます。これ足して大丈夫なのかな？って思うんですが。 位置埋め込みは、学習されることもあるようですが、Transformerが最初に提案されたAttention Is All You Needの論文では、以下のように構成されています。 \\[\n\\begin{align*}\nW_p[2i - 1, t] &= \\sin (\\frac{t}{L^{2i / d_e}}) \\\\\nW_p[2i, t] &= \\cos (\\frac{t}{L^{2i / d_e}}) \\\\\n&~~~~~(0 &lt; 2i \\leq d_e)\n\\end{align*}\n\\] これを\\(L=50, d_e = 5\\)として可視化してみましょう。\n\n\nCode\nimport matplotlib\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nmatplotlib.font_manager.fontManager.addfont(\"NotoEmoji-Medium.ttf\")\n\nL = 50\nd_e = 5\nx = np.arange(L)\nfor i in range(1, 1 + d_e):\n    if i % 2 == 0:\n        w_p = np.sin(x / L ** (i / d_e))\n    else:\n        w_p = np.cos(x / L ** ((i - 1) / d_e))\n    _ = plt.plot(x, w_p, label=f\"i={i}\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nというわけで、この埋めこみは各成分ごとに異なる周波数での単語を埋め込むようです。これにより、短いコンテキストの中での位置も同時に考慮できるのかな。"
  },
  {
    "objectID": "posts/understanding-attention.html#マルコフモデルの学習",
    "href": "posts/understanding-attention.html#マルコフモデルの学習",
    "title": "Attentionが何をやっているのか理解しよう",
    "section": "マルコフモデルの学習",
    "text": "マルコフモデルの学習\nまず簡単なモデルで天気を生成してみます。次の日の天気は、前の日の天気にもとづいて確率的に決まることにしましょう。🌧️・☁️・☀️がマルチバイト文字であることに注意して、以下のように実装します。\n\n\nCode\nimport dataclasses\n\n_GEN = np.random.Generator(np.random.PCG64(20230508))\n_MARKOV = {\n    \"\": [0.3, 0.4, 0.3],\n    \"🌧️\": [0.6, 0.3, 0.1],\n    \"☁️\": [0.3, 0.4, 0.3],\n    \"☀️\": [0.2, 0.3, 0.5],\n}\n\ndef markov(prev: str) -&gt; str:\n    prob = _MARKOV[prev[-2:]]\n    return prev + _GEN.choice(WEATHERS, p=prob)\n\n\ndef generate(f, n: int, init: str = \"\"):\n    value = init\n    for _ in range(n):\n        value = f(value)\n    return value\n\n\n@dataclasses.dataclass\nclass Dataset:\n    weathers: list[str]\n    embeddings: jax.Array\n    next_weather_indices: jax.Array\n    \n    def __len__(self) -&gt; int:\n        return len(self.weathers)\n\n\ndef make_dataset(f, seq_len, size) -&gt; Dataset:\n    w_list, e_list, nw_list = [], [], []\n    for _ in range(size):\n        weathers = generate(f, seq_len + 1)\n        e = jnp.array(get_embedding(weathers[:-2]))\n        w_list.append(weathers)\n        e_list.append(e)\n        nw_list.append(WEATHERS.index(weathers[-2:]))\n    return Dataset(w_list, jnp.stack(e_list), jnp.array(nw_list))\n\n\ngenerated = generate(markov, 10)\ngenerated, get_embedding(generated)\n\n\n('🌧️🌧️🌧️☀️🌧️☁️🌧️🌧️☀️☀️',\n array([[1.  , 1.  , 1.  , 0.  , 1.  , 0.  , 1.  , 1.  , 0.  , 0.  ],\n        [0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  ],\n        [0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 1.  , 1.  ],\n        [0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 ]]))\n\n\nこんな感じです。いま、次の日の天気だけ予測したいので、モデルの出力は集合{🌧️・☁️・☀️}上での確率分布が適切でしょう。Attentionは長さ\\(T\\)の埋め込み列に対して長さ\\(d_\\textrm{out} \\times T\\)の行列をかえします。なので、\\(d_\\textrm{out} = 3\\)とし、Attentionの出力\\(\\tilde{V}\\)に対してソフトマックス関数を適用し、\\(P_t = \\textrm{softmax}(\\tilde{V}[:, t])\\)とします。このとき、\\(P_t\\)の各要素が次の日🌧️・☁️・☀️になる確率を表すとして、モデル化します。これを、対数尤度の和\\(\\sum_t \\log P_t(\\textrm{next weather})\\)を最大化するように学習しましょう。学習のコードを定義します。\n\n\nCode\nfrom typing import Callable\n\nimport optax\n\n\ndef attn_neglogp(model: eqx.Module, seq: jax.Array, next_w: jax.Array) -&gt; jax.Array:\n    batch_size = seq.shape[0]\n    tilde_v = jax.vmap(model)(seq)  # B x OUT x SEQ_LEN\n    logp = jax.nn.log_softmax(tilde_v, axis=1)  # B x OUT x SEQ_LEN\n    logp_masked = logp * jax.nn.one_hot(next_w, num_classes=3).reshape(-1, 3, 1)\n    return -jnp.mean(jnp.sum(logp_masked.reshape(batch_size, -1), axis=-1))\n\ndef train(\n    n_total_epochs: int,\n    minibatch_size: int,\n    model: eqx.Module,\n    ds: Dataset,\n    test_ds: Dataset,\n    key: jax.Array,\n    learning_rate: float = 1e-2,\n    loss_fn: Callable[[eqx.Module, jax.Array, jax.Array], jax.Array] = attn_neglogp,\n) -&gt; tuple[eqx.Module, jax.Array, list[float], list[float]]:\n    n_data = len(ds)\n    optim = optax.adam(learning_rate)\n\n    @eqx.filter_jit\n    def train_1step(\n        model: eqx.Module,\n        seq: jax.Array,\n        next_w: jax.Array,\n        opt_state: optax.OptState,\n    ) -&gt; tuple[jax.Array, eqx.Module, optax.OptState]:\n        loss, grads = eqx.filter_value_and_grad(loss_fn)(model, seq, next_w)\n        updates, opt_state = optim.update(grads, opt_state)\n        model = eqx.apply_updates(model, updates)\n        return loss, model, opt_state\n\n    opt_state = optim.init(model)\n    n_optim_epochs = n_data // minibatch_size\n    loss_list, eval_list = [], []\n    for epoch in range(n_total_epochs // n_optim_epochs):\n        key, perm_key = jax.random.split(key)\n        indices = jax.random.permutation(perm_key, n_data, independent=True)\n        for _ in range(n_optim_epochs):\n            e = ds.embeddings[indices]\n            next_w = ds.next_weather_indices[indices]\n            loss, model, opt_state = train_1step(model, e, next_w, opt_state)\n            loss_list.append(loss.item())\n            test_loss = jax.jit(loss_fn)(\n                model,\n                test_ds.embeddings,\n                test_ds.next_weather_indices,\n            )\n            eval_list.append(test_loss.item())\n    return model, key, loss_list, eval_list\n\n\nこれを実際に走らせてみます。適当に、Attentionの次元を6、天気列の長さを10にします。\n\n\nCode\nD_ATTN = 6\nSEQ_LEN = 10\nkey = jax.random.PRNGKey(1234)\nmodel = MaskedAttention(4, D_ATTN, 3, key)\nds = make_dataset(markov, SEQ_LEN, 1000)\ntest_ds = make_dataset(markov, SEQ_LEN, 1000)\nmodel, key, loss_list, eval_list = train(500, 100, model, ds, test_ds, key, 1e-2)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on Markov model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\n@jax.jit\ndef accuracy(model: eqx.Module, seq: jax.Array, next_w: jax.Array) -&gt; float:\n    tilde_v = jax.vmap(model)(seq)  # B x OUT x SEQ_LEN\n    inferred = jnp.argmax(tilde_v[:, :, 0], axis=1)\n    n_correct = jnp.sum(inferred == next_w)\n    return n_correct / seq.shape[0]\n\nf\"Accuracy: {accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.49800002574920654'\n\n\n\n\n\n\n\n\n\n100エポックあたりでロスが落ちなくなっているので収束はしていそうです。実際に何を学習したのか確認してみましょう。とりあえず天気を生成してみます。こんなもの見ても何もわからないのですが、生成の流れを確認しておくにはいいかなと。ビームサーチが使われることが多いようですが、面倒なので今回はもっと簡単な方法を使います。☁️からスタートして、カテゴリカル分布から次の天気をサンプルし、どんどん足していくことにします。\n\n\nCode\ndef generate_from_model(\n    model: eqx.Module,\n    key: jax.Array,\n    seq_len: int,\n    init: str = \"☁️\",\n) -&gt; tuple[str, jax.Array]:\n    @jax.jit\n    def step(\n        model: eqx.Module,\n        seq: jax.Array,\n        key: jax.Array,\n    ) -&gt; tuple[jax.Array, jax.Array]:\n        sample_key, key = jax.random.split(key)\n        tilde_v = model(seq)  # 3 x len(seq)\n        sampled = jax.random.categorical(key=sample_key, logits=tilde_v[:, 0])\n        return sampled, key\n\n    generated = init\n    for _ in range(seq_len):\n        next_w, key = step(model, get_embedding(generated), key)\n        generated += WEATHERS[next_w.item()]\n    return generated, key\n\n\ngenerated, key = generate_from_model(model, key, 20)\ngenerated\n\n\n'☁️🌧️🌧️☀️🌧️☁️🌧️☀️☀️☀️☁️☁️☀️☀️☀️☀️☀️☁️☁️☀️☁️'\n\n\nこんな感じになりました。当たり前ですがこれを見たところで何もわからないですね。次に、テストデータ中の適当なデータに対しAttentionの中身を可視化してみます。\n\n\nCode\n@jax.jit\ndef get_attn(model: eqx.Module, seq: jax.Array) -&gt; jax.Array:\n    q = model.w_q @ seq + model.b_q\n    k = model.w_k @ seq + model.b_k\n    score = causal_mask(q.T @ k) / model.sqrt_d_attn\n    return jax.nn.softmax(score, axis=-1)\n\n\ndef visualize_attn(ax, model: eqx.Module, ds: Dataset, index: int = 0) -&gt; None:\n    attn = np.array(get_attn(model, ds.embeddings[index]))\n    im = ax.imshow(attn)\n    ax.set_xticks(\n        np.arange(10),\n        labels=[ds.weathers[index][i * 2] for i in range(10)],\n        fontname=\"Noto Emoji\",\n    )\n    ax.set_yticks(\n        np.arange(10),\n        labels=[ds.weathers[index][i * 2] for i in range(10)],\n        fontname=\"Noto Emoji\",\n    )\n    for i in [np.argmin(attn), np.argmax(attn)]:\n        # Show min and max values\n        im.axes.text(i % 10, i // 10, f\"{attn.flatten()[i]:.1f}\", color=\"gray\")\n\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\nvisualize_attn(ax1, model, test_ds, 1)\nvisualize_attn(ax2, model, test_ds, 2)\n\n\n\n\n\n\n\n\n\nmatplotlibでカラー絵文字が使えなかったのでモノクロの絵文字にしました。というわけで、 1. 直前の天気→直前の天気 のAttentionが最も大きい 2. 他の日の天気→直前の天気 のAttentionも大きい 3. 他はほとんど関係ない\nといったことがわかります。マルコフモデルから生成した天気列を学習させたので、1は当たり前ですよね。2の他の日の天気→直前の天気の関係も実際はいらないのですが、注意されているようです。"
  },
  {
    "objectID": "posts/understanding-attention.html#独立に発生した過去の複数の事象に依存して将来の出来事が決まる場合",
    "href": "posts/understanding-attention.html#独立に発生した過去の複数の事象に依存して将来の出来事が決まる場合",
    "title": "Attentionが何をやっているのか理解しよう",
    "section": "独立に発生した過去の複数の事象に依存して将来の出来事が決まる場合",
    "text": "独立に発生した過去の複数の事象に依存して将来の出来事が決まる場合\n次に、もう少し複雑なデータを学習させてみましょう。今度は、以下のような方法で11日ぶんの天気を生成します。 1. 1日目、4日目、8日目の天気を独立に生成する 2. 2,3日目の天気を1日目の天気を初期状態とするマルコフ連鎖により生成する。5,6,7,9,10日目の天気についても、4日目・8日目の天気にもとづいて同様に生成する。 3. 11日目の天気を1日目、4日目、8日目の天気から確率的に生成する。\nこれを学習できるか試してみましょう。\n\n\nCode\ndef _make_table() -&gt; dict[str, list[float]]:\n    candidates = []\n    for i in range(1, 9):\n        for j in range(1, 9):\n            for k in range(1, 9):\n                if i + j + k == 10:\n                    candidates.append((i, j, k))\n    table = {}\n    for i in WEATHERS:\n        for j in WEATHERS:\n            for k in WEATHERS:\n                table[i + j + k] = [p / 10 for p in _GEN.choice(candidates)]\n    return table\n\n_ONE_FOUR_8_TABLE = _make_table()\n\ndef one_four_8(prev: str) -&gt; str:\n    length = len(prev) // 2\n    if length == 10:\n        p = _ONE_FOUR_8_TABLE[prev[0: 2] + prev[6: 8] + prev[14: 16]]\n        return prev + _GEN.choice(WEATHERS, p=p)\n    elif length == 4 or length == 8:\n        return prev + _GEN.choice(WEATHERS, p=_MARKOV[\"\"])\n    else:\n        return markov(prev)\n    \ngenerate(one_four_8, 11)\n\n\n'☀️☀️☀️🌧️🌧️🌧️🌧️☁️☁️🌧️☁️'\n\n\nこんな感じですね。では学習させましょう。さっきよりも少しデータが複雑なので、サンプルの数を増やしてみます。\n\n\nCode\nmodel = MaskedAttention(4, D_ATTN, 3, key)\nds = make_dataset(one_four_8, SEQ_LEN, 5000)\ntest_ds = make_dataset(one_four_8, SEQ_LEN, 1000)\nmodel, key, loss_list, eval_list = train(500, 100, model, ds, test_ds, key, 1e-2)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on 1-4-8 model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\nvisualize_attn(ax1, model, test_ds, 1)\nvisualize_attn(ax2, model, test_ds, 2)\n\nf\"Accuracy: {accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.3890000283718109'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n損失は小さくなっていますがAccuracyが悪くまたAttentionの出方も微妙ですね。一応1・48日目にも注意がいっていますが、先ほどの実験と同じく最後の日の注意が大きめに出ていますね。"
  },
  {
    "objectID": "posts/understanding-attention.html#attentionいらないんじゃ",
    "href": "posts/understanding-attention.html#attentionいらないんじゃ",
    "title": "Attentionが何をやっているのか理解しよう",
    "section": "Attentionいらないんじゃ…",
    "text": "Attentionいらないんじゃ…\n勘のいい読者の方はお気づきかと思いますが、ここまで学習させた2つの天気列を表現するのに、self-attentionのような小難しいものはいらないですよね。最初のものは前日（10日目)の天気、次のやつは1・4・8日目から11日目の天気が決定されるため、入力された天気列の内部相関がタスクに一切関係ないからです。というわけで、線形モデル+ソフトマックス(いわゆるmultinomial logistic regressionというやつ)で学習してみましょう。\n\n\nCode\nclass LinearModel(eqx.Module):\n    w: jax.Array\n    b: jax.Array\n\n    def __init__(self, d_in: int, d_out: int, key: jax.Array) -&gt; None:\n        w_key, b_key = jax.random.split(key)\n        self.w = jax.random.normal(w_key, (d_out, d_in))\n        self.b = jax.random.normal(b_key, (d_out,))\n\n    def __call__(self, seq: jax.Array) -&gt; jax.Array:\n        return self.w @ seq.flatten() + self.b\n\n\ndef linear_neglogp(model: eqx.Module, seq: jax.Array, next_w: jax.Array) -&gt; jax.Array:\n    logp = jax.nn.log_softmax(jax.vmap(model)(seq), axis=1)  # B x OUT\n    logp_masked = logp * jax.nn.one_hot(next_w, num_classes=3)\n    return -jnp.mean(jnp.sum(logp_masked, axis=1))\n\n\nmodel = LinearModel(4 * SEQ_LEN, 3, key)\nmodel, key, loss_list, eval_list = train(\n    500, 100, model, ds, test_ds, key, 1e-2, linear_neglogp\n)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on 1-4-8 model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\n\n@jax.jit\ndef linear_accuracy(model: eqx.Module, seq: jax.Array, next_w: jax.Array) -&gt; float:\n    tilde_v = jax.vmap(model)(seq)  # B x OUT\n    inferred = jnp.argmax(tilde_v, axis=1)\n    n_correct = jnp.sum(inferred == next_w)\n    return n_correct / seq.shape[0]\n\n\nf\"Accuracy: {linear_accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.4230000078678131'\n\n\n\n\n\n\n\n\n\n普通にこっちのほうが良さそうですね…。では、Attentionはどういう時に役に立つのでしょうか。\n\n(MLP等と比較) パラメータ数をトークン列の長さ\\(L\\)に依存させたくないとき\n\nAttentionではパラメータの数が \\((d_\\textrm{in} + 1)(2d_\\textrm{attn} + d_\\textrm{out})\\)になるのに対し、線形モデルでは\\((d_\\textrm{in}L + 1)d_\\textrm{out}\\)になることに注意しましょう。線形モデルではトークン列の長さに比例してパラメータの数が増えてしまいます。ただし、Attentionでは\\(q^\\top k\\)を保持するのに\\(O(L^2)\\)のメモリ使用量が必要な点に注意が必要です。もっとも、Self-attention Does Not Need \\(O(n^2)\\) Memoryでは効率的な\\(O(\\sqrt{L})\\)の実装が示されており、まあ何とかなるといえばなるようですが、それでも単純なRNNやCNNより遅くなります。\n\n(RNN・CNN等と比較) トークン系列に長期間の依存関係が存在する場合\n\nCNNやRNNと比べたとき、\\(q^\\top k\\)により一層のレイヤーで任意のトークン間の依存関係が表現できるのはAttentionの利点と言えるでしょう。ただし\\(q^\\top k[i, j]\\)は2つの埋め込み\\(e[i], e[j]\\)に対して線形な演算のみで得られるため、この2つの埋め込みが何か非線形な関数を介して依存している場合、その関係は一層のAttentionでは表現できません。\nというわけで、一層の線形レイヤーと比較すると、パラメタ数が\\(L\\)に依存しないというメリットはあるものの、実際Attentionを使うともっと色々な関数が学習できるのかというとよくわかりません。もう少し試してみます。"
  },
  {
    "objectID": "posts/understanding-attention.html#隠れ変数がある場合",
    "href": "posts/understanding-attention.html#隠れ変数がある場合",
    "title": "Attentionが何をやっているのか理解しよう",
    "section": "隠れ変数がある場合",
    "text": "隠れ変数がある場合\n以下の方法で天気列を生成します。過去\\(n\\)日間の天気を見て、🌧️の登場回数が\\(k\\)回なら、次の日の天気が🌧️になる確率を\\(\\frac{n - k}{2n}\\)とします。☁️、☀️についても同様に確率を割り当てます。この方法で大量に天気列を生成して適当な部分列からデータセットを作ります。\n\n\nCode\nfrom functools import partial\n\ndef ndays_model(prev: str, n: int = 10) -&gt; str:\n    counts = np.zeros(3)\n    prev_n = prev[-2 * n: ]\n    for i in range(n):\n        prev_w_i = prev_n[i * 2: i * 2 + 2]\n        counts[WEATHERS.index(prev_w_i)] += 1\n    prob = (n - counts) / (n * 2)\n    return prev + _GEN.choice(WEATHERS, p=prob)\n\ngenerate(ndays_model, 100, generate(markov, 10))                \n\n\n'☀️☀️☀️🌧️☀️☀️☁️☀️☁️☀️☀️🌧️☁️🌧️☁️☁️🌧️🌧️☁️☁️☀️🌧️☀️☀️☀️☀️☁️☁️🌧️🌧️🌧️🌧️🌧️☁️☀️☁️☀️☀️☀️☀️🌧️🌧️🌧️🌧️🌧️☀️☁️🌧️☁️☀️☀️☀️☁️☁️☀️🌧️☁️🌧️☁️☀️☀️☁️☁️☁️☁️☁️🌧️☁️🌧️☀️🌧️🌧️☀️☀️☁️🌧️☀️☀️🌧️☀️☁️☀️☀️☁️☁️☀️☀️🌧️🌧️🌧️☀️🌧️☁️🌧️🌧️☀️☀️🌧️☁️☀️🌧️🌧️☀️🌧️☀️☁️🌧️☀️☁️☁️'\n\n\n生成された天気列はこんな感じです。まず線形モデルを10日モデルで学習させます。この場合隠れ変数はありません。\n\n\nCode\ndef make_ndays_dataset(seq_len, size, n: int = 10) -&gt; Dataset:\n    weathers = generate(partial(ndays_model, n=n), seq_len * size * 2, generate(markov, n * 2))\n    w_list, e_list, nw_list = [], [], []\n    for _ in range(size):\n        start = _GEN.integers(0, seq_len * size * 2 - 11)\n        w = weathers[start * 2 : start * 2 + (seq_len + 1) * 2]\n        e = jnp.array(get_embedding(w[:-2]))\n        w_list.append(w)\n        e_list.append(e)\n        nw_list.append(WEATHERS.index(w[-2:]))\n    return Dataset(w_list, jnp.stack(e_list), jnp.array(nw_list))\n\n\nds = make_ndays_dataset(SEQ_LEN, 5000, n=10)\ntest_ds = make_ndays_dataset(SEQ_LEN, 1000, n=10)\n\nmodel = LinearModel(4 * SEQ_LEN, 3, key)\nmodel, key, loss_list, eval_list = train(\n    500, 100, model, ds, test_ds, key, 1e-2, linear_neglogp\n)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on 10days model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nf\"Accuracy: {linear_accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.3760000169277191'\n\n\n\n\n\n\n\n\n\n次にSelf-Attentionを学習させます。\n\n\nCode\nmodel = MaskedAttention(4, D_ATTN, 3, key)\nmodel, key, loss_list, eval_list = train(500, 100, model, ds, test_ds, key, 1e-2)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on 10days model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\nvisualize_attn(ax1, model, test_ds, 1)\nvisualize_attn(ax2, model, test_ds, 2)\n\nf\"Accuracy: {accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.3320000171661377'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nほとんど変わりませんね。次に隠れ変数があるモデルとして、先程の10日モデルを15日モデルにしてみます。まず、線形モデルを学習させます。\n\n\nCode\nds = make_ndays_dataset(SEQ_LEN, 5000, n=15)\ntest_ds = make_ndays_dataset(SEQ_LEN, 1000, n=15)\n\nmodel = LinearModel(4 * SEQ_LEN, 3, key)\nmodel, key, loss_list, eval_list = train(\n    500, 100, model, ds, test_ds, key, 1e-2, linear_neglogp\n)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on 15days model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nf\"Accuracy: {linear_accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.36900001764297485'\n\n\n\n\n\n\n\n\n\n次にSelf-Attentionを学習させます。\n\n\nCode\nmodel = MaskedAttention(4, D_ATTN, 3, key)\nmodel, key, loss_list, eval_list = train(500, 100, model, ds, test_ds, key, 1e-2)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on 15days model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\nvisualize_attn(ax1, model, test_ds, 1)\nvisualize_attn(ax2, model, test_ds, 2)\n\nf\"Accuracy: {accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.3140000104904175'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nこの場合も結局Self-Attentionのほうが悪くなってしまいました。悲しい。"
  },
  {
    "objectID": "posts/understanding-attention.html#非線形ならどうか",
    "href": "posts/understanding-attention.html#非線形ならどうか",
    "title": "Attentionが何をやっているのか理解しよう",
    "section": "非線形ならどうか",
    "text": "非線形ならどうか\n隠れ変数があっても線形モデルの方が性能がいいということは、たぶん線形で解けるタスクではどうあがいても線形モデルに勝てないということなのでしょう。なのでもっと難しいデータを考えます。 10日ぶんの天気列の🌧️、☁️、☀️にそれぞれ0, 1, 2を割り当てて作ったベクトルを\\(y\\)とします。また、\\(\\beta = (0, 1, 2, 3, 2, 1, 0, 1, 2, 3)^\\top\\)とします。このとき、次の日の天気を\\((y(2 - y)\\cdot \\beta) \\mod 3\\)とします。これだと芸がないので一応他の天気も2%くらいの確率で出るようにしておきます。\n\n\nCode\n_BETA = np.tile([0, 1, 2, 3, 2, 1], (10,))\n\ndef dotmod_model(prev: str, n: int =10) -&gt; str:\n    y = np.zeros(n, dtype=int)\n    prev_n = prev[-2 * n:]\n    for i in range(n):\n        prev_w_i = prev_n[i * 2: i * 2 + 2]\n        y[i] = WEATHERS.index(prev_w_i) + 1\n    prob = [0.02, 0.02, 0.02]\n    prob[np.dot(y * (2 - y), _BETA[: n]) % 3] = 0.96\n    return prev + _GEN.choice(WEATHERS, p=prob)\n\ndef make_dotmod_dataset(seq_len, size, n: int = 10) -&gt; Dataset:\n    weathers = generate(partial(dotmod_model, n=n), seq_len * size * 2, generate(markov, n * 2))\n    w_list, e_list, nw_list = [], [], []\n    for _ in range(size):\n        start = _GEN.integers(0, seq_len * size * 2 - 11)\n        w = weathers[start * 2 : start * 2 + (seq_len + 1) * 2]\n        e = jnp.array(get_embedding(w[:-2]))\n        w_list.append(w)\n        e_list.append(e)\n        nw_list.append(WEATHERS.index(w[-2:]))\n    return Dataset(w_list, jnp.stack(e_list), jnp.array(nw_list))\n\ngenerate(dotmod_model, 100, generate(markov, 10))\n\n\n'🌧️🌧️☁️☁️☁️☀️🌧️🌧️☁️☀️☀️☁️🌧️☀️☁️☁️☁️☁️☀️🌧️☀️🌧️☁️☀️☀️☀️☁️☁️☁️☀️☁️🌧️🌧️☀️🌧️☁️🌧️☁️☁️☁️☀️☀️☁️☁️☀️☁️🌧️🌧️☀️🌧️☁️🌧️☁️☁️☁️☁️☀️☁️☁️☀️☁️🌧️🌧️☀️🌧️☁️🌧️☁️☁️☁️☀️☀️☁️☁️☀️☁️🌧️🌧️☀️🌧️☁️🌧️☁️☁️🌧️☀️☁️☀️☁️🌧️🌧️☀️☀️☀️☁️🌧️☀️☁️☁️☁️☁️☀️🌧️☀️☀️☁️🌧️☁️☁️☁️'\n\n\nぱっと見ではまるで法則性がわからない天気列が生成できました。これを学習させてみましょう。まずは線形モデルです。\n\n\nCode\nds = make_dotmod_dataset(SEQ_LEN, 5000)\ntest_ds = make_dotmod_dataset(SEQ_LEN, 1000)\n\nmodel = LinearModel(4 * SEQ_LEN, 3, key)\nmodel, key, loss_list, eval_list = train(\n    500, 100, model, ds, test_ds, key, 1e-2, linear_neglogp\n)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on Dotmod model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nf\"Accuracy: {linear_accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.6990000605583191'\n\n\n\n\n\n\n\n\n\n意外に7割近く正解していますね。次にSelf-Attentionを学習してみます。\n\n\nCode\nmodel = MaskedAttention(4, D_ATTN, 3, key)\nmodel, key, loss_list, eval_list = train(500, 100, model, ds, test_ds, key, 1e-2)\nplt.plot(loss_list, label=\"Training Loss\")\nplt.plot(eval_list, label=\"Test Loss\")\nplt.title(\"Trained on Dotmod model\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Negative Log Likelihood\")\nplt.legend()\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\nvisualize_attn(ax1, model, test_ds, 1)\nvisualize_attn(ax2, model, test_ds, 2)\n\nf\"Accuracy: {accuracy(model, test_ds.embeddings, test_ds.next_weather_indices).item()}\"\n\n\n'Accuracy: 0.4480000138282776'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nまたしてもSelf-Attentionのほうがだめという結果になりました。"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html",
    "href": "posts/understanding-what-makes-rl-difficult.html",
    "title": "より良い問題設計へ向けて： 何が強化学習を難しくするのかを理解しよう",
    "section": "",
    "text": "強化学習苦手の会 Advent Calendar 2020 20日目 # 1. はじめに\n強化学習は、逐次的に意思決定する問題を定義するためのフレームワークです。 従来は大変だった1 ニューラルネットワークの訓練が簡単になったことや、 Alpha GOなど深層強化学習(Deep RL)の成功を背景に、対話システム・経済 など、様々なアプリケーションで強化学習の利用が試みられているように思います。 僕個人としても、強化学習は汎用的で面白いツールだと思うので、将来的には色々な応用分野で広く活用されるといいな、と思います。\n一方で、強化学習を何か特定の問題に応用してみよう、という場面では、 その汎用性ゆえかえってとっつきにくい・扱いづらい面があるように思います。 実際に、苦手の会などで応用研究をされている方から、 - 問題を定義するのがそもそも大変 - 色々な手法があって、何がなんだかよくわからない\nなどの意見を観測できました。\nでは応用研究に対する「ツール」として強化学習を扱う上で何が大事なのだろう、と考えたとき、 僕は簡単な問題を設計することこそが大事だという仮説に思いいたりました。 簡単な問題を設計するためには、強化学習の中でもどういう問題が難しいのか、 ということをきちんと理解しておく必要があるように思います。\nそこでこのブログ記事では、強化学習の中でも「難しい問題」がどういうものなのか、 そういう問題はなぜ難しいのかについて、例を通してなるべく直感的に説明することを試みます。 強化学習の難しさがわかった暁には、きっと - そもそも強化学習を使わないという選択ができるし、 - なるべく簡単に解けるような強化学習の問題を設計できるし、 - 問題に合わせて手法を選択できる\nことでしょう。\n記事の構成として、強化学習の難しさについて「場合分け」を行い、 - MDPを解くことの難しさ - データを収集することの難しさ\nという2つの観点から整理していきます。\n前提知識について、初心者の方でも読めるように、 強化学習についての知識についてはなるべく記事の中で補足します。 しかし、すごく雑に書くので、詳細はReinforcement Learning: An Introduction などの教科書を参照されるといいと思います。 また、プログラムを見たほうがイメージしやすい（方もいる）かと思ってPythonのコード例をたまに出しています。 コード例では、\\(\\sum_{s} f(s) g(s, a)\\)のようにテンソルの適当な軸で掛け算して足し込む演算に numpy.einsum を多用しているので、知っていたほうが読みやすいかもしれません。"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html#vpicdot-vを評価する難しさ",
    "href": "posts/understanding-what-makes-rl-difficult.html#vpicdot-vを評価する難しさ",
    "title": "より良い問題設計へ向けて： 何が強化学習を難しくするのかを理解しよう",
    "section": "2.1: \\(V^\\pi\\cdot V^*\\)を評価する難しさ",
    "text": "2.1: \\(V^\\pi\\cdot V^*\\)を評価する難しさ\nでは、もう少し難しいMDPを考えてみましょう。\n\n\nCode\nmdp3 = ChainMDP(\n    [[1.0, 0.0], [0.8, 1.0], [1.0, 0.9]], [[0.0, 0.0], [0.5, 0.0], [0.0, 1.0]]\n)\n_ = mdp3.show(\"MDP3\")\n\n\n\n\n\n\n\n\n\n今度は、State 1で右に、State 2で左に行けば良さそうです。 \\[\n\\begin{aligned}\nV^* (1) = 0.5 + \\gamma (0.1 * V^*(1) + 0.9 * V^*(2)) \\\\\nV^* (2) = 1.0 + \\gamma (0.8 * V^*(1) + 0.2 * V^*(2))\n\\end{aligned}\n\\]\n先ほどの問題と違って1も2も吸引状態ではないので、\\(V(1)\\)と\\(V(2)\\)がお互いに依存する面倒な 方程式が出てきてしまいました。 このようなループの存在が、強化学習を難しくしている要素の一つです。\nとはいえ、コンピューターで数値的に解くのは簡単です。 状態\\(s\\)にいて、あと\\(n\\)回行動できる時の価値関数を\\(V_n^\\pi(s)\\)と書きます。 任意の\\(s\\)について、\\(V_0^\\pi(s) = 0\\)です（1回も行動できないので!）。 \\(V_i^\\pi\\) から \\(V_{i + 1}^\\pi\\) を求めるには、1ステップだけ先読みすればいいので、 \\[\nV_{i + 1}^\\pi(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s) \\left( r(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s'|s, a) V_{i}^\\pi(s') \\right)\n\\] で計算できます。\\(\\gamma &lt; 1\\)によりこの反復計算は収束し、\\(V^\\pi\\) が求まります。 実際にプログラムで書いてみましょう。\n\n\nCode\nMAX_ITER_V_PI: int = int(1e5)\n\n\ndef v_pi(\n    r: Array2,\n    p: Array3,\n    pi: Array2,\n    gamma: float,\n    epsilon: float,\n) -&gt; Tuple[Array1, int]:\n    v = np.zeros(r.shape[0])  # Vπ\n    r_pi = np.einsum(\"sa,sa-&gt;s\", pi, r)  # |S|, πを使ったときに貰う報酬のベクトル\n    p_pi = np.einsum(\"saS,sa-&gt;sS\", p, pi)  # |S| x |S|, πを使ったときの状態遷移確率\n    for n_iter in range(MAX_ITER_V_PI):\n        v_next = r_pi + gamma * np.einsum(\"s,sS\", v, p_pi.T)\n        if np.all(np.absolute(v_next - v) &lt; epsilon):\n            return v_next, n_iter + 1\n        v = v_next\n    # 理論的には必ず収束するので、バグ予防\n    raise RuntimeError(\"Policy Evaluation did not converge &gt;_&lt;\")\n\n\npi_star = np.array([[1.0, 0.0], [1.0, 0.0], [0.0, 1.0]])\nv_star_mdp3, n_iter = v_pi(mdp3.r, mdp3.p, pi_star, gamma=0.9, epsilon=1e-4)\nprint(f\"反復回数: {n_iter}\")\nprint(\" \".join([f\"V({i}): {v:.3}\" for i, v in enumerate(v_star_mdp3)]))\n\n\n反復回数: 86\nV(0): 6.49 V(1): 7.21 V(2): 7.51\n\n\n86回この計算を反復した後、なんかそれっぽい数字が出てきました。 この反復回数は、何によって決まるのでしょうか？\n任意の \\(s\\) について \\(|V_{i+1}^\\pi(s) - V_i^\\pi(s)| &lt; \\epsilon\\) なら計算終わり、とします4。 \\(V_n^\\pi(s)\\)は「あと\\(n\\)ステップ行動できる時の状態価値の期待値」なので、\\(i\\) ステップ目にもらった報酬を \\(R_i\\)とすると、 \\[\nV_n^\\pi(s) = \\mathbb{E}_{s, \\pi} \\left[ R_1 + \\gamma R_2 + \\gamma^2 R_3 + ... \\right]\n\\] と書けます。 なので、報酬の範囲が\\(0 \\leq R_t &lt; R_\\textrm{max}\\)だと仮定すると、 \\(\\gamma^{k - 1} R_\\textrm{max} &lt; \\epsilon\\)ならこの数値計算が収束することがわかります。 簡単のため\\(R_\\textrm{max}=1\\)としてみると、\\(k\\)が満たすべき条件は \\[\n\\gamma^{k-1} &lt; \\epsilon\n\\Leftrightarrow\nk &lt; \\frac{\\log\\epsilon}{\\log\\gamma} + 1\n\\] となります。 コードの中で \\(\\gamma = 0.9, \\epsilon = 0.0001\\) としたので、たかだか89回の反復で収束することがわかります。 実験結果では86回だったので、だいたい同じくらいですね。\nよって、\\(V^\\pi\\)を反復法により評価した時、その反復回数は報酬・\\(\\epsilon\\)・\\(\\gamma\\)に依存することがわかりました。 報酬と\\(\\epsilon\\)には\\(\\log\\)のオーダーでしか依存しないのに対し、\\(\\gamma\\)に対しては \\(O((-\\log\\gamma)^{-1})\\)のオーダーで依存していることに注意してください。 試しに、\\(\\epsilon=0.0001\\)の時の\\(\\frac{\\log\\epsilon}{\\log\\gamma}\\)をプロットしてみましょう。\n\n\nCode\n#hide_input\n_, ax = plt.subplots(1, 1)\nx = np.logspace(-0.1, -0.001, 1000)\ny = np.log(1e-4) / np.log(x)\nax.set_xlabel(\"γ\", fontsize=16)\nax.set_ylabel(\"log(ε)/log(γ)\", fontsize=16)\n_ = ax.plot(x, y, \"b-\", lw=3, alpha=0.7)\n\n\n\n\n\n\n\n\n\nこのように、\\(\\gamma\\)が大きくなると一気に反復回数が増えることがわかります。 また、この数値計算が収束した時、真の\\(V^\\pi\\)との差が \\[\n\\begin{aligned}\nV^\\pi(s) - V_k^\\pi(s) &= \\mathbb{E}_{s, \\pi} \\left[ \\gamma^k R_{k + 1} + \\gamma^{k + 1} R_{k + 2} ... \\right] \\\\\n&&lt; \\frac{\\gamma^k R_\\textrm{max}}{1 - \\gamma} &lt; \\frac{\\gamma \\epsilon}{1 - \\gamma}\n\\end{aligned}\n\\] で抑えられることもわかります。\n次は、いきなり\\(V^*\\)を求めてみましょう。 \\(V^\\pi\\)を求めた時と同じように、状態\\(s\\)にいて、 あと\\(n\\)回行動できる時の最適価値関数を\\(V_n^*(s)\\)と書きます。 先ほどと同様に、\\(V_i^*\\)から1ステップ先読みして\\(V^*_{i + 1}\\)を求めます。 残り\\(i + 1\\)ステップある時、 \\(r(s, a) + \\sum_{s' \\in \\mathcal{S}} P(s'|s, a) V_i^*(s)\\) が最大になるような行動\\(a\\)を選ぶのが最適です。 ですから、\\(V^*_{i + 1}\\)は \\[\nV_{i + 1}^*(s) = \\max_a \\left( r(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s'|s, a) V_{i}^\\pi(s') \\right)\n\\] で求められます。 さっきより簡単な式になってしまいました。 プログラムで書いてみます。\n\n\nCode\nMAX_ITER_VI: int = int(1e6)\n\n\ndef value_iteration(\n    r: Array2,\n    p: Array3,\n    gamma: float,\n    epsilon: float,\n) -&gt; Tuple[Array1, int]:\n    v = np.zeros(p.shape[0])\n    for n_iter in range(MAX_ITER_VI):\n        # これ↓はQ Valueとも言います\n        r_plus_gamma_pv = r + gamma * np.einsum(\"saS,S-&gt;sa\", p, v)\n        v_next = r_plus_gamma_pv.max(axis=1)\n        if np.all(np.absolute(v_next - v) &lt; epsilon):\n            return v_next, n_iter + 1\n        v = v_next\n    raise RuntimeError(\"Value Iteration did not converge &gt;_&lt;\")\n\n\nv_star_mdp3_vi, n_iter = value_iteration(mdp3.r, mdp3.p, 0.9, 1e-4)\nprint(f\"反復回数: {n_iter}\")\nprint(\" \".join([f\"V({i}): {v:.3}\" for i, v in enumerate(v_star_mdp3_vi)]))\n\n\n反復回数: 86\nV(0): 6.49 V(1): 7.21 V(2): 7.51\n\n\n先程と同じく、86回の反復で\\(V^*\\)が求まりました。 この反復回数も、先ほどの\\(V^\\pi\\)と同じように\\(\\gamma,\\epsilon\\)を用いて抑えられます。\nしかし、\\(\\epsilon\\)は人手で設定するパラメタです。 最適方策が求まれば\\(V^*\\)は大して正確でなくとも困らないという場合は、もっと\\(\\epsilon\\)を大きくして、 計算を早く終わらせたい気がします。 では、「どんな場合に\\(\\epsilon\\)を大きくできるか」を考えてみましょう。\n簡単のため、 \\(Q^\\pi(s, a) = r(s, a) + \\gamma \\sum_{s' in \\mathcal{S}} P(s'|s, a) V^\\pi(s')\\) (QはQualityのQらしい)を導入します。 残り\\(k\\)ステップある時の最適行動を\\(a_k^* = \\textrm{argmax}_a Q_k^*(s, a)\\)とします。 すると、\\(k+1\\)ステップ目以降の割引報酬和は \\(\\frac{\\gamma^{k}R_\\textrm{max}}{1 -\\gamma}\\)で抑えられるので、 \\[\nQ_k^*(s, a_k^*) - \\max_{a \\neq a_k^*} Q_k^*(s, a) &lt; \\frac{\\gamma^k R_\\textrm{max}}{1 -\\gamma}\n\\] が成り立つなら、\\(a_k^*\\)が今後他の行動に逆転されることはありません。 なので\\(a_k^*\\)が最適でいいよね、はいこの話終わり、ということになります。 以下略記して \\(A_\\textrm{min}^*(s, a_k) = Q_k^*(s, a_k^*) - \\max_{a \\neq a_k^*} Q_k^*(s, a)\\) と書きます（他の行動に対するアドバンテージの最小値という意味）。 いま\\(\\gamma^{k-1} R_\\textrm{max}&lt;\\epsilon\\)が終了条件なので、 \\[\nA_\\textrm{min}^*(s, a_k) &lt; \\frac{\\epsilon\\gamma}{1 -\\gamma}\n\\Leftrightarrow\n\\frac{A_\\textrm{min}^*(s, a_k)(1 - \\gamma)}{\\gamma}&lt; \\epsilon\n\\] が成り立ちます。 これが意味するのは、\\(V*\\)と二番目にいい\\(Q^*(s, a)\\)との差が大きいほど\\(\\epsilon\\)を大きくできるということです。\nここまでの議論から、計算量の観点では、 - \\(\\gamma\\)が大きいほどMDPを解くのが難しい - 最適解と二番目にいい解との差が小さいほどMDPを解くのが難しい\nという2点が言えそうですね。"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html#方策最適化の難しさ",
    "href": "posts/understanding-what-makes-rl-difficult.html#方策最適化の難しさ",
    "title": "より良い問題設計へ向けて： 何が強化学習を難しくするのかを理解しよう",
    "section": "2.2: 方策最適化の難しさ",
    "text": "2.2: 方策最適化の難しさ\n前節で用いた再帰的な数値計算は動的計画法(DP)と呼ばれるものです。 Q学習など、多くの強化学習アルゴリズムがDPをもとにしています。 一方で、単に強化学習をブラックボックス最適化だと考えることもできます。 特に、方策パラメタ\\(\\theta\\)を最適化して解く方法を方策最適化と呼びます。\nいま、\\(\\pi(0|s) = \\theta(s), \\pi(1|s) = 1.0 - \\theta(s)\\)によって\\(\\pi\\)をパラメタ\\(\\theta\\)により表すことにします （これをdirect parameterizationと呼びます）。 ためしに、先ほどのMDP3で\\(\\pi(0|0)=1.0\\)を固定して、\\(\\theta(1), \\theta(2)\\)を動かした時の\\(\\sum_{s \\in \\mathcal{S}} V^\\pi(s)\\)の変動をプロットしてみましょう。\n\n\nCode\ndef v_pi_sum_2dim(\n    r: Array2,\n    p: Array3,\n    gamma: float,\n    epsilon: float,\n    initial_pi: Array2,\n    states: Tuple[int, int],\n    n_discretization: int,\n) -&gt; Array2:\n    res = []\n    for i2 in range(n_discretization + 1):\n        p2 = (1.0 / n_discretization) * i2\n        for i1 in range(n_discretization + 1):\n            p1 = (1.0 / n_discretization) * i1\n            pi = initial_pi.copy()\n            pi[states[0]] = p1, 1 - p1\n            pi[states[1]] = p2, 1 - p2\n            res.append(v_pi(r, p, pi, gamma, epsilon)[0].sum())\n    return np.array(res).reshape(n_discretization + 1, -1)\n\n\ndef plot_piv_heatmap(\n    data: Array2,\n    xlabel: str = \"\",\n    ylabel: str = \"\",\n    title: str = \"\",\n    ax: Optional[Axes] = None,\n) -&gt; Axes:\n    from matplotlib.ticker import LinearLocator\n\n    if ax is None:\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection=\"3d\")\n    n_discr = data.shape[0]\n    x, y = np.meshgrid(np.linspace(0, 1, n_discr), np.linspace(0, 1, n_discr))\n    ax.plot_surface(x, y, data, cmap=\"inferno\", linewidth=0, antialiased=False)\n    ax.zaxis.set_major_locator(LinearLocator(10))\n    ax.zaxis.set_major_formatter('{x:.01f}')\n    ax.set_xlabel(xlabel, fontsize=14)\n    ax.set_ylabel(ylabel, fontsize=14)\n    ax.set_zlabel(\"∑Vπ\", fontsize=14, horizontalalignment=\"right\")\n    ax.set_title(title, fontsize=15)\n    return ax\n\n\ninitial_pi = np.array([[1.0, 0.0], [0.5, 0.5], [0.5, 0.5]])\nv_pi_sums = v_pi_sum_2dim(mdp3.r, mdp3.p, 0.9, 1e-4, initial_pi, (1, 2), 20)\nax = plot_piv_heatmap(v_pi_sums, \"θ(1)\", \"θ(2)\", \"MDP3\")\n_ = ax.set_xlim(tuple(reversed(ax.get_xlim())))\n_ = ax.set_ylim(tuple(reversed(ax.get_ylim())))\n\n\n\n\n\n\n\n\n\nなんかいい感じに山になっていますね。 この問題の場合は、山登り法（勾配上昇法）で\\(\\theta\\)を更新していけば大域解 \\(\\theta(1) = 0.0, \\theta(2) = 1.0\\)にたどり着きそうです5。\nしかし、\\(f(\\theta) = \\sum_{s\\in\\mathcal{S}} V^{\\pi_\\theta}(s)\\)は、いつもこのような性質 のいい関数になっているのでしょうが？ 結論から言うとそうではないです。 例えば、以下のようなMDPではどうでしょうか？(\\(\\gamma=0.95\\)にしています)\n\n\nCode\nmdp4 = ChainMDP(\n    [[1.0, 0.0], [0.6, 0.9], [0.9, 0.6], [1.0, 1.0]],\n    [[0.0, 1.0], [0.0, 0.0], [0.0, 0.0], [0.9, 0.0]],\n)\nwidth, height = mdp4.figure_shape()\nfig = plt.figure(\"MDP4-pi-vis\", (width * 1.25, height))\nmdp_ax = fig.add_axes([0.42, 0.0, 1.0, 1.0])\n_ = mdp4.show(\"MDP4\", ax=mdp_ax)\npi_ax = fig.add_axes([0.0, 0.0, 0.4, 1.0], projection=\"3d\")\ninitial_pi = np.array([[0.0, 1.0], [0.5, 0.5], [0.5, 0.5], [1.0, 0.0]])\nv_pi_sums = v_pi_sum_2dim(mdp4.r, mdp4.p, 0.95, 1e-4, initial_pi, (1, 2), 24)\n_ = plot_piv_heatmap(v_pi_sums, \"θ(1)\", \"θ(2)\", ax=pi_ax)\nprint(\n    f\"f(θ(1) = 0.0, θ(2) = 0.0): {v_pi_sums[0][0]}\\n\"\n    f\"f(θ(1) = 0.5, θ(2) = 0.5): {v_pi_sums[12][12]}\\n\"\n    f\"f(θ(1) = 1.0, θ(2) = 1.0): {v_pi_sums[24][24]}\"\n)\n\n\nf(θ(1) = 0.0, θ(2) = 0.0): 74.25901721830479\nf(θ(1) = 0.5, θ(2) = 0.5): 72.01388270994806\nf(θ(1) = 1.0, θ(2) = 1.0): 70.6327625115528\n\n\n\n\n\n\n\n\n\n一番右だと永遠に0.9がもらえて、一番左だと1.0がもらえるので、より最適方策を見分けるのが難しそうな感じがします。\nプロットしてみると、\\(f(\\theta)\\)は先程とは逆に谷のような形になっていて、山登り法で解いても 必ずしも大域解に収束しなそうに見えます。 これをもっと専門的な言葉で言うと、\\(f(0.0) + f(1.0) &gt; 2 * f(0.5)\\)よりこれは凹関数ではありません。 あまり詳しく説明しませんが、凹関数だと山登り法が大域解に収束するなど嬉しい点があるので、 これは最適化する上で厄介な特徴だと言えます。\n以上より、方策最適化で問題を解く時は\\(\\sum_{s\\in\\mathcal{S}} V(s)\\)が凹関数かどうかが、 問題の難しさに影響を与えそうだということがわかりました。"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html#a-方策反復法の難しさ",
    "href": "posts/understanding-what-makes-rl-difficult.html#a-方策反復法の難しさ",
    "title": "より良い問題設計へ向けて： 何が強化学習を難しくするのかを理解しよう",
    "section": "2.A 方策反復法の難しさ",
    "text": "2.A 方策反復法の難しさ\n\nNote: この節は特に内容がないのでアペンディクス扱いになっています。飛ばしても問題ありません。\n\nところで2.1で\\(V^*\\)を求めたときに使った手法を価値反復法と言います。 もう一つ、方策反復法という手法で\\(V^*\\)を求めることができます。\n\\(\\pi^*\\)が満たすべき性質について考えてみます。 \\(\\pi\\)が最適であるとき、 \\[\nV^\\pi(s) \\geq \\max_{a \\in \\mathcal{A}} r(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s'|s,a) V^\\pi(s')\n\\] が成り立ちます。 これが成り立たないとすると、 \\[\n\\pi'(s, a) = \\begin{cases}\n1.0 &(\\textrm{if}~a = \\textrm{argmax}_a r(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s'|s,a) V^\\pi(s')) \\\\\n0.0 &(\\textrm{otherwise})\n\\end{cases}\n\\] の方が性能が良くなり、\\(\\pi\\)が最適であることと矛盾します。\nでは、この性質が成り立つまで方策を改善し続けるというアルゴリズムを試してみましょう。 さっき書いたv_pi関数を使って実装できます。\n\n\nCode\nMAX_ITER_PI: int = 10000\n\ndef policy_iteration(\n    r: Array2,\n    p: Array3,\n    gamma: float,\n    epsilon: float,\n) -&gt; Tuple[Array1, Array2, int]:\n    pi = np.zeros(p.shape[:2])  # |S| x |A|\n    pi[:, 1] = 1.0  # 最初の方策は決定的ならなんでもいいが、行動1を選ぶ方策にしてみる\n    state_indices = np.arange(0, p.shape[0], dtype=np.uint)\n    for n_iter in range(MAX_ITER_PI):\n        v_pi_, _ = v_pi(r, p, pi, gamma, epsilon)\n        q_pi = r + gamma * np.einsum(\"saS,S-&gt;sa\", p, v_pi_)\n        greedy_actions = np.argmax(q_pi, axis=1)\n        pi_next = np.zeros_like(pi)\n        pi_next[state_indices, greedy_actions] = 1.0\n        # pi == pi_next なら収束\n        if np.linalg.norm(pi - pi_next) &lt; 1.0:\n            return v_pi_, pi_next, n_iter + 1\n        pi = pi_next\n    raise RuntimeError(\"Policy Iteration did not converge &gt;_&lt;\")\n\nv_star_mdp3_vi, _, n_iter = policy_iteration(mdp3.r, mdp3.p, 0.9, 1e-4)\nprint(f\"反復回数: {n_iter}\")\nprint(\" \".join([f\"V({i}): {v:.3}\" for i, v in enumerate(v_star_mdp3_vi)]))\n\n\n反復回数: 2\nV(0): 6.49 V(1): 7.21 V(2): 7.51\n\n\nなんか2回反復しただけで求まってしまいましたが…。 このアルゴリズムは方策反復法と呼ばれ、なんやかんやで最適方策に収束することが知られています。 では、この反復回数は、何によって決まるのでしょうか？ 方策の組み合わせは\\(|A|^{|S|}\\)通りありますが、上の実験だとずっと速く収束しているので、もっといいバウンドがありそうに思えます。 しかし、実際のところ最悪ケースでは指数時間かかることが知られています。 この記事では、この方策反復法が難しくなる場合についても解説したかったのですが、 理解できなかったので、諦めました。ヾ(｡&gt;﹏&lt;｡)ﾉ"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html#b-参考文献など",
    "href": "posts/understanding-what-makes-rl-difficult.html#b-参考文献など",
    "title": "より良い問題設計へ向けて： 何が強化学習を難しくするのかを理解しよう",
    "section": "2.B 参考文献など",
    "text": "2.B 参考文献など\n\nOn the Complexity of Solving Markov Decision Problems\nCournell CS 6789: Foundations of Reinforcement Learning\n\n参考文献では\\(\\frac{1}{1 - \\gamma}\\)で反復回数を抑えているじゃないか、話が違うじゃないか、という気が一見してしまいます。 これは有名不等式\\(\\log x \\leq x - 1\\) からなんやかんやで\\(\\frac{1}{1 - \\gamma} \\geq -\\frac{1}{\\log\\gamma}\\) だから〜という感じで考えればなんとかなると思います。 この不等式は\\(x=1\\)で等号なので、よく使う\\(\\gamma=0.99\\)とかの設定ならかなり差は近くなります。"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html#報酬なし探査の問題",
    "href": "posts/understanding-what-makes-rl-difficult.html#報酬なし探査の問題",
    "title": "より良い問題設計へ向けて： 何が強化学習を難しくするのかを理解しよう",
    "section": "3.1 報酬なし探査の問題",
    "text": "3.1 報酬なし探査の問題\nというわけで、とりあえず別に学習しなくていいので、環境から情報を集めてこよう、という問題を考えてみましょう。\n\n\nCode\nfrom matplotlib.figure import Figure\nfrom matplotlib.image import FigureImage\n\n\nclass GridMDP:\n    from matplotlib.colors import ListedColormap\n\n    #: Up, Down, Left, Right\n    ACTIONS = np.array([[-1, 0], [+1, 0], [0, -1], [0, +1]])\n    #: Symbols\n    EMPTY, BLOCK, START, GOAL = range(4)\n    DESCRIPTIONS = [\"Empty\", \"Block\", \"Start\", \"Goal\"]\n    #: Colormap for visualizing the map\n    CM = ListedColormap([\"w\", \".75\", \"xkcd:leaf green\", \"xkcd:vermillion\"])\n    REWARD_COLORS = [\"xkcd:light royal blue\", \"xkcd:vermillion\"]\n    FIG_AREA = 28\n\n    # Returns PIL.Image\n    def __download_agent_image():\n        from io import BytesIO\n        from urllib import request\n\n        from PIL import Image\n\n        fd = BytesIO(\n            request.urlopen(\n                \"https://2.bp.blogspot.com/-ZwYKR5Zu28s/U6Qo2qAjsqI\"\n                + \"/AAAAAAAAhkM/HkbDZEJwvPs/s400/omocha_robot.png\"\n            ).read()\n        )\n        return Image.open(fd)\n\n    AGENT_IMAGE = __download_agent_image()\n\n    def __init__(\n        self,\n        map_array: Sequence[Sequence[int]],\n        reward_array: Optional[Sequence[Sequence[float]]] = None,\n        action_noise: float = 0.1,\n        horizon: Optional[int] = None,\n        seed: int = 123456789,\n        legend_loc: str = \"upper right\",\n    ) -&gt; None:\n        def add_padding(seq: Sequence[Sequence[T]], value: T) -&gt; list:\n            width = len(seq[0]) + 2\n            ret_list = [[value for _ in range(width)]]\n            for col in seq:\n                ret_list.append([value] + list(col) + [value])\n            ret_list.append([value for _ in range(width)])\n            return ret_list\n\n        self.map_array = np.array(add_padding(map_array, 1), dtype=np.uint8)\n        assert self.map_array.max() &lt;= 3\n        assert 0 &lt;= self.map_array.min()\n        self.rows, self.cols = self.map_array.shape\n\n        if reward_array is None:\n            self.reward_array = np.zeros((self.rows, self.cols), np.float64)\n        else:\n            self.reward_array = np.array(\n                add_padding(reward_array, 0.0), np.float64\n            )\n\n        self.action_noise = action_noise\n        self.horizon = horizon\n        self.start_positions = np.argwhere(self.map_array == self.START)\n        if len(self.start_positions) == 0:\n            raise ValueError(\"map_array needs at least one start posiiton\")\n        self.random_state = np.random.RandomState(seed)\n        _ = self.reset()\n\n        # Visualization stuffs\n        self.legend_loc = legend_loc\n        self.map_fig, self.map_ax, self.map_img = None, None, None\n        self.agent_img, self.agent_fig_img = None, None\n\n    def n_states(self) -&gt; int:\n        return np.prod(self.map_array.shape)\n\n    @staticmethod\n    def n_actions() -&gt; int:\n        return 4\n\n    def reset(self) -&gt; Array1:\n        idx = self.random_state.randint(self.start_positions.shape[0])\n        self.state = self.start_positions[idx]\n        self.n_steps = 0\n        return self.state.copy()\n\n    def state_index(self, state: Array1) -&gt; int:\n        y, x = state\n        return y * self.map_array.shape[1] + x\n\n    def _load_agent_img(self, fig_height: float) -&gt; None:\n        from io import BytesIO\n        from urllib import request\n\n        fd = BytesIO(request.urlopen(self.ROBOT).read())\n        img = Image.open(fd)\n        scale = fig_height / img.height\n        self.agent_img = img.resize((int(img.width * scale), int(img.height * scale)))\n\n    def _fig_inches(self) -&gt; Tuple[int, int]:\n        prod = self.rows * self.cols\n        scale = np.sqrt(self.FIG_AREA / prod)\n        return self.cols * scale, self.rows * scale\n\n    def _is_valid_state(self, *args) -&gt; bool:\n        if len(args) == 2:\n            y, x = args\n        else:\n            y, x = args[0]\n        return 0 &lt;= y &lt; self.rows and 0 &lt;= x &lt; self.cols\n\n    def _possible_actions(self) -&gt; Array1:\n        possible_actions = []\n        for i, act in enumerate(self.ACTIONS):\n            y, x = self.state + act\n            if self._is_valid_state(y, x) and self.map_array[y, x] != self.BLOCK:\n                possible_actions.append(i)\n        return np.array(possible_actions)\n\n    def _reward(self, next_state: Array1) -&gt; float:\n        y, x = next_state\n        return self.reward_array[y, x]\n\n    def _is_terminal(self) -&gt; bool:\n        if self.horizon is not None and self.n_steps &gt; self.horizon:\n            return True\n        y, x = self.state\n        return self.map_array[y, x] == self.GOAL\n\n    def step(self, action: int) -&gt; Tuple[Tuple[int, int], float, bool]:\n        self.n_steps += 1\n        possible_actions = self._possible_actions()\n        if self.random_state.random_sample() &lt; self.action_noise:\n            action = self.random_state.choice(possible_actions)\n\n        if action in possible_actions:\n            next_state = self.state + self.ACTIONS[action]\n        else:\n            next_state = self.state.copy()\n\n        reward = self._reward(next_state)\n        self.state = next_state\n        is_terminal = self._is_terminal()\n        return next_state.copy(), reward, is_terminal\n\n    def _draw_agent(self, fig: Figure) -&gt; FigureImage:\n        unit = self.map_img.get_window_extent().y1 / self.rows\n        y, x = self.state\n        return fig.figimage(\n            self.agent_img,\n            unit * (x + 0.3),\n            unit * (self.rows - 0.8 - y),\n        )\n\n    def _draw_rewards(self) -&gt; None:\n        for y in range(self.rows):\n            for x in range(self.cols):\n                rew = self.reward_array[y, x]\n                if abs(rew) &lt; 1e-3:\n                    continue\n                if self.map_array[y, x] == self.GOAL:\n                    color = \"w\"\n                else:\n                    color = self.REWARD_COLORS[int(rew &gt;= 0)]\n                self.map_ax.text(\n                    x + 0.1,\n                    y + 0.5,\n                    f\"{rew:+.2}\",\n                    color=color,\n                    fontsize=12,\n                )\n\n    def show(self, title: str = \"\", explicit: bool = False) -&gt; Axes:\n        if self.map_fig is None:\n            self.map_fig = plt.figure(title or \"GridMDP\", self._fig_inches())\n            ax = self.map_fig.add_axes([0, 0, 1, 1])\n            ax.set_aspect(\"equal\")\n            ax.set_xticks([])\n            ax.set_yticks([])\n            self.map_img = ax.imshow(\n                self.map_array,\n                cmap=self.CM,\n                extent=(0, self.cols, self.rows, 0),\n                vmin=0,\n                vmax=4,\n                alpha=0.6,\n            )\n            for i in range(1, 4):\n                if np.any(self.map_array == i):\n                    ax.plot([0.0], [0.0], color=self.CM(i), label=self.DESCRIPTIONS[i])\n            ax.legend(fontsize=12, loc=self.legend_loc)\n            ax.text(0.1, 0.8, title or \"GridMDP\", fontsize=16)\n            self.map_ax = ax\n\n            imw, imh = self.AGENT_IMAGE.width, self.AGENT_IMAGE.height\n            scale = (self.map_img.get_window_extent().y1 / self.rows) / imh\n            self.agent_img = self.AGENT_IMAGE.resize(\n                (int(imw * scale), int(imh * scale))\n            )\n\n            if np.linalg.norm(self.reward_array) &gt; 1e-3:\n                self._draw_rewards()\n        if self.agent_fig_img is not None:\n            self.agent_fig_img.remove()\n        self.agent_fig_img = self._draw_agent(self.map_fig)\n        if explicit:\n            from IPython.display import display\n\n            self.map_fig.canvas.draw()\n            display(self.map_fig)\n        return self.map_ax\n\n\n\n\nCode\ngrid_mdp1 = GridMDP(\n    [[0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0], \n     [0, 0, 2, 0, 0],\n     [0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0]],\n    horizon=50,\n)\n_ = grid_mdp1.show(\"GridMDP1\")\n\n\n\n\n\n\n\n\n\nGridMDPと題されたこちらが、今回使用する「環境」になります。 環境の中で行動する主体をエージェントと呼びます。 今回は、いらすとや様のロボットの画像を使用させていただきました。 各マス目の中で、エージェントは行動は上下左右に移動の4種類の行動を選択できます。 行動は時々失敗して、一様ランダムな状態遷移が発生します。 ここで、前章で用いなかったいくつかの新しい概念を導入します。\n\n「スタート地点」の存在\n\nロボットくんは、決められた場所から行動を始めなくてはなりません。この問題では、初期状態はあらかじめ決まられたいくつかのスタート地点から均等に選びます。理論的なフレームワークでは、初期状態分布\\(\\mu: \\mathcal{S} \\rightarrow \\mathbb{R}\\)として表現すればいいです。\n\n「終了地点」の存在\n\nロボットくんは、いくつかの決められた場所に到達したら強制的にスタートまで戻されます。\n\nエピソード\n\nスタートから終了するまでの一連の流れをエピソードと呼びます。\n\n「エピソード長さ（ホライゾン）」の存在\n\n一定のターンが経過した時、ロボットくんはスタート地点まで戻されます。強化学習ではしばしば、シミュレーターを何度もリセットして学習します。理論を実際に近づけるため、MDPのフレームワークにもこれが導入される場合があります。\n\n\nではさっそく、適当に行動してもらいましょう。 エージェントをランダムに行動させて、訪問した場所に色をつけていきます。 なお、エピソード長さは50とします。\n\n\nCode\nfrom abc import ABC, abstractmethod\nfrom typing import Callable\n\n\nclass VisitationHeatmap:\n    def __init__(\n        self,\n        map_shape: Tuple[int, int],\n        figsize: Tuple[float, float],\n        ax: Optional[Axes] = None,\n        max_visit: int = 1000,\n        title: str = \"\",\n    ) -&gt; None:\n        from matplotlib import colors as mc\n        from mpl_toolkits.axes_grid1 import make_axes_locatable\n\n        self.counter = np.zeros(map_shape, np.int64)\n        self.title = title\n        self.fig = plt.figure(self.title, figsize, facecolor=\"w\")\n        self.ax = self.fig.add_axes([0, 0, 1, 1])\n        self.ax.set_aspect(\"equal\")\n        self.ax.set_xticks([])\n        self.ax.set_yticks([])\n        r, g, b = mc.to_rgb(\"xkcd:fuchsia\")\n        cdict = {\n            \"red\": [(0.0, r, r), (1.0, r, r)],\n            \"green\": [(0.0, g, g), (1.0, g, g)],\n            \"blue\": [(0.0, b, b), (1.0, b, b)],\n            \"alpha\": [(0.0, 0.0, 0.0), (1.0, 1.0, 1.0)],\n        }\n        self.img = self.ax.imshow(\n            np.zeros(map_shape),\n            cmap=mc.LinearSegmentedColormap(\"visitation\", cdict),\n            extent=(0, map_shape[1], map_shape[0], 0),\n            vmin=0,\n            vmax=max_visit,\n        )\n\n        divider = make_axes_locatable(self.ax)\n        cax = divider.append_axes(\"right\", size=\"4%\", pad=0.1)\n        self.fig.colorbar(self.img, cax=cax, orientation=\"vertical\")\n        cax.set_ylabel(\"Num. Visitation\", rotation=0, position=(1.0, 1.1), fontsize=14)\n\n        self._update_text()\n        self.agent = None\n\n    def _update_text(self) -&gt; None:\n        self.text = self.ax.text(\n            0.1,\n            -0.5,\n            f\"{self.title} After {self.counter.sum()} steps\",\n            fontsize=16,\n        )\n\n    def _draw_agent(self, draw: Callable[[Figure], FigureImage]) -&gt; None:\n        if self.agent is not None:\n            self.agent.remove()\n        self.agent = draw(self.fig)\n\n    def visit(self, state: Array1) -&gt; int:\n        y, x = state\n        res = self.counter[y, x]\n        self.counter[y, x] += 1\n        self.img.set_data(self.counter)\n        self.text.remove()\n        self._update_text()\n        return res\n\n    def show(self) -&gt; None:\n        from IPython.display import display\n\n        display(self.fig)\n\n\ndef do_nothing(\n    _state: int,\n    _action: int,\n    _next_state: int,\n    _reward: float,\n    _is_terminal: bool,\n) -&gt; None:\n    return\n\n\ndef simulation(\n    mdp: GridMDP,\n    n: int,\n    act: Callable[[int], int],\n    learn: Callable[[int, int, int, float, bool], None] = do_nothing,\n    max_visit: Optional[int] = None,\n    vis_freq: Optional[int] = None,\n    vis_last: bool = False,\n    title: str = \"\",\n) -&gt; None:\n    visitation = VisitationHeatmap(\n        mdp.map_array.shape,\n        mdp._fig_inches(),\n        max_visit=max_visit or n // 10,\n        title=title,\n    )\n    state = mdp.reset()\n    visitation.visit(state)\n    vis_interval = n + 1 if vis_freq is None else n // vis_freq\n    for i in range(n):\n        if (i + 1) % vis_interval == 0 and (vis_last or i &lt; n - 1):\n            visitation._draw_agent(mdp._draw_agent)\n            visitation.show()\n        action = act(mdp.state_index(state))\n        next_state, reward, terminal = mdp.step(action)\n        visitation.visit(next_state)\n        learn(\n            mdp.state_index(state),\n            action,\n            mdp.state_index(next_state),\n            reward,\n            terminal,\n        )\n        if terminal:\n            state = mdp.reset()\n        else:\n            state = next_state\n    visitation._draw_agent(mdp._draw_agent)\n\n\nsimulation(grid_mdp1, 1000, lambda _: np.random.randint(4), vis_freq=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nランダムに行動させただけですが、それなりにまんべんなく色が塗られていて、まあまあいいのではないか、という気がします。 しかし、もっと広い環境ではどうでしょうか。\n\n\nCode\ngrid_mdp2_map = [[0] * 15 for _ in range(15)]\ngrid_mdp2_map[7][7] = 2\ngrid_mdp2 = GridMDP(grid_mdp2_map, horizon=50)\n_ = grid_mdp2.show()\nrandom_state = np.random.RandomState(1)\nsimulation(\n    grid_mdp2,\n    5000,\n    lambda _: random_state.randint(4),\n    max_visit=100,\n    title=\"Random Exploration\",\n    vis_freq=None,\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nなんか駄目っぽい感じですね。 場所によっては全く色がついていません。 環境が広いと、ランダムに歩き回るのでは、効率よく情報を集めてこれないようです。 具体的にどのくらい難しいのかと言うと、平均一回訪問するのにかかる時間が、だいたい - 一方通行の直線: \\(O(|S|)\\) - 二次元ランダムウォーク: \\(O(|S|^2)\\)? (参考: plane上のランダムウォーク） - 最悪ケース: \\(O(2^{|S|})\\)\nくらいになります。 一方通行なのはなんとなくわかりますね。 ランダムウォークの場合、同じ場所を行ったりきたりできるので、そのぶん時間がかかってしまいます。 最悪ケースは、以下のように構成すればいいです。\n\n\nCode\n# hide-input\nclass WorstCaseMDP(ChainMDP):\n    def __init__(self, n: int) -&gt; None:\n        self.n_states = n\n        # For plotting\n        self.circles = []\n        self.cached_ax = None\n\n    def show(self, title: str = \"\", ax: Optional[Axes] = None) -&gt; Axes:\n        # だいたいChainMDPからコピペ...\n        if self.cached_ax is not None:\n            return self.cached_ax\n\n        from matplotlib.patches import Circle\n\n        width, height = self.figure_shape()\n        circle_position = height / 2 - height / 10\n        if ax is None:\n            fig = plt.figure(title or \"ChainMDP\", (width, height))\n            ax = fig.add_axes([0, 0, 1, 1], aspect=1.0)\n        ax.set_xlim(0, width)\n        ax.set_ylim(0, height)\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n        def xi(si: int) -&gt; float:\n            return self.OFFSET + (1.0 + self.INTERVAL) * si + 0.5\n\n        self.circles = [\n            Circle((xi(i), circle_position), 0.5, fc=\"w\", ec=\"k\")\n            for i in range(self.n_states)\n        ]\n        for i in range(self.n_states):\n            x = self.OFFSET + (1.0 + self.INTERVAL) * i + 0.1\n            ax.text(x, height * 0.85, f\"State {i}\", fontsize=16)\n\n        def annon(act: int, *args, **kwargs) -&gt; None:\n            # We don't hold references to annotations (i.e., we treat them immutable)\n            a_to_b(\n                ax,\n                *args,\n                **kwargs,\n                arrowcolor=self.ACT_COLORS[act],\n                text=f\"P: 1.0\",\n                fontsize=11,\n            )\n\n        for si in range(self.n_states):\n            ax.add_patch(self.circles[si])\n            x = xi(si)\n            # Action 0:\n            y = circle_position + self.SHIFT\n            if si &lt; self.n_states - 1:\n                annon(\n                    0,\n                    (x + self.SHIFT, y),\n                    (xi(si + 1) - self.SHIFT * 1.2, y - self.SHIFT * 0.3),\n                    verticalalignment=\"center_baseline\",\n                )\n            else:\n                annon(\n                    0,\n                    (x - self.SHIFT * 1.2, y),\n                    (x + self.SHIFT * 0.5, y - self.SHIFT * 0.1),\n                    style=\"self\",\n                    verticalalignment=\"bottom\",\n                )\n            # Action 1:\n            y = circle_position - self.SHIFT\n            if si &gt; 0:\n                annon(\n                    1,\n                    (x - self.SHIFT * 1.6, y),\n                    (xi(0), y - self.SHIFT * 0.6),\n                    style=\"arc3,rad=-0.15\",\n                    verticalalignment=\"top\",\n                )\n            else:\n                annon(\n                    1,\n                    (x + self.SHIFT * 0.4, y),\n                    (x - self.SHIFT * 0.45, y + self.SHIFT * 0.1),\n                    style=\"self\",\n                    verticalalignment=\"top\",\n                )\n\n        for i in range(2):\n            ax.plot([0.0], [0.0], color=self.ACT_COLORS[i], label=f\"Action {i}\")\n        ax.legend(fontsize=11, loc=\"upper right\")\n        if len(title) &gt; 0:\n            ax.text(0.06, height * 0.9, title, fontsize=18)\n        self.cached_ax = ax\n        return ax\n\n\n_ = WorstCaseMDP(6).show()\n\n\n\n\n\n\n\n\n\nこの環境で状態\\(0\\)からランダムに行動すると、右端にたどり着くまでに平均\\(2^5\\)くらいのステップ数がかかります。 そんなんありかよ…って感じですが。\nこの結果から、最悪の場合だと指数時間かかるから賢くデータ収集しないといけないよね、 思うこともできます。 その一方で、ランダムウォークのように遷移の対称性がある環境なら、 ランダムに行動してもそんなに悪くないんじゃないかな、とも思えます。\nさてその話は一旦おいておいて、もっと効率よくデータを集める方法を考えてみましょう。\n\n訪問した場所を覚えておいて、訪問していない場所を優先して探査する\n状態と状態の間に距離が定義できると仮定して、遠くに行くようにする\n環境がユークリッド空間だと仮定してSLAMで自己位置推定する\n\nなど、色々な方法が考えられると思いますが、ここでは1の方法を使ってみましょう。\n以下のようなアルゴリズムを考えます。 1. 適当な方策\\(\\pi_0\\)から開始する 2. 状態行動訪問回数\\(n(s, a)\\)、状態行動次状態訪問回数\\(n(s, a, s')\\)を記録しておく - ただし、初期値は\\(n_0(s, a) = 1.0, n_0(s, a, s) = \\frac{1}{|S|}\\)とする(0除算防止のため) 3. エピソードが終わったとき、以下のように方策を更新する 1. 状態遷移関数の推定値\\(\\hat{P}(s'|s, a) = \\frac{n(s, a, s')}{n(s, a}\\)、疑似報酬\\(r_k(s, a)=\\frac{1}{n(s, a)}\\)、適当な\\(\\gamma\\)から成るMDP\\(\\mathcal{M}_k\\)を解く 2. \\(\\mathcal{M}_k\\)の最適価値関数\\(V^*_k,Q^*_k\\)から以下のように方策\\(pi_{k+1}\\)を構成する - \\(V^*_k(s) &lt; \\frac{1}{|S|}\\sum_{s'\\in\\mathcal{S}}V^*_k(s')\\) なら \\(\\pi_{k+1}(s)\\)は\\(Q^*_k\\)に基づく貪欲行動 - それ以外の場合、\\(\\pi_{k+1}(s)\\)は一様ランダムな行動をとる (=方策を緩和する)\n疑似報酬\\(r_k=\\frac{1}{n(s, a)}\\)を使用してプランニングするのが、最も重要なポイントです。 この値は、一度も状態行動ペア\\((s,a)\\)を経験していないなら\\(1\\)、一度経験したら\\(1/2\\)、2回経験したら\\(1/3\\)のように減衰します。 これを報酬とするMDPを解くことで、あまり経験していない状態行動ペアをとろうとする方策が得られます。 完全な貪欲方策ではなく緩和をいれているのは、高い報酬の状態をループしないようにするためです。 では、やってみましょう。\n\n\nCode\nclass RewardFreeExplore:\n    def __init__(\n        self,\n        n_states: int,\n        n_actions: int,\n        seed: int = 1,\n        gamma: float = 0.95,\n    ) -&gt; None:\n        self.sa_count = np.ones((n_states, n_actions))\n        self.sas_count = np.ones((n_states, n_actions, n_states)) / n_states\n        self.pi = np.ones((n_states, n_actions)) / n_actions\n        self.random_state = np.random.RandomState(seed)\n        self.n_states, self.n_actions = n_states, n_actions\n        self.n_updates = 0\n        self.gamma = gamma\n        self.pi_indices = np.arange(n_states, dtype=np.uint32)\n\n    def learn(\n        self,\n        state: int,\n        action: int,\n        next_state: int,\n        reward: float,\n        is_terminal: bool,\n    ) -&gt; None:\n        # 訪問記録を更新する\n        self.sa_count[state, action] += 1\n        if is_terminal:\n            # エピソードが終わったら、Value Iterationを解いて方策を更新する\n            r = 1.0 / self.sa_count\n            p = self.sas_count / np.expand_dims(self.sa_count, axis=-1)\n            v, _n_iter = value_iteration(r, p, self.gamma, 1e-2)\n            v_is_larger_than_mean = v &gt; v.mean()\n            q = r + self.gamma * np.einsum(\"saS,S-&gt;sa\", p, v)\n            self.pi.fill(0.0)\n            for state in range(self.n_states):\n                # Vが大きい場所では方策を緩和する\n                if v_is_larger_than_mean[state]:\n                    self.pi[state] = 1.0 / self.n_actions\n                # そうでない場合は貪欲\n                else:\n                    self.pi[state][q[state].argmax()] = 1.0\n            self.n_updates += 1\n        else:\n            self.sas_count[state, action, next_state] += 1\n\n    def act(self, state: int) -&gt; int:\n        return self.random_state.choice(self.n_actions, p=self.pi[state])\n\n\nagent = RewardFreeExplore(grid_mdp2.n_states(), grid_mdp2.n_actions())\nsimulation(\n    grid_mdp2,\n    5000,\n    agent.act,\n    learn=agent.learn,\n    max_visit=100,\n    title=\"Strategic Exploration\",\n    vis_freq=None,\n)\n\n\n\n\n\n\n\n\n\nさっきよりも満遍なく、色々な状態を訪問してくれるようになりましたね。"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html#報酬あり探査",
    "href": "posts/understanding-what-makes-rl-difficult.html#報酬あり探査",
    "title": "より良い問題設計へ向けて： 何が強化学習を難しくするのかを理解しよう",
    "section": "3.2 報酬あり探査",
    "text": "3.2 報酬あり探査\n次は、環境から報酬が与えられるので、なるべく早く学習を終わらせたい、という問題を考えます。 訪問していない場所に積極的にいけばいい、という方針はさっきと変わりません。 一方で、あまりに報酬がもらえなさそうな状態はとっとと諦めることをしなくてはいけない点が異なっています。\n例えば、報酬の推定値を\\(\\hat{r}(s, a)\\)とするとき、先ほどのアルゴリズムの疑似報酬を \\(r_k(s, a)=\\hat{r}(s, a)+\\frac{\\beta}{\\sqrt{n(s, a)}}\\)とすればいいです。 これを、単に\\(r_k(s, a)=\\hat{r}(s, a)\\)とするアルゴリズム(Approximate Value Iteration)と比較してみましょう。 こちらは、確率\\(\\epsilon\\)で一様分布から行動をサンプリングし、\\(1-\\epsilon\\)で\\(Q^*_k\\)が一番大きい行動を選択するという行動方策を使ってみましょう（\\(\\epsilon\\)-Greedyと言います）。 今回は\\(\\epsilon=0.9 \\rightarrow 0.4\\)とします。\n\n\nCode\ngrid_mdp3 = GridMDP(\n    [[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n    [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9.0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0, 0]],\n    horizon=100,\n    legend_loc=\"lower right\"\n)\n_ = grid_mdp3.show(\"GridMDP3\")\n\n\n\n\n\n\n\n\n\nまず、こちらの環境で実験してみます。素直に\\(0.1\\)の報酬→\\(9.0\\)の報酬を目指せばいい感じです。 また、\\(\\gamma=0.99\\)とします。\n\n\nCode\nclass EpsgApproxVI:\n    def __init__(\n        self,\n        n_states: int,\n        n_actions: int,\n        seed: int = 1,\n        gamma: float = 0.99,\n        epsilon: float = 0.9,\n        epsilon_delta: float = 0.0001,\n    ) -&gt; None:\n        self.sa_count = np.ones((n_states, n_actions))\n        self.sas_count = np.ones((n_states, n_actions, n_states)) / n_states\n        self.r_sum = np.zeros((n_states, n_actions))\n        self.pi = np.ones((n_states, n_actions)) / n_actions\n        self.random_state = np.random.RandomState(seed)\n        self.n_states, self.n_actions = n_states, n_actions\n        self.n_updates = 0\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.epsilon_delta = epsilon_delta\n        self.pi_indices = np.arange(n_states, dtype=np.uint32)\n\n    def learn(\n        self,\n        state: int,\n        action: int,\n        next_state: int,\n        reward: float,\n        is_terminal: bool,\n    ) -&gt; None:\n        self.sa_count[state, action] += 1\n        self.r_sum[state, action] += reward\n        if is_terminal:\n            r = self.r_sum / self.sa_count\n            p = self.sas_count / np.expand_dims(self.sa_count, axis=-1)\n            v, _n_iter = value_iteration(r, p, self.gamma, 1e-2)\n            q = r + self.gamma * np.einsum(\"saS,S-&gt;sa\", p, v)\n            self.pi.fill(0)\n            for state in range(self.n_states):\n                self.pi[state][q[state].argmax()] = 1.0\n            self.n_updates += 1\n        else:\n            self.sas_count[state, action, next_state] += 1\n\n    def act(self, state: int) -&gt; int:\n        if self.random_state.rand() &lt; self.epsilon:\n            self.epsilon -= self.epsilon_delta\n            return self.random_state.choice(self.n_actions)\n        else:\n            return self.random_state.choice(self.n_actions, p=self.pi[state])\n\n\nclass MBIB_EB:\n    def __init__(\n        self,\n        n_states: int,\n        n_actions: int,\n        seed: int = 1,\n        gamma: float = 0.99,\n        beta: float = 0.1,\n    ) -&gt; None:\n        self.sa_count = np.ones((n_states, n_actions))\n        self.sas_count = np.ones((n_states, n_actions, n_states)) / n_states\n        self.r_sum = np.zeros((n_states, n_actions))\n        self.pi = np.ones((n_states, n_actions)) / n_actions\n        self.random_state = np.random.RandomState(seed)\n        self.n_states, self.n_actions = n_states, n_actions\n        self.n_updates = 0\n        self.gamma = gamma\n        self.beta = beta\n        self.pi_indices = np.arange(n_states, dtype=np.uint32)\n    \n    def learn(\n        self,\n        state: int,\n        action: int,\n        next_state: int,\n        reward: float,\n        is_terminal: bool,\n    ) -&gt; None:\n        self.sa_count[state, action] += 1\n        self.r_sum[state, action] += reward\n        if is_terminal:\n            r = self.r_sum / self.sa_count + self.beta / np.sqrt(self.sa_count)\n            p = self.sas_count / np.expand_dims(self.sa_count, axis=-1)\n            v, _n_iter = value_iteration(r, p, self.gamma, 1e-2)\n            v_is_larger_than_mean = v &gt; v.mean()\n            q = r + self.gamma * np.einsum(\"saS,S-&gt;sa\", p, v)\n            self.pi.fill(0.0)\n            for state in range(self.n_states):\n                if v_is_larger_than_mean[state]:\n                    self.pi[state] = 1.0 / self.n_actions\n                else:\n                    self.pi[state][q[state].argmax()] = 1.0\n            self.n_updates += 1\n        else:\n            self.sas_count[state, action, next_state] += 1\n\n    def act(self, state: int) -&gt; int:\n        return self.random_state.choice(self.n_actions, p=self.pi[state])\n\n\nepsg_vi = EpsgApproxVI(grid_mdp3.n_states(), grid_mdp3.n_actions())\nsimulation(\n    grid_mdp3,\n    10000,\n    epsg_vi.act,\n    learn=epsg_vi.learn,\n    max_visit=100,\n    title=\"ε-Greedy\",\n    vis_freq=None,\n)\nmbib_eb = MBIB_EB(grid_mdp3.n_states(), grid_mdp3.n_actions())\nsimulation(\n    grid_mdp3,\n    10000,\n    mbib_eb.act,\n    learn=mbib_eb.learn,\n    max_visit=100,\n    title=\"Startegic Exploration\",\n    vis_freq=None,\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n両方とも、いい感じに探査してくれているように見えます。 \\(\\epsilon\\)-Greedyの方が、\\(R_t=9.0\\)がもらえるゴールの周辺を多く探査していて、 良さそうに見えます。 一方で、もう少し意地悪な環境の場合はどうでしょうか？\n\n\nCode\ngrid_mdp4 = GridMDP(\n    [[0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 3],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n    [[0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 9.0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n    horizon=100,\n    legend_loc=\"lower right\"\n)\n_ = grid_mdp4.show(\"GridMDP3\")\n\n\n\n\n\n\n\n\n\nこの環境では、\\(+5.0\\)とかいう邪魔くさい報酬があります。 しかもここはゴールなので、ここに行くとまたリセットしてやり直しです。 ここを目指すように学習してしまうと、なかなか\\(+9.0\\)の方に行くのは厳しそうに見えます。 実験してみましょう。\n\n\nCode\nepsg_vi = EpsgApproxVI(grid_mdp4.n_states(), grid_mdp4.n_actions())\nsimulation(\n    grid_mdp4,\n    10000,\n    epsg_vi.act,\n    learn=epsg_vi.learn,\n    max_visit=100,\n    title=\"ε-Greedy\",\n    vis_freq=None,\n)\nmbib_eb = MBIB_EB(grid_mdp4.n_states(), grid_mdp4.n_actions())\nsimulation(\n    grid_mdp4,\n    10000,\n    mbib_eb.act,\n    learn=mbib_eb.learn,\n    max_visit=100,\n    title=\"Startegic Exploration\",\n    vis_freq=None,\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n予想通り、\\(\\epsilon\\)-Greedyの方は右上ばかり行ってしまってイマイチな感じになりました。\n以上の結果から、 - 邪魔がなく遷移関数が対称な状態空間（ランダムウォークのように考えられるもの）では、わりあい簡単にデータ収集ができる - 邪魔な報酬がない環境では、わりあい簡単にデータ収集ができる\nという2点が言えるかと思います。 ワーストケースを考えると探査が難しいのも事実ですが、実用上は難しいケースを考えるより邪魔な報酬を排除する ことを考えるのが重要です。"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html#a-参考文献など",
    "href": "posts/understanding-what-makes-rl-difficult.html#a-参考文献など",
    "title": "より良い問題設計へ向けて： 何が強化学習を難しくするのかを理解しよう",
    "section": "3.A 参考文献など",
    "text": "3.A 参考文献など\n\nOn the Sample Complexity of Reinforcement Learning\nReward-Free Exploration for Reinforcement Learning\nSample Complexity Bounds of Exploration\nAn analysis of model-based Interval Estimation for Markov Decision Processes\n\n3.1で紹介したアルゴリズムは一応2.の文献を参考にしていますが、僕がさっき適当に考えた(は？)ものです。 理論保証があるかはあやしいと思います。 3.2のやつはMBIB-EB(4.)に似ていますが、方策の緩和が入っている点が違います。 緩和も僕が適当に考えたものなのですが、入れた方が性能が良かったので入れてみました。 良い子の皆さんは真似しないでください。"
  },
  {
    "objectID": "posts/understanding-what-makes-rl-difficult.html#footnotes",
    "href": "posts/understanding-what-makes-rl-difficult.html#footnotes",
    "title": "より良い問題設計へ向けて： 何が強化学習を難しくするのかを理解しよう",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSuttonも誤差逆伝播を使うのにはトリッキーな工夫が必要だと言っています。↩︎\nこの報酬関数は最も簡単な定義です。他に\\(r: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow \\mathbb{R}\\)(遷移先に依存)、\\(r: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{E}[R_{s, a}]\\)（確率的）があります。↩︎\n初期状態分布(雑に言うと、スタート地点の分布)を\\(\\mu(s)\\)とすると、$ {s} (s)V(s)$ がエージェントが獲得する割引報酬和の期待値です。\\(V_\\pi(s)\\) が最大ならこれも最大になります。↩︎\nこの\\(\\epsilon\\)は\\(\\epsilon\\)-Optimal Policyの\\(\\epsilon\\)ではありません。↩︎\nこの記事では勾配の導出については一切触れないので、別途資料などを参照してください。↩︎"
  },
  {
    "objectID": "posts/python_basic_en.html",
    "href": "posts/python_basic_en.html",
    "title": "Quick Python basic course for OIST new students",
    "section": "",
    "text": "In this course, I aim to cover introductory topics in software engeneering and Python programming in short.\nThis teaching material is prepared for programming/CS course for OIST new students that was cancelled due to low interests.\nYou can open this notebook in Google colab or download .ipynb file from the right pane. Please understand that Colab notebook is a variant of jupyter notebook customized by Google, so many features are different from standard jupyter notebook or jupyter lab."
  },
  {
    "objectID": "posts/python_basic_en.html#exercise-0",
    "href": "posts/python_basic_en.html#exercise-0",
    "title": "Quick Python basic course for OIST new students",
    "section": "Exercise 0",
    "text": "Exercise 0\nExecute the cell below.\n\n\nCode\n3 + 1\n\n\n4"
  },
  {
    "objectID": "posts/python_basic_en.html#what-is-a-programming-language",
    "href": "posts/python_basic_en.html#what-is-a-programming-language",
    "title": "Quick Python basic course for OIST new students",
    "section": "What is a programming language?",
    "text": "What is a programming language?\nLet’s recall that mondern compueters have this kind of architecture:\n\n\nCode\nfrom io import BytesIO\nfrom urllib import request\n\nfrom PIL import Image\n\nfd = BytesIO(\n    request.urlopen(\n        \"https://upload.wikimedia.org/wikipedia/commons/0/08/Computer_architecture_block_diagram.png\"\n    ).read()\n)\nImage.open(fd)\n\n\n\n\n\n\n\n\n\nThis is called von Neumann architecture and the basis of all modern computers. However, it is unclear how you can give instructions to the computer. Software, or program, is a way to do that. So, basically, software is the set of instructions to what to do. We can send software to the computer, and the software is stored in its memory and executed.\nNote that software can be either very lower level or higher level. Computer itself can only understand very lower level language. For example, in X86-64 assembly which is common in today’s intel and AMD desktop CPU, \\(3 + 1\\) in Python is written as below.\nmov al, 3   ; Move 3 into the AL register\nadd al, 1   ; Add 1 to AL\nThis is quite low-level operation, and we, lazy programmers, don’t want to write this kind of very low-level operation by hand at every time!\nThus, we need a high-level language that can bridge our thoughts and low level machine language. There are two types of laguages with different execution scheme:\n\nCompiled language (C, C++, Rust)\nInterpreter language (Javascript, Python, Ruby)\n\nIn compiled language, the program is converted to the machine language and then executed. For example, 3 + 1 is converted to mov al, 3; add al, 1 as the example above. Because it’s once covnerted to machine languages, the code in compiled langauges are as fast as machine language.\nAnother one is interpreter language. In this case, software is executed within another software called interpreter without compiling it to machine language. They are often slower than compiled languages, but they are easier and good for begginners. Python fits this category, and Python interpreter is running behind this notebook on a part of really big computer clusters in Google.\nAlso, there are some kinds of programming languages with different philosophy:\n\nProcedural (C, Rust)\nObject-oriented (C++, Python)\nFunctional (Lisp, Haskell)\n\nPython is an object-oriented language, but it inherits lots of features from procedural langauges. We generally focus on the Python as procedural languages in the first half of this course, because it may be the easiest way to get into programming."
  },
  {
    "objectID": "posts/python_basic_en.html#arithmetic-operations",
    "href": "posts/python_basic_en.html#arithmetic-operations",
    "title": "Quick Python basic course for OIST new students",
    "section": "Arithmetic operations",
    "text": "Arithmetic operations\nPrease also refer to the official tutorial. We can use +-*/ for basic arithmetic operations.\n\n\nCode\n3 + 5 * 4\n\n\n23\n\n\nPower is **, and the modulo is %.\n\n\nCode\n(2 ** 4) % 10\n\n\n6\n\n\nAddition is evaluated later than multiplication and division. We can use () to change the order.\n\n\nCode\n(3 + 5) * 4\n\n\n32"
  },
  {
    "objectID": "posts/python_basic_en.html#exercise-1.1",
    "href": "posts/python_basic_en.html#exercise-1.1",
    "title": "Quick Python basic course for OIST new students",
    "section": "Exercise 1.1",
    "text": "Exercise 1.1\nCompute how many seconds are in a day."
  },
  {
    "objectID": "posts/python_basic_en.html#variable",
    "href": "posts/python_basic_en.html#variable",
    "title": "Quick Python basic course for OIST new students",
    "section": "Variable",
    "text": "Variable\nVariable is the most important concept. You may think of it is a container that has some value, and we can name it.\n\n\nCode\nvar1 = 30\nprint(var1)\nvar1 = 40\nprint(var1)\n\n\n30\n40\n\n\n= is the syntax that assigns some value into the variable. There are some operators that changes the value in the variable in place.\nprint is what I’m not explained yet, but please understand that it just shows the content of the given variable (or, more precisely, the evaluated result of the given experssion).\n\n\nCode\nvar1 += 40\nprint(var1)\n\n\n80"
  },
  {
    "objectID": "posts/python_basic_en.html#expression-and-statements",
    "href": "posts/python_basic_en.html#expression-and-statements",
    "title": "Quick Python basic course for OIST new students",
    "section": "Expression and statements",
    "text": "Expression and statements\nIn Python, what we can assign into the variable is called expression. Expression includes numerical values, texts, and values themselves.\n\n\nCode\nvar1 = 20\nvar2 = -100\nvar3 = var2\nvar3\n\n\n-100\n\n\nIn Colab/Jupyter notebook, the last expression in the cell is printed out. In the cell above, the content of var3 is printed out.\nOn the other hand, the assignement like var1 = 20 is called statement. If the cell ends with statement, the notebook doesn’t show anything."
  },
  {
    "objectID": "posts/python_basic_en.html#for-loop",
    "href": "posts/python_basic_en.html#for-loop",
    "title": "Quick Python basic course for OIST new students",
    "section": "For loop",
    "text": "For loop\nThe power of the computer is in its speed. Todays computers can executre millionds of operations in a second. Thus, the most basic usage to utilize the computation power of computers is to make it repeat something. for loop the most basic way to do this. There is a range object that is often used with for loop, which represents certain numerical range.\n\n\nCode\nrange(10)\n\n\nrange(0, 10)\n\n\nFor loop repeats something with this range.\n\n\nCode\nfor i in range(10):\n    print(i)\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nHere, the program in the for loop is executed with variable i with all values in the range [0, 10) assigned.\nWait, did you notice this redundant four spaces?\nfor i in range(10):\n□□□□print(i)\nThese spaces are called indentation and used to indicate that these statements are inside the for loop block. This indentation is used everywhere in Python, as we’ll see later. Note that two spaces and three spaces also work, but you should to have the same indentation width throughout your code.\nLet’s go back to the for loop. It’s useful to accumulate values by for loop.\n\n\nCode\nsum_0_100 = 0\nfor i in range(101):\n    sum_0_100 += i\nsum_0_100\n\n\n5050\n\n\n\nExercise 2.1\nCompute \\(\\sum_{i=0}^{100} 2^k\\)."
  },
  {
    "objectID": "posts/python_basic_en.html#if-statement",
    "href": "posts/python_basic_en.html#if-statement",
    "title": "Quick Python basic course for OIST new students",
    "section": "If statement",
    "text": "If statement\nThe for loop is powerful, but not very frexible. It does the same thing everytime. However, we sometimes want to make the computer to do a differnt thing sometimes. Here, if statement is useful. if statement works with boolean values.\n\n\nCode\nTrue\n\n\nTrue\n\n\n\n\nCode\nFalse\n\n\nFalse\n\n\nIn Python, True and False are boolean types. Some expression returns boolean type as the result. For example, comparing numbers by &gt; and &lt; returns True or False.\n\n\nCode\n10 &lt; 40\n\n\nTrue\n\n\n\n\nCode\n40 &gt; 10\n\n\nTrue\n\n\nWe can use a == b for checking two numbers a and b are equal. Also, &gt;= and &lt;= represent \\(\\geq\\) and \\(\\leq\\).\n\n\nCode\n40 == 40\n\n\nTrue\n\n\nThe if statement executes the code inside the block if the expression is evaluated as True.\n\n\nCode\nvar_a = 100\n\nif var_a  &gt; 90:\n    print(\"Hey\")\n\n\nHey\n\n\nHere, print(\"Hey\") is executed because var_a &gt; 90 is True.\n\n\nCode\nif var_a  &gt; 100:\n    print(\"No hey\")\n\n\nIf the given expression is evaluated as False, nothing happens.\nWe can use if statement combined with for loop. For example, we can accumulate the sum of even numbers between \\(0\\) and \\(100\\) by:\n\n\nCode\neven_sum_0_100 = 0\n\nfor i in range(0, 101):\n    if i % 2 == 0:\n        even_sum_0_100 += i\n\neven_sum_0_100\n\n\n2550"
  },
  {
    "objectID": "posts/python_basic_en.html#more-complex-logic-with-if-statement",
    "href": "posts/python_basic_en.html#more-complex-logic-with-if-statement",
    "title": "Quick Python basic course for OIST new students",
    "section": "More complex logic with if statement",
    "text": "More complex logic with if statement\nif statement can have else branch if needed. If the given expression is False, statements in the else block is executed.\n\n\nCode\nif var_a  &gt; 100:\n    print(\"Hey\")\nelse:\n    print(\"Not hey\")\n\n\nNot hey\n\n\nIf you need more branches, if statement can be really complex with elif (shorthand of else if) blocks.\n\n\nCode\nif var_a &gt; 100:\n    print(\"Cond 1\")\nelif var_a == 100:\n    print(\"Cond 2\")\nelif var_a == 99:\n    print(\"Cond 3\")\nelse:\n    print(\"Cond 4\")\n\n\nCond 2\n\n\nThe condition also can be a bit more complex. Python has and and or operators for boolean values, which does logical operations.\n\n\nCode\nTrue and True, True and False, False and False\n\n\n(True, False, False)\n\n\n\n\nCode\nTrue or True, True or False, False or False\n\n\n(True, True, False)\n\n\nThese operators are often combined with if statement to express complex conditions.\n\n\nCode\nvar_b = 200\n\nif var_a &gt;= 100 and var_b &gt;= 200:\n    print(\"Hey!\")\n\n\nHey!\n\n\n\nExercise 2.2\nFix the program below (From https://utokyo-ipp.github.io/2/2-3.html).\n\n\nCode\nx = -1\nif x &lt; 3:\n    print(\"x is larger than or equal to 2, and less than 3\")\nelif x &lt; 2:\n    print(\"x is larger than or equal to 1, and less than 2\")\nelif x &lt; 1:\n    print(\"x is less than 1\")\nelse:\n    print(\"x is larger or equal to 3\")\n\n\nx is larger than or equal to 2, and less than 3"
  },
  {
    "objectID": "posts/python_basic_en.html#numerical-types",
    "href": "posts/python_basic_en.html#numerical-types",
    "title": "Quick Python basic course for OIST new students",
    "section": "Numerical types",
    "text": "Numerical types\n\nint(interger) type\nPython has some builtin data types. So far, we used int type.\n\n\nCode\ntype(30)\n\n\nint\n\n\nOn machine, int types have a simple representation by binary. Let’s see it:\n\n\nCode\nbin(4)\n\n\n'0b100'\n\n\nWe call the minimum unit of this binary representation (i.e., 0 or 1) bit. Although there is some tricks to where we store the bit for minus (-), for intergers, that’s it.\n\n\nfloat type\nBut, for general real numbers like 3.14, the situation is a bit complex.\n\n\nCode\ntype(3.14)\n\n\nfloat\n\n\nReal values are called float type in Python and some other programming languages. What does it mean? This example of representing \\(12.345\\) from Wikipedia article may be easy to understand.\n\\[\n12.345=\\underbrace{12345}_{\\text{significand}}\\times\\underbrace{10}_{\\text{base}}\\underbrace{{}^{-3}}^{\\text{exponent}}\n\\]\nSo, in computer, real values are stored by two intergers (significand and exponent) with the fixed base. Base \\(2\\) is commonly used. This approximation of real numbers are called floating-point numbers, from which the type name float is derived.\nOn most computers available nowadays, Python’s float type has 1 bit for sign (\\(+/-\\)), 11 bits for exponent, and 52 bits for significand. Thus, it has some limitations in precision.\n\n\nCode\n10 / 3\n\n\n3.3333333333333335\n\n\nThe answer should be \\(3.33333....\\), but because it has only 52 bits for significand, it can’t express \\(3.333333333333333\\).\nThis limit of floating point representation sometimes causes large errors in some scienctific applications, and there are bunch of researches how to deal with the error.\nBTW, please it is worth noting that the division operator / always returns float type. To get the interger as the result of division, use // instead.\n\n\nCode\n8 / 4, type(8 / 4)\n\n\n(2.0, float)\n\n\n\n\nCode\n8 //4, type(8 // 4)\n\n\n(2, int)"
  },
  {
    "objectID": "posts/python_basic_en.html#execercise-3.1",
    "href": "posts/python_basic_en.html#execercise-3.1",
    "title": "Quick Python basic course for OIST new students",
    "section": "Execercise 3.1",
    "text": "Execercise 3.1\nDisplay the largest number in Python float.\n\nOptional: the hidden power of Python integer\nSo, as we learned the limitation of Python float type, it seems natural to assume that Python int type has the same limitation by the restricted number of bits. Let’s try.\n\n\nCode\n2 ** 120, 2 ** 240\n\n\n(1329227995784915872903807060280344576,\n 1766847064778384329583297500742918515827483896875618958121606201292619776)\n\n\nWait, what is that? We can somehow compute very large number using Python int.\nThis is because Python interger has 2 internal representations:\n\nStandard 64bit interger ranges from \\(-9223372036854775808\\) to \\(9223372036854775808\\)\nList of 64bit intergers to represent large numbers\n\nPython automatically switches into the later representation, so it can compute really big numbers.\nNote that it’s unusual. In many programming languages, int is often 64bit and sometimes 32bit, thus the precision is limited."
  },
  {
    "objectID": "posts/python_basic_en.html#text-and-list",
    "href": "posts/python_basic_en.html#text-and-list",
    "title": "Quick Python basic course for OIST new students",
    "section": "Text and List",
    "text": "Text and List\n\nText\nWe can use “” and ’’ to represent texts. Both have the same effect.\n\n\nCode\n\"Hey, programming is fun!\"\n\n\n'Hey, programming is fun!'\n\n\n\n\nCode\n'Hey, programming is fun!'\n\n\n'Hey, programming is fun!'\n\n\nThis is called str type (prefix of string).\n\n\nList\nList is a convenient data type to store multiple values in one variable. We can construct a list by [].\n\n\nCode\n[\"Hey\", 2, 4, [\"Yay\", \"Me\"]]\n\n\n['Hey', 2, 4, ['Yay', 'Me']]\n\n\n\n\nIndexing and List operatons\n[] has another meaning: it can be a special operator for getting a part of list and str.\n\n\nCode\na = [1, 2, 3, 4, 5]\na[0]\n\n\n1\n\n\nWe can update the value with this indexing syntax.\n\n\nCode\na[0] -= 1\na\n\n\n[0, 2, 3, 4, 5]\n\n\nThe indexing starts from zero. If the index is negative (say, -i, it indicates length of the list - i - 1.\n\n\nCode\na[-1], a[-2]\n\n\n(5, 4)\n\n\nThere is a special syntax called slice combined with []. a[i:j] returns a part of the list from index i to j - 1.\n\n\nCode\na[2: 4]\n\n\n[3, 4]\n\n\nWe can skip either start and end of the slice. Then, the default values (0 and the length of the list) are used. We can even skip both and write a[:], but it’s just the same as a.\n\n\nCode\na[2:], a[:4], a[:]\n\n\n([3, 4, 5], [1, 2, 3, 4], [1, 2, 3, 4, 5])\n\n\nWe can concatanate lists by +. If you just want to add a value, you can use the syntax list.append.\n\n\nCode\na.append(-3)\na\n\n\n[1, 2, 3, 4, 5, -1]\n\n\n\n\nCode\na + [-2, -1]\n\n\n[1, 2, 3, 4, 5, -1, -2, -1]\n\n\nSame operations can be done for str.\n\n\nCode\n\"Hey, programming is fun!\"[: -4] + \"not fun!\"\n\n\n'Hey, programming is not fun!'\n\n\nBecause text is so common in human society, there are plenty of methods to manipulate str. One example is the templating string.\n\n\nCode\n\"{} is fun\".format(\"Programming\")\n\n\n'Programming is fun'\n\n\nWith this str.format, we can embed arbitary string and some values implicitly convertible to str to the point where {} indicates in the string. Because this is so common, Python has a special syntax for formatting called f-string.\n\n\nCode\nf\"The answer of 1 + 2 = {1 + 2}\"\n\n\n'The answer of 1 + 2 = 3'\n\n\nf-string is so special in that inside {} we can write any Python expression. The result is converted to string and embed in the result."
  },
  {
    "objectID": "posts/python_basic_en.html#internal-representation-of-text",
    "href": "posts/python_basic_en.html#internal-representation-of-text",
    "title": "Quick Python basic course for OIST new students",
    "section": "Internal representation of text",
    "text": "Internal representation of text\nstr in the machine is just numbers. We can convert it like:\n\n\nCode\nb = \"Hey\".encode(\"utf-8\")\nb\n\n\nb'Hey'\n\n\nb has a special type called bytes, which is a specialized list for small intergers. Actually it’s just a sequence of numbers\n\n\nCode\nb[0], b[1], b[2]\n\n\n(72, 101, 121)\n\n\nSo, H is 72, e is 101, and y is 121 on the machine. For complex characters like ☀, more complex rules are used to convert it to numbers. See UTF-8 if you are curious.\n\nExcercise 3.1\nConstruct a list that contains all English alphabet in small case.\nNote that you can convert number to str by:\n\n\nCode\na = 97\na.to_bytes().decode(\"utf-8\")\n\n\n'a'"
  },
  {
    "objectID": "posts/python_basic_en.html#other-convenient-types",
    "href": "posts/python_basic_en.html#other-convenient-types",
    "title": "Quick Python basic course for OIST new students",
    "section": "Other Convenient Types",
    "text": "Other Convenient Types\n\nTuple\nVery similar to list, but has a bit different syntax and immutable. Use () to construct tuple.\n\n\nCode\n(1, 2, \"Sun\", 4)\n\n\n(1, 2, 'Sun', 4)\n\n\nWe don’t need to use () in some situations. For example, as the last value of Notebook cell.\n\n\nCode\n1, 2, \"Sun\", 4\n\n\n(1, 2, 'Sun', 4)\n\n\nBut I’d recommend you to always use () for tuple, especially for begginers.\nBecause tuple is immutable, we can’t update values in tuple.\n\n\nCode\nt = (1, 2, \"Sun\", 4)\nt[0] += 4\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[97], line 2\n      1 t = (1, 2, \"Sun\", 4)\n----&gt; 2 t[0] += 4\n\nTypeError: 'tuple' object does not support item assignment\n\n\n\n+ is allowed because it creates a new tuple.\n\n\nCode\nt + (5, 6)\n\n\n(1, 2, 'Sun', 4, 5, 6)\n\n\n\n\nDict\nDict is a set of key and value pairs with fast access by key. We can make list by {} syntax.\n\n\nCode\nuser = {\n    \"Name\": \"Me\",\n    \"ID\": 345,\n    0: 10,\n}\n\n\nWe can use the index syntax to dict, though slicing [:] is not supported.\n\n\nCode\nuser[\"Name\"]\n\n\n'Me'\n\n\n\n\nNoneType\nIt sounds a bit weird, but None is the special type that represents nothing.\n\n\nCode\nNone\n\n\nThe type of None is NoneType, and None is the only value of this type.\n\n\nCode\ntype(None)\n\n\nNoneType\n\n\nNone is surprisingly useful in programming when some values can be missing, but maybe difficult for begginers to understand its value."
  },
  {
    "objectID": "posts/python_basic_en.html#recursion",
    "href": "posts/python_basic_en.html#recursion",
    "title": "Quick Python basic course for OIST new students",
    "section": "Recursion",
    "text": "Recursion\nAs long as reusability, there is another clever way to use function, called recursion.\nFor example, let’s assume that we want to compute \\(\\sum_{i = 0}^{100} i\\). As we learned, we can compute this easily by for loop. However, we can do the same thing using a function recursion.\n\n\nCode\ndef sum_to_0(i):\n    if i == 0:\n        return 0\n    else:\n        return sum_to_0(i - 1) + i\n\nsum_to_0(100)\n\n\n5050\n\n\nBecause \\(\\sum_{i = 0}^{k} i = k + \\sum_{i = 0}^{k - 1} i\\) for all \\(k &gt; 0\\), we can call the function sum_to_0 in the function sum_to_0. This technique is called recursion. I’d say that for loop is often better because it’s simple, but sometimes recursion works like a charm.\n\nExecercise 4.1\nImplement Euclidian Algorithm using recursion."
  },
  {
    "objectID": "posts/python_basic_en.html#scope",
    "href": "posts/python_basic_en.html#scope",
    "title": "Quick Python basic course for OIST new students",
    "section": "Scope",
    "text": "Scope\nVariable scope around the function is so important. You can remember two rules:\n\nIn functions, we can refer to the variable outside of the function.\nVariables inside the function is not visible from outside.\n\n\n\nCode\nvar_out = 100\n\ndef scope_demo():\n    print(var_out)\n    var_in = 200\n\nscope_demo()\nprint(var_in)\n\n\n100\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[143], line 8\n      5     var_in = 200\n      7 scope_demo()\n----&gt; 8 print(var_in)\n\nNameError: name 'var_in' is not defined\n\n\n\nSo, while we can read var_out inside scope_demo function, var_in is only available inside the function.\nImportantly, we can’t change the value of outside variable by assignment inside a function.\n\n\nCode\nvar_out = 100\n\ndef scope_demo2():\n    var_out = 200\n\nscope_demo2()\nvar_out\n\n\n100\n\n\nIt’s a bit confusing, but when a new variable is assigned in a function, it is treated as the variable inside the function scope. If you really want to avoid this, there are two ways:\n\nUse global\nUse container types like list or dict\n\n\n\nCode\nvar_out = 100\n\ndef scope_demo3():\n    global var_out\n    var_out = 200\n\nscope_demo3()\nvar_out\n\n\n200\n\n\nBy placing global var_out, it magically worked. Also, we can change the value of dict inside a function.\n\n\nCode\nvar_out = {0: 100}\n\ndef scope_demo3():\n    var_out[0] *= 2\n\nscope_demo3()\nvar_out\n\n\n{0: 200}\n\n\nThis is maybe a bit weird behavior for you (and even for me). But my recommendation is avoid referencing variables outside the function as much as possible. It can confuse your brain. Also, not depending on outside variables improves copy-pastablity of your code. I mean, if your code depends on some variables outside the function, you need to copy everything to another file if you want to use. But, if the function is completely independent, it just works by only copy-pasting the function.\nNote that your Python can work without writing any function, but I recommend to put everything inside function in your Python code. That will make your life easier."
  },
  {
    "objectID": "posts/python_basic_en.html#type-annotation",
    "href": "posts/python_basic_en.html#type-annotation",
    "title": "Quick Python basic course for OIST new students",
    "section": "Type annotation",
    "text": "Type annotation\nWe can annotate function arguments with type.\n\n\nCode\ndef typed_function(a: int, b: str) -&gt; str:\n    return f\"a: {a} b: {b}\"\n\n\nThe value after -&gt; indicates the type of return value. It is good to annotate arguments for readability."
  },
  {
    "objectID": "posts/python_basic_en.html#basic-class",
    "href": "posts/python_basic_en.html#basic-class",
    "title": "Quick Python basic course for OIST new students",
    "section": "Basic class",
    "text": "Basic class\nSo far, we learned that Python has many builtin types like list or dict. You can define you own one using class syntax.\n\n\nCode\nclass YourClass:\n    pass\n\n\n\n\nCode\ny = YourClass()\ntype(y)\n\n\n__main__.YourClass\n\n\nThat’s it. Your class can have some special functions called method, that is called by .method() syntax. For example, \"{}\".format is a method of str class.\n\n\nCode\nclass ClassWithMethod:\n    def method(self) -&gt; int:\n        return 5\n\n\n\n\nCode\nClassWithMethod().method()\n\n\n5\n\n\nYour class can have any values called members. To initialize your class with specific members, you can use a special method called __init__.\n\n\nCode\nclass ClassWithMembers:\n    def __init__(self):\n        self.name = \"Me\"\n        self.number = 10\n\n\n\n\nCode\nc = ClassWithMembers()\nc.name, c.number\n\n\n('Me', 10)\n\n\nThe special function used to create an instance of class is called constructor, and it also can take arguments.\nclass ClassWithFlexibleMembers: def init(self, name: str, number: int) -&gt; None: self.name = name self.number = number\nc = ClassWithFlexibleMembers(“Mew”, 33) c.name, c.number\nNote that the return type of __init__ is always None. __init__ is called in the constructor inside Python, and it’s not equal to the constructor."
  },
  {
    "objectID": "posts/try_gemma3_rlhf.html",
    "href": "posts/try_gemma3_rlhf.html",
    "title": "Gemma3でRLHFを試してみる",
    "section": "",
    "text": "前回のエントリを見たら2年近く前で衝撃を受けたのですが…\nそれはさておき、昨今ChatGPTをはじめとする「大規模言語モデルと対話できるWebサービス」が大流行しており、Anthropicの調査によると特にプログラマのようなコンピューターを使う職種にはよく広まっているようです。それを支えている技術がRLHF(Reinforcement Learning from Human Feedback)と呼ばれる技術です。素のLLMを人間の嗜好に沿うように訓練していい感じに対話してくれるようにしよう、そのために問題を定式化しよう、ということでこれができたのだと思います。実際には（強化学習の中でも）コンテキストつきバンディット問題だと思うんですが、RLと名前についているからには、まあ試しておくべきかな、と思いつつ結局何もしていなかったので、今回重い腰を上げてやってみることにしました。\nというわけで、このブログでは簡単にRLHFについて概観したあと、公開されているモデルを実際にRLHFで訓練してみようと思います。NVIDIA RTX4090が4台のったマシンを使うのでGPUメモリが20GBくらいで訓練できるモデルなら別になんでもいいのですが、僕はJaxが好きなのでGoogleが最近公開したGemma 3のコードを使ってみることにしました。後述しますが、この判断はわりと間違いだったと思います。"
  },
  {
    "objectID": "posts/try_gemma3_rlhf.html#rlhfの目的関数",
    "href": "posts/try_gemma3_rlhf.html#rlhfの目的関数",
    "title": "Gemma3でRLHFを試してみる",
    "section": "2.1: RLHFの目的関数",
    "text": "2.1: RLHFの目的関数\n嗜好データを使って何を最適化するのかというと、RLHFではまずBradley-Terry modelというモデルにもとづいて嗜好を報酬に変換します。このモデルでは\\(\\sigma(x) = \\frac{1}{1 + e^{-1}}\\)を使い、\\(p(y \\succ y' | x) = \\sigma(r(x, y) - r(x, y'))\\)が成り立つような実数関数\\(r\\)が存在すると仮定します。すると、データセット\\(\\mathcal{D} = (x_i, y_{w,i} \\succ y_{l, i})^N_{i=1}\\)に対して、\\(r\\)は\\(L(r)= -\\mathbb{E}_{(x, y_w, y_l)~D} \\left[ \\log ( \\sigma(r(x, y_w) - r(x, y_l)) ) \\right]\\)を損失関数とするロジスティック回帰により学習できます。\nこの報酬関数のもとで、制約付き期待報酬和\\(J(\\pi) = \\mathbb{E}\\pi [r(x, y)] − \\tau D_\\textbf{KL}(\\pi || \\pi_\\textbf{ref})\\)を最大化するのがRLHFの目的関数になります。\\(\\tau D_\\textbf{KL}(\\pi || \\pi_\\textbf{ref})\\)が方策に対する制約で、\\(\\pi_\\textbf{ref}\\)として事前学習済みモデルを使って大きく壊れないようにするのが一般的なようです。"
  },
  {
    "objectID": "posts/try_gemma3_rlhf.html#dpoの目的関数",
    "href": "posts/try_gemma3_rlhf.html#dpoの目的関数",
    "title": "Gemma3でRLHFを試してみる",
    "section": "2.2: DPOの目的関数",
    "text": "2.2: DPOの目的関数\n2.1の目的関数を最大化するためにはいったん\\(r\\)を学習する必要があるのですが、これを直接最適化する形式にしたのがDPO(Direct Preference Optimization)です。DPOでは以下の最小化項を目的関数とします\n\\(\\min_{\\pi} \\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}} \\left[ - \\log \\sigma \\left( \\tau \\log \\frac{\\pi(y_w|x)}{\\pi(y_l|x)} - \\tau \\log \\frac{\\pi_{ref}(y_w|x)}{\\pi_{ref}(y_l|x)}  \\right) \\right]\\)\nちょっとわかりにくいので、\\(\\frac{\\pi(y_w|x)}{\\pi(y_l|x)}\\)をx軸として、制約項と定数を無視した\\(L_\\textbf{simple}= - \\log \\sigma(\\log \\frac{\\pi(y_w|x)}{\\pi(y_l|x)})\\)をプロットしてみましょう。\n\n\nCode\nimport numpy as np\nimport seaborn.objects as so\n\n\ndef sigmoid(x):\n    return 1.0 / (1.0 + np.exp(-x))\n\n\nx = np.linspace(0.1, 10.0, 100)\ny = -np.log(sigmoid(np.log(x)))\n\n(\n    so.Plot(\n        data={\"x\": x, \"y\": y},\n        x=\"x\",\n        y=\"y\",\n    )\n    .add(\n        so.Line(),\n        orient=\"y\",\n    )\n    .label(x=r\"$\\frac{\\pi(y_w|x)}{\\pi(y_l|x)}$\", y=r\"$L_\\text{simple}$\")\n)\n\n\n\n\n\n\n\n\n\nというわけで、より好まれる\\(y_w\\)と好まれない\\(y_l\\)を生成する確率の密度比が大きいほど、この損失関数は小さくなります。Bradley-Terry modelが成り立つなど、いくつかの仮定のもとでDPOの目的関数とRLHFの目的関数が成り立つことを示すことができます。"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "RLog2: RL blog 2",
    "section": "",
    "text": "Blog posts on reinforcement learning and other technical stuff with some code"
  }
]