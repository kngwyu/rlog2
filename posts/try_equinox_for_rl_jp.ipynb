{
 "cells": [
  {
   "cell_type": "raw",
   "id": "02857a90-76ff-41d8-a163-45df6489d423",
   "metadata": {},
   "source": [
    "---\n",
    "title: equinoxで強化学習してみる\n",
    "date: 06/9/2023\n",
    "categories: [ja, RL, deep]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f24cd72-d1b9-431b-a7f5-1c599be663df",
   "metadata": {},
   "source": [
    "最近[equinox](https://docs.kidger.site/equinox/)という[jax](https://jax.readthedocs.io/en/latest/)ベースの深層学習モデルを定義するライブラリを使ってみたのですが、これが中々いいと思ったので紹介ついでに強化学習してみます。他のjaxベースのライブラリにはDeepmindの[haiku]()やGoogle Researchの[flax]()があります。この2つのライブラリは実際のところあまり変わりはありません。というのも、jaxには試験的に書かれた[stax](https://jax.readthedocs.io/en/latest/jax.example_libraries.stax.html)という深層学習ライブラリのリファレンス実装があり、haikuもflaxもstaxをベースにオブジェクト志向的な`Module`を採用したものだからです。あるいは、haikuやflaxは「staxをPyTorchっぽくしたもの」と言ってもいいかもしれません。equinoxのドキュメントにある[Compatibility with init-apply libraries](https://docs.kidger.site/equinox/examples/init_apply/)というページでは、これらのライブラリのやり方を「init-applyアプローチ」と呼んで軽く説明しています。これについてざっと見てみましょう。\n",
    "\n",
    "# Jaxにできることとできないこと\n",
    "\n",
    "そもそも、jaxというのは何をしてくれるライブラリなのでしょうか。ホームページのトップにはこう書かれています。\n",
    "\n",
    "> JAX is Autograd and XLA, brought together for high-performance numerical computing.\n",
    "\n",
    "[XLA](https://www.tensorflow.org/xla)というのは、Tensorflowのバックエンドとして開発された深層学習用の中間言語で、CPU/GPU/TPU用に数値計算コードを最適化してくるものです。深層学習に求められる並列化の性質から、特にSIMD演算/ベクトル並列化に特化しています。Jaxは、`jax.numpy`というNumPyに似せたライブラリをXLAのフロントエンドとして提供することで、「NumPyコードをベクトル並列化された高速なGPU用コードに実行時コンパイルすること」を可能にしています。では、Autogradというのはなんでしょうか？これは、jaxの開発者が以前に開発していたライブラリの名前でもありますが、自動微分全般のことを指すと考えていいでしょう。自分で勾配逆伝播のコードを書かなくても、jaxでは損失関数の各パラメタにおいての偏微分を勝手に計算してくれます。試しに、$f(x, y) = x^2 + y$の偏微分を計算してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff14064e-d950-4450-bd62-cf9c590b96ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-06 17:50:45.977087: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Array([ 6.,  8., 10., 12., 14.], dtype=float32),\n",
       " Array([1., 1., 1., 1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def f(x: jax.Array, y: jax.Array) -> jax.Array:\n",
    "    return jnp.sum(x ** 2 + y)\n",
    "\n",
    "x = jnp.array([3, 4, 5, 6, 7], dtype=jnp.float32)\n",
    "y = jnp.array([5, 2, 5, 7, 2], dtype=jnp.float32)\n",
    "jax.grad(f, argnums=(0, 1))(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583a0026-e6a9-43fe-9541-d47ef341d777",
   "metadata": {},
   "source": [
    "$\\frac{\\partial}{\\partial x}f = 2x, \\frac{\\partial}{\\partial y}f = 1$なので、正しく計算されているようですね。こんな感じで、jaxは自動的に偏微分を計算してくれるので、これをそのまま勾配降下法に使ってモデルを学習させることができます。深層ニューラルネットワークを学習させる際も、この`grad`を使えば全部のパラメタについて偏微分を効率的に計算してくれるので、それを使って学習できます。ついでに、この勾配をとる計算やモデルを更新する計算を`jax.jit`につっこめば、高速に計算してくれます。\n",
    "ここで問題になるのは、深層ニューラルネットワークの場合パラメタが多すぎるので、こんな風にいちいち全てのパラメタについて変数を割り当ててプログラムを書いていたら大変すぎる、ということです。大変すぎる以外に何か問題はあるのかというと、特にないと思います。コードの再利用性くらいでしょうか。しかし、まあ面倒なものは面倒ですから、**パラメタを管理してくれる仕組み**がほしいなあ、と思うわけですね。\n",
    "\n",
    "# init-applyアプローチによるパラメタ管理\n",
    "\n",
    "stax, haiku, flaxでは、この「パラメタ管理の問題」を、「init-applyアプローチ」により解決しています。このアプローチは以下のようにまとめられます。\n",
    "1. モデルはパラメータを持たない\n",
    "2. モデルは`init`と`apply`の2つの関数を持つ\n",
    "  - `init`は、入力例を受け取ってパラメタを初期化し、最初のパラメータを返す\n",
    "  - `apply`は、入力とパラメタを受け取り、モデルの計算結果を返す\n",
    "  \n",
    "なので、flaxやhaikuのAPIは以下のような感じになります。flaxでは`__call__`を使うがhaikuはPyTorchと同じ`forward`を使う、`flax`のModuleは [`dataclasses.dataclass`](https://docs.python.org/ja/3/library/dataclasses.html#dataclasses.dataclass)デコレータにより定義されたクラスと同じような性質を持つなどの違いがありますが、まあそれくらいで、大して違いはないです。以下僕がflaxとhaikuの間をとって書いた適当な疑似コードです。ニューラルネットワークを表すクラスとして、PyTorch風に「Module」という名前を使っています。\n",
    "```python\n",
    "class Linear(Module):\n",
    "    def __call__(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        w = self.parameter(output, shape=(batch_size, self.output_size) init=self.w_init)\n",
    "        b = self.parameter(output, shape=(1, self.output_size) init=self.w_init)\n",
    "        return w @ x.T + b\n",
    "\n",
    "model = Linear(output_size=10)\n",
    "params = model.init(jnp.zeros(1, 100))\n",
    "result = model.apply(params, jnp.zeros(10, 100))\n",
    "```\n",
    "こんな感じですかね。なお、上疑似コード中の`self.parameter`というメソッドは`flax`や`haiku`にある「パラメータをクラスに登録する機能」のことです。この機能により、各パラメタを値としてもつ`dict`を`init`により返すことができます。haikuの場合は、`params`は以下のような`dict`になっています。\n",
    "```python\n",
    "{\n",
    "    \"Linear/~/w\": [...],\n",
    "    \"Linear/~/b\": [...],\n",
    "}\n",
    "```\n",
    "\n",
    "`stax`はただのreference implementationなのでこのような機能がなく、かわりに、数のレイヤーを組み合わせるコンビネーターを提供しています。ではこのアプローチにはどのようなメリット、デメリットがあるでしょうか。\n",
    "\n",
    "**メリット**\n",
    "- Moduleを初期化する際に入力されるArrayのshapeを指定しなくていい\n",
    "  - パラメタは`init`が呼ばれた際に初期化される\n",
    "- 関数とデータを分離できる\n",
    "  - Moduleは変更可能な変数を持たず、パラメタと完全に別に扱われる\n",
    "- `init`、`apply`に`jit`や`vmap`などの関数デコレータを適用するコードが自然に書ける\n",
    "  \n",
    "**デメリット**\n",
    "- Moduleは冗長\n",
    "  - Moduleは「出力の次元」などの「モデルに関する設定」を持っているだけ\n",
    "- パラメタの要素に直接アクセスするのが面倒\n",
    "  - 例えばhaikuなら`params[\"Linear/~/w\"]`のようにしてパラメタの各要素にアクセスできるが、複雑なクラスだと`dict`の鍵の名前が長くなりわかりにくい\n",
    "- あまりオブジェクト指向的ではない\n",
    "- (haiku/flaxに特有) Module内でのパラメタの呼び出しを`jax.grad`が使えるような関数に変換する必要がある\n",
    "  - 例えば`haiku.transform`は、`haiku.get_parameter`によるパラメタ呼び出しを含む関数を、パラメタを引数としてとる関数に変換する\n",
    "\n",
    "こんなところでしょうか。\n",
    "\n",
    "# equinoxの特徴\n",
    "\n",
    "equinoxの特徴は、init-applyアプローチと異なり、「よりオブジェクト指向的な」（または、PyTorchに近い）インターフェースを志向している点にあります。\n",
    "ドキュメントのトップページにある[Quick example](https://docs.kidger.site/equinox/#quick-example)を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e12206f3-c53e-42d3-89f2-1dfdf01c85f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weight? [[0.59902626 0.2172144 ]\n",
      " [0.660603   0.03266738]\n",
      " [1.2164948  1.1940813 ]]\n",
      "Grad? Linear(weight=f32[3,2], bias=f32[3])\n"
     ]
    }
   ],
   "source": [
    "import equinox as eqx\n",
    "import jax\n",
    "\n",
    "class Linear(eqx.Module):\n",
    "    weight: jax.Array\n",
    "    bias: jax.Array\n",
    "\n",
    "    def __init__(self, in_size, out_size, key):\n",
    "        wkey, bkey = jax.random.split(key)\n",
    "        self.weight = jax.random.normal(wkey, (out_size, in_size))\n",
    "        self.bias = jax.random.normal(bkey, (out_size,))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.weight @ x + self.bias\n",
    "    \n",
    "@jax.jit\n",
    "@jax.grad\n",
    "def loss_fn(model, x, y):\n",
    "    pred_y = jax.vmap(model)(x)\n",
    "    return jax.numpy.mean((y - pred_y) ** 2)\n",
    "\n",
    "batch_size, in_size, out_size = 32, 2, 3\n",
    "model = Linear(in_size, out_size, key=jax.random.PRNGKey(0))\n",
    "x = jax.numpy.zeros((batch_size, in_size))\n",
    "y = jax.numpy.zeros((batch_size, out_size))\n",
    "grads = loss_fn(model, x, y)\n",
    "print(\"Model weight?\", model.weight)\n",
    "print(\"Grad?\", grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aeb7a5-8159-4f19-a55d-639da0c23de0",
   "metadata": {},
   "source": [
    "ここで、equinoxの最大の特徴は\n",
    "```python\n",
    "class Linear(eqx.Module):\n",
    "    weight: jax.Array\n",
    "    bias: jax.Array\n",
    "```\n",
    "というコードに表れていますが、「Moduleがパラメタを直接持つ」という点です。init-applyアプローチと比べると、これはどのような利点・欠点があるでしょうか？\n",
    "\n",
    "**メリット**\n",
    "- わかりやすい\n",
    "- デバッグしやすい\n",
    "  - flax/haikuは一度`transform`しないとデバッグできない\n",
    "- パラメタを直接操作するのが簡単\n",
    "\n",
    "**デメリット**\n",
    "- モデルを初期化する時に、入力する特徴量の次元が必要\n",
    "- `grad`を使う際に、パラメタとその他の変数を分離する必要がある\n",
    "\n",
    "ここで、最後の「`grad`を使う際に、パラメタとその他の変数を分離する必要がある」というのは、どういう意味でしょうか？例えば、self-attentionを計算する以下のようなModuleを考えます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cade3801-2a52-4669-93e6-c3e2986b7ed5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SelfAttention(eqx.Module):\n",
    "    q: eqx.nn.Linear\n",
    "    k: eqx.nn.Linear\n",
    "    v: eqx.nn.Linear\n",
    "    sqrt_d_attn: float\n",
    "\n",
    "    def __init__(self, d_in: int, d_attn: int, d_out: int, key: jax.Array) -> None:\n",
    "        q_key, k_key, v_key = jax.random.split(key, 3)\n",
    "        self.q = eqx.nn.Linear(d_in, d_attn, key=q_key)\n",
    "        self.k = eqx.nn.Linear(d_in, d_attn, key=k_key)\n",
    "        self.v = eqx.nn.Linear(d_in, d_attn, key=k_key)\n",
    "        self.sqrt_d_attn = float(jnp.sqrt(d_attn))\n",
    "\n",
    "    def __call__(self, e: jax.Array) -> jax.Array:\n",
    "        q = jax.vmap(self.q)(e)\n",
    "        k = jax.vmap(self.k)(e)\n",
    "        alpha = jax.nn.softmax(q.T @ k / self.sqrt_d_attn, axis=-1)\n",
    "        return jax.vmap(self.v)(e) @ alpha.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed2b32c-94ed-4277-871a-115fb336a7b3",
   "metadata": {},
   "source": [
    "`eqx.Module`は内部で`dataclasses.dataclass`を使うので、`__init__`等の初期化メソッドはあらかじめ用意されていますが、これをオーバーライドしてパラメタの初期化に使います。\n",
    "$\\sqrt{d_\\mathrm{attn}}$は定数なので、これもメンバ変数にしてしまいましょう。勾配を計算してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "594006e8-ca53-44cd-870b-c3b6d7f0a4a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelfAttention(\n",
       "  q=Linear(\n",
       "    weight=f32[8,4],\n",
       "    bias=f32[8],\n",
       "    in_features=4,\n",
       "    out_features=8,\n",
       "    use_bias=True\n",
       "  ),\n",
       "  k=Linear(\n",
       "    weight=f32[8,4],\n",
       "    bias=f32[8],\n",
       "    in_features=4,\n",
       "    out_features=8,\n",
       "    use_bias=True\n",
       "  ),\n",
       "  v=Linear(\n",
       "    weight=f32[8,4],\n",
       "    bias=f32[8],\n",
       "    in_features=4,\n",
       "    out_features=8,\n",
       "    use_bias=True\n",
       "  ),\n",
       "  sqrt_d_attn=f32[]\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SelfAttention(4, 8, 4, jax.random.PRNGKey(10))\n",
    "jax.grad(lambda model, x: jnp.mean(model(x)))(model, jnp.ones((3, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c4050b-1887-4636-8f6d-ca45dc79df19",
   "metadata": {},
   "source": [
    "ここで困ったことに、`sqrt_d_attn`での偏微分も計算されてしまいました。`eqx.Module`そのものがパラメタを持つことによって、定数であるようなメンバ変数に対しても偏微分が計算されてしまいます。この問題を、equinoxでは、[`eqx.partition`](https://docs.kidger.site/equinox/api/filtering/partition-combine/#equinox.partition)と[`eqx.is_inexact_array`](https://docs.kidger.site/equinox/api/filtering/partition-combine/#equinox.is_inexact_array)を使って、「32bit浮動小数点の`jax.Array`または`numpy.ndarray`」と「その他のメンバ変数」を分離することにより解決しています。やってみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baff873a-9915-49b0-960d-6f582e452183",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(SelfAttention(\n",
       "   q=Linear(\n",
       "     weight=f32[8,4],\n",
       "     bias=f32[8],\n",
       "     in_features=4,\n",
       "     out_features=8,\n",
       "     use_bias=True\n",
       "   ),\n",
       "   k=Linear(\n",
       "     weight=f32[8,4],\n",
       "     bias=f32[8],\n",
       "     in_features=4,\n",
       "     out_features=8,\n",
       "     use_bias=True\n",
       "   ),\n",
       "   v=Linear(\n",
       "     weight=f32[8,4],\n",
       "     bias=f32[8],\n",
       "     in_features=4,\n",
       "     out_features=8,\n",
       "     use_bias=True\n",
       "   ),\n",
       "   sqrt_d_attn=None\n",
       " ),\n",
       " SelfAttention(\n",
       "   q=Linear(weight=None, bias=None, in_features=4, out_features=8, use_bias=True),\n",
       "   k=Linear(weight=None, bias=None, in_features=4, out_features=8, use_bias=True),\n",
       "   v=Linear(weight=None, bias=None, in_features=4, out_features=8, use_bias=True),\n",
       "   sqrt_d_attn=2.8284270763397217\n",
       " ))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eqx.partition(model, eqx.is_inexact_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e2243f-296a-41cb-9e92-5e7ed671e245",
   "metadata": {},
   "source": [
    "`sqrt_d_attn=None`のものと、全てのパラメタがNoneで`sqrt_d_attn=2.8284270763397217`のものに分離されています。なので、定数を持つModuleに対して勾配を計算したい場合は\n",
    "1. Moduleをパラメタとそれ以外に分割\n",
    "2. 勾配を求めたい関数`f(module, ...)`をラップする関数`g(params, others, ..)`みたいなものを作る\n",
    "3. `jax.grad(g)(params, others, ...)`で勾配を計算\n",
    "という流れになります。面倒ですね。\n",
    "長々説明したのですが、これを全部やってくれるのが、`equinox.filter_value_and_grad`です。基本的にもうこれを使えばいいです。やってみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b76481a-6851-42a4-b936-31eb3cf5dcfa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(-0.06870803, dtype=float32),\n",
       " SelfAttention(\n",
       "   q=Linear(\n",
       "     weight=f32[8,4],\n",
       "     bias=f32[8],\n",
       "     in_features=4,\n",
       "     out_features=8,\n",
       "     use_bias=True\n",
       "   ),\n",
       "   k=Linear(\n",
       "     weight=f32[8,4],\n",
       "     bias=f32[8],\n",
       "     in_features=4,\n",
       "     out_features=8,\n",
       "     use_bias=True\n",
       "   ),\n",
       "   v=Linear(\n",
       "     weight=f32[8,4],\n",
       "     bias=f32[8],\n",
       "     in_features=4,\n",
       "     out_features=8,\n",
       "     use_bias=True\n",
       "   ),\n",
       "   sqrt_d_attn=None\n",
       " ))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eqx.filter_value_and_grad(lambda model, x: jnp.mean(model(x)))(model, jnp.ones((3, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6ad199-ba26-4686-80c8-1119232cc6a2",
   "metadata": {},
   "source": [
    "計算結果と勾配を返してくれました。定数の`sqrt_d_attn`はきっちり`None`でマスクされています。なので、Moduleで定数を持ちたかったらとりあえず`jax.Array`か`ndarray`以外の型にしておけばいいです。じゃあ32bit浮動小数点型の`jax.Array`を定数として持ちたかったらどうすればいいんだというと、`filter_value_and_grad`を直接使えずめちゃくちゃ面倒になるので、避けたほうが良さげです。ただPython組み込みの`float`や`bool`もjitコンパイルしてしまえばただの定数になるので、パフォーマンス的には気にする必要はないです。\n",
    "\n",
    "# 強化学習してみる\n",
    "\n",
    "一通り`equinox`の特徴をおさらいしたところで、これを使って強化学習してみます。せっかくjaxを使っているの~~とgymのAPIが変わりまくった上にgymnasiumに変わって全然ついていけないの~~で、[gynmax](https://github.com/RobertTLange/gymnax)というjax製のRLベンチマークライブラリを使ってみます。環境は`FourRooms-misc`というのを使ってみます。`-misc`というのがどういう意味なのかよくわかりませんが、これは[Sutton et al. 1999](https://people.cs.umass.edu/~barto/courses/cs687/Sutton-Precup-Singh-AIJ99.pdf)に出てくる有名な迷路です。なんだかんだ迷路がわかりやすいですよね。[Githubの例](https://github.com/RobertTLange/gymnax#key-selling-points-)を少し改変して、迷路を表示してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6edeabd9-809d-4a59-8e35-c9c5cbd8fd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yu/Documents/rlog2/.venv/lib/python3.10/site-packages/gymnax/environments/minatar/freeway.py:283: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in array is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return jnp.array(cars, dtype=jnp.int_)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcEAAAHaCAYAAAB4qvKwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAc40lEQVR4nO3de3CU5dnH8d8GckKSAIkBAggoCgVBjJystglpIRwElYgigQkjYkE72jIdEa0KtVVhKLxDdZyKVSmC6AylVMEUqyROtYiKnOogoRhbBC0gCRnBQNjr/YN392XZnMDggtf3M5NRn8Pm3ie7+ebZZ+81YGYmAAAciov1AAAAiBUiCABwiwgCANwiggAAt4ggAMAtIggAcIsIAgDcIoIAALeIIADALSIYY4FAoN6v3NzcWA/xO6ekpCR8fNPS0vT111/Xue2dd94Z3nbSpEkR655//nkFAgHNmjWryccY+r5xcXH69NNPm/z2z0XBYFDPPvusBg8erDZt2ig+Pl5t27bVFVdcoSlTpmjp0qWxHuIZKS0t1ezZszVy5EhdeOGFCgQC6tKlS6yHhf/TPNYDwAlFRUW1Lu/Ro8e3PBJfDh06pFdffVU33XRT1Lpjx47p5Zdf/tbHdPToUb300kuSJDPT0qVLdf/999e67aRJk7R48WKtW7fuvP6D6ejRo7r++utVXFysuLg4DRgwQJ07d1Z1dbU2b96sZ555RkuWLFFhYWF4n/LycnXt2lU5OTkqKSmJ3eAbcM8992jz5s2xHgbqQATPEc8//3ysh+BO79699dFHH+mFF16oNYKvvfaaDhw4oOzsbG3cuDFq/Y033qhBgwYpIyOjSce1Zs0affnll2rfvr327t2rJUuW1BnB74onnnhCxcXF6tSpk9auXRv1x98///lPLV68OEaj+2aGDh2qsWPHqn///urYsaN69eoV6yHhJEQQbmVmZqpDhw567bXXdPDgQbVu3Tpi/QsvvKC4uDiNHz++1gimpaUpLS2tyce1ZMkSSdLs2bM1Z84cbd++Xe+//7769evX5N/rXLFixQpJ0kMPPVTrqx+9evXS3Llzv+1hNYmTx/3555/HcCSoDdcEzxOTJk1SIBBQSUmJ/vrXv2rw4MFq1aqVAoGAKioqwtsVFxeHrz0kJibq4osv1vTp03XgwIFab9fM9OKLLyovL0+tW7dWUlKSvve972nWrFk6fPhwnWM42Z///OfwdbOdO3dGrHviiScUCAQ0b9688LK9e/dq7ty5ysnJUYcOHZSQkKB27dppzJgxeu+99yL2r66uVkZGhlq0aBFxP0/2zjvvKBAIKCcnp4GjGG3ChAk6evRo1Muehw4d0iuvvKLc3Fx16NCh1n3ruiZ48nF66623lJeXp5SUFKWmpmrkyJH66KOP6hxPRUWFVq9ercTERN18883hl/9CYTxZIBAInx0NHjw44lpyeXl5+PZ+97vfKT8/X507d1ZiYqLS09M1bNgwvf7667WOITc3N3wby5Yt06BBg5SSkqJWrVrVOe5vat++fZKkCy+8sFHbz5o1S127dpV04prbyff91Gu3X375pWbOnKmePXsqOTlZaWlpysvL06uvvhp1u+Xl5eFr8YcOHdI999yjTp06hZ8XCxYsUDAY/GZ3FucUInieWbZsmYYPH66vvvpKw4cPV//+/RUIBCRJ9913n4YPH66//e1v6t69u0aPHq3mzZtrwYIFGjhwoL744ouI2woGgyosLNT48eP13nvvqW/fvhoxYoS++uorzZ49W4MHD9aRI0fC24cic2oE161bF/73utadfL1q1apVmjFjhr744gv16dNHN954o7KysrRy5Updc801Wrt2bXjbxMREFRUV6ciRI3W+MWLRokWSpDvuuKMRRzDSDTfcoAsuuCDqtlesWKGvv/464hrU6XrllVeUl5enw4cPa8SIEWrfvr3WrFmjH/7wh3WeEbz88suqrq7Wddddp7S0NE2YMEGStHz5ctXU1ERsW1RUpEsuuUSSlJ+fr6KiovBXy5YtJUnr16/X3XffrR07dqh79+668cYb1b17d61du1b5+fl69tln6xz/Y489pokTJyohIUHXXXedLr/88jM+Fg3p1KmTJOmZZ57RsWPHGty+b9++KigokCS1bds24r5fe+214e127Nihvn376vHHH9eRI0eUn5+vfv366d1339WoUaMi/jg7WXV1tfLy8vTHP/5RAwYM0JAhQ/Tpp59q+vTpuu2225rgHuOcYYgpSdaYH0NRUVF42+XLl0etf/nll02SXX755VZWVhZeHgwG7aGHHjJJdsstt0TsM3fuXJNkubm5tnfv3vDy6upqmzx5skmyGTNmhJfv2rXLJFlOTk7E7fTp08cuueQSS0pKssLCwojvnZGRYampqVZTUxNevmXLFtu2bVvUfSguLraEhAS75JJLLBgMhpd//PHHFggE7Iorrojap7Ky0lq0aGGtW7e2I0eO1HLkoq1bt84k2Y9+9CMzMyssLLRAIGDl5eXhbfLy8iwpKckqKyvtxRdfNElWVFQUcTvPPfecSbKHH344YnnoZxUXF2crV64ML6+pqbGCggKTZA8++GCtY7v22mtNUsR+AwYMMEm2evXqqO1D32vdunW13t6uXbvsH//4R9TyjRs3WqtWrSw1NdWqqqoi1uXk5JgkS0pKspKSklpvt6mFjrEku+iii+xnP/uZvfTSS7Zz58469/nkk09qfTyG1NTUWO/evU2SzZ07144fPx5eV1ZWZl27drVmzZrZ1q1bo25TkvXp08f27dsXXrdz507LysqK+vmcjr1795ok69y58xntj6ZHBGMs9ISr6+uTTz4xs///ZTdy5Mhab+eKK64wSRFP6JBgMGh9+/a1Zs2ahZ/Ux44ds4yMDLvgggvs888/j9rn8OHD1q5dO2vdunXEL4+LLrrIEhMTw8E5cOCABQIBmzZtmuXk5FjHjh3D227ZssUk2YgRIxp9PAoLC02SbdmyJWJ5Xl6eSbINGzZELH/qqadMkt19992N/h6nRnDNmjUmyR599FEzM9u9e7fFxcXZ2LFjzczOOIIn/0EQ8v7779f5i3vXrl0WCASsTZs2Vl1dHV6+cOFCk2Tjxo2L2qehCNbngQceMEn2l7/8JWJ5KIJ33XXXad/mNzF//ny74IILop4DXbp0scceeyzqj5yGIrhy5UqTZAUFBbWu/9Of/hT12Dk5gmvXro3aJ/R4Cz12ThcRPPfwxphzRF1TJEIva4WMHj06apv//ve/2rx5sy699NJaX7IKBAK65pprtGnTJn3wwQfKz8/Xxo0btX//fg0ZMkRt27aN2ic5OVlXXXWVVq9erbKyMnXv3l3SiZdElyxZovXr1ys3N1elpaUyM+Xm5iozM1OlpaXauXOnunXrFn5ptLa37ldXV6u4uFgbNmzQvn37dPToUUnS1q1bJUllZWXq3bt3ePupU6fqzTff1KJFi9S/f//w8m/yUmjIkCFDlJmZqaVLl2rmzJlatmyZgsFg+KXIMzV06NCoZZdddpmkE9dFT7V06VKZmW6++WYlJCSEl48bN07Tp0/XqlWrVFVVpZSUlNMax/Hjx/XGG2/onXfe0d69e1VdXS3pxDE++Z+nqu2xdjb9/Oc/V1FRkVasWKGSkhK99957KisrU3l5uWbOnKlVq1bpzTffVHJycqNuL/Sy+pgxY2pd/4Mf/ECStGHDhqh1bdq00ZAhQ6KW33rrrZo2bZreeecdBYNBxcVxRel8RwTPEY2dInHRRRdFLQu9CaKsrCx8fbAu+/fvj9jn9ddfb9Q+oQjm5uZqyZIlKikpUW5ubkToMjMzNXv2bJWUlNQbwa1bt2r06NHhMdSmqqoq4r9vuOEGtWvXTi+++KLmz5+vli1bauPGjdq4caOuvvrqiLedP/7449q+fXvE/j169NB9991X6/dq3ry5xo0bp4ULF2rTpk164YUXlJ6eruHDh9d7XBrSsWPHqGWhgIVCdLLQm19Oje+FF16o/Px8rV69WitWrIh640d9du/ereuuu67eeWqnHuuQ2h5r9fn73/+uZ555Jmr5vHnzGj2NpE2bNpoyZYqmTJkiSfr000/15JNPav78+Vq/fr3mz5+vBx54oFG3FXp8FRYW1nttN/ScOFnnzp1r3TYtLU2tWrVSRUWFDh48qPT09EaNBecuInieSUpKiloWerdau3btlJ+fX+/+oSd3aJ9u3brpmmuuqXefk5/ooaCFAldSUqKePXsqMzNTqampSkxMVElJiSZPnqy33npLKSkpys7ODu8fOtMpLy/X1KlTNXXqVF188cVq2bKlAoGA7r//fj322GMys4gxxMfH67bbbtOjjz6q5cuX6/bbbw//wg39wgwpLi5WaWlpxLKcnJw6Iyid+EW5cOFCzZw5U1u2bNG0adMUHx9f73FpyOmcJWzYsEE7duyQJM2YMSNq/e7duyWdCOXpRPD222/X5s2bVVBQoHvvvVfdu3dXSkqK4uLi9PTTT+snP/lJ1LEOqe2xVp+dO3fWOpdv1qxZZzyXsnPnzpo7d65qamq0YMECrV69utERDD3Ghw0bVuurHSFNPc8T5xci+B0QOuPIyMho9BllaJ8ePXqc1kT9iy++WJ06ddL69eu1Z88ebd26VdOmTZN04pfmoEGDVFpaqm3btmn//v0aPny4mjVrFt5/+/bt2r59u/r166ennnoq6vZ37dpV5/e+44479Pjjj2vRokUaP368li1bptTUVN1yyy0R253Jp4cMGDBAl156qYqLiyVFn42dbSdPgXj77bfr3K6kpES7d++u9SzzVF999ZVef/11tW3bVi+99FLEz0Gq/1ifiUmTJp1WoE9HXl6eFixYUOtZW11Cx+j2228Pv5O0sf7973/XuvzQoUOqqKhQcnLyWZ0ygm8PL2h/B3Ts2FE9evTQRx99FD6baEj//v2Vlpam0tJSffnll6f1/XJyclRdXa05c+bIzDR48ODwutzcXO3evTt8lnbqS6EHDx4Mj/lUBw8erHPumnTirGDYsGHasGGDfvnLX6qyslKFhYVq0aLFaY2/LrfddpvS09PVt29fff/732+S22yMmpqa8Mekbdu2TXbiDWtRX5MmTVIwGIyYzhG6dnjq9AlJqqysVDAYVPv27aMCeOzYMa1cufIs3qvTU9fZaEho/unJczbru++Swtf0zuR+HjhwQG+88UbU8uXLl0uSrr766qhjivMTEfyOePDBBxUMBlVQUKBNmzZFrT9w4ED4TSTSifl39957r6qqqjRmzJhazwo+++yzWidph8L29NNPR01SP3mdpKgJ7N26dVNcXJzefPPNiDdkfP3115o6dWqDQZ46daokacGCBZKiXwr9Ju677z7t379fH374YZPdZmMUFxdr37596t27d70fqXXrrbdKOvFJNiFZWVmSpI8//jhq+8zMTKWlpWnbtm0RZ5fHjx/XjBkzGv0H07dh9OjRWrhwYa0//3fffVePPPKIJEV8vF1GRobi4+P1r3/9S8ePH4/ar6CgQD179tTSpUv1yCOPRF2HNTO9/fbbdZ55/+IXv4j4kIlPPvlEv/rVryRJd9111+nfSZybYvGWVPw/neY8wfreCn///feH56dlZ2fb2LFj7aabbrIrr7zSmjVrZmlpaRHbHz9+3CZOnGiSLCEhwQYOHGjjxo2zMWPGWK9eveqcm1dWVhYed69evSLWHTlyxBITE02SpaSk2LFjx6L2nzJlikmy5ORkGzlypN10003Wtm1by8jIsEmTJpkke+6552q9jzU1NdapUyeTZP369WvosNXq1CkSDTnTKRJ1/ax0ylvkb7755ogpGnWpqamxzMxMk2QffvihmZ2YchEIBCwpKcmuv/56mzx5sk2ePNn2799vZma/+c1vTJI1a9bMhgwZYrfccot16dLFkpOT7a677qp1/KEpEqHpOd+G0BSf5s2bW79+/Wzs2LFWUFBgffv2DT/WRo0aFfV4GjVqVPhxOHHiRJs8ebI9++yz4fU7duywrl27miTLzMy0H//4xzZ+/HgbOnRo+FguWLAgvH1oisSgQYMsOzvbWrVqZWPGjLFRo0ZZixYtTJJNmDDhtO7bokWLbODAgTZw4EDLzs6OeL6Fvj744INvdPxw5ohgjDVlBM3MSktLbezYsZaVlWXx8fGWnp5uffr0sZ/+9KdWWlpa6z6rVq2ykSNHWmZmpsXHx1tmZqZdddVVdu+999b55OzYsWOdc8lCv0SHDRtW6741NTX229/+1nr27GlJSUnWtm1bKywstPLycnv44YfrjaCZ2YQJE0yS/f73v6/3WNTlXIpgZWWlJSUlmSTbtWtXg2MJhWv69OnhZUuXLrXs7GxLTk6Oml9qZrZ48WK78sorrUWLFpaenm7XX3+9bd68uc7xxyKCZWVltnDhQhs1apRddtll1rJlS4uPj7f27dvbyJEjbdmyZREfoBDyxRdf2MSJE61du3bWrFmzWn9OFRUV9utf/9qys7OtZcuWlpSUZF26dLH8/Hx78sknIybEnzz3sKKiwu68807LysqyhIQE6969u82bNy/igx8aI/SYru/rTOZ5omkEzBp4MR44hxw+fFgdOnRQTU2N9uzZc9pz5oD6nC//eyY0Ha4J4rzy5JNPqqKiQkVFRQQQwDfGFAmc8w4cOBD+wO01a9aoZcuW9c75A4DGIoI451VVVekPf/iDEhISdOWVV2revHmNmicHAA3hmiAAwC2uCQIA3CKCAAC3GnVNMBgMht+O3tD/cQAAgFgzM1VVVSkrK6veD7NvVAT37NmjTp06NdngAAD4NvznP/+p9410jXo5lPlYAIDzUUP9alQEeQkUAHA+aqhfvDEGAOAWEQQAuEUEAQBuEUEAgFtEEADgFhEEALhFBAEAbhFBAIBbRBAA4BYRBAC4RQQBAG4RQQCAW0QQAOAWEQQAuEUEAQBuEUEAgFtEEADgFhEEALhFBAEAbhFBAIBbRBAA4BYRBAC4RQQBAG4RQQCAW0QQAOAWEQQAuEUEAQBuEUEAgFtEEADgFhEEALhFBAEAbhFBAIBbRBAA4BYRBAC4RQQBAG4RQQCAW0QQAOAWEQQAuEUEAQBuEUEAgFvNT2fj9NFDFRcff7bG0mT2rVgd6yEAwHfS4pkjYj2ERjlSfUxT57/e4HacCQIA3CKCAAC3iCAAwC0iCABwiwgCANwiggAAt4ggAMAtIggAcIsIAgDcIoIAALeIIADALSIIAHCLCAIA3CKCAAC3iCAAwC0iCABwiwgCANwiggAAt4ggAMAtIggAcIsIAgDcIoIAALeIIADALSIIAHCLCAIA3CKCAAC3iCAAwC0iCABwiwgCANwiggAAt4ggAMAtIggAcIsIAgDcIoIAALeIIADALSIIAHCLCAIA3CKCAAC3iCAAwC0iCABwiwgCANwiggAAtwJmZg1tdOjQIaWlpSl99FDFxcd/G+P6Rvp0+yzWQ2i0N+ZsivUQAOA7q7KyUqmpqXWu50wQAOAWEQQAuEUEAQBuEUEAgFtEEADgFhEEALhFBAEAbhFBAIBbRBAA4BYRBAC4RQQBAG4RQQCAW0QQAOAWEQQAuEUEAQBuEUEAgFtEEADgFhEEALhFBAEAbhFBAIBbRBAA4BYRBAC4RQQBAG4RQQCAW0QQAOAWEQQAuEUEAQBuEUEAgFtEEADgFhEEALhFBAEAbhFBAIBbRBAA4BYRBAC4RQQBAG4RQQCAW0QQAOAWEQQAuEUEAQBuEUEAgFtEEADgFhEEALjVPNYDOBvemLMp1kMAgO+kH83oG+shNEpN9XGV/s/WBrfjTBAA4BYRBAC4RQQBAG4RQQCAW0QQAOAWEQQAuEUEAQBuEUEAgFtEEADgFhEEALhFBAEAbhFBAIBbRBAA4BYRBAC4RQQBAG4RQQCAW0QQAOAWEQQAuEUEAQBuEUEAgFtEEADgFhEEALhFBAEAbhFBAIBbRBAA4BYRBAC4RQQBAG4RQQCAW0QQAOAWEQQAuEUEAQBuEUEAgFtEEADgFhEEALhFBAEAbhFBAIBbRBAA4BYRBAC4RQQBAG4RQQCAW0QQAOAWEQQAuNU81gM4G9559UCsh9Bo378uPdZDAIBGe2POplgPoUlxJggAcIsIAgDcIoIAALeIIADALSIIAHCLCAIA3CKCAAC3iCAAwC0iCABwiwgCANwiggAAt4ggAMAtIggAcIsIAgDcIoIAALeIIADALSIIAHCLCAIA3CKCAAC3iCAAwC0iCABwiwgCANwiggAAt4ggAMAtIggAcIsIAgDcIoIAALeIIADALSIIAHCLCAIA3CKCAAC3iCAAwC0iCABwiwgCANwiggAAt4ggAMAtIggAcIsIAgDcIoIAALeIIADALSIIAHCLCAIA3CKCAAC3iCAAwC0iCABwiwgCANwiggAAt4ggAMAtIggAcIsIAgDcIoIAALeIIADALSIIAHCLCAIA3CKCAAC3iCAAwC0iCABwiwgCANwiggAAt4ggAMAtIggAcIsIAgDcIoIAALeIIADALSIIAHCLCAIA3CKCAAC3iCAAwC0iCABwiwgCANwiggAAt4ggAMAtIggAcIsIAgDcIoIAALeIIADALSIIAHCLCAIA3CKCAAC3iCAAwC0iCABwiwgCANwiggAAt4ggAMAtIggAcKt5rAdwNpS9PTHWQwAAnAc4EwQAuEUEAQBuEUEAgFtEEADgFhEEALhFBAEAbhFBAIBbRBAA4BYRBAC4RQQBAG4RQQCAW0QQAOAWEQQAuEUEAQBuEUEAgFtEEADgFhEEALhFBAEAbhFBAIBbRBAA4BYRBAC4RQQBAG4RQQCAW0QQAOAWEQQAuEUEAQBuEUEAgFtEEADgFhEEALhFBAEAbhFBAIBbRBAA4BYRBAC4RQQBAG4RQQCAW0QQAOAWEQQAuEUEAQBuEUEAgFtEEADgFhEEALhFBAEAbjWP9QDOhj8G98R6CACA8wBnggAAt4ggAMAtIggAcIsIAgDcIoIAALeIIADALSIIAHCLCAIA3CKCAAC3iCAAwC0iCABwiwgCANwiggAAt4ggAMAtIggAcIsIAgDcIoIAALeIIADALSIIAHCLCAIA3CKCAAC3iCAAwC0iCABwiwgCANwiggAAt4ggAMAtIggAcIsIAgDcIoIAALeIIADALSIIAHCLCAIA3CKCAAC3iCAAwC0iCABwiwgCANwiggAAt4ggAMAtIggAcIsIAgDcIoIAALeIIADAreaxHsDZ8PjP1sV6CI3Wf07rWA8BANziTBAA4BYRBAC4RQQBAG4RQQCAW0QQAOAWEQQAuEUEAQBuEUEAgFtEEADgFhEEALhFBAEAbhFBAIBbRBAA4BYRBAC4RQQBAG4RQQCAW0QQAOAWEQQAuEUEAQBuEUEAgFtEEADgFhEEALhFBAEAbhFBAIBbRBAA4BYRBAC4RQQBAG4RQQCAW0QQAOAWEQQAuEUEAQBuEUEAgFtEEADgFhEEALhFBAEAbhFBAIBbRBAA4BYRBAC4RQQBAG4RQQCAW0QQAOAWEQQAuBUwM2too0OHDiktLe3bGA/QJLI6XRPrITTanv+8HeshAN9ZlZWVSk1NrXM9Z4IAALeIIADALSIIAHCLCAIA3CKCAAC3iCAAwC0iCABwiwgCANwiggAAt4ggAMAtIggAcIsIAgDcIoIAALeIIADALSIIAHCLCAIA3CKCAAC3iCAAwC0iCABwiwgCANwiggAAt4ggAMAtIggAcIsIAgDcIoIAALeIIADALSIIAHCLCAIA3CKCAAC3iCAAwC0iCABwiwgCANwiggAAt4ggAMAtIggAcIsIAgDcIoIAALeIIADALSIIAHCLCAIA3CKCAAC3mjdmIzM72+MAmlQwWBPrIQA4BzTUr0ZFsKqqqkkGA3xbPv/s3VgPAcA5oKqqSmlpaXWuD1gjTvOCwaD27NmjlJQUBQKBJh0gAABNzcxUVVWlrKwsxcXVfeWvUREEAOC7iDfGAADcIoIAALeIIADALSIIAHCLCAIA3CKCAAC3iCAAwK3/BbPVYUFZZsPeAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gymnax\n",
    "from gymnax.visualize import Visualizer\n",
    "\n",
    "key = jax.random.PRNGKey(42)\n",
    "key_reset, key_act, key_step, key = jax.random.split(key, 4)\n",
    "\n",
    "# Instantiate the environment & its settings.\n",
    "env, env_params = gymnax.make(\"Freeway-MinAtar\")\n",
    "\n",
    "# Reset the environment.\n",
    "obs, state = env.reset(key_reset, env_params)\n",
    "\n",
    "# Sample a random action.\n",
    "action = env.action_space(env_params).sample(key_act)\n",
    "\n",
    "# Perform the step transition.\n",
    "n_obs, n_state, reward, done, _ = env.step(key_step, state, action, env_params)\n",
    "\n",
    "vis = Visualizer(env, env_params, [state])\n",
    "vis.init()\n",
    "vis.update(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46e4ee0-d14a-4053-80ab-321f927ecb84",
   "metadata": {},
   "source": [
    "正直このVisualizerのAPIはどうなんだろう？という気もしますが...きちんとFour Roomsを表示してくれました。\n",
    "\n",
    "# DQNを試してみる\n",
    "とりあえずPPOをやってみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fca7a0eb-ead2-4952-8900-b87c96aea1ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space(env_params).n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2e08e02-e07c-4b96-90ed-26a77f24682b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space(env_params).high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf54cb73-f5e6-4528-9942-d24aca83d924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28760915-3974-4931-b1d9-300918b8ace2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8d524b-8973-444c-b7a1-d1fdb9dc7442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d7ee97-0681-4707-9538-e08b2f09956e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlog2",
   "language": "python",
   "name": "rlog2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
