{
 "cells": [
  {
   "cell_type": "raw",
   "id": "02857a90-76ff-41d8-a163-45df6489d423",
   "metadata": {},
   "source": [
    "---\n",
    "title: equinoxで強化学習してみる\n",
    "date: 06/9/2023\n",
    "categories: [ja, RL, deep]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f24cd72-d1b9-431b-a7f5-1c599be663df",
   "metadata": {},
   "source": [
    "最近[equinox](https://docs.kidger.site/equinox/)という[jax](https://jax.readthedocs.io/en/latest/)ベースの深層学習モデルを定義するライブラリを使ってみたのですが、これが中々いいと思ったので紹介ついでに強化学習してみます。他のjaxベースのライブラリにはDeepmindの[haiku]()やGoogle Researchの[flax]()があります。この2つは兄弟みたいなもので、実際のところあまり変わりはありません。というのも、jaxには試験的に書かれた[stax](https://jax.readthedocs.io/en/latest/jax.example_libraries.stax.html)というモデル定義用ライブラリがあり、haikuもflaxもstaxをベースにしているからです。equinoxのドキュメントにある[Compatibility with init-apply libraries](https://docs.kidger.site/equinox/examples/init_apply/)というページでは、これらのライブラリのやり方を「init-applyアプローチ」と呼んで軽く説明しているのですが、僕もこれについて簡単な説明を試みてみます。\n",
    "\n",
    "# Jaxにできることとできないこと\n",
    "\n",
    "そもそも、jaxというのは何をしてくれるライブラリなのでしょうか。ホームページのトップにはこう書かれています。\n",
    "\n",
    "> JAX is Autograd and XLA, brought together for high-performance numerical computing.\n",
    "\n",
    "[XLA](https://www.tensorflow.org/xla)というのは、Tensorflowのバックエンドとして開発された深層学習用の中間言語で、CPU/GPU用に数値計算コードを最適化してくれます。Jaxは、numpyライクなシンタックスのコードをXLAに変換することで、「numpyコードを高速にjitコンパイルすること」を可能にしています。では、Autogradというのはなんでしょうか？これは、jaxの開発者が以前に開発していたライブラリの名前でもありますが、自動微分全般のことを指すと考えていいでしょう。自分で勾配逆伝播のコードを書かなくても、jaxでは損失関数の各パラメタにおいての偏微分を勝手に計算してくれます。試しに、$f(x, y) = x^2 + y$の偏微分を計算してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff14064e-d950-4450-bd62-cf9c590b96ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([ 6.,  8., 10., 12., 14.], dtype=float32),\n",
       " Array([1., 1., 1., 1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def f(x: jax.Array, y: jax.Array) -> jax.Array:\n",
    "    return jnp.sum(x ** 2 + y)\n",
    "\n",
    "x = jnp.array([3, 4, 5, 6, 7], dtype=jnp.float32)\n",
    "y = jnp.array([5, 2, 5, 7, 2], dtype=jnp.float32)\n",
    "jax.grad(f, argnums=(0, 1))(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583a0026-e6a9-43fe-9541-d47ef341d777",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "fのxでの偏微分は$2x$、yでの偏微分は$1$なので、正しく計算されているようですね。こんな感じで、jaxは自動的に偏微分を計算してくれるので、これをそのまま勾配降下法に使ってモデルを学習させることができます。深層ニューラルネットワークも結局パラメタと入力を引数にとる関数なので、全部のパラメタについて偏微分を計算してもらえば、それを使って学習できます。ついでに、この勾配をとる計算やモデルを更新する計算を`jax.jit`につっこめば、高速に計算してくれます。\n",
    "ここで問題になるのは、深層ニューラルネットワークの場合パラメタが多すぎるので、こんな風にいちいち全てのパラメタについて変数を割り当ててプログラムを書いていたら大変すぎる、ということです。大変すぎる以外に何か問題はあるのかというと、特にないと思います。コードの再利用性くらいでしょうか。しかし、まあ面倒なものは面倒ですから、**パラメタを管理してくれる仕組み**がほしいなあ、と思うわけですね。\n",
    "\n",
    "# init-applyアプローチによるパラメタ管理\n",
    "\n",
    "stax, haiku, flaxでは、この「パラメタ管理の問題」を、「init-applyアプローチ」により解決しています。このアプローチは以下のようにまとめられます。\n",
    "1. モデルはパラメータを持たない\n",
    "2. モデルは`init`と`apply`の2つの関数を持つ\n",
    "  - `init`は、入力例を受け取ってパラメタを初期化し、最初のパラメータを返す\n",
    "  - `apply`は、入力とパラメタを受け取り、モデルの計算結果を返す\n",
    "  \n",
    "なので、flaxやhaikuのAPIは以下のような感じになります。flaxでは`__call__`を使うがhaikuはPyTorchと同じ`forward`を使う、`flax`のModuleは [`dataclasses.dataclass`](https://docs.python.org/ja/3/library/dataclasses.html#dataclasses.dataclass)デコレータにより定義されたクラスと同じような性質を持つなどの違いがありますが、まあそれくらいで、大して違いはないです。以下僕がflaxとhaikuの間をとって書いた適当な疑似コードです。\n",
    "```python\n",
    "class Linear(Module):\n",
    "    def __call__(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        w = self.parameter(output, shape=(batch_size, self.output_size) init=self.w_init)\n",
    "        b = self.parameter(output, shape=(1, self.output_size) init=self.w_init)\n",
    "        return w @ x.T + b\n",
    "\n",
    "model = Linear(output_size=10)\n",
    "params = model.init(jnp.zeros(1, 100))\n",
    "result = model.apply(params, jnp.zeros(10, 100))\n",
    "```\n",
    "こんな感じですかね。なお、上疑似コード中の`self.parameter`というメソッドは`flax`や`haiku`にある「パラメータをクラスに登録する機能」のことです。この機能により、各パラメタを値としてもつ`dict`を`init`により返すことができます。haikuの場合は、`params`は以下のような`dict`になっています。\n",
    "```python\n",
    "{\n",
    "    \"Linear/~/w\": [...],\n",
    "    \"Linear/~/b\": [...],\n",
    "}\n",
    "```\n",
    "\n",
    "`stax`はただのreference implementationなのでこのような機能がなく、かわりに、数のレイヤーを組み合わせるコンビネーターを提供しています。ではこのアプローチにはどのようなメリット、デメリットがあるでしょうか。\n",
    "\n",
    "**メリット**\n",
    "- Moduleを初期化する際に入力されるArrayのshapeを指定しなくていい\n",
    "  - Moduleの定義は`init`と`apply`という2つの関数を定義するだけで、パラメタは`init`が呼ばれた際に初期化されるため。\n",
    "- 関数とデータを分離できる\n",
    "  - Moduleは変更可能な変数を持たず、パラメタと完全に別に扱われるので、見通しがよくなる\n",
    "- `init`、`apply`に`jit`や`vmap`などの関数デコレータを適用するコードが自然に書ける\n",
    "  \n",
    "**デメリット**\n",
    "- Moduleは冗長\n",
    "  - Moduleは「出力の次元」などの「モデルに関する設定」を持っているだけ\n",
    "  - これらは`Config`など専用のクラスで管理されることが多く、`Config`と`Module`を両方持つのは冗長\n",
    "- パラメタの要素のアクセスに型検査をするのが難しい\n",
    "  - 例えばhaikuなら`params[\"Linear/~/w\"]`のようにしてパラメタの各要素にアクセスするが、これは`pyright`や`mypy`のような静的型検査ツールにより検査されないので、実行時エラーを起こしやすい\n",
    "- あまりオブジェクト指向的ではない\n",
    "- (特にflax, haiku)仕組みがわかりにくい\n",
    "\n",
    "こんなところでしょうか。\n",
    "\n",
    "# equinoxの特徴\n",
    "\n",
    "equinoxの特徴は、init-applyアプローチと異なり、「よりオブジェクト指向的な」（または、PyTorchに近い）インターフェースを志向している点にあります。\n",
    "ドキュメントのトップページにある[Quick example](https://docs.kidger.site/equinox/#quick-example)を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e12206f3-c53e-42d3-89f2-1dfdf01c85f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weight? [[0.59902626 0.2172144 ]\n",
      " [0.660603   0.03266738]\n",
      " [1.2164948  1.1940813 ]]\n",
      "Grad? Linear(weight=f32[3,2], bias=f32[3])\n"
     ]
    }
   ],
   "source": [
    "import equinox as eqx\n",
    "import jax\n",
    "\n",
    "class Linear(eqx.Module):\n",
    "    weight: jax.Array\n",
    "    bias: jax.Array\n",
    "\n",
    "    def __init__(self, in_size, out_size, key):\n",
    "        wkey, bkey = jax.random.split(key)\n",
    "        self.weight = jax.random.normal(wkey, (out_size, in_size))\n",
    "        self.bias = jax.random.normal(bkey, (out_size,))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.weight @ x + self.bias\n",
    "    \n",
    "@jax.jit\n",
    "@jax.grad\n",
    "def loss_fn(model, x, y):\n",
    "    pred_y = jax.vmap(model)(x)\n",
    "    return jax.numpy.mean((y - pred_y) ** 2)\n",
    "\n",
    "batch_size, in_size, out_size = 32, 2, 3\n",
    "model = Linear(in_size, out_size, key=jax.random.PRNGKey(0))\n",
    "x = jax.numpy.zeros((batch_size, in_size))\n",
    "y = jax.numpy.zeros((batch_size, out_size))\n",
    "grads = loss_fn(model, x, y)\n",
    "print(\"Model weight?\", model.weight)\n",
    "print(\"Grad?\", grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aeb7a5-8159-4f19-a55d-639da0c23de0",
   "metadata": {},
   "source": [
    "ここで、equinoxが他の「"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlog2",
   "language": "python",
   "name": "rlog2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
