{
 "cells": [
  {
   "cell_type": "raw",
   "id": "9690a3c6-69f5-4eab-a416-725cdcaf97ed",
   "metadata": {},
   "source": [
    "---\n",
    "title: Transformerが何をやっているのか理解する\n",
    "date: 28/04/2023\n",
    "categories: [ja, NLP, deep]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecfbeb3-8521-4f34-8080-7cfbcf028edb",
   "metadata": {},
   "source": [
    "ChatGPTが大バズリしている昨今です。僕はプロンプトを考えるのが面倒なので（ええ...)あまり使わないのですが、友人が論文を書くのに使っていたり、僕の母親が話し相手に使っていたりするようです。親不孝な息子でごめんなさいという感じもしますが、僕はいまだにTransformerが何なのかすらよくわかっていないで、これを機に何をやっているのかくらいは理解してみます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d724c74d-6ac1-463e-b6e7-847b66b1aaeb",
   "metadata": {},
   "source": [
    "# 参考にしたもの\n",
    "- [Formal Algorithms for Transformers](https://arxiv.org/abs/2207.09238)\n",
    "  - 疑似コードをまとめた論文です。これが一番わかりやすいと思うので、とりあえずこれを見ればいいと思います。他にも[The Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/)などを見たのですが、よくわかりませんでした。\n",
    "- [Shumi-Note Transformer](https://github.com/syuntoku14/Shumi-Note/blob/main/notebooks/NN_transformer.ipynb)\n",
    "  - これを見て真似しようと思ったのがこの記事のきっかけです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f645938a-ee71-4bec-a32f-933a8e243059",
   "metadata": {},
   "source": [
    "# トークン列のエンコーディング\n",
    "Transformerでは、入力されたトークン列に対し、multihead attentionと呼ばれるレイヤーを繰り返し適用します。そこで、まずどのようにトークン列をベクトルに射影するのかを説明します。\n",
    "\n",
    "## 最初に列があった\n",
    "トークン列というのは文字通りトークンからなる列のことです。トークンは有限集合の要素です。実用上はbyte pair encodingにより得られたsubwordなどがこれに該当しますが、とりあえず気にしなくていいです。トークンの集合を$V$とし、$[Nv] := {1, ..., Nv}$と番号付けしておきます。トークン列を$x = x[1: l]$と書きます。また、トークン列の最大の長さを$L$とします。トークンとして連続値や無限集合は扱えないと思いますが、素人なので何か抜け道があるかどうかは知りません。\n",
    "\n",
    "## トークンからベクトルに\n",
    "適当な$d_e \\times Nv$次元の行列$W_e$を使って、$v$番目のトークンから埋め込み（Token embedding）を $e = W_e[:, v]$により得ます。これは$d_e$次元のベクトルになります。なお、numpy風に$i$番目の行ベクトルを$W[i, :]$、$j$番目の列ベクトルを$W[:, j]$と書いています。この行列$W_e$は勾配降下により学習されるようです。\n",
    "\n",
    "## ついでに位置もベクトルに\n",
    "適当な$d_p \\times L$次元の行列$W_p$を使って、トークン列中の$l$番目にトークンがあるという情報から、位置埋め込み（Positional embedding）を $p = W_p[:, l]$により得ます。これも$d_e$次元のベクトルになります。正直なんの意味があるのかよくわからないのですが、これを先程のトークン埋め込みに足してトークン列$x$の$t$番目のトークン$x[t]$に対する埋め込みを$e = W_e[:, x[t]] + W_p[:, t]$によって得ます。これ足して大丈夫なのかな？って思うんですが。\n",
    "位置埋め込みは、学習されることもあるようですが、Transformerが最初に提案された[Attention Is All You Need](https://arxiv.org/abs/1706.03762)の論文では、以下のように構成されています。\n",
    "$$\n",
    "\\begin{align*}\n",
    "W_p[2i - 1, t] &= \\sin (\\frac{t}{L^{2i / d_e}}) \\\\\n",
    "W_p[2i, t] &= \\cos (\\frac{t}{L^{2i / d_e}}) \\\\\n",
    "&~~~~~(0 < 2i \\leq d_e)\n",
    "\\end{align*}\n",
    "$$\n",
    "これを$L=50, d_e = 5$として可視化してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef87949-a62c-4e0f-a139-11753389e2c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "L = 50\n",
    "d_e = 5\n",
    "x = np.arange(L)\n",
    "for i in range(1, 1 + d_e):\n",
    "    if i % 2 == 0:\n",
    "        w_p = np.sin(x / L ** (i / d_e))\n",
    "    else:\n",
    "        w_p = np.cos(x / L ** ((i - 1) / d_e)) \n",
    "    _ = plt.plot(x, w_p, label=f\"i={i}\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc464505-a4d0-4b3b-86f0-3cf0fcc333ea",
   "metadata": {},
   "source": [
    "というわけで、この埋めこみは各成分ごとに異なる周波数での単語を埋め込むようです。これにより、短いコンテキストの中での位置も同時に考慮できるのかな。\n",
    "\n",
    "# Attention\n",
    "Transformerの主要な構成要素になるのがAttentionです。Attentionでは、入力されたトークン列中のすべてのトークンの組み合わせについて、そいつらの組み合わせがどれくらい重要なのかというモデル化を行います。具体的に、単一クエリに対するAttentionでは、現在のトークンから得た埋め込み$e_t$と$x$中のすべてのトークンの埋め込み$e_0, e_1, ..., e_{Nv} \\in E$に対し、以下のような操作を行います。\n",
    "$$\n",
    "\\begin{align*}\n",
    "q_t &\\leftarrow W_q e_t + b_q \\\\\n",
    "k_{t'} &\\leftarrow W_k e_{t'} + b_k,~\\forall e_{t'} \\in E \\\\\n",
    "v_{t'} &\\leftarrow W_v e_{t'} + b_v,~\\forall e_{t'} \\in E \\\\\n",
    "\\alpha_{t'} &\\leftarrow \\frac{\\exp(q_t^\\top k_{t'} / \\sqrt{d_{\\textrm{attn}}})}{\\sum_u \\exp(q_t^\\top k_{t'} / \\sqrt{d_{\\textrm{attn}}})},~\\forall e_{t'} \\in E \\\\\n",
    "v_\\textrm{attr} &\\leftarrow \\sum_{t = 1}^T \\alpha_{t'} v_{t'}\n",
    "\\end{align*}\n",
    "$$\n",
    "埋め込みの次元を$d_\\textrm{in}$、出力の次元と$d_\\textrm{out}$とすると、$W_q, Q_k$は$d_\\textrm{attn} \\times e$の行列、$W_q, Q_k$は$d_\\textrm{out} \\times d_\\textrm{in}$の行列、$b_*$はベクトル（バイアス項）です。ここで、$q^\\top k_{t'}$の値でソフトマックスをとって$v$にマスクをかけるので、これは現在のトークンと$t'$番目のトークンが「どれくらい対応しているか」を表していてほしいです。$v_{t'}$が何を表しているかはタスクによって異なると思いますが、$t'$番目のトークンの埋め込みに線形に関係する値が入っているはずです。このトークン列に後ろ向きの因果関係がない場合（あるトークン$x[t]$が、任意の未来のトークン$x[t']~\\textrm{where}~t < t'$に依存しない場合）は、$\\alpha_{t'}$にマスクをかける($\\alpha_{t'}[i] = 0 ~\\textrm{if}~t < i$)こともあります。なので、未来を予測する際にはこのマスクをかけるのが一般的なようです。\n",
    "\n",
    "実際に、時系列から何か（次の単語、ラベルなど）を予測する際には、この単一クエリに対するAttentionを長さ$T$の系列内のすべてのトークンに対して計算し、$d_\\textrm{out} \\times T$の行列$\\tilde{V}$を得ます。\n",
    "\n",
    "とりあえずこれを学習させてみましょう。今回は[jax](https://jax.readthedocs.io/en/latest/)と[equinox](https://docs.kidger.site/equinox/)を使ってモデルを書いてみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb9e788-b0b0-4ae6-b671-ed8c1b23876c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "class Attention(eqx.Module):\n",
    "    w_q: jax.Array\n",
    "    b_q: jax.Array\n",
    "    w_k: jax.Array\n",
    "    b_k: jax.Array\n",
    "    w_v: jax.Array\n",
    "    b_v: jax.Array\n",
    "    sqrt_d_attn: float\n",
    "\n",
    "    def __init__(self, d_in: int, d_attn: int, d_out: int, key: jax.Array) -> None:\n",
    "        wq_key, bq_key, wk_key, bk_key, wv_key, bv_key = jax.random.split(key, 6)\n",
    "        self.w_q = jax.random.normal(wq_key, (d_attn, d_in))\n",
    "        self.b_q = jax.random.normal(bq_key, (d_attn, 1))\n",
    "        self.w_k = jax.random.normal(wk_key, (d_attn, d_in))\n",
    "        self.b_k = jax.random.normal(bk_key, (d_attn, 1))\n",
    "        self.w_v = jax.random.normal(wv_key, (d_out, d_in))\n",
    "        self.b_v = jax.random.normal(bv_key, (d_out, 1))\n",
    "        self.sqrt_d_attn = float(np.sqrt(d_attn))\n",
    "\n",
    "    def __call__(self, e: jax.Array) -> jax.Array:\n",
    "        \"\"\"Take a matrix e with shape [d_in x seq_len], compute attention for all tokens in e.\n",
    "        Outputs a matrix with shape [d_out x seq_len]\n",
    "        \"\"\"\n",
    "        q = self.w_q @ e + self.b_q\n",
    "        k = self.w_k @ e + self.b_k\n",
    "        v = self.w_v @ e + self.b_v\n",
    "        alpha = jax.nn.softmax(q.T @ k / self.sqrt_d_attn, axis=-1)\n",
    "        return v @ alpha.T\n",
    "\n",
    "\n",
    "def causal_mask(x: jax.Array) -> jax.Array:\n",
    "    ltri = jnp.tri(x.shape[0], dtype=bool, k=-1)\n",
    "    return jax.lax.select(ltri, jnp.ones_like(x) * -jnp.inf, x)\n",
    "\n",
    "\n",
    "class MaskedAttention(Attention):\n",
    "    def __call__(self, e: jax.Array) -> jax.Array:\n",
    "        q = self.w_q @ e + self.b_q\n",
    "        k = self.w_k @ e + self.b_k\n",
    "        v = self.w_v @ e + self.b_v\n",
    "        score = causal_mask(q.T @ k) / self.sqrt_d_attn\n",
    "        alpha = jax.nn.softmax(score, axis=-1)\n",
    "        return v @ alpha.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eba767b-c7c9-4fe2-a1f5-8a3cf6fcf263",
   "metadata": {},
   "source": [
    "これを学習させてみましょう。トークンとして、天気🌧️・☁️・☀️を考えます。この3つの記号に対し適当な埋め込みを与えて、次の日の天気を学習させてみます。よくわからないので、ダブらないようにトークン埋め込みを$[-1, 0, 1]$、位置埋め込みを$1 / t$としてみましょう。最大文字列長は適当に10にします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab8ccf5-bbd0-4d20-8324-a5254ef9df57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TOKEN_EMBEDDING = {\n",
    "    \"🌧️\": -1.0,\n",
    "    \"☁️\": 0.0,\n",
    "    \"☀️\": 1.0,\n",
    "}\n",
    "\n",
    "def get_embedding(seq: str, max_seq_len: int | None = None) -> np.ndarray:\n",
    "    if max_seq_len is None:\n",
    "        max_seq_len = len(seq)\n",
    "    length = len(seq) // 2\n",
    "    e = np.zeros(length)\n",
    "    for i in range(length):\n",
    "        x = seq[i * 2: i * 2 + 2]\n",
    "        e[i] = TOKEN_EMBEDDING[x] + (i + 1) / max_seq_len\n",
    "    return e.reshape(1, length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fac285c-fae4-4cd9-a1e0-a2dea1179670",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Markovモデルの学習\n",
    "\n",
    "まず簡単なモデルで天気を生成してみます。**次の日の天気は、前の日の天気にもとづいて確率的に決まる**ことにしましょう。🌧️・☁️・☀️がマルチバイト文字であることに注意して、以下のように実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c945d0e4-fb80-49b1-bdcf-e9c5eb9245c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "_GEN = np.random.Generator(np.random.PCG64(11111))\n",
    "_MARKOV = {\n",
    "    \"\": [0.3, 0.4, 0.3],\n",
    "    \"🌧️\": [0.6, 0.3, 0.1],\n",
    "    \"☁️\": [0.3, 0.4, 0.3],\n",
    "    \"☀️\": [0.2, 0.3, 0.5],\n",
    "}\n",
    "\n",
    "WEATHERS = [\"🌧️\", \"☁️\", \"☀️\"]\n",
    "\n",
    "\n",
    "def markov(prev: str) -> str:\n",
    "    prob = _MARKOV[prev[-2:]]\n",
    "    return prev + _GEN.choice(WEATHERS, p=prob)\n",
    "\n",
    "\n",
    "def generate(f, n: int):\n",
    "    value = \"\"\n",
    "    for _ in range(n):\n",
    "        value = f(value)\n",
    "    return value\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Dataset:\n",
    "    weathers: list[str]\n",
    "    embeddings: jax.Array\n",
    "    next_weather_indices: jax.Array\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.weathers)\n",
    "\n",
    "\n",
    "def make_dataset(f, seq_len, size) -> Dataset:\n",
    "    w_list, e_list, nw_list = [], [], []\n",
    "    for _ in range(size):\n",
    "        weathers = generate(f, seq_len + 1)\n",
    "        e = jnp.array(get_embedding(weathers[:-2]))\n",
    "        w_list.append(weathers)\n",
    "        e_list.append(e)\n",
    "        nw_list.append(WEATHERS.index(weathers[-2:]))\n",
    "    return Dataset(w_list, jnp.stack(e_list), jnp.array(nw_list))\n",
    "\n",
    "\n",
    "generate(markov, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03961817-2903-4643-9009-b65a8285cb3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "こんな感じです。いま、次の日の天気だけ予測したいので、モデルの出力は集合{🌧️・☁️・☀️}上での確率分布が適切でしょう。Attentionは長さ$T$の埋め込み列に対して長さ$d_\\textrm{out} \\times T$の行列をかえします。なので、$d_\\textrm{out} = 3$とし、Attentionの出力$\\tilde{V}$に対してソフトマックス関数を適用し、$P_t = \\textrm{softmax}(\\tilde{V}[:, t])$とします。このとき、$P_t$の各要素が次の日🌧️・☁️・☀️になる確率を表すとして、モデル化します。これを、対数尤度の和$\\sum_t \\log P_t(\\textrm{next weather})$を最大化するように学習しましょう。学習のコードを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf1d94d-da0b-4e5e-9009-122584989969",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "\n",
    "def neg_logp(model: eqx.Module, seq: jax.Array, next_w: jax.Array) -> jax.Array:\n",
    "    batch_size = seq.shape[0]\n",
    "    tilde_v = jax.vmap(model)(seq)  # B x OUT x SEQ_LEN\n",
    "    logp = jax.nn.log_softmax(tilde_v, axis=1)  # B x OUT x SEQ_LEN\n",
    "    logp_masked = logp * jax.nn.one_hot(next_w, num_classes=3).reshape(-1, 3, 1)\n",
    "    return -jnp.mean(jnp.sum(logp_masked.reshape(batch_size, -1), axis=-1))\n",
    "\n",
    "\n",
    "compute_loss = eqx.filter_value_and_grad(neg_logp)\n",
    "evaluate_model = jax.jit(neg_logp)\n",
    "\n",
    "\n",
    "def train(\n",
    "    n_epochs: int,\n",
    "    minibatch_size: int,\n",
    "    model: eqx.Module,\n",
    "    ds: Dataset,\n",
    "    test_ds: Dataset,\n",
    "    key: jax.Array,\n",
    "    learning_rate: float = 1e-2,\n",
    ") -> tuple[eqx.Module, jax.Array, list[float], list[float]]:\n",
    "    n_data = len(ds)\n",
    "    indices = jnp.arange(n_data)\n",
    "    optim = optax.adam(learning_rate)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def train_1step(\n",
    "        model: eqx.Module,\n",
    "        seq: jax.Array,\n",
    "        next_w: jax.Array,\n",
    "        opt_state: optax.OptState,\n",
    "    ) -> tuple[jax.Array, eqx.Module, optax.OptState]:\n",
    "        loss, grads = compute_loss(model, seq, next_w)\n",
    "        updates, opt_state = optim.update(grads, opt_state)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return loss, model, opt_state\n",
    "\n",
    "    opt_state = optim.init(model)\n",
    "    n_optim_epochs = n_data // minibatch_size\n",
    "    loss_list, eval_list = [], []\n",
    "    for epoch in range(n_epochs):\n",
    "        key, shuffle_key = jax.random.split(key)\n",
    "        shuffled_indices = jax.random.shuffle(shuffle_key, indices)\n",
    "        for _ in range(n_optim_epochs):\n",
    "            e = ds.embeddings[shuffled_indices]\n",
    "            next_w = ds.next_weather_indices[shuffled_indices]\n",
    "            loss, model, opt_state = train_1step(model, e, next_w, opt_state)\n",
    "            loss_list.append(loss.item())\n",
    "            test_loss = evaluate_model(\n",
    "                model, test_ds.embeddings, test_ds.next_weather_indices\n",
    "            )\n",
    "            eval_list.append(test_loss.item())\n",
    "    return model, key, loss_list, eval_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481f39a1-3cb7-4526-893e-effff5c36e48",
   "metadata": {
    "tags": []
   },
   "source": [
    "これを実際に走らせてみます。適当に、Attentionの次元を8、天気列の長さを10にしましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793558de-6389-44b2-9982-da31c66f8688",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "D_ATTN = 8\n",
    "SEQ_LEN = 10\n",
    "key = jax.random.PRNGKey(123)\n",
    "model = MaskedAttention(1, D_ATTN, 3, key)\n",
    "ds = make_dataset(markov, SEQ_LEN, 1000)\n",
    "test_ds = make_dataset(markov, SEQ_LEN, 100)\n",
    "model, key, loss_list, eval_list = train(50, 100, model, ds, test_ds, key, 1e-3)\n",
    "plt.plot(loss_list, label=\"Training Loss\")\n",
    "plt.plot(eval_list, label=\"Test Loss\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Negative Log Likelihood\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde21f3a-a9cd-4fc6-ac72-04df8356f913",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlog2",
   "language": "python",
   "name": "rlog2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
