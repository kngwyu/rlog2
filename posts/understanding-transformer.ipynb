{
 "cells": [
  {
   "cell_type": "raw",
   "id": "9690a3c6-69f5-4eab-a416-725cdcaf97ed",
   "metadata": {},
   "source": [
    "---\n",
    "title: Transformerが何をやっているのか理解する\n",
    "date: 28/04/2023\n",
    "categories: [ja, NLP, deep]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecfbeb3-8521-4f34-8080-7cfbcf028edb",
   "metadata": {},
   "source": [
    "ChatGPTが大バズリしている昨今です。僕はプロンプトを考えるのが面倒なので（ええ...)あまり使わないのですが、友人が論文を書くのに使っていたり、僕の母親が話し相手に使っていたり（親不孝な息子でごめんなさい）するようです。そんな中僕はいまだにTransformerが何なのかすらよくわかっていなかったので、とりあえず何をやっているのかくらいは理解してみようとしてみます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d724c74d-6ac1-463e-b6e7-847b66b1aaeb",
   "metadata": {},
   "source": [
    "# 参考にしたもの\n",
    "- [Formal Algorithms for Transformers](https://arxiv.org/abs/2207.09238)\n",
    "  - 疑似コードをまとめた論文です。これが一番わかりやすいと思うので、とりあえずこれを見ればいいと思います。他にも[The Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/)などを見たのですが、よくわかりませんでした。\n",
    "- [Shumi-Note Transformer](https://github.com/syuntoku14/Shumi-Note/blob/main/notebooks/NN_transformer.ipynb)\n",
    "  - これを見て真似しようと思ったのがこの記事のきっかけです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f645938a-ee71-4bec-a32f-933a8e243059",
   "metadata": {},
   "source": [
    "# トークン列のエンコーディング\n",
    "Transformerでは、入力されたトークン列に対し、multihead attentionと呼ばれるレイヤーを繰り返し適用します。そこで、まずどのようにトークン列をベクトルに射影するのかを説明します。\n",
    "\n",
    "## 最初に列があった\n",
    "トークン列というのは文字通りトークンからなる列のことです。トークンは有限集合の要素です。実用上はbyte pair encodingにより得られたsubwordなどがこれに該当しますが、とりあえず気にしなくていいです。トークンの集合を$V$とし、$[Nv] := {1, ..., Nv}$と番号付けしておきます。トークン列を$x = x[1: l]$と書きます。また、トークン列の最大の長さを$L$とします。トークンとして連続値や無限集合は扱えないと思いますが、素人なので何か抜け道があるかどうかは知りません。\n",
    "\n",
    "## トークンからベクトルに\n",
    "適当な$d_e \\times Nv$次元の行列$W_e$を使って、$v$番目のトークンから埋め込み（Token embedding）を $e = W_e[:, v]$により得ます。これは$d_e$次元のベクトルになります。なお、numpy風に$i$番目の行ベクトルを$W[i, :]$、$j$番目の列ベクトルを$W[:, j]$と書いています。この行列$W_e$は勾配降下により学習されるようです。\n",
    "\n",
    "## ついでに位置もベクトルに\n",
    "適当な$d_p \\times L$次元の行列$W_p$を使って、トークン列中の$l$番目にトークンがあるという情報から、位置埋め込み（Positional embedding）を $p = W_p[:, l]$により得ます。これも$d_e$次元のベクトルになります。正直なんの意味があるのかよくわからないのですが、これを先程のトークン埋め込みに足してトークン列$x$の$t$番目のトークン$x[t]$に対する埋め込みを$e = W_e[:, x[t]] + W_p[:, t]$によって得ます。これ足して大丈夫なのかな？って思うんですが。\n",
    "位置埋め込みは、学習されることもあるようですが、Transformerが最初に提案された[Attention Is All You Need](https://arxiv.org/abs/1706.03762)の論文では、以下のように構成されています。\n",
    "$$\n",
    "\\begin{align*}\n",
    "W_p[2i - 1, t] &= \\sin (\\frac{t}{L^{2i / d_e}}) \\\\\n",
    "W_p[2i, t] &= \\cos (\\frac{t}{L^{2i / d_e}}) \\\\\n",
    "&~~~~~(0 < 2i \\leq d_e)\n",
    "\\end{align*}\n",
    "$$\n",
    "これを$L=50, d_e = 5$として可視化してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef87949-a62c-4e0f-a139-11753389e2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "L = 50\n",
    "d_e = 5\n",
    "x = np.arange(L)\n",
    "for i in range(1, 1 + d_e):\n",
    "    if i % 2 == 0:\n",
    "        w_p = np.sin(x / L ** (i / d_e))\n",
    "    else:\n",
    "        w_p = np.cos(x / L ** ((i - 1) / d_e)) \n",
    "    _ = plt.plot(x, w_p, label=f\"i={i}\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc464505-a4d0-4b3b-86f0-3cf0fcc333ea",
   "metadata": {},
   "source": [
    "というわけで、この埋めこみは各成分ごとに異なる周波数での単語を埋め込むようです。これにより、短いコンテキストの中での位置も同時に考慮できるのかな。\n",
    "\n",
    "# Attention\n",
    "Transformerの主要な構成要素になるのがAttentionです。同様に系列を扱えるネットワークとしてCNNやRNNがあります。CNNが現在のトークン$x[t]$の近くのトークンだけを見る、RNNが現在のトークンと過去のトークンだけを見るのに対し、Attentionでは$x$中のすべてのトークンについて、現在のトークンとの関係を直接モデル化します。これにより、CNNは何層も重ねないと長期的な依存関係が見れないのですが、Attentionでは一つのレイヤーでトークン列の中の長期的な依存関係を見ることができます。具体的に、現在のトークンから得た埋め込み$e$と$x$中のすべてのトークンの埋め込み$e_0, e_1, ..., e_{Nv} \\in E$に対し、以下のような操作を行います。\n",
    "$$\n",
    "\\begin{align*}\n",
    "q &\\leftarrow W_q e + b_q \\\\\n",
    "k_t &\\leftarrow W_k e_t + b_k,~\\forall e_t \\in E \\\\\n",
    "v_t &\\leftarrow W_v e_t + b_v,~\\forall e_t \\in E \\\\\n",
    "\\alpha_t &\\leftarrow \\frac{\\exp(q^\\top k_t / \\sqrt{d_{\\textrm{attn}}})}{\\sum_u \\exp(q^\\top k_u / \\sqrt{d_{\\textrm{attn}}})},~\\forall e_t \\in E \\\\\n",
    "v_\\textrm{attr} &\\leftarrow \\sum_{t = 1}^T \\alpha_t v_t\n",
    "\\end{align*}\n",
    "$$\n",
    "$W_*$は$d_\\textrm{attn} \\times e$の次元の行列、$b_*$はベクトル（バイアス項）です。ここで、$q^\\top k_t$の値でソフトマックスをとって$v$にマスクをかけるので、これは現在のトークンと$t$番目のトークンが「どれくらい対応しているか」を表していてほしいです。$v_t$が何を表しているかはタスクによって異なると思いますが、何か$t$番目のトークンの埋め込みに線形に関係する値が入っているはずです。\n",
    "\n",
    "実際のTransoformerはもっと複雑ですが、とりあえずこれを学習させてみましょう。今回は[jax](https://jax.readthedocs.io/en/latest/)と[quinox](https://docs.kidger.site/equinox/)を使ってモデルを書いてみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb9e788-b0b0-4ae6-b671-ed8c1b23876c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "class SingleQueryAttention(eqx.Module):\n",
    "    w_q: jax.Array\n",
    "    b_q: jax.Array\n",
    "    w_k: jax.Array\n",
    "    b_k: jax.Array\n",
    "    w_v: jax.Array\n",
    "    b_v: jax.Array\n",
    "    sqrt_d_attn: jax.Array\n",
    "\n",
    "    def __init__(self, d_in: int, d_attn: int, key: jax.Array) -> None:\n",
    "        wq_key, bq_key, wk_key, bk_key, wv_key, bv_key = jax.random.split(key, 6)\n",
    "        self.w_q = jax.random.normal(wq_key, (d_attn, d_in))\n",
    "        self.b_q = jax.random.normal(bq_key, (d_attn, 1))\n",
    "        self.w_k = jax.random.normal(wk_key, (d_attn, d_in))\n",
    "        self.b_k = jax.random.normal(bk_key, (d_attn, 1))\n",
    "        self.w_k = jax.random.normal(wv_key, (d_attn, d_in))\n",
    "        self.b_k = jax.random.normal(bv_key, (d_attn, 1))\n",
    "        self.d_attn = jnp.sqrt(d_attn)\n",
    "\n",
    "    def __call__(self, e: jax.Array) -> jax.Array:\n",
    "        q = self.w_q @ e + self.b_q\n",
    "        k = self.w_k @ e + self.b_k\n",
    "        v = self.w_v @ e + self.b_v\n",
    "        alpha = jax.nn.softmax(q.T @ k / self.sqrt_d_attn)\n",
    "        return alpha @ v.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eba767b-c7c9-4fe2-a1f5-8a3cf6fcf263",
   "metadata": {},
   "source": [
    "これを学習させてみましょう。トークンとして、天気🌧️・☁️・☀️を考えます。この3つの記号に対し適当な埋め込みを与えて、次の日の天気を学習させてみます。よくわからないので、ダブらないようにトークン埋め込みを$[-1, 0, 1]$、位置埋め込みを$1 / t$としてみましょう。\n",
    "\n",
    "まず適当なモデルで天気を生成してみます。🌧️・☁️・☀️がマルチバイト文字であることに注意して、以下のように実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c945d0e4-fb80-49b1-bdcf-e9c5eb9245c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_GEN = np.random.Generator(np.random.PCG64(11111))\n",
    "_MARKOV = {\n",
    "    \"\": [0.3, 0.4, 0.3],\n",
    "    \"🌧️\": [0.6, 0.3, 0.1],\n",
    "    \"☁️\": [0.3, 0.4, 0.3],\n",
    "    \"☀️\": [0.2, 0.3, 0.5],\n",
    "}\n",
    "_MARKOV2 = {\n",
    "    \"\": [0.3, 0.4, 0.3],\n",
    "    \"🌧️\": [0.6, 0.3, 0.1],\n",
    "    \"☁️\": [0.3, 0.4, 0.3],\n",
    "    \"☀️\": [0.2, 0.3, 0.5],\n",
    "}\n",
    "WEATHERS = [\"🌧️\", \"☁️\", \"☀️\"]\n",
    "\n",
    "\n",
    "def markov(prev: str) -> str:\n",
    "    prob = _MARKOV[prev[-2:]]\n",
    "    return prev + _GEN.choice(WEATHERS, p=prob)\n",
    "\n",
    "\n",
    "def apply_n(f, init, n: int):\n",
    "    value = init\n",
    "    for _ in range(n):\n",
    "        value = f(value)\n",
    "    return value\n",
    "\n",
    "apply_n(markov, \"\", 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03961817-2903-4643-9009-b65a8285cb3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "こんな感じです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf1d94d-da0b-4e5e-9009-122584989969",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TOKEN_EMBEDDING = {\n",
    "    \"🌧️\": -1.0,\n",
    "    \"☁️\": 0.0,\n",
    "    \"☀️\": 1.0,\n",
    "}\n",
    "L = 10\n",
    "\n",
    "def get_embedding(seq: str) -> np.ndarray:\n",
    "    e = np.zeros(len(seq) // 2)\n",
    "    for i in range(len(seq) // 2):\n",
    "        x = seq[i * 2: i * 2 + 2]\n",
    "        e[i] = TOKEN_EMBEDDING[x] + (i + 1) / L\n",
    "    return e\n",
    "\n",
    "@jax.jit\n",
    "@jax.grad\n",
    "def loss_fn(model: eqx.Module, seq: jax.Array, next_w: jax.Array) -> jax.Array:\n",
    "    predicted = jax.vmap(model)(seq)\n",
    "    return jax.numpy.mean((y - pred_y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1178d3bc-50b5-4331-b6be-4adffad21f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(f, seq_len, batch_size):\n",
    "    dataset = []\n",
    "    for _ in range(batch_size):\n",
    "        weathers = apply_n(f, \"\", seq_len + 1)\n",
    "        e = get_embedding(weathers[: -2])\n",
    "        next_w = TOKEN_EMBEDDING[e]\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlog2",
   "language": "python",
   "name": "rlog2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
